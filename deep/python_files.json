[
  {
    "path": "extract_voice_track.py",
    "size": 3470,
    "lines": 106,
    "source": "import os\nimport subprocess\n\n# Configuration\nLIZARD_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD\"\nINPUT_AUDIO = \"/Users/gaia/resurrecting atlantis/LIZARD/ElevenLabs_2025-05-30T10_18_59_Maputo jazz mortician_gen_sp100_s50_sb75_se70_b_m2.mp3\"\nOUTPUT_VOICE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VoiceTrack.mp3\")\nOUTPUT_WAV = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VoiceTrack.wav\")\nVIDEO_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\nFINAL_VIDEO = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_WithVoice.mp4\")\n\ndef get_video_duration():\n    \"\"\"Get the exact duration of the video in seconds.\"\"\"\n    cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        VIDEO_FILE\n    ]\n    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n    return float(result.stdout.strip())\n\ndef extract_voice_track():\n    \"\"\"Extract and optimize voice track to exactly match video duration.\"\"\"\n    # Get video duration\n    video_duration = get_video_duration()\n    print(f\"Video duration: {video_duration:.2f} seconds\")\n    \n    # Get audio duration\n    audio_cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        INPUT_AUDIO\n    ]\n    audio_duration = float(subprocess.run(audio_cmd, stdout=subprocess.PIPE, text=True).stdout.strip())\n    print(f\"Original audio duration: {audio_duration:.2f} seconds\")\n    \n    # Calculate speed factor\n    speed_factor = audio_duration / video_duration\n    print(f\"Speed adjustment factor: {speed_factor:.2f}x\")\n    \n    # Extract and adjust voice track\n    extract_cmd = [\n        'ffmpeg',\n        '-i', INPUT_AUDIO,\n        # Voice enhancement filters\n        '-af', f'atempo={speed_factor},highpass=f=200,lowpass=f=3000,equalizer=f=1000:width_type=h:width=200:g=3,acompressor=threshold=0.05:ratio=4:attack=200:release=1000,volume=1.5',\n        '-y',\n        OUTPUT_VOICE\n    ]\n    \n    print(\"Extracting and optimizing voice track...\")\n    subprocess.run(extract_cmd)\n    \n    # Also create WAV version for easier editing\n    wav_cmd = [\n        'ffmpeg',\n        '-i', OUTPUT_VOICE,\n        '-y',\n        OUTPUT_WAV\n    ]\n    subprocess.run(wav_cmd)\n    \n    # Verify output duration\n    verify_cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        OUTPUT_VOICE\n    ]\n    output_duration = float(subprocess.run(verify_cmd, stdout=subprocess.PIPE, text=True).stdout.strip())\n    print(f\"Adjusted voice track duration: {output_duration:.2f} seconds\")\n    \n    # Create video with voice track\n    video_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-i', OUTPUT_VOICE,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'copy',\n        '-c:a', 'aac',\n        '-shortest',\n        '-y',\n        FINAL_VIDEO\n    ]\n    \n    print(\"Creating video with voice track...\")\n    subprocess.run(video_cmd)\n    \n    print(\"\\nVoice track extraction complete!\")\n    print(f\"Voice track MP3: {OUTPUT_VOICE}\")\n    print(f\"Voice track WAV: {OUTPUT_WAV}\")\n    print(f\"Video with voice: {FINAL_VIDEO}\")\n    \n    if os.path.exists(OUTPUT_VOICE) and os.path.exists(FINAL_VIDEO):\n        return True\n    return False\n\nif __name__ == \"__main__\":\n    extract_voice_track()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/LIZARD/ElevenLabs_2025-05-30T10_18_59_Maputo jazz mortician_gen_sp100_s50_sb75_se70_b_m2.mp3",
      "WhereYouGoWhenYouLeave_VoiceTrack.mp3",
      "WhereYouGoWhenYouLeave_VoiceTrack.wav",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "WhereYouGoWhenYouLeave_WithVoice.mp4",
      "/Users/gaia/resurrecting atlantis/LIZARD",
      "/Users/gaia/resurrecting atlantis/LIZARD/ElevenLabs_2025-05-30T10_18_59_Maputo jazz mortician_gen_sp100_s50_sb75_se70_b_m2.mp3",
      ")\n    \n    # Calculate speed factor\n    speed_factor = audio_duration / video_duration\n    print(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "audio_cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "extract_cmd"
      },
      {
        "type": "run",
        "snippet": "wav_cmd"
      },
      {
        "type": "run",
        "snippet": "verify_cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "video_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "sequential_overlay.py",
    "size": 3715,
    "lines": 80,
    "source": "import os\nimport subprocess\nimport shutil\n\ndef sequential_overlay():\n    \"\"\"Overlays poems one by one onto the video to ensure stability.\"\"\"\n    base_video = \"/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS/DOG_NEW_AUDIO_ASSEMBLAGE_0s.mp4\"\n    audio_dir = \"/Users/gaia/resurrecting atlantis/ANT/VO_Mixdowns_RY\"\n    output_dir = \"/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS\"\n    final_video_path = os.path.join(output_dir, \"DOG_WITH_POEMS_new_timestamps.mp4\")\n    temp_dir = os.path.join(output_dir, \"temp_video_files\")\n\n    # --- Codex ---\n    codex = [\n        {\"file\": \"01_Out of Life_mixdown_RY_250626.wav\", \"start_time\": 0},\n        {\"file\": \"02_Flashing Lights_mixdown_RY_250626.wav\", \"start_time\": 136},\n        {\"file\": \"03_How to Break off an Engagement_mixdown_RY_250626.wav\", \"start_time\": 272},\n        {\"file\": \"04_Nevermore_mixdown_RY_250626.wav\", \"start_time\": 408},\n        {\"file\": \"05_Bloodline_mixdown_RY_250626.wav\", \"start_time\": 544},\n        {\"file\": \"06_Resurrecting Atlantis_mixdown_RY_250626.wav\", \"start_time\": 680},\n        {\"file\": \"07_DJ Turn Me Up_mixdown_RY_250626.wav\", \"start_time\": 816},\n        {\"file\": \"08_Newly Single_mixdown_RY_250626.wav\", \"start_time\": 952},\n        {\"file\": \"09_Yet Heard_mixdown_RY_250626.wav\", \"start_time\": 1088},\n        {\"file\": \"10_Magic Ride_mixdown_RY_250626.wav\", \"start_time\": 1224},\n        {\"file\": \"12_Reunion_mixdown_RY_250626.wav\", \"start_time\": 1360},\n        {\"file\": \"13_How to Win My Heart_mixdown_RY_250626.wav\", \"start_time\": 1496},\n        {\"file\": \"14_Hot Minute_mixdown_RY_250626.wav\", \"start_time\": 1632},\n    ]\n\n    # --- Setup --- \n    if os.path.exists(temp_dir):\n        shutil.rmtree(temp_dir)\n    os.makedirs(temp_dir)\n\n    current_video_input = base_video\n\n    # --- Loop through poems and overlay one by one ---\n    for i, poem in enumerate(codex):\n        poem_file = os.path.join(audio_dir, poem['file'])\n        start_time = poem['start_time']\n        output_video_step = os.path.join(temp_dir, f\"step_{i+1}.mp4\")\n        \n        print(f\"--- Step {i+1}/{len(codex)}: Overlaying {poem['file']} ---\")\n\n        command = [\n            'ffmpeg', '-y',\n            '-i', current_video_input, # Input 0: Video (from previous step)\n            '-i', poem_file,           # Input 1: Current poem audio\n            '-filter_complex',\n            # Delay the poem audio, then mix it with the video's existing audio.\n            # Use 'longest' to ensure the output matches the video length.\n            f\"[1:a]adelay={start_time*1000}|{start_time*1000}[poem_audio];\" \\\n            f\"[0:a][poem_audio]amix=inputs=2:duration=longest[aout]\",\n            '-map', '0:v',             # Map the video stream\n            '-map', '[aout]',          # Map the newly mixed audio\n            '-c:v', 'libx264',         # Re-encode video for stability\n            '-preset', 'veryfast',\n            '-crf', '23',\n            '-c:a', 'aac', '-b:a', '256k',\n            output_video_step\n        ]\n\n        try:\n            subprocess.run(command, check=True, capture_output=True, text=True)\n            current_video_input = output_video_step\n        except subprocess.CalledProcessError as e:\n            print(f\"ERROR during step {i+1} with {poem['file']}:\")\n            print(e.stderr)\n            shutil.rmtree(temp_dir)\n            return\n\n    # --- Final Step: Move the last temp file to the final output path ---\n    print(\"\\n--- All poems overlaid successfully! Finalizing video. ---\")\n    shutil.move(current_video_input, final_video_path)\n    shutil.rmtree(temp_dir) # Clean up\n    print(f\"\\nSuccessfully created final video!\\n{final_video_path}\")\n\nif __name__ == \"__main__\":\n    sequential_overlay()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS/DOG_NEW_AUDIO_ASSEMBLAGE_0s.mp4",
      "DOG_WITH_POEMS_new_timestamps.mp4",
      "01_Out of Life_mixdown_RY_250626.wav",
      "02_Flashing Lights_mixdown_RY_250626.wav",
      "03_How to Break off an Engagement_mixdown_RY_250626.wav",
      "04_Nevermore_mixdown_RY_250626.wav",
      "05_Bloodline_mixdown_RY_250626.wav",
      "06_Resurrecting Atlantis_mixdown_RY_250626.wav",
      "07_DJ Turn Me Up_mixdown_RY_250626.wav",
      "08_Newly Single_mixdown_RY_250626.wav",
      "09_Yet Heard_mixdown_RY_250626.wav",
      "10_Magic Ride_mixdown_RY_250626.wav",
      "12_Reunion_mixdown_RY_250626.wav",
      "13_How to Win My Heart_mixdown_RY_250626.wav",
      "14_Hot Minute_mixdown_RY_250626.wav",
      "step_{i+1}.mp4",
      "/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS/DOG_NEW_AUDIO_ASSEMBLAGE_0s.mp4",
      "/Users/gaia/resurrecting atlantis/ANT/VO_Mixdowns_RY",
      "/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS",
      "--- Step {i+1}/{len(codex)}: Overlaying {poem["
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "command, check=True, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "shutil"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "find_long_videos.py",
    "size": 2597,
    "lines": 63,
    "source": "import os\nimport subprocess\nimport json\n\ndef find_long_videos_with_audio(directory, min_duration_minutes=25):\n    \"\"\"\n    Finds video files in a directory longer than a specified duration that have an audio track.\n\n    Args:\n        directory (str): The directory to search for video files.\n        min_duration_minutes (int): The minimum duration in minutes.\n    \"\"\"\n    min_duration_seconds = min_duration_minutes * 60\n    long_videos = []\n\n    print(f\"Searching for .mp4 files in '{directory}' longer than {min_duration_minutes} minutes with audio...\")\n\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            if filename.endswith('.mp4'):\n                filepath = os.path.join(root, filename)\n                try:\n                    # Get video duration\n                    duration_command = [\n                        'ffprobe',\n                        '-v', 'error',\n                        '-show_entries', 'format=duration',\n                        '-of', 'default=noprint_wrappers=1:nokey=1',\n                        filepath\n                    ]\n                    duration_output = subprocess.check_output(duration_command, text=True).strip()\n                    duration = float(duration_output) if duration_output else 0\n\n                    if duration > min_duration_seconds:\n                        # Check for audio stream\n                        audio_command = [\n                            'ffprobe',\n                            '-v', 'error',\n                            '-select_streams', 'a:0',\n                            '-show_entries', 'stream=codec_type',\n                            '-of', 'default=noprint_wrappers=1:nokey=1',\n                            filepath\n                        ]\n                        audio_output = subprocess.check_output(audio_command, text=True).strip()\n                        \n                        if 'audio' in audio_output.lower():\n                            long_videos.append(filepath)\n\n                except (subprocess.CalledProcessError, FileNotFoundError, ValueError) as e:\n                    # print(f\"Could not process {filepath}: {e}\")\n                    pass # Ignore files that cause errors\n\n    if long_videos:\n        print(\"\\nFound the following videos longer than 25 minutes with audio:\")\n        for video in long_videos:\n            print(video)\n    else:\n        print(\"\\nNo videos longer than 25 minutes with audio were found.\")\n\nif __name__ == \"__main__\":\n    search_directory = \"/Users/gaia/resurrecting atlantis\"\n    find_long_videos_with_audio(search_directory)\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "subprocess",
      "json"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "create_voice_variations_fixed.py",
    "size": 7882,
    "lines": 203,
    "source": "import os\nimport subprocess\nimport time\n\n# Configuration\nLIZARD_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD\"\nINPUT_VOICE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VoiceTrack.wav\")\nVIDEO_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\nOUTPUT_DIR = os.path.join(LIZARD_DIR, \"VOICE_VARIATIONS\")\n\n# Voice variation definitions with simplified filters\nVARIATIONS = [\n    {\n        \"name\": \"Radio\",\n        \"description\": \"AM/FM radio transmission effect with static and frequency limitations\",\n        \"filters\": \"highpass=f=500,lowpass=f=4000,equalizer=f=1000:width_type=o:width=1:g=3,bandpass=f=1800:width_type=h:width=700,aphaser,volume=1.5\",\n        \"output_suffix\": \"Radio\",\n        \"output_name\": \"Old-time radio broadcast\"\n    },\n    {\n        \"name\": \"Echo\",\n        \"description\": \"Cathedral-like echo with layered reverberations\",\n        \"filters\": \"aecho=0.8:0.9:1000|1800:0.3|0.25,aecho=0.8:0.7:40|60|80:0.4|0.3|0.2,highpass=f=200,lowpass=f=3000,volume=1.5\",\n        \"output_suffix\": \"Echo\",\n        \"output_name\": \"Subterranean echo chambers\"\n    },\n    {\n        \"name\": \"Harmonious\",\n        \"description\": \"Choir-like harmonization with subtle pitch variation\",\n        \"filters\": \"chorus=0.5:0.9:50|60|40:0.4|0.32|0.3:0.25|0.4|0.3:2|2.3|1.3,equalizer=f=250:width_type=h:width=100:g=2,volume=1.4\",\n        \"output_suffix\": \"Harmonious\",\n        \"output_name\": \"Spectral harmonies\"\n    },\n    {\n        \"name\": \"Robotic\",\n        \"description\": \"Mechanical vocoder-style voice with digital artifacts\",\n        \"filters\": \"aresample=11025,aresample=44100,vibrato=f=10:d=0.2,tremolo=f=40:d=0.1,volume=1.8\",\n        \"output_suffix\": \"Robotic\",\n        \"output_name\": \"Digital consciousness\"\n    },\n    {\n        \"name\": \"OrganicMorph\",\n        \"description\": \"Fluid, organic morphing with time-based transformations\",\n        \"filters\": \"tremolo=f=4:d=0.4,vibrato=f=5:d=0.3,flanger=delay=5:speed=1:depth=5,aphaser=type=t:speed=0.6:decay=0.3,volume=1.6\",\n        \"output_suffix\": \"OrganicMorph\",\n        \"output_name\": \"Liquid metamorphosis\"\n    },\n    {\n        \"name\": \"SpectralGhosts\",\n        \"description\": \"Haunting spectral remains of voices, like sonic ghosts\",\n        \"filters\": \"aecho=0.8:0.88:60|30:0.4|0.3,apulsator=mode=sine:hz=0.125,volume=2.0\",\n        \"output_suffix\": \"SpectralGhosts\",\n        \"output_name\": \"Spectral hauntings\"\n    },\n    {\n        \"name\": \"TimeCollapse\",\n        \"description\": \"Time-stretched and collapsed, like multiple timelines overlapping\",\n        \"filters\": \"adelay=100|0,aecho=0.8:0.7:500|400|300|200|100:0.4|0.3|0.2|0.3|0.4,flanger=delay=10:speed=0.2:depth=5,volume=1.7\",\n        \"output_suffix\": \"TimeCollapse\",\n        \"output_name\": \"Temporal compression\"\n    },\n    {\n        \"name\": \"CrystalResonance\",\n        \"description\": \"Crystal-like resonance with harmonic overtones\",\n        \"filters\": \"treble=g=8:f=4000:w=0.5,bass=g=3:f=100:w=0.5,aphaser=type=t:speed=0.6:decay=0.3,aecho=0.8:0.9:60|80:0.25|0.15,volume=1.4\",\n        \"output_suffix\": \"CrystalResonance\",\n        \"output_name\": \"Crystalline frequencies\"\n    }\n]\n\ndef ensure_output_dir():\n    \"\"\"Ensure output directory exists.\"\"\"\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f\"Created output directory: {OUTPUT_DIR}\")\n\ndef create_voice_variation(variation):\n    \"\"\"Create a specific voice variation.\"\"\"\n    name = variation[\"name\"]\n    filters = variation[\"filters\"]\n    suffix = variation[\"output_suffix\"]\n    description = variation[\"description\"]\n    \n    output_audio = os.path.join(OUTPUT_DIR, f\"Voice_{suffix}.wav\")\n    output_video = os.path.join(OUTPUT_DIR, f\"WhereYouGoWhenYouLeave_{suffix}.mp4\")\n    \n    print(f\"\\nCreating {name} voice variation: {description}\")\n    \n    # Apply audio effect\n    effect_cmd = [\n        'ffmpeg',\n        '-i', INPUT_VOICE,\n        '-af', filters,\n        '-y',\n        output_audio\n    ]\n    \n    print(f\"Applying {name} effect...\")\n    subprocess.run(effect_cmd)\n    \n    if not os.path.exists(output_audio):\n        print(f\"Error creating {name} voice variation.\")\n        return False\n    \n    # Get duration to verify\n    duration_cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        output_audio\n    ]\n    \n    try:\n        result = subprocess.run(duration_cmd, stdout=subprocess.PIPE, text=True)\n        duration_str = result.stdout.strip()\n        if duration_str:\n            duration = float(duration_str)\n            print(f\"{name} voice duration: {duration:.2f} seconds\")\n        else:\n            print(f\"Warning: Could not determine duration for {name} voice\")\n            duration = 0\n    except Exception as e:\n        print(f\"Error getting duration: {str(e)}\")\n        duration = 0\n    \n    # Combine with video\n    video_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-i', output_audio,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'copy',\n        '-c:a', 'aac',\n        '-shortest',\n        '-metadata', f'title=Where You Go When You Leave - {variation[\"output_name\"]}',\n        '-metadata', f'comment={description}',\n        '-y',\n        output_video\n    ]\n    \n    print(f\"Creating {name} video...\")\n    subprocess.run(video_cmd)\n    \n    if os.path.exists(output_video):\n        print(f\"\u2713 {name} variation complete: {output_video}\")\n        return True\n    \n    print(f\"Error creating {name} video.\")\n    return False\n\ndef create_all_variations():\n    \"\"\"Create all voice variations.\"\"\"\n    ensure_output_dir()\n    \n    # Create a markdown report\n    report_path = os.path.join(OUTPUT_DIR, \"Voice_Variations_Guide.md\")\n    \n    with open(report_path, 'w') as report:\n        report.write(\"# WHERE YOU GO WHEN YOU LEAVE - VOICE VARIATIONS\\n\\n\")\n        report.write(\"*Audio transformations for the poetic journey*\\n\\n\")\n        report.write(\"## Description\\n\\n\")\n        report.write(\"These voice variations create distinct atmospheric interpretations of the \\\"Where You Go When You Leave\\\" sequence, \")\n        report.write(\"each offering a different emotional and experiential relationship to the poetry's content. \")\n        report.write(\"The transformations align with your Afro-futurist aesthetic framework while exploring the auditory dimensions \")\n        report.write(\"of Metz's Syntagmatic Analysis and Deleuze's Cineosis theories.\\n\\n\")\n        report.write(\"## Variations\\n\\n\")\n        report.write(\"| Variation | Description | Audio File | Video File |\\n\")\n        report.write(\"|-----------|-------------|------------|------------|\\n\")\n        \n        # Process each variation\n        completed = []\n        for variation in VARIATIONS:\n            try:\n                success = create_voice_variation(variation)\n                if success:\n                    completed.append(variation)\n            except Exception as e:\n                print(f\"Error processing {variation['name']}: {str(e)}\")\n                continue\n            \n        # Add successful variations to report\n        for variation in completed:\n            name = variation[\"name\"]\n            description = variation[\"description\"]\n            suffix = variation[\"output_suffix\"]\n            output_name = variation[\"output_name\"]\n            \n            audio_file = f\"Voice_{suffix}.wav\"\n            video_file = f\"WhereYouGoWhenYouLeave_{suffix}.mp4\"\n            \n            report.write(f\"| **{output_name}** | {description} | `{audio_file}` | `{video_file}` |\\n\")\n    \n    print(f\"\\nVoice variations complete. Guide created at: {report_path}\")\n\nif __name__ == \"__main__\":\n    start_time = time.time()\n    print(\"Starting voice variation creation...\")\n    create_all_variations()\n    end_time = time.time()\n    print(f\"\\nProcess completed in {(end_time - start_time):.2f} seconds.\")\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_VoiceTrack.wav",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "Voice_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "Voice_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "/Users/gaia/resurrecting atlantis/LIZARD",
      "AM/FM radio transmission effect with static and frequency limitations"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "effect_cmd"
      },
      {
        "type": "run",
        "snippet": "duration_cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "video_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "time"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "create_professional_vintage_mix.py",
    "size": 11478,
    "lines": 312,
    "source": "import os\nimport subprocess\nimport glob\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nORDERED_DIR = os.path.join(LIZARD_DIR, \"ORDERED\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_VINTAGE\")\nINPUT_VOICE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VoiceTrack.wav\")\nVIDEO_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n\n# Define vintage microphone and era variations\nVARIATIONS = [\n    {\n        \"name\": \"DeepVintageRadio\",\n        \"description\": \"Deep, pitched-down voice with vintage AM radio characteristics and subtle distortion\",\n        \"voice_filters\": \"aresample=44100,atempo=1.0,asetrate=44100*0.85,aresample=44100,highpass=f=200,lowpass=f=3500,equalizer=f=300:width_type=h:width=200:g=3,acompressor=threshold=0.105:ratio=3:attack=30:release=300,volume=1.5\",\n        \"background_filters\": \"aecho=0.8:0.6:500|800:0.5|0.3,volume=0.6\",\n        \"output_suffix\": \"DeepVintageRadio\"\n    },\n    {\n        \"name\": \"VinylCrackle\",\n        \"description\": \"Slightly pitched-down voice with vinyl record characteristics including crackles and warmth\",\n        \"voice_filters\": \"aresample=44100,atempo=0.9,asetrate=44100*0.9,aresample=44100,highpass=f=150,lowpass=f=8000,equalizer=f=250:width_type=h:width=150:g=2,volume=1.3\",\n        \"background_filters\": \"aecho=0.8:0.7:300|600:0.4|0.2,highpass=f=80,lowpass=f=12000,volume=0.5\",\n        \"output_suffix\": \"VinylCrackle\"\n    },\n    {\n        \"name\": \"GrittyBroadcast\",\n        \"description\": \"Gritty broadcast voice with analog compression and frequency limitations\",\n        \"voice_filters\": \"highpass=f=300,lowpass=f=5000,equalizer=f=2000:width_type=h:width=800:g=3,acompressor=threshold=0.12:ratio=6:attack=10:release=100,volume=1.6\",\n        \"background_filters\": \"aecho=0.7:0.5:1000|1500:0.3|0.2,volume=0.45\",\n        \"output_suffix\": \"GrittyBroadcast\"\n    },\n    {\n        \"name\": \"WarpedTape\",\n        \"description\": \"Warped tape recorder effect with flutter, wobble and magnetic distortion\",\n        \"voice_filters\": \"aresample=44100,vibrato=f=4:d=0.2,tremolo=f=2:d=0.2,bandpass=f=1200:width_type=h:width=1000,equalizer=f=500:width_type=h:width=300:g=2,volume=1.4\",\n        \"background_filters\": \"aecho=0.8:0.6:800|1200:0.4|0.2,flanger=delay=1:depth=0.5:regen=0.3:speed=0.1:shape=sinusoidal,volume=0.5\",\n        \"output_suffix\": \"WarpedTape\"\n    },\n    {\n        \"name\": \"ModernMorphed\",\n        \"description\": \"Modern professional mix with subtle pitch morphing and clean vintage characteristics\",\n        \"voice_filters\": \"aresample=44100,atempo=0.95,asetrate=44100*0.95,aresample=44100,aphaser=in_gain=0.4:out_gain=0.4:delay=3:decay=0.4,equalizer=f=400:width_type=h:width=200:g=2,volume=1.3\",\n        \"background_filters\": \"aecho=0.6:0.4:600|900:0.3|0.2,volume=0.6\",\n        \"output_suffix\": \"ModernMorphed\"\n    }\n]\n\ndef ensure_temp_dir():\n    \"\"\"Ensure temporary directory exists.\"\"\"\n    if not os.path.exists(TEMP_DIR):\n        os.makedirs(TEMP_DIR)\n        print(f\"Created temporary directory: {TEMP_DIR}\")\n\ndef extract_audio_from_videos():\n    \"\"\"Extract combined audio from the ordered video directory.\"\"\"\n    combined_audio = os.path.join(TEMP_DIR, \"original_combined.wav\")\n    \n    # Extract audio directly from the final assembly video\n    extract_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-vn',\n        '-acodec', 'pcm_s16le',\n        '-y',\n        combined_audio\n    ]\n    \n    print(\"Extracting original audio from assembled video...\")\n    subprocess.run(extract_cmd)\n    \n    return combined_audio if os.path.exists(combined_audio) else None\n\ndef generate_vintage_elements():\n    \"\"\"Generate vintage audio elements.\"\"\"\n    # Generate vinyl crackle\n    vinyl_file = os.path.join(TEMP_DIR, \"vinyl_crackle.wav\")\n    vinyl_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.05:c=brown:d=101',\n        '-af', 'highpass=f=1000,lowpass=f=16000,volume=0.1',\n        '-y',\n        vinyl_file\n    ]\n    \n    print(\"Generating vinyl crackle...\")\n    subprocess.run(vinyl_cmd)\n    \n    # Generate tape hiss\n    tape_file = os.path.join(TEMP_DIR, \"tape_hiss.wav\")\n    tape_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.03:c=white:d=101',\n        '-af', 'highpass=f=3000,lowpass=f=18000,volume=0.08',\n        '-y',\n        tape_file\n    ]\n    \n    print(\"Generating tape hiss...\")\n    subprocess.run(tape_cmd)\n    \n    # Generate radio static\n    radio_file = os.path.join(TEMP_DIR, \"radio_static.wav\")\n    radio_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.03:c=pink:d=101',\n        '-af', 'bandpass=f=1000:width_type=h:width=800,volume=0.05',\n        '-y',\n        radio_file\n    ]\n    \n    print(\"Generating radio static...\")\n    subprocess.run(radio_cmd)\n    \n    return {\n        'vinyl': vinyl_file if os.path.exists(vinyl_file) else None,\n        'tape': tape_file if os.path.exists(tape_file) else None,\n        'radio': radio_file if os.path.exists(radio_file) else None\n    }\n\ndef create_vintage_variation(variation, original_audio, vintage_elements):\n    \"\"\"Create a vintage microphone soundscape variation.\"\"\"\n    name = variation[\"name\"]\n    description = variation[\"description\"]\n    voice_filters = variation[\"voice_filters\"]\n    background_filters = variation[\"background_filters\"]\n    suffix = variation[\"output_suffix\"]\n    \n    # Output files\n    processed_voice = os.path.join(TEMP_DIR, f\"voice_{suffix}.wav\")\n    processed_bg = os.path.join(TEMP_DIR, f\"bg_{suffix}.wav\")\n    final_mix = os.path.join(TEMP_DIR, f\"mix_{suffix}.wav\")\n    output_video = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{suffix}.mp4\")\n    \n    print(f\"\\nCreating {name} vintage mix: {description}\")\n    \n    # Process voice with vintage microphone effects\n    voice_cmd = [\n        'ffmpeg',\n        '-i', INPUT_VOICE,\n        '-af', voice_filters,\n        '-y',\n        processed_voice\n    ]\n    \n    print(f\"Processing voice with {name} effects...\")\n    subprocess.run(voice_cmd)\n    \n    if not os.path.exists(processed_voice):\n        print(f\"Error creating processed voice for {name}\")\n        return False\n    \n    # Process background with reverb and ambient effects\n    bg_cmd = [\n        'ffmpeg',\n        '-i', original_audio,\n        '-af', background_filters,\n        '-y',\n        processed_bg\n    ]\n    \n    print(f\"Processing background with {name} effects...\")\n    subprocess.run(bg_cmd)\n    \n    if not os.path.exists(processed_bg):\n        print(f\"Error creating processed background for {name}\")\n        return False\n    \n    # Setup for the final mix\n    inputs = ['-i', processed_voice, '-i', processed_bg]\n    filter_complex = []\n    \n    # Add vintage elements based on the variation type\n    if name == \"DeepVintageRadio\" and vintage_elements['radio']:\n        inputs.extend(['-i', vintage_elements['radio']])\n        filter_complex = [\n            \"[0:a][1:a]amix=inputs=2:duration=longest[main];\",\n            \"[main][2:a]amix=inputs=2:duration=longest[aout]\"\n        ]\n        map_out = \"[aout]\"\n    elif name == \"VinylCrackle\" and vintage_elements['vinyl']:\n        inputs.extend(['-i', vintage_elements['vinyl']])\n        filter_complex = [\n            \"[0:a][1:a]amix=inputs=2:duration=longest[main];\",\n            \"[main][2:a]amix=inputs=2:duration=longest[aout]\"\n        ]\n        map_out = \"[aout]\"\n    elif name == \"WarpedTape\" and vintage_elements['tape']:\n        inputs.extend(['-i', vintage_elements['tape']])\n        filter_complex = [\n            \"[0:a][1:a]amix=inputs=2:duration=longest[main];\",\n            \"[main][2:a]amix=inputs=2:duration=longest[aout]\"\n        ]\n        map_out = \"[aout]\"\n    else:\n        filter_complex = [\"[0:a][1:a]amix=inputs=2:duration=longest[aout]\"]\n        map_out = \"[aout]\"\n    \n    # Create the final mix\n    mix_cmd = [\n        'ffmpeg',\n        *inputs,\n        '-filter_complex', ''.join(filter_complex),\n        '-map', map_out,\n        '-y',\n        final_mix\n    ]\n    \n    print(f\"Creating final audio mix for {name}...\")\n    subprocess.run(mix_cmd)\n    \n    if not os.path.exists(final_mix):\n        print(f\"Error creating final mix for {name}\")\n        return False\n    \n    # Combine with video\n    video_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-i', final_mix,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'copy',\n        '-c:a', 'aac',\n        '-shortest',\n        '-metadata', f'title=Where You Go When You Leave - {name}',\n        '-metadata', f'comment={description}',\n        '-y',\n        output_video\n    ]\n    \n    print(f\"Creating {name} video with vintage audio...\")\n    subprocess.run(video_cmd)\n    \n    if os.path.exists(output_video):\n        print(f\"\u2713 {name} vintage mix complete: {output_video}\")\n        return True\n    \n    print(f\"Error creating {name} video\")\n    return False\n\ndef create_all_variations():\n    \"\"\"Create all vintage microphone variations.\"\"\"\n    ensure_temp_dir()\n    \n    # Extract original audio\n    original_audio = extract_audio_from_videos()\n    if not original_audio:\n        print(\"Error extracting original audio from videos\")\n        return False\n    \n    # Generate vintage elements\n    vintage_elements = generate_vintage_elements()\n    \n    # Create a markdown report\n    report_path = os.path.join(LIZARD_DIR, \"Vintage_Voice_Guide.md\")\n    \n    with open(report_path, 'w') as report:\n        report.write(\"# WHERE YOU GO WHEN YOU LEAVE - VINTAGE VOICE MIXES\\n\\n\")\n        report.write(\"*Professional vintage microphone and era transformations*\\n\\n\")\n        report.write(\"## Description\\n\\n\")\n        report.write(\"These professional mixes apply vintage microphone and era characteristics to the voice track, \")\n        report.write(\"focusing on pitch manipulation, depth, and texture rather than excessive reverb. \")\n        report.write(\"Each variation creates a distinct sonic character that aligns with different historical \")\n        report.write(\"recording technologies while maintaining professional audio quality.\\n\\n\")\n        report.write(\"## Vintage Variations\\n\\n\")\n        report.write(\"| Variation | Description | Video File |\\n\")\n        report.write(\"|-----------|-------------|------------|\\n\")\n        \n        # Process each variation\n        completed = []\n        for variation in VARIATIONS:\n            try:\n                success = create_vintage_variation(variation, original_audio, vintage_elements)\n                if success:\n                    completed.append(variation)\n            except Exception as e:\n                print(f\"Error processing {variation['name']}: {str(e)}\")\n                continue\n            \n        # Add successful variations to report\n        for variation in completed:\n            name = variation[\"name\"]\n            description = variation[\"description\"]\n            suffix = variation[\"output_suffix\"]\n            \n            video_file = f\"WhereYouGoWhenYouLeave_{suffix}.mp4\"\n            \n            report.write(f\"| **{name}** | {description} | `{video_file}` |\\n\")\n    \n    print(f\"\\nVintage voice mixes complete. Guide created at: {report_path}\")\n    return True\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Starting vintage voice mix creation...\")\n    success = create_all_variations()\n    \n    if success:\n        print(\"Vintage voice mix creation complete!\")\n    else:\n        print(\"Error creating vintage voice mixes\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_VoiceTrack.wav",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "original_combined.wav",
      "vinyl_crackle.wav",
      "tape_hiss.wav",
      "radio_static.wav",
      "voice_{suffix}.wav",
      "bg_{suffix}.wav",
      "mix_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "extract_cmd"
      },
      {
        "type": "run",
        "snippet": "vinyl_cmd"
      },
      {
        "type": "run",
        "snippet": "tape_cmd"
      },
      {
        "type": "run",
        "snippet": "radio_cmd"
      },
      {
        "type": "run",
        "snippet": "voice_cmd"
      },
      {
        "type": "run",
        "snippet": "bg_cmd"
      },
      {
        "type": "run",
        "snippet": "mix_cmd"
      },
      {
        "type": "run",
        "snippet": "video_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "glob"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "assemble_lizard_videos.py",
    "size": 4012,
    "lines": 128,
    "source": "import os\nimport subprocess\nimport datetime\n\n# Configuration\nORDERED_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD/ORDERED\"\nOUTPUT_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD\"\nOUTPUT_FILENAME = \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\"\nLIST_FILE = os.path.join(OUTPUT_DIR, \"video_list.txt\")\nLOG_FILE = os.path.join(OUTPUT_DIR, \"assembly_log.txt\")\n\ndef check_ffmpeg():\n    \"\"\"Check if FFmpeg is installed.\"\"\"\n    try:\n        subprocess.run(['ffmpeg', '-version'], \n                      stdout=subprocess.PIPE, \n                      stderr=subprocess.PIPE)\n        return True\n    except:\n        return False\n\ndef create_video_list():\n    \"\"\"Create a list file for FFmpeg with all ordered videos.\"\"\"\n    videos = []\n    \n    # Get all video files\n    for filename in sorted(os.listdir(ORDERED_DIR)):\n        if filename.endswith(('.mp4', '.mov', '.avi')):\n            video_path = os.path.join(ORDERED_DIR, filename)\n            videos.append(video_path)\n    \n    # Create the list file for FFmpeg\n    with open(LIST_FILE, 'w') as f:\n        for video in videos:\n            f.write(f\"file '{video}'\\n\")\n    \n    return videos\n\ndef assemble_videos(videos):\n    \"\"\"Assemble all videos into a single file using FFmpeg.\"\"\"\n    output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n    \n    # Create log header\n    with open(LOG_FILE, 'w') as log:\n        log.write(\"# LIZARD VIDEO ASSEMBLY LOG\\n\\n\")\n        log.write(f\"Assembly started at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n        log.write(f\"Output file: {output_path}\\n\\n\")\n        log.write(\"## Videos being assembled (in order):\\n\\n\")\n        \n        for i, video in enumerate(videos):\n            log.write(f\"{i+1}. {os.path.basename(video)}\\n\")\n        \n        log.write(\"\\n## FFmpeg Operation Log:\\n\\n\")\n    \n    # Prepare FFmpeg command\n    ffmpeg_cmd = [\n        'ffmpeg',\n        '-f', 'concat',\n        '-safe', '0',\n        '-i', LIST_FILE,\n        '-c', 'copy',  # Use copy codec for faster processing\n        output_path\n    ]\n    \n    # Run FFmpeg and capture output\n    process = subprocess.Popen(\n        ffmpeg_cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True\n    )\n    \n    # Write FFmpeg output to log\n    with open(LOG_FILE, 'a') as log:\n        for line in process.stdout:\n            log.write(line)\n    \n    # Wait for process to complete\n    return_code = process.wait()\n    \n    # Add completion status to log\n    with open(LOG_FILE, 'a') as log:\n        log.write(f\"\\n\\nAssembly completed at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        log.write(f\"Return code: {return_code}\\n\")\n        \n        if return_code == 0:\n            log.write(\"Status: SUCCESS\\n\")\n            file_size = os.path.getsize(output_path) / (1024 * 1024)  # Size in MB\n            log.write(f\"Output file size: {file_size:.2f} MB\\n\")\n        else:\n            log.write(\"Status: FAILED\\n\")\n    \n    return return_code == 0\n\ndef main():\n    \"\"\"Main function to assemble videos.\"\"\"\n    print(\"Starting video assembly process...\")\n    \n    # Check if FFmpeg is installed\n    if not check_ffmpeg():\n        print(\"ERROR: FFmpeg is not installed or not in PATH.\")\n        print(\"Please install FFmpeg to continue.\")\n        return\n    \n    # Create the list of videos to assemble\n    print(\"Creating list of videos to assemble...\")\n    videos = create_video_list()\n    \n    if not videos:\n        print(\"No videos found in the ORDERED directory.\")\n        return\n    \n    print(f\"Found {len(videos)} videos to assemble.\")\n    \n    # Assemble the videos\n    print(\"Assembling videos (this may take some time)...\")\n    success = assemble_videos(videos)\n    \n    if success:\n        output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n        print(f\"Assembly complete! Output file: {output_path}\")\n        print(f\"Log file: {LOG_FILE}\")\n    else:\n        print(f\"Assembly failed. Check the log for details: {LOG_FILE}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "/Users/gaia/resurrecting atlantis/LIZARD/ORDERED",
      "/Users/gaia/resurrecting atlantis/LIZARD",
      ")\n            file_size = os.path.getsize(output_path) / (1024 * 1024)  # Size in MB\n            log.write(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "['ffmpeg', '-version'], \n                      stdout=subprocess.PIPE, \n                      stderr=subprocess.PIPE"
      },
      {
        "type": "Popen",
        "snippet": "\n        ffmpeg_cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True\n    "
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "create_vintage_fixed_length.py",
    "size": 12702,
    "lines": 341,
    "source": "import os\nimport subprocess\nimport glob\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_FIXED\")\nINPUT_VOICE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VoiceTrack.wav\")\nVIDEO_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\nVIDEO_DURATION = 100.91  # Target duration in seconds (1:40)\n\n# Define vintage microphone variations with timing correction\nVARIATIONS = [\n    {\n        \"name\": \"DeepVintageRadio\",\n        \"description\": \"Deep, pitched-down voice with vintage AM radio characteristics and subtle distortion\",\n        \"voice_filters\": \"aresample=44100,atempo=1.0,asetrate=44100*0.85,aresample=44100,highpass=f=200,lowpass=f=3500,equalizer=f=300:width_type=h:width=200:g=3,acompressor=threshold=0.105:ratio=3:attack=30:release=300,volume=1.5\",\n        \"bg_volume\": 0.6,\n        \"output_suffix\": \"DeepVintageRadio\"\n    },\n    {\n        \"name\": \"VinylCrackle\",\n        \"description\": \"Slightly pitched-down voice with vinyl record characteristics including crackles and warmth\",\n        \"voice_filters\": \"aresample=44100,atempo=0.9,asetrate=44100*0.9,aresample=44100,highpass=f=150,lowpass=f=8000,equalizer=f=250:width_type=h:width=150:g=2,volume=1.3\",\n        \"bg_volume\": 0.5,\n        \"output_suffix\": \"VinylCrackle\"\n    },\n    {\n        \"name\": \"GrittyBroadcast\",\n        \"description\": \"Gritty broadcast voice with analog compression and frequency limitations\",\n        \"voice_filters\": \"highpass=f=300,lowpass=f=5000,equalizer=f=2000:width_type=h:width=800:g=3,acompressor=threshold=0.12:ratio=6:attack=10:release=100,volume=1.6\",\n        \"bg_volume\": 0.45,\n        \"output_suffix\": \"GrittyBroadcast\"\n    },\n    {\n        \"name\": \"WarpedTape\",\n        \"description\": \"Warped tape recorder effect with flutter, wobble and magnetic distortion\",\n        \"voice_filters\": \"aresample=44100,vibrato=f=4:d=0.2,tremolo=f=2:d=0.2,bandpass=f=1200:width_type=h:width=1000,equalizer=f=500:width_type=h:width=300:g=2,volume=1.4\",\n        \"bg_volume\": 0.5,\n        \"output_suffix\": \"WarpedTape\"\n    },\n    {\n        \"name\": \"ModernMorphed\",\n        \"description\": \"Modern professional mix with subtle pitch morphing and clean vintage characteristics\",\n        \"voice_filters\": \"aresample=44100,atempo=0.95,asetrate=44100*0.95,aresample=44100,aphaser=in_gain=0.4:out_gain=0.4:delay=3:decay=0.4,equalizer=f=400:width_type=h:width=200:g=2,volume=1.3\",\n        \"bg_volume\": 0.6,\n        \"output_suffix\": \"ModernMorphed\"\n    }\n]\n\ndef ensure_temp_dir():\n    \"\"\"Ensure temporary directory exists.\"\"\"\n    if not os.path.exists(TEMP_DIR):\n        os.makedirs(TEMP_DIR)\n        print(f\"Created temporary directory: {TEMP_DIR}\")\n\ndef extract_audio_from_videos():\n    \"\"\"Extract combined audio from the assembled video.\"\"\"\n    combined_audio = os.path.join(TEMP_DIR, \"original_combined.wav\")\n    \n    # Extract audio directly from the final assembly video\n    extract_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-vn',\n        '-acodec', 'pcm_s16le',\n        '-y',\n        combined_audio\n    ]\n    \n    print(\"Extracting original audio from assembled video...\")\n    subprocess.run(extract_cmd)\n    \n    return combined_audio if os.path.exists(combined_audio) else None\n\ndef get_video_duration():\n    \"\"\"Get the exact duration of the video in seconds.\"\"\"\n    cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        VIDEO_FILE\n    ]\n    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n    return float(result.stdout.strip())\n\ndef generate_vintage_elements():\n    \"\"\"Generate vintage audio elements at exact video length.\"\"\"\n    video_duration = get_video_duration()\n    \n    # Generate vinyl crackle\n    vinyl_file = os.path.join(TEMP_DIR, \"vinyl_crackle.wav\")\n    vinyl_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', f'anoisesrc=a=0.05:c=brown:d={video_duration}',\n        '-af', 'highpass=f=1000,lowpass=f=16000,volume=0.1',\n        '-y',\n        vinyl_file\n    ]\n    \n    print(\"Generating vinyl crackle...\")\n    subprocess.run(vinyl_cmd)\n    \n    # Generate tape hiss\n    tape_file = os.path.join(TEMP_DIR, \"tape_hiss.wav\")\n    tape_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', f'anoisesrc=a=0.03:c=white:d={video_duration}',\n        '-af', 'highpass=f=3000,lowpass=f=18000,volume=0.08',\n        '-y',\n        tape_file\n    ]\n    \n    print(\"Generating tape hiss...\")\n    subprocess.run(tape_cmd)\n    \n    # Generate radio static\n    radio_file = os.path.join(TEMP_DIR, \"radio_static.wav\")\n    radio_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', f'anoisesrc=a=0.03:c=pink:d={video_duration}',\n        '-af', 'bandpass=f=1000:width_type=h:width=800,volume=0.05',\n        '-y',\n        radio_file\n    ]\n    \n    print(\"Generating radio static...\")\n    subprocess.run(radio_cmd)\n    \n    return {\n        'vinyl': vinyl_file if os.path.exists(vinyl_file) else None,\n        'tape': tape_file if os.path.exists(tape_file) else None,\n        'radio': radio_file if os.path.exists(radio_file) else None\n    }\n\ndef create_vintage_variation(variation, original_audio, vintage_elements):\n    \"\"\"Create a vintage microphone soundscape variation with fixed length.\"\"\"\n    name = variation[\"name\"]\n    description = variation[\"description\"]\n    voice_filters = variation[\"voice_filters\"]\n    bg_volume = variation[\"bg_volume\"]\n    suffix = variation[\"output_suffix\"]\n    \n    # Get exact video duration\n    video_duration = get_video_duration()\n    print(f\"Target video duration: {video_duration:.2f} seconds\")\n    \n    # Output files\n    processed_voice = os.path.join(TEMP_DIR, f\"voice_{suffix}.wav\")\n    processed_voice_fixed = os.path.join(TEMP_DIR, f\"voice_{suffix}_fixed.wav\")\n    final_mix = os.path.join(TEMP_DIR, f\"mix_{suffix}.wav\")\n    output_video = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{suffix}_Fixed.mp4\")\n    \n    print(f\"\\nCreating {name} vintage mix: {description}\")\n    \n    # Process voice with vintage microphone effects\n    voice_cmd = [\n        'ffmpeg',\n        '-i', INPUT_VOICE,\n        '-af', voice_filters,\n        '-y',\n        processed_voice\n    ]\n    \n    print(f\"Processing voice with {name} effects...\")\n    subprocess.run(voice_cmd)\n    \n    if not os.path.exists(processed_voice):\n        print(f\"Error creating processed voice for {name}\")\n        return False\n    \n    # Get processed voice duration\n    duration_cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        processed_voice\n    ]\n    \n    result = subprocess.run(duration_cmd, stdout=subprocess.PIPE, text=True)\n    voice_duration = float(result.stdout.strip())\n    print(f\"Processed voice duration: {voice_duration:.2f} seconds\")\n    \n    # Calculate tempo adjustment to match video duration exactly\n    tempo_factor = voice_duration / video_duration\n    print(f\"Tempo adjustment factor: {tempo_factor:.4f}\")\n    \n    # Apply tempo adjustment to make voice track exactly match video duration\n    fix_cmd = [\n        'ffmpeg',\n        '-i', processed_voice,\n        '-af', f'atempo={tempo_factor}',\n        '-y',\n        processed_voice_fixed\n    ]\n    \n    print(f\"Adjusting voice timing to match video duration...\")\n    subprocess.run(fix_cmd)\n    \n    if not os.path.exists(processed_voice_fixed):\n        print(f\"Error fixing voice duration for {name}\")\n        return False\n    \n    # Verify fixed duration\n    result = subprocess.run(duration_cmd, stdout=subprocess.PIPE, text=True, input=processed_voice_fixed)\n    fixed_duration = float(result.stdout.strip() if result.stdout.strip() else \"0\")\n    print(f\"Fixed voice duration: {fixed_duration:.2f} seconds\")\n    \n    # Create the mix\n    inputs = ['-i', processed_voice_fixed, '-i', original_audio]\n    filter_str = f\"[0:a][1:a]amix=inputs=2:weights=1 {bg_volume}[aout]\"\n    \n    # Add vintage elements based on the variation type\n    if name == \"DeepVintageRadio\" and vintage_elements['radio']:\n        inputs.extend(['-i', vintage_elements['radio']])\n        filter_str = f\"[0:a][1:a]amix=inputs=2:weights=1 {bg_volume}[main];[main][2:a]amix=inputs=2:weights=1 0.15[aout]\"\n    elif name == \"VinylCrackle\" and vintage_elements['vinyl']:\n        inputs.extend(['-i', vintage_elements['vinyl']])\n        filter_str = f\"[0:a][1:a]amix=inputs=2:weights=1 {bg_volume}[main];[main][2:a]amix=inputs=2:weights=1 0.15[aout]\"\n    elif name == \"WarpedTape\" and vintage_elements['tape']:\n        inputs.extend(['-i', vintage_elements['tape']])\n        filter_str = f\"[0:a][1:a]amix=inputs=2:weights=1 {bg_volume}[main];[main][2:a]amix=inputs=2:weights=1 0.15[aout]\"\n    \n    # Create the final mix\n    mix_cmd = [\n        'ffmpeg',\n        *inputs,\n        '-filter_complex', filter_str,\n        '-map', '[aout]',\n        '-y',\n        final_mix\n    ]\n    \n    print(f\"Creating final audio mix for {name}...\")\n    subprocess.run(mix_cmd)\n    \n    if not os.path.exists(final_mix):\n        print(f\"Error creating final mix for {name}\")\n        return False\n    \n    # Combine with video\n    video_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-i', final_mix,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'copy',\n        '-c:a', 'aac',\n        '-shortest',\n        '-metadata', f'title=Where You Go When You Leave - {name}',\n        '-metadata', f'comment={description}',\n        '-y',\n        output_video\n    ]\n    \n    print(f\"Creating {name} video with vintage audio...\")\n    subprocess.run(video_cmd)\n    \n    if os.path.exists(output_video):\n        print(f\"\u2713 {name} vintage mix complete: {output_video}\")\n        return True\n    \n    print(f\"Error creating {name} video\")\n    return False\n\ndef create_all_variations():\n    \"\"\"Create all vintage microphone variations with fixed length.\"\"\"\n    ensure_temp_dir()\n    \n    # Get exact video duration\n    video_duration = get_video_duration()\n    print(f\"Target video duration: {video_duration:.2f} seconds\")\n    \n    # Extract original audio\n    original_audio = extract_audio_from_videos()\n    if not original_audio:\n        print(\"Error extracting original audio from videos\")\n        return False\n    \n    # Generate vintage elements\n    vintage_elements = generate_vintage_elements()\n    \n    # Create a markdown report\n    report_path = os.path.join(LIZARD_DIR, \"Fixed_Length_Voice_Guide.md\")\n    \n    with open(report_path, 'w') as report:\n        report.write(\"# WHERE YOU GO WHEN YOU LEAVE - FIXED LENGTH VINTAGE MIXES\\n\\n\")\n        report.write(\"*Professional vintage voice transformations matched exactly to 1:40 video length*\\n\\n\")\n        report.write(\"## Description\\n\\n\")\n        report.write(\"These professional mixes apply vintage microphone and era characteristics to the voice track, \")\n        report.write(\"focusing on pitch manipulation, depth, and texture rather than excessive reverb. \")\n        report.write(\"Each variation is precisely timed to match the 1:40 (100.91 second) video length \")\n        report.write(\"using automatic tempo adjustment to ensure perfect synchronization.\\n\\n\")\n        report.write(\"## Vintage Variations\\n\\n\")\n        report.write(\"| Variation | Description | Video File |\\n\")\n        report.write(\"|-----------|-------------|------------|\\n\")\n        \n        # Process each variation\n        completed = []\n        for variation in VARIATIONS:\n            try:\n                success = create_vintage_variation(variation, original_audio, vintage_elements)\n                if success:\n                    completed.append(variation)\n            except Exception as e:\n                print(f\"Error processing {variation['name']}: {str(e)}\")\n                continue\n            \n        # Add successful variations to report\n        for variation in completed:\n            name = variation[\"name\"]\n            description = variation[\"description\"]\n            suffix = variation[\"output_suffix\"]\n            \n            video_file = f\"WhereYouGoWhenYouLeave_{suffix}_Fixed.mp4\"\n            \n            report.write(f\"| **{name}** | {description} | `{video_file}` |\\n\")\n    \n    print(f\"\\nFixed length vintage mixes complete. Guide created at: {report_path}\")\n    return True\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Starting fixed length vintage mix creation...\")\n    success = create_all_variations()\n    \n    if success:\n        print(\"Fixed length vintage mix creation complete!\")\n    else:\n        print(\"Error creating fixed length vintage mixes\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_VoiceTrack.wav",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "original_combined.wav",
      "vinyl_crackle.wav",
      "tape_hiss.wav",
      "radio_static.wav",
      "voice_{suffix}.wav",
      "voice_{suffix}_fixed.wav",
      "mix_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}_Fixed.mp4",
      "WhereYouGoWhenYouLeave_{suffix}_Fixed.mp4",
      "/Users/gaia/resurrecting atlantis",
      ")\n    \n    # Calculate tempo adjustment to match video duration exactly\n    tempo_factor = voice_duration / video_duration\n    print(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "extract_cmd"
      },
      {
        "type": "run",
        "snippet": "cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "vinyl_cmd"
      },
      {
        "type": "run",
        "snippet": "tape_cmd"
      },
      {
        "type": "run",
        "snippet": "radio_cmd"
      },
      {
        "type": "run",
        "snippet": "voice_cmd"
      },
      {
        "type": "run",
        "snippet": "duration_cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "fix_cmd"
      },
      {
        "type": "run",
        "snippet": "duration_cmd, stdout=subprocess.PIPE, text=True, input=processed_voice_fixed"
      },
      {
        "type": "run",
        "snippet": "mix_cmd"
      },
      {
        "type": "run",
        "snippet": "video_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "glob"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "create_voice_variations.py",
    "size": 7231,
    "lines": 178,
    "source": "import os\nimport subprocess\nimport time\n\n# Configuration\nLIZARD_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD\"\nINPUT_VOICE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VoiceTrack.wav\")\nVIDEO_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\nOUTPUT_DIR = os.path.join(LIZARD_DIR, \"VOICE_VARIATIONS\")\n\n# Voice variation definitions\nVARIATIONS = [\n    {\n        \"name\": \"Radio\",\n        \"description\": \"AM/FM radio transmission effect with static and frequency limitations\",\n        \"filters\": \"highpass=f=500,lowpass=f=4000,equalizer=f=1000:width_type=o:width=1:g=3,areverse,afftdn=nr=40:nf=-20,areverse,bandpass=f=1800:width_type=h:width=700,aphaser,volume=1.5,acontrast=0.8\",\n        \"output_suffix\": \"Radio\",\n        \"output_name\": \"Old-time radio broadcast\"\n    },\n    {\n        \"name\": \"Echo\",\n        \"description\": \"Cathedral-like echo with layered reverberations\",\n        \"filters\": \"aecho=0.8:0.9:1000|1800:0.3|0.25,aecho=0.8:0.7:40|60|80:0.4|0.3|0.2,areverse,aecho=0.6:0.3:60|30:0.3|0.3,areverse,highpass=f=200,lowpass=f=3000,volume=1.5\",\n        \"output_suffix\": \"Echo\",\n        \"output_name\": \"Subterranean echo chambers\"\n    },\n    {\n        \"name\": \"Harmonious\",\n        \"description\": \"Choir-like harmonization with subtle pitch variation\",\n        \"filters\": \"chorus=0.5:0.9:50|60|40:0.4|0.32|0.3:0.25|0.4|0.3:2|2.3|1.3,chorus=0.6:0.4:25|50:0.5|0.4:0.25|0.4:3|5,equalizer=f=250:width_type=h:width=100:g=2,equalizer=f=2500:width_type=h:width=500:g=3,volume=1.4\",\n        \"output_suffix\": \"Harmonious\",\n        \"output_name\": \"Spectral harmonies\"\n    },\n    {\n        \"name\": \"Robotic\",\n        \"description\": \"Mechanical vocoder-style voice with digital artifacts\",\n        \"filters\": \"asetrate=11025,aresample=44100,vibrato=f=20:d=0.2,rubberband=pitch=1.5,areverse,vibrato=f=40:d=0.1,areverse,tremolo=f=60:d=0.1,acontrast=0.8,volume=1.8\",\n        \"output_suffix\": \"Robotic\",\n        \"output_name\": \"Digital consciousness\"\n    },\n    {\n        \"name\": \"OrganicMorph\",\n        \"description\": \"Fluid, organic morphing with time-based transformations\",\n        \"filters\": \"rubberband=tempo=0.8:pitch=0.9,tremolo=f=4:d=0.4,vibrato=f=5:d=0.3,flanger=delay=5:speed=1:depth=5,atempo=1.25,aphaser=type=t:speed=0.6:decay=0.3,volume=1.6\",\n        \"output_suffix\": \"OrganicMorph\",\n        \"output_name\": \"Liquid metamorphosis\"\n    },\n    {\n        \"name\": \"SpectralGhosts\",\n        \"description\": \"Haunting spectral remains of voices, like sonic ghosts\",\n        \"filters\": \"afftfilt=real='hypot(re,im)*sin(0)':imag='hypot(re,im)*cos(0)':win_size=1024:overlap=0.75,areverse,silenceremove=1:0:-50dB,areverse,aecho=0.8:0.88:60|30:0.4|0.3,apulsator=mode=sine:hz=0.125,stereotools=mlev=0.1,volume=2.0\",\n        \"output_suffix\": \"SpectralGhosts\",\n        \"output_name\": \"Spectral hauntings\"\n    },\n    {\n        \"name\": \"TimeCollapse\",\n        \"description\": \"Time-stretched and collapsed, like multiple timelines overlapping\",\n        \"filters\": \"adelay=100|0,aecho=0.8:0.7:500|400|300|200|100:0.4|0.3|0.2|0.3|0.4,asetrate=44100*0.5,aresample=44100,atempo=2,flanger=delay=10:speed=0.2:depth=5,vibrato=f=4:d=0.5,volume=1.7\",\n        \"output_suffix\": \"TimeCollapse\",\n        \"output_name\": \"Temporal compression\"\n    },\n    {\n        \"name\": \"CrystalResonance\",\n        \"description\": \"Crystal-like resonance with harmonic overtones\",\n        \"filters\": \"crystalizer=i=5:c=1,treble=g=8:f=4000:w=0.5,bass=g=3:f=100:w=0.5,aphaser=type=t:speed=0.6:decay=0.3,aecho=0.8:0.9:60|80:0.25|0.15,volume=1.4\",\n        \"output_suffix\": \"CrystalResonance\",\n        \"output_name\": \"Crystalline frequencies\"\n    }\n]\n\ndef ensure_output_dir():\n    \"\"\"Ensure output directory exists.\"\"\"\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f\"Created output directory: {OUTPUT_DIR}\")\n\ndef create_voice_variation(variation):\n    \"\"\"Create a specific voice variation.\"\"\"\n    name = variation[\"name\"]\n    filters = variation[\"filters\"]\n    suffix = variation[\"output_suffix\"]\n    description = variation[\"description\"]\n    \n    output_audio = os.path.join(OUTPUT_DIR, f\"Voice_{suffix}.wav\")\n    output_video = os.path.join(OUTPUT_DIR, f\"WhereYouGoWhenYouLeave_{suffix}.mp4\")\n    \n    print(f\"\\nCreating {name} voice variation: {description}\")\n    \n    # Apply audio effect\n    effect_cmd = [\n        'ffmpeg',\n        '-i', INPUT_VOICE,\n        '-af', filters,\n        '-y',\n        output_audio\n    ]\n    \n    print(f\"Applying {name} effect...\")\n    subprocess.run(effect_cmd, capture_output=True)\n    \n    if not os.path.exists(output_audio):\n        print(f\"Error creating {name} voice variation.\")\n        return False\n    \n    # Get duration to verify\n    duration_cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        output_audio\n    ]\n    duration = float(subprocess.run(duration_cmd, stdout=subprocess.PIPE, text=True).stdout.strip())\n    print(f\"{name} voice duration: {duration:.2f} seconds\")\n    \n    # Combine with video\n    video_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-i', output_audio,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'copy',\n        '-c:a', 'aac',\n        '-shortest',\n        '-metadata', f'title=Where You Go When You Leave - {variation[\"output_name\"]}',\n        '-metadata', f'comment={description}',\n        '-y',\n        output_video\n    ]\n    \n    print(f\"Creating {name} video...\")\n    subprocess.run(video_cmd, capture_output=True)\n    \n    if os.path.exists(output_video):\n        print(f\"\u2713 {name} variation complete: {output_video}\")\n        return True\n    \n    print(f\"Error creating {name} video.\")\n    return False\n\ndef create_all_variations():\n    \"\"\"Create all voice variations.\"\"\"\n    ensure_output_dir()\n    \n    # Create a markdown report\n    report_path = os.path.join(OUTPUT_DIR, \"Voice_Variations_Guide.md\")\n    \n    with open(report_path, 'w') as report:\n        report.write(\"# WHERE YOU GO WHEN YOU LEAVE - VOICE VARIATIONS\\n\\n\")\n        report.write(\"*Audio transformations for the poetic journey*\\n\\n\")\n        report.write(\"| Variation | Description | Audio File | Video File |\\n\")\n        report.write(\"|-----------|-------------|------------|------------|\\n\")\n        \n        # Process each variation\n        for variation in VARIATIONS:\n            success = create_voice_variation(variation)\n            \n            # Add to report\n            name = variation[\"name\"]\n            description = variation[\"description\"]\n            suffix = variation[\"output_suffix\"]\n            output_name = variation[\"output_name\"]\n            \n            audio_file = f\"Voice_{suffix}.wav\"\n            video_file = f\"WhereYouGoWhenYouLeave_{suffix}.mp4\"\n            \n            report.write(f\"| **{output_name}** | {description} | `{audio_file}` | `{video_file}` |\\n\")\n    \n    print(f\"\\nAll voice variations complete. Guide created at: {report_path}\")\n\nif __name__ == \"__main__\":\n    start_time = time.time()\n    print(\"Starting voice variation creation...\")\n    create_all_variations()\n    end_time = time.time()\n    print(f\"\\nProcess completed in {(end_time - start_time):.2f} seconds.\")\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_VoiceTrack.wav",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "Voice_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "Voice_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "/Users/gaia/resurrecting atlantis/LIZARD",
      "AM/FM radio transmission effect with static and frequency limitations"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "effect_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "duration_cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "video_cmd, capture_output=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "time"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "extract_compiled_poem.py",
    "size": 4231,
    "lines": 121,
    "source": "import os\nimport re\n\n# Configuration\nLIZARD_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD\"\nCONTENT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_PoemContent.md\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_CompiledPoem.md\")\n\ndef extract_poem_lines():\n    \"\"\"Extract content lines from the poem content file and compile them into a new poem.\"\"\"\n    if not os.path.exists(CONTENT_FILE):\n        print(f\"Error: Content file not found at {CONTENT_FILE}\")\n        return False\n        \n    with open(CONTENT_FILE, 'r') as f:\n        content = f.read()\n        \n    # Extract section headers and content lines\n    section_pattern = r'## (\\d+)_([A-Z]{2})_([^\\n]+)'\n    content_pattern = r'\\*\\*Content:\\*\\* ([^\\n]+)'\n    \n    sections = re.findall(section_pattern, content)\n    content_lines = re.findall(content_pattern, content)\n    \n    # Group content by sections\n    current_section = None\n    poem_sections = {}\n    \n    # Extract section headers to get their order\n    section_headers = []\n    for match in re.finditer(section_pattern, content):\n        section_id = match.group(1)\n        section_code = match.group(2)\n        section_title = match.group(3)\n        section_headers.append((section_id, section_code, section_title))\n    \n    # Sort section headers by ID\n    section_headers.sort(key=lambda x: x[0])\n    \n    # Process content in sections\n    section_index = 0\n    line_index = 0\n    section_lines = {}\n    \n    # Initialize empty lists for each section\n    for section_id, section_code, _ in section_headers:\n        section_lines[section_code] = []\n    \n    # Group content lines by section\n    current_section = None\n    for match in re.finditer(section_pattern, content):\n        section_code = match.group(2)\n        if current_section != section_code:\n            current_section = section_code\n            \n        # Find all content lines in this section\n        section_start = match.end()\n        section_end = len(content)\n        \n        # Find next section start, if any\n        next_match = re.search(section_pattern, content[section_start:])\n        if next_match:\n            section_end = section_start + next_match.start()\n            \n        # Extract content lines from this section\n        section_content = content[section_start:section_end]\n        for content_match in re.finditer(content_pattern, section_content):\n            line = content_match.group(1).strip()\n            section_lines[current_section].append(line)\n    \n    # Create the compiled poem\n    with open(OUTPUT_FILE, 'w') as outfile:\n        outfile.write(\"# WHERE YOU GO WHEN YOU LEAVE\\n\\n\")\n        outfile.write(\"*A compiled poem from the video sequence*\\n\\n\")\n        \n        # Add each section\n        for section_id, section_code, section_title in section_headers:\n            lines = section_lines.get(section_code, [])\n            if not lines:\n                continue\n                \n            outfile.write(f\"## {section_title}\\n\\n\")\n            \n            # Remove duplicates while preserving order\n            unique_lines = []\n            for line in lines:\n                if line not in unique_lines:\n                    unique_lines.append(line)\n            \n            # Write the unique lines as a stanza\n            for line in unique_lines:\n                outfile.write(f\"{line}\\n\")\n            \n            outfile.write(\"\\n\")\n        \n        # Add a combined version of the entire poem\n        outfile.write(\"---\\n\\n\")\n        outfile.write(\"## Complete Poem\\n\\n\")\n        \n        all_lines = []\n        for section_id, section_code, _ in section_headers:\n            lines = section_lines.get(section_code, [])\n            \n            # Remove duplicates while preserving order\n            unique_lines = []\n            for line in lines:\n                if line not in unique_lines:\n                    unique_lines.append(line)\n                    \n            all_lines.extend(unique_lines)\n        \n        # Write the full poem with line breaks\n        for line in all_lines:\n            outfile.write(f\"{line}\\n\")\n            \n    print(f\"Compiled poem created at: {OUTPUT_FILE}\")\n    return True\n\nif __name__ == \"__main__\":\n    extract_poem_lines()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/LIZARD"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "generate_olog_model.py",
    "size": 10723,
    "lines": 260,
    "source": "import ast\nimport os\nimport hashlib\nfrom pathlib import Path\nimport re\n\nclass OlogGenerator(ast.NodeVisitor):\n    def __init__(self, script_path):\n        self.script_path = script_path\n        self.script_name = os.path.basename(script_path)\n        self.phenotype = \"Unknown\"\n        self.model = [f\"\u25fb {self.script_name}\"]\n        self.variables = {}\n        self.temp_dir_vars = set()\n\n    def add_morphism(self, description, level=1):\n        self.model.append(f\"{'    ' * level}\u2192 {description}\")\n\n    def get_value(self, node):\n        if isinstance(node, ast.Constant):\n            return node.value\n        if isinstance(node, ast.Name):\n            return self.variables.get(node.id)\n        if isinstance(node, ast.Attribute):\n            # Simplistic handling for attributes like Path.cwd()\n            val = self.get_value(node.value)\n            if val:\n                return f\"{val}.{node.attr}\"\n        return f\"`{ast.unparse(node)}`\" # fallback\n\n    def visit_Assign(self, node):\n        # Track variable assignments\n        if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):\n            target_name = node.targets[0].id\n            self.variables[target_name] = self.get_value(node.value)\n\n            # Detect temporary directory creation\n            if isinstance(node.value, ast.Call) and 'TemporaryDirectory' in ast.unparse(node.value.func):\n                self.temp_dir_vars.add(target_name)\n\n        self.generic_visit(node)\n\n    def visit_Call(self, node):\n        func_name = ast.unparse(node.func)\n\n        # Handle subprocess.run with ffmpeg\n        if 'subprocess.run' in func_name:\n            self.handle_ffmpeg_call(node)\n\n        # Handle moviepy\n        elif 'write_videofile' in func_name:\n            output_file = self.get_value(node.args[0])\n            self.add_morphism(f\"generates \u2192 \u25fb Video File (`{output_file}`)\")\n            self.add_morphism(f\"utilizes \u2192 \u25fb moviepy\", level=2)\n\n        # Handle glob\n        elif 'glob' in func_name:\n            pattern = self.get_value(node.args[0])\n            self.add_morphism(f\"reads \u2192 \u25fb File Sequence (matching `{pattern}`)\")\n\n        # Handle JSON\n        elif 'json.dump' in func_name:\n            output_file_obj = self.get_value(node.args[1])\n            self.add_morphism(f\"writes \u2192 \u25fb JSON Data (to `{output_file_obj}`)\")\n        elif 'json.load' in func_name:\n            input_file_obj = self.get_value(node.args[0])\n            self.add_morphism(f\"reads \u2192 \u25fb JSON Data (from `{input_file_obj}`)\")\n\n        self.generic_visit(node)\n\n    def visit_For(self, node):\n        iterable = self.get_value(node.iter)\n        self.add_morphism(f\"for each item in `{iterable}`:\")\n        # Process the loop body with increased indentation\n        for sub_node in node.body:\n            if isinstance(sub_node, ast.Expr) and isinstance(sub_node.value, ast.Call):\n                # A bit of a hack to represent the action inside the loop\n                call_str = ast.unparse(sub_node.value.func)\n                self.add_morphism(f\"performs action \u2192 `{call_str}`\", level=2)\n            else:\n                self.generic_visit(sub_node) # Visit other nodes in the loop\n\n    def visit_With(self, node):\n        for item in node.items:\n            context_expr = self.get_value(item.context_expr)\n            if isinstance(item.optional_vars, ast.Name) and item.optional_vars.id in self.temp_dir_vars:\n                self.add_morphism(f\"manages \u2192 \u25fb Temporary Directory (`{context_expr}`)\")\n                self.add_morphism(f\"creation and cleanup\", level=2)\n        self.generic_visit(node)\n\n    def handle_ffmpeg_call(self, node):\n        try:\n            cmd_list = []\n            cmd_str = \"\"\n            # Assuming the command is a list of strings in the first arg\n            cmd_list_node = node.args[0]\n            if isinstance(cmd_list_node, (ast.List, ast.Tuple)):\n                cmd_list = [self.get_value(e) for e in cmd_list_node.elts]\n                cmd_str = ' '.join(map(str, filter(None, cmd_list)))\n            else:\n                # Fallback for f-strings or other complex command constructions\n                cmd_str = self.get_value(cmd_list_node)\n                # Create a best-effort cmd_list from the string representation\n                cmd_list = cmd_str.replace('`', '').replace('{', ' ').replace('}', ' ').split()\n\n            self.add_morphism(f\"executes \u2192 \u25fb ffmpeg command\")\n\n            if '-i' in cmd_list:\n                try:\n                    i_index = cmd_list.index('-i') + 1\n                    input_file = cmd_list[i_index]\n                    self.add_morphism(f\"reads \u2192 \u25fb Input (`{input_file}`)\", level=2)\n                except (ValueError, IndexError):\n                    pass # Ignore if -i is present but the arg is not found\n\n            if '-f concat' in cmd_str:\n                self.add_morphism(f\"performs \u2192 \u25fb Lossless Concatenation\", level=2)\n\n            if '-c copy' in cmd_str:\n                self.add_morphism(f\"using codec \u2192 `copy` (no re-encoding)\", level=2)\n\n            if '-an' in cmd_str:\n                self.add_morphism(f\"removes \u2192 \u25fb Audio Stream (-an)\", level=2)\n\n            if cmd_list:\n                output_file = cmd_list[-1]\n                self.add_morphism(f\"generates \u2192 \u25fb Output File (`{output_file}`)\", level=2)\n\n        except (IndexError, AttributeError, UnboundLocalError):\n            self.add_morphism(\"executes complex ffmpeg command\")\n\n    def classify_phenotype(self):\n        morphisms_str = ' '.join(self.model)\n\n        # Genetic markers\n        has_ffmpeg = 'ffmpeg' in morphisms_str\n        has_ffprobe = 'ffprobe' in morphisms_str\n        has_moviepy = 'moviepy' in morphisms_str or 'ImageSequenceClip' in morphisms_str\n        has_json = 'json.load' in morphisms_str or 'json.dump' in morphisms_str\n        has_concat = 'Lossless Concatenation' in morphisms_str\n\n        if has_moviepy:\n            self.phenotype = \"Generative Assembler\"\n            return\n\n        if has_ffmpeg and has_ffprobe:\n            self.phenotype = \"Conditional Processor & Assembler\"\n            return\n\n        if has_ffmpeg and has_concat:\n            self.phenotype = \"Sequential Assembler\"\n            return\n\n        if has_json and not has_ffmpeg and not has_moviepy:\n            self.phenotype = \"Metadata Manager & Analyst\"\n            return\n\n        # Fallback for simple ffmpeg calls that aren't concatenations (e.g. single-file processing)\n        if has_ffmpeg:\n            self.phenotype = \"Conditional Processor & Assembler\" # Broader category for single-file manipulations\n            return\n\n        # Fallback for scripts that just do analysis or file management\n        if 'analyze' in self.script_name or 'index' in self.script_name or 'olog' in self.script_name:\n            self.phenotype = \"Utility & Analysis\"\n            return\n\n        self.phenotype = \"Composite\"\n\n    def analyze(self):\n        with open(self.script_path, 'r', encoding='utf-8', errors='ignore') as f:\n            source = f.read()\n        try:\n            tree = ast.parse(source)\n            self.visit(tree)\n        except Exception as e:\n            self.model.append(f\"    \u2192 ERROR: Could not analyze script: {e}\")\n        self.classify_phenotype()\n        return self.model\n\ndef get_script_hash(script_path):\n    with open(script_path, 'r', encoding='utf-8', errors='ignore') as f:\n        return hashlib.sha256(f.read().encode()).hexdigest()\n\ndef find_assembly_scripts(root_dir):\n    root_path = Path(root_dir)\n    # Find all python files with 'assembl' in the name\n    return [str(p) for p in root_path.rglob('*assembl*.py')]\n\ndef generate_olog_report(generators, output_file):\n    toc = []\n    report_parts = []\n\n    # Sort generators by script path for consistent report order\n    sorted_generators = sorted(generators, key=lambda g: g.script_path)\n\n    for gen in sorted_generators:\n        anchor = gen.script_path.replace('/', '').replace('.', '')\n        toc.append(f'- [{gen.script_path} (*{gen.phenotype}*)](#{anchor})')\n        \n        report_parts.append(f'<a name=\"{anchor}\"></a>')\n        report_parts.append(f'### {gen.script_path}')\n        report_parts.append(f'**Phenotype: {gen.phenotype}**\\n')\n        report_parts.extend(gen.model)\n        report_parts.append('\\n---')\n\n    with open(output_file, 'w', encoding='utf-8') as f:\n        f.write(\"# Comprehensive Ological Index of Assembly Scripts\\n\\n\")\n        f.write(\"This document provides a deep, semantic model of each Python assembly script, detailing its internal logic, dependencies, and data transformations. This file was auto-generated.\\n\\n\")\n        f.write(\"## Table of Contents\\n\\n\")\n        f.write(\"\\n\".join(toc))\n        f.write(\"\\n\\n---\\n\\n\")\n        f.write(\"## Ological Index of Scripts\\n\\n\")\n        f.write(\"\\n\".join(report_parts))\n\ndef main():\n    project_root = '/Users/gaia/resurrecting atlantis'\n    print(f\"Starting deep ological analysis of assembly scripts in: {project_root}\")\n\n    assembly_scripts = find_assembly_scripts(project_root)\n    print(f\"Found {len(assembly_scripts)} assembly scripts to model.\")\n\n    unique_scripts = {}\n    for script_path in assembly_scripts:\n        script_name = os.path.basename(script_path)\n        script_hash = get_script_hash(script_path)\n        key = (script_name, script_hash)\n        if key not in unique_scripts:\n            unique_scripts[key] = script_path\n\n    print(f\"Found {len(unique_scripts)} unique scripts to model.\")\n\n    all_generators = []\n    processed_paths = list(unique_scripts.values())\n\n    for script_path in sorted(processed_paths):\n        print(f\"  - Modeling {os.path.basename(script_path)}...\")\n        # We wrap the analysis of each script in a try/except to be robust\n        try:\n            generator = OlogGenerator(script_path)\n            generator.analyze() # This runs the analysis and classification\n            all_generators.append(generator)\n        except Exception as e:\n            # This will catch errors in file reading, parsing, or visiting\n            print(f\"    ERROR: Could not process {script_path}: {e}\")\n            # Optionally create a dummy generator to report the error\n            error_gen = OlogGenerator(script_path)\n            error_gen.model.append(f\"    \u2192 FATAL ERROR during analysis: {e}\")\n            error_gen.phenotype = \"Error\"\n            all_generators.append(error_gen)\n\n    output_md_file = os.path.join(project_root, 'assembly_scripts_olog.md')\n    print(\"\\nGenerating comprehensive olog model report...\")\n    generate_olog_report(all_generators, output_md_file)\n\n    print(f\"\\nSuccessfully generated olog model at: {output_md_file}\")\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      "></a>",
      "/Users/gaia/resurrecting atlantis",
      ")\n        # We wrap the analysis of each script in a try/except to be robust\n        try:\n            generator = OlogGenerator(script_path)\n            generator.analyze() # This runs the analysis and classification\n            all_generators.append(generator)\n        except Exception as e:\n            # This will catch errors in file reading, parsing, or visiting\n            print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "ast",
      "os",
      "hashlib",
      "pathlib",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "assemble_boy_video.py",
    "size": 4492,
    "lines": 108,
    "source": "import os\nimport subprocess\nimport traceback\n\ndef assemble_video_with_ffmpeg(video_dir, audio_file, output_file, final_duration):\n    \"\"\"\n    Assembles video clips using ffmpeg, adds an audio track, and trims to a specific duration.\n\n    Args:\n        video_dir (str): The directory containing the video clips.\n        audio_file (str): The path to the audio file.\n        output_file (str): The path to save the final video.\n        final_duration (int): The duration of the final video in seconds.\n    \"\"\"\n    print(\"Starting video assembly with ffmpeg...\")\n    file_list_path = os.path.join(video_dir, \"file_list.txt\")\n    concatenated_video_path = os.path.join(video_dir, \"concatenated_video.mp4\")\n\n    try:\n        # 1. Get a sorted list of video files\n        print(f\"Searching for video files in: {video_dir}\")\n        video_files = sorted([\n            os.path.join(video_dir, f) \n            for f in os.listdir(video_dir) \n            if f.endswith(('.mp4', '.mov', '.avi', '.mkv'))\n        ])\n        \n        if not video_files:\n            print(f\"No video files found in {video_dir}\")\n            return\n        print(f\"Found {len(video_files)} video files.\")\n\n        # 2. Create a temporary file list for ffmpeg's concat demuxer\n        print(f\"Creating temporary file list at: {file_list_path}\")\n        with open(file_list_path, 'w') as f:\n            for file_path in video_files:\n                # For ffmpeg, a single quote ' in a filename needs to be escaped as '\\''\n                # (a single quote, followed by a backslash, followed by another single quote).\n                replacement_string_for_single_quote = \"'\\\\''\" # This assigns the string: ' \\\\ '\n                sanitized_file_path = file_path.replace(\"'\", replacement_string_for_single_quote)\n                f.write(f\"file '{sanitized_file_path}'\\n\")\n        print(\"Temporary file list created successfully.\")\n\n        # 3. Concatenate videos using ffmpeg\n        print(\"Concatenating video clips with ffmpeg...\")\n        concat_command = [\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', file_list_path,\n            '-c', 'copy',\n            '-y',\n            concatenated_video_path\n        ]\n        subprocess.run(concat_command, check=True, capture_output=True, text=True)\n        print(\"Video clips concatenated successfully.\")\n\n        # 4. Add audio and trim the video\n        print(f\"Adding audio from {audio_file} and trimming to {final_duration} seconds...\")\n        final_command = [\n            'ffmpeg',\n            '-i', concatenated_video_path,\n            '-i', audio_file,\n            '-c:v', 'libx264',\n            '-c:a', 'aac',\n            '-b:a', '192k',\n            '-t', str(final_duration),\n            '-map', '0:v:0',\n            '-map', '1:a:0',\n            '-shortest',\n            '-y',\n            output_file\n        ]\n        result = subprocess.run(final_command, check=True, capture_output=True, text=True)\n        print(\"Final video created successfully.\")\n        if result.stderr:\n            print(\"ffmpeg log:\\n\", result.stderr)\n\n    except subprocess.CalledProcessError as e:\n        print(\"An error occurred while running ffmpeg.\")\n        print(f\"Command: {' '.join(e.cmd)}\")\n        print(f\"Return code: {e.returncode}\")\n        print(f\"Output (stdout):\\n{e.stdout}\")\n        print(f\"Output (stderr):\\n{e.stderr}\")\n        traceback.print_exc()\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        traceback.print_exc()\n    finally:\n        # 5. Clean up the temporary files\n        print(\"Cleaning up temporary files...\")\n        if os.path.exists(file_list_path):\n            os.remove(file_list_path)\n        if os.path.exists(concatenated_video_path):\n             os.remove(concatenated_video_path)\n        print(\"Cleanup complete.\")\n\nif __name__ == \"__main__\":\n    video_directory = \"/Users/gaia/resurrecting atlantis/BOY/WN_LEAVE/\"\n    audio_path = \"/Users/gaia/resurrecting atlantis/BOY/WN_LEAVE/\u652c\u4f6cSKAI ISYOUGOD\u516b\u65b9\u4f86\u8ca1\u56e0\u679cHD \u9ad8\u6e05\u5b98\u65b9\u5b8c\u6574\u7248 MV_no_voice_split_by_lalalai.mp3\"\n    output_path = \"/Users/gaia/resurrecting atlantis/BOY/assembled_video.mp4\"\n    duration = 180  # Default duration in seconds\n\n    assemble_video_with_ffmpeg(video_directory, audio_path, output_path, duration)\n    output_path = \"/Users/gaia/resurrecting atlantis/boy_video.mp4\"\n    duration = 351\n    assemble_video_with_ffmpeg(video_directory, audio_path, output_path, duration)\n",
    "file_references": [
      "concatenated_video.mp4",
      "/Users/gaia/resurrecting atlantis/BOY/WN_LEAVE/\u652c\u4f6cSKAI ISYOUGOD\u516b\u65b9\u4f86\u8ca1\u56e0\u679cHD \u9ad8\u6e05\u5b98\u65b9\u5b8c\u6574\u7248 MV_no_voice_split_by_lalalai.mp3",
      "/Users/gaia/resurrecting atlantis/BOY/assembled_video.mp4",
      "/Users/gaia/resurrecting atlantis/boy_video.mp4",
      "/Users/gaia/resurrecting atlantis/BOY/WN_LEAVE/",
      "/Users/gaia/resurrecting atlantis/BOY/WN_LEAVE/\u652c\u4f6cSKAI ISYOUGOD\u516b\u65b9\u4f86\u8ca1\u56e0\u679cHD \u9ad8\u6e05\u5b98\u65b9\u5b8c\u6574\u7248 MV_no_voice_split_by_lalalai.mp3",
      "/Users/gaia/resurrecting atlantis/BOY/assembled_video.mp4",
      "/Users/gaia/resurrecting atlantis/boy_video.mp4"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "concat_command, check=True, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "final_command, check=True, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "traceback"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "generate_assembly_index.py",
    "size": 3192,
    "lines": 84,
    "source": "import os\nfrom pathlib import Path\n\n# --- Configuration ---\n# The root directory to search for assembly scripts.\nROOT_DIR = Path('/Users/gaia/resurrecting atlantis/')\n\n# The glob pattern to identify assembly scripts.\n# It looks for any python file with 'assemble' in the name.\nSCRIPT_PATTERN = '*assembl*.py'\n\n# The name of the output markdown file.\nOUTPUT_FILENAME = 'assembly_scripts_index.md'\n\n# The full path for the output file.\nOUTPUT_FILE_PATH = ROOT_DIR / OUTPUT_FILENAME\n\n# The name of this script, to exclude it from the index.\nSELF_SCRIPT_NAME = 'generate_assembly_index.py'\n\ndef find_assembly_scripts():\n    \"\"\"Finds all assembly scripts in the root directory, excluding this script itself.\"\"\"\n    print(f\"Searching for scripts matching '{SCRIPT_PATTERN}' in '{ROOT_DIR}'...\")\n    all_scripts = list(ROOT_DIR.rglob(SCRIPT_PATTERN))\n    \n    # Filter out this script itself\n    filtered_scripts = [\n        script for script in all_scripts \n        if script.name != SELF_SCRIPT_NAME\n    ]\n    \n    print(f\"Found {len(filtered_scripts)} assembly scripts.\")\n    return sorted(filtered_scripts)\n\ndef generate_index_file(script_paths):\n    \"\"\"Generates the markdown index file with the content of all found scripts.\"\"\"\n    print(f\"Generating index file at: {OUTPUT_FILE_PATH}\")\n    \n    try:\n        with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:\n            f.write(\"# Comprehensive Index of Python Assembly Scripts\\n\\n\")\n            f.write(\"This document provides a consolidated index of all Python assembly scripts found across the project directories. This file was auto-generated.\\n\\n\")\n            \n            # --- Write Table of Contents ---\n            f.write(\"## Table of Contents\\n\")\n            for i, script_path in enumerate(script_paths):\n                # Create a simple, valid anchor link\n                anchor_name = str(script_path).replace('/', '').replace('.', '').replace(' ', '-')\n                f.write(f\"{i+1}. [`{script_path}`](#{anchor_name})\\n\")\n            \n            f.write(\"\\n---\\n\\n\")\n            \n            # --- Write Script Contents ---\n            f.write(\"## Script Source Code\\n\\n\")\n            for script_path in script_paths:\n                anchor_name = str(script_path).replace('/', '').replace('.', '').replace(' ', '-')\n                \n                f.write(f\"### <a name=\\\"{anchor_name}\\\"></a>`{script_path}`\\n\\n\")\n                f.write(\"```python\\n\")\n                try:\n                    content = script_path.read_text(encoding='utf-8')\n                    f.write(content)\n                except Exception as e:\n                    f.write(f\"# Error reading file: {e}\")\n                f.write(\"\\n```\\n\\n\")\n        \n        print(\"\\nSuccessfully generated the assembly scripts index file!\")\n        print(f\"You can find it at: {OUTPUT_FILE_PATH}\")\n\n    except IOError as e:\n        print(f\"Error writing to file: {e}\")\n\ndef main():\n    \"\"\"Main function to run the script.\"\"\"\n    scripts = find_assembly_scripts()\n    if not scripts:\n        print(\"No assembly scripts found. The index file will not be created.\")\n        return\n    \n    generate_index_file(scripts)\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/",
      "\n\n# The full path for the output file.\nOUTPUT_FILE_PATH = ROOT_DIR / OUTPUT_FILENAME\n\n# The name of this script, to exclude it from the index.\nSELF_SCRIPT_NAME = ",
      "></a>`{script_path}`\\n\\n"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "check_syntagmas.py",
    "size": 2731,
    "lines": 76,
    "source": "import json\nimport re\n\ndef find_non_canonical_syntagmas(json_file_path, canonical_syntagmas_list):\n    \"\"\"\n    Identifies syntagmaType values in a JSON timeline file that are not\n    present in a canonical list of syntagma types.\n\n    Args:\n        json_file_path (str): Path to the timeline JSON file.\n        canonical_syntagmas_list (list): A list of canonical syntagma type strings.\n\n    Returns:\n        set: A set of unique syntagmaType base names found in the JSON\n             that are not in the canonical list.\n    \"\"\"\n    non_canonical_found = set()\n    unique_syntagmas_in_json = set()\n\n    try:\n        with open(json_file_path, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: File not found at {json_file_path}\")\n        return non_canonical_found\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {json_file_path}\")\n        return non_canonical_found\n\n    for entry in timeline_data:\n        if \"syntagmaType\" in entry:\n            stype = entry[\"syntagmaType\"]\n            unique_syntagmas_in_json.add(stype)\n            \n            # Normalize the syntagmaType to get its base name\n            # This removes common patterns like \" (XX)\" or \" (X)\"\n            base_name = re.sub(r'\\s*\\([A-Z]{1,2}\\)$', '', stype).strip()\n            \n            if base_name not in canonical_syntagmas_list:\n                non_canonical_found.add(base_name)\n                \n    print(f\"\\nUnique syntagmaType values found in '{json_file_path}':\")\n    for ust in sorted(list(unique_syntagmas_in_json)):\n        print(f\"- {ust}\")\n        \n    return non_canonical_found\n\nif __name__ == \"__main__\":\n    # Define the canonical syntagma types from glyphs.md\n    # (Ensure these exactly match the names in glyphs.md, without glyphs or comments)\n    canonical_syntagmas = [\n        \"Autonomous Syntagma\",\n        \"Chronological Syntagma\",\n        \"Crystal Syntagma\",\n        \"Descriptive Syntagma\",\n        \"Flashback Syntagma\",\n        \"Thematic Montage\"\n    ]\n\n    # Path to your JSON file\n    timeline_file = \"/Users/gaia/resurrecting atlantis/ELEPHANT/BL-TIMELINE-ADDENDUM.json\"\n\n    print(\"Canonical Syntagma Types (from glyphs.md):\")\n    for cs in canonical_syntagmas:\n        print(f\"- {cs}\")\n\n    off_syntagmas = find_non_canonical_syntagmas(timeline_file, canonical_syntagmas)\n\n    if off_syntagmas:\n        print(\"\\nThe following syntagmaType base names from the JSON are NOT in the canonical list:\")\n        for syntagma in sorted(list(off_syntagmas)):\n            print(f\"- {syntagma}\")\n    else:\n        print(\"\\nAll syntagmaType base names in the JSON appear to be in the canonical list (after normalization).\")\n\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/ELEPHANT/BL-TIMELINE-ADDENDUM.json",
      "/Users/gaia/resurrecting atlantis/ELEPHANT/BL-TIMELINE-ADDENDUM.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "re"
    ],
    "generates": [],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "create_immersive_soundscape.py",
    "size": 11051,
    "lines": 303,
    "source": "import os\nimport subprocess\nimport glob\nimport re\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nORDERED_DIR = os.path.join(LIZARD_DIR, \"ORDERED\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_SOUND\")\nINPUT_VOICE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VoiceTrack.wav\")\nFINAL_OUTPUT = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ImmersiveSoundscape.mp4\")\nFINAL_AUDIO = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ImmersiveSoundscape.wav\")\nVIDEO_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n\n# Define soundscape variations\nVARIATIONS = [\n    {\n        \"name\": \"CathedralResonance\",\n        \"description\": \"Massive cathedral-like reverb with tuned resonance and subtle harmonic chimes\",\n        \"voice_filters\": \"areverse,aecho=0.8:0.88:60|30:0.4|0.3,areverse,aecho=0.8:0.9:1000|1800:0.3|0.25,equalizer=f=200:width_type=h:width=100:g=3,equalizer=f=1000:width_type=h:width=300:g=2,aphaser=in_gain=0.6:out_gain=0.6:delay=3:decay=0.6,volume=1.4\",\n        \"bg_music_volume\": 0.2,\n        \"original_audio_volume\": 0.6,\n        \"output_suffix\": \"CathedralResonance\"\n    },\n    {\n        \"name\": \"SpatialHarmonics\",\n        \"description\": \"Spatial harmonics with tuned resonators and subtle ambient drones\",\n        \"voice_filters\": \"aecho=0.8:0.7:300|400|500|600:0.4|0.3|0.2|0.1,chorus=0.7:0.9:60|90|40|20:0.5|0.4|0.3|0.2:0.4|0.3|0.5|0.2:2|3|1.5|2.5,equalizer=f=300:width_type=h:width=200:g=3,equalizer=f=3000:width_type=h:width=500:g=2,volume=1.3\",\n        \"bg_music_volume\": 0.3,\n        \"original_audio_volume\": 0.5,\n        \"output_suffix\": \"SpatialHarmonics\"\n    },\n    {\n        \"name\": \"ResonantChamber\",\n        \"description\": \"Deep resonant chamber with harmonically tuned reflections and subtle chimes\",\n        \"voice_filters\": \"bass=g=4:f=80:w=0.5,treble=g=2:f=8000:w=0.5,aecho=0.8:0.7:100|200|300:0.5|0.3|0.2,areverse,aecho=0.7:0.7:50|150:0.3|0.2,areverse,aphaser=in_gain=0.7:out_gain=0.7:delay=3:decay=0.5,volume=1.5\",\n        \"bg_music_volume\": 0.25,\n        \"original_audio_volume\": 0.6,\n        \"output_suffix\": \"ResonantChamber\"\n    }\n]\n\ndef ensure_temp_dir():\n    \"\"\"Ensure temporary directory exists.\"\"\"\n    if not os.path.exists(TEMP_DIR):\n        os.makedirs(TEMP_DIR)\n        print(f\"Created temporary directory: {TEMP_DIR}\")\n\ndef extract_audio_from_videos():\n    \"\"\"Extract audio from each video in the ORDERED directory.\"\"\"\n    videos = sorted(glob.glob(os.path.join(ORDERED_DIR, \"*.mp4\")))\n    combined_audio = os.path.join(TEMP_DIR, \"original_combined.wav\")\n    \n    # Extract audio from each video\n    extract_files = []\n    for i, video in enumerate(videos):\n        video_name = os.path.basename(video)\n        output_audio = os.path.join(TEMP_DIR, f\"{i+1:03d}_original.wav\")\n        \n        extract_cmd = [\n            'ffmpeg',\n            '-i', video,\n            '-vn',\n            '-acodec', 'pcm_s16le',\n            '-y',\n            output_audio\n        ]\n        \n        print(f\"Extracting audio from {video_name}...\")\n        subprocess.run(extract_cmd, capture_output=True)\n        \n        if os.path.exists(output_audio):\n            extract_files.append(f\"file '{output_audio}'\")\n    \n    # Create a list file for concatenation\n    list_file = os.path.join(TEMP_DIR, \"audio_list.txt\")\n    with open(list_file, 'w') as f:\n        f.write(\"\\n\".join(extract_files))\n    \n    # Concatenate all extracted audio\n    concat_cmd = [\n        'ffmpeg',\n        '-f', 'concat',\n        '-safe', '0',\n        '-i', list_file,\n        '-c', 'copy',\n        '-y',\n        combined_audio\n    ]\n    \n    print(\"Combining original audio...\")\n    subprocess.run(concat_cmd, capture_output=True)\n    \n    return combined_audio if os.path.exists(combined_audio) else None\n\ndef generate_ambient_elements():\n    \"\"\"Generate ambient elements like chimes and drones.\"\"\"\n    # Generate a soft drone with slowly evolving harmonics\n    drone_file = os.path.join(TEMP_DIR, \"ambient_drone.wav\")\n    drone_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.1:c=brown:d=100',\n        '-af', 'highpass=f=100,lowpass=f=1000,treble=g=1:f=500:w=0.5,aecho=0.6:0.3:1000|2000|3000:0.2|0.1|0.05,volume=0.15',\n        '-y',\n        drone_file\n    ]\n    \n    print(\"Generating ambient drone...\")\n    subprocess.run(drone_cmd, capture_output=True)\n    \n    # Generate subtle chimes\n    chimes_file = os.path.join(TEMP_DIR, \"ambient_chimes.wav\")\n    chimes_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.02:c=blue:d=100',\n        '-af', 'bandpass=f=2000:width_type=h:width=800,treble=g=5:f=3000:w=0.5,aphaser,volume=0.1,silenceremove=1:0:-40dB:1:1:-40dB',\n        '-y',\n        chimes_file\n    ]\n    \n    print(\"Generating ambient chimes...\")\n    subprocess.run(chimes_cmd, capture_output=True)\n    \n    return {\n        'drone': drone_file if os.path.exists(drone_file) else None,\n        'chimes': chimes_file if os.path.exists(chimes_file) else None\n    }\n\ndef create_immersive_variation(variation, original_audio, ambient_elements):\n    \"\"\"Create an immersive soundscape variation.\"\"\"\n    name = variation[\"name\"]\n    description = variation[\"description\"]\n    voice_filters = variation[\"voice_filters\"]\n    bg_music_volume = variation[\"bg_music_volume\"]\n    original_audio_volume = variation[\"original_audio_volume\"]\n    suffix = variation[\"output_suffix\"]\n    \n    # Output files\n    processed_voice = os.path.join(TEMP_DIR, f\"voice_{suffix}.wav\")\n    final_mix = os.path.join(TEMP_DIR, f\"mix_{suffix}.wav\")\n    output_video = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{suffix}.mp4\")\n    \n    print(f\"\\nCreating {name} immersive soundscape: {description}\")\n    \n    # Process voice with reverb and resonance\n    voice_cmd = [\n        'ffmpeg',\n        '-i', INPUT_VOICE,\n        '-af', voice_filters,\n        '-y',\n        processed_voice\n    ]\n    \n    print(f\"Processing voice with {name} effects...\")\n    subprocess.run(voice_cmd, capture_output=True)\n    \n    if not os.path.exists(processed_voice):\n        print(f\"Error creating processed voice for {name}\")\n        return False\n    \n    # Create the complex filter graph for mixing all audio elements\n    filter_graph = []\n    \n    # Input pads\n    filter_graph.append(f\"[0:a]volume={original_audio_volume}[original]\")\n    filter_graph.append(f\"[1:a]volume=1.0[voice]\")\n    \n    inputs = \"[original][voice]\"\n    \n    if ambient_elements['drone'] and os.path.exists(ambient_elements['drone']):\n        filter_graph.append(f\"[2:a]volume=0.15,apad[drone]\")\n        inputs += \"[drone]\"\n    \n    if ambient_elements['chimes'] and os.path.exists(ambient_elements['chimes']):\n        filter_graph.append(f\"[3:a]volume=0.1,apad[chimes]\")\n        inputs += \"[chimes]\"\n    \n    # Final mix\n    filter_graph.append(f\"{inputs}amix=inputs={len(inputs.split('])')[:-1])}:duration=longest[aout]\")\n    \n    # Input files\n    input_files = [\n        '-i', original_audio,\n        '-i', processed_voice\n    ]\n    \n    if ambient_elements['drone'] and os.path.exists(ambient_elements['drone']):\n        input_files.extend(['-i', ambient_elements['drone']])\n    \n    if ambient_elements['chimes'] and os.path.exists(ambient_elements['chimes']):\n        input_files.extend(['-i', ambient_elements['chimes']])\n    \n    # Create the final mix\n    mix_cmd = [\n        'ffmpeg',\n        *input_files,\n        '-filter_complex', \";\".join(filter_graph),\n        '-map', '[aout]',\n        '-y',\n        final_mix\n    ]\n    \n    print(f\"Creating final audio mix for {name}...\")\n    subprocess.run(mix_cmd, capture_output=True)\n    \n    if not os.path.exists(final_mix):\n        print(f\"Error creating final mix for {name}\")\n        return False\n    \n    # Combine with video\n    video_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-i', final_mix,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'copy',\n        '-c:a', 'aac',\n        '-shortest',\n        '-metadata', f'title=Where You Go When You Leave - {name}',\n        '-metadata', f'comment={description}',\n        '-y',\n        output_video\n    ]\n    \n    print(f\"Creating {name} video with immersive audio...\")\n    subprocess.run(video_cmd, capture_output=True)\n    \n    if os.path.exists(output_video):\n        print(f\"\u2713 {name} immersive soundscape complete: {output_video}\")\n        return True\n    \n    print(f\"Error creating {name} video\")\n    return False\n\ndef create_all_variations():\n    \"\"\"Create all immersive soundscape variations.\"\"\"\n    ensure_temp_dir()\n    \n    # Extract original audio\n    original_audio = extract_audio_from_videos()\n    if not original_audio:\n        print(\"Error extracting original audio from videos\")\n        return False\n    \n    # Generate ambient elements\n    ambient_elements = generate_ambient_elements()\n    \n    # Create a markdown report\n    report_path = os.path.join(LIZARD_DIR, \"Immersive_Soundscape_Guide.md\")\n    \n    with open(report_path, 'w') as report:\n        report.write(\"# WHERE YOU GO WHEN YOU LEAVE - IMMERSIVE SOUNDSCAPES\\n\\n\")\n        report.write(\"*Resonant blended soundscapes for the poetic journey*\\n\\n\")\n        report.write(\"## Description\\n\\n\")\n        report.write(\"These immersive soundscapes blend the voice with original audio from the videos, \")\n        report.write(\"creating deeply resonant environments with tuned reverberation, spatial harmonics, \")\n        report.write(\"and subtle ambient elements. Each variation explores different acoustic spaces \")\n        report.write(\"that recontextualize the poetry within rich, evocative sonic landscapes.\\n\\n\")\n        report.write(\"## Immersive Variations\\n\\n\")\n        report.write(\"| Variation | Description | Video File |\\n\")\n        report.write(\"|-----------|-------------|------------|\\n\")\n        \n        # Process each variation\n        completed = []\n        for variation in VARIATIONS:\n            try:\n                success = create_immersive_variation(variation, original_audio, ambient_elements)\n                if success:\n                    completed.append(variation)\n            except Exception as e:\n                print(f\"Error processing {variation['name']}: {str(e)}\")\n                continue\n            \n        # Add successful variations to report\n        for variation in completed:\n            name = variation[\"name\"]\n            description = variation[\"description\"]\n            suffix = variation[\"output_suffix\"]\n            \n            video_file = f\"WhereYouGoWhenYouLeave_{suffix}.mp4\"\n            \n            report.write(f\"| **{name}** | {description} | `{video_file}` |\\n\")\n    \n    print(f\"\\nImmersive soundscapes complete. Guide created at: {report_path}\")\n    return True\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Starting immersive soundscape creation...\")\n    success = create_all_variations()\n    \n    if success:\n        print(\"Immersive soundscape creation complete!\")\n    else:\n        print(\"Error creating immersive soundscapes\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_VoiceTrack.wav",
      "WhereYouGoWhenYouLeave_ImmersiveSoundscape.mp4",
      "WhereYouGoWhenYouLeave_ImmersiveSoundscape.wav",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "*.mp4",
      "original_combined.wav",
      "{i+1:03d}_original.wav",
      "ambient_drone.wav",
      "ambient_chimes.wav",
      "voice_{suffix}.wav",
      "mix_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "extract_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "concat_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "drone_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "chimes_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "voice_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "mix_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "video_cmd, capture_output=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "glob",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "process_videos.py",
    "size": 1857,
    "lines": 56,
    "source": "import os\nimport subprocess\n\ndef process_videos():\n    source_dir = '/Users/gaia/resurrecting atlantis/IMPALA/original_files_backup'\n    dest_dir = '/Users/gaia/resurrecting atlantis/ANT/00-TITLES'\n    \n    # Create the destination directory if it doesn't exist\n    os.makedirs(dest_dir, exist_ok=True)\n    \n    print(f\"Source directory: {source_dir}\")\n    print(f\"Destination directory: {dest_dir}\")\n    \n    # List all files in the source directory\n    try:\n        files = os.listdir(source_dir)\n    except FileNotFoundError:\n        print(f\"Error: Source directory not found at {source_dir}\")\n        return\n\n    # Filter for common video file extensions\n    video_files = [f for f in files if f.lower().endswith(('.mp4', '.mov', '.avi', '.mkv', '.webm'))]\n    \n    if not video_files:\n        print(f\"No video files found in {source_dir}\")\n        return\n\n    print(f\"Found {len(video_files)} video files to process.\")\n\n    for filename in video_files:\n        source_path = os.path.join(source_dir, filename)\n        dest_path = os.path.join(dest_dir, filename)\n        \n        print(f\"Processing {filename}...\")\n        \n        # Construct the ffmpeg command to scale video and copy audio\n        command = [\n            'ffmpeg',\n            '-i', source_path,\n            '-vf', 'scale=1280:720',\n            '-c:a', 'copy',  # Copy audio stream without re-encoding\n            '-y', # Overwrite output file if it exists\n            dest_path\n        ]\n        \n        try:\n            # Execute the command\n            result = subprocess.run(command, check=True, capture_output=True, text=True)\n            print(f\"Successfully processed {filename}\")\n        except subprocess.CalledProcessError as e:\n            print(f\"Error processing {filename}:\")\n            print(f\"Stderr: {e.stderr}\")\n\nif __name__ == \"__main__\":\n    process_videos()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/IMPALA/original_files_backup",
      "/Users/gaia/resurrecting atlantis/ANT/00-TITLES"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "command, check=True, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "emulate_assembly_outputs.py",
    "size": 11464,
    "lines": 227,
    "source": "import os\nimport sys\nimport subprocess\nimport importlib.util\nimport contextlib\nimport runpy\nfrom io import StringIO\nimport builtins\nfrom unittest.mock import patch, MagicMock\nfrom pathlib import Path\nimport hashlib\nimport tempfile\nimport shutil\nimport ast\nfrom collections import Counter\n\n# --- \"Project Jailed\" Dependency Scanner ---\nclass DependencyScanner(ast.NodeVisitor):\n    \"\"\"Scans a script's AST for file paths, but only those inside the project root.\"\"\"\n    def __init__(self, script_dir, project_root):\n        self.script_dir = os.path.abspath(script_dir)\n        self.project_root = os.path.abspath(project_root)\n        self.dependencies = set()\n\n    def visit_Constant(self, node):\n        if isinstance(node.value, str):\n            val = node.value\n            if len(val) < 3 or (not '/' in val and not '\\\\' in val and not '.' in val):\n                return\n            potential_path = os.path.normpath(os.path.join(self.script_dir, val))\n            if os.path.abspath(potential_path).startswith(self.project_root) and os.path.exists(potential_path):\n                self.dependencies.add(os.path.abspath(potential_path))\n        self.generic_visit(node)\n\n# --- Mock Objects & Sandbox Hardening ---\nclass ScriptExitException(Exception):\n    \"\"\"Custom exception to be raised when a script calls sys.exit().\"\"\"\n    pass\n\ndef mock_sys_exit(code=0):\n    \"\"\"Mock for sys.exit() that raises a custom exception instead of exiting.\"\"\"\n    print(f\"[SANDBOX-EXIT] Neutralized sys.exit({code}) call.\")\n    raise ScriptExitException(f\"Script called sys.exit({code})\")\n\nclass MockCompletedProcess:\n    def __init__(self, stdout='', stderr='', returncode=0, args=None):\n        self.stdout, self.stderr, self.returncode, self.args = stdout, stderr, returncode, args or []\n    def check_returncode(self):\n        if self.returncode != 0: raise subprocess.CalledProcessError(self.returncode, self.args, self.stdout, self.stderr)\n\ndef mock_subprocess_run(*args, **kwargs):\n    cmd_str = ' '.join(map(str, args[0]))\n    print(f\"[SANDBOX-SUBPROCESS] Neutralized command: {cmd_str}\")\n    if 'ffprobe' in cmd_str: return MockCompletedProcess(stdout='{\"format\": {\"duration\": \"1.0\"}}', args=args[0])\n    return MockCompletedProcess(args=args[0])\n\nclass MockPopen:\n    \"\"\"A mock for subprocess.Popen that simulates a streamable stdout and a wait() method.\"\"\"\n    def __init__(self, args, **kwargs):\n        cmd_str = ' '.join(map(str, args))\n        print(f\"[SANDBOX-SUBPROCESS-POPEN] Neutralized command: {cmd_str}\")\n        output = f\"Mock streaming output for: {cmd_str}\\n...\\n...\\n...\\nProcess complete.\"\n        self.stdout = StringIO(output)\n        self.returncode = 0\n\n    def wait(self):\n        return self.returncode\n\n# --- Sandboxed Execution Engine ---\nclass SandboxedEmulator:\n    def __init__(self, script_path, project_root):\n        self.script_path = os.path.abspath(script_path)\n        self.script_name = os.path.basename(script_path)\n        self.project_root = Path(project_root).resolve()\n        self.sandbox_dir = None\n        self.original_open = builtins.open\n        self.original_listdir = os.listdir\n\n    def mock_open(self, file, mode='r', *args, **kwargs):\n        # Remap absolute paths that are within the project root to the sandbox\n        if isinstance(file, str) and os.path.isabs(file) and file.startswith(str(self.project_root)):\n            sandboxed_path = os.path.join(self.sandbox_dir, os.path.relpath(file, self.project_root))\n            print(f\"[SANDBOX-IO] Remapping absolute path {os.path.basename(file)} to sandbox.\")\n            os.makedirs(os.path.dirname(sandboxed_path), exist_ok=True)\n            return self.original_open(sandboxed_path, mode, *args, **kwargs)\n        \n        # Block any other absolute path attempts\n        if isinstance(file, str) and os.path.isabs(file):\n            print(f\"[SANDBOX-IO] Blocked attempt to access absolute path outside project: {file}\")\n            raise FileNotFoundError(f\"Access to {file} is denied by the sandbox.\")\n\n        # For relative paths, they are already contained within the sandbox CWD.\n        return self.original_open(file, mode, *args, **kwargs)\n\n    def mock_os_listdir(self, path):\n        \"\"\"Mock for os.listdir to enforce sandbox boundaries.\"\"\"\n        requested_path = Path(path).resolve()\n\n        # Allow listing only within the project root.\n        if self.project_root in requested_path.parents or self.project_root == requested_path:\n            try:\n                print(f\"[SANDBOX-OS.LISTDIR] Permitted listing of project path: {path}\")\n                return self.original_listdir(path)\n            except FileNotFoundError:\n                print(f\"[SANDBOX-OS.LISTDIR] Script tried to list non-existent project path: {path}\")\n                return []\n        \n        print(f\"[SANDBOX-OS.LISTDIR] Blocked access to system path outside project root: {path}\")\n        return []\n\n    def run(self):\n        with tempfile.TemporaryDirectory() as self.sandbox_dir:\n            self._copy_dependencies(self.sandbox_dir)\n            stdout_capture = StringIO()\n            original_cwd = os.getcwd()\n            try:\n                os.chdir(self.sandbox_dir)\n                # This context manager now also patches os.remove to prevent self-deleting evidence.\n                with contextlib.redirect_stdout(stdout_capture), \\\n                     patch('subprocess.run', side_effect=mock_subprocess_run), \\\n                     patch('subprocess.Popen', MockPopen), \\\n                     patch('os.system', side_effect=lambda cmd: print(f'[SANDBOX-OS.SYSTEM] Neutralized: {cmd}')), \\\n                     patch('sys.exit', side_effect=mock_sys_exit), \\\n                     patch('os.remove', side_effect=lambda path: print(f'[SANDBOX-OS.REMOVE] Neutralized deletion of: {os.path.basename(path)}')), \\\n                     patch('os.listdir', self.mock_os_listdir), \\\n                     patch('builtins.open', self.mock_open, create=True):\n                    sys.modules['moviepy.editor'] = MagicMock()\n                    sys.modules['pandas'] = MagicMock()\n                    # Use runpy to execute the script in the sandbox as if it were the main program.\n                    # This is the correct way to ensure the if __name__ == '__main__' block is run.\n                    sandbox_script_path = os.path.join(self.sandbox_dir, self.script_name)\n                    runpy.run_path(sandbox_script_path, run_name='__main__')\n            except ScriptExitException as e:\n                print(f\"\\n[SANDBOX-INFO] {self.script_name}: {e}\\n\")\n            except Exception as e:\n                print(f\"\\n[SANDBOX-ERROR] in {self.script_name}: {e}\\n\")\n            finally:\n                stdout_capture.flush() # Ensure all buffer is written\n                written_files = self._scan_for_text_artifacts(self.sandbox_dir)\n                os.chdir(original_cwd)\n            return {'stdout': stdout_capture.getvalue(), 'written_files': written_files}\n\n    def _copy_dependencies(self, sandbox_dir):\n        try:\n            with open(self.script_path, 'r', encoding='utf-8') as f: source = f.read()\n            tree = ast.parse(source)\n            scanner = DependencyScanner(os.path.dirname(self.script_path), self.project_root)\n            scanner.visit(tree)\n            dependencies = scanner.dependencies | {self.script_path}\n            for dep_path in dependencies:\n                relative_path = os.path.relpath(dep_path, self.project_root)\n                sandbox_dest = os.path.join(sandbox_dir, relative_path)\n                if os.path.isdir(dep_path): shutil.copytree(dep_path, sandbox_dest, dirs_exist_ok=True)\n                else: os.makedirs(os.path.dirname(sandbox_dest), exist_ok=True); shutil.copy(dep_path, sandbox_dest)\n        except Exception as e: print(f\"[SANDBOX-WARNING] Dependency copy failed for {self.script_name}: {e}\")\n\n    def _scan_for_text_artifacts(self, sandbox_dir):\n        artifacts = {}\n        text_exts = ['.txt', '.json', '.md', '.log', '.sh']\n        for root, _, files in os.walk(sandbox_dir):\n            for name in files:\n                if any(name.endswith(ext) for ext in text_exts):\n                    try:\n                        file_path = os.path.join(root, name)\n                        relative_path = os.path.relpath(file_path, sandbox_dir)\n                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                            artifacts[relative_path] = f.read()\n                    except Exception: pass\n        return artifacts\n\n# --- Meta-Analysis and Report Generation ---\ndef analyze_and_report(results, output_file):\n    all_filenames = [name for res in results for name in res['written_files'].keys()]\n    file_ext_counts = Counter(Path(name).suffix for name in all_filenames)\n    common_filenames = Counter(all_filenames).most_common(5)\n    with open(output_file, 'w', encoding='utf-8') as f:\n        f.write(\"# Text Flow Analysis Report\\n\\nThis report analyzes the collective textual output of all assembly scripts.\\n\\n\")\n        f.write(\"## Meta-Analysis Summary (The Forest)\\n\\n\")\n        f.write(\"### Common File Types Created\\n\")\n        for ext, count in file_ext_counts.items(): f.write(f\"- `{ext if ext else '[no extension]'}`: {count} times\\n\")\n        f.write(\"\\n### Most Common Filenames\\n\")\n        for name, count in common_filenames: f.write(f\"- `{name}`: {count} times\\n\")\n        f.write(\"\\n---\\n\\n## Detailed Script Analysis (The Trees)\\n\\n\")\n        for res in sorted(results, key=lambda x: x['script_name']):\n            f.write(f\"### \ud83d\udcdc Script: `{res['script_name']}`\\n\\n\")\n            f.write(f\"#### \ud83d\udda5\ufe0f Console Output\\n```text\\n{res['stdout'].strip() or '(No console output)'}\\n```\\n\\n\")\n            if res['written_files']:\n                f.write(\"#### \ud83d\udcc4 Generated Text Files\\n\")\n                for name, content in sorted(res['written_files'].items()):\n                    f.write(f\"- **`{name}`**\\n  ```\\n{content.strip()}\\n  ```\\n\")\n            else: f.write(\"_(No text files generated)_\\n\")\n            f.write(\"\\n---\\n\")\n\n# --- Main Execution ---\ndef find_assembly_scripts(project_root):\n    scripts_to_analyze = {}\n    for root, _, files in os.walk(project_root):\n        for name in files:\n            if 'assembl' in name and name.endswith('.py'):\n                script_path = os.path.join(root, name)\n                scripts_to_analyze[name] = script_path\n    return scripts_to_analyze\n\ndef main():\n    project_root = \"/Users/gaia/resurrecting atlantis\"\n    output_file = os.path.join(project_root, \"text_flow_analysis.md\")\n    print(f\"--- INITIATING FULL ANALYSIS ---\")\n    scripts_to_analyze = find_assembly_scripts(project_root)\n    print(f\"Found {len(scripts_to_analyze)} assembly scripts to analyze.\\n\")\n    results = []\n    for script_name, script_path in scripts_to_analyze.items():\n        print(f\"--- Analyzing: {script_name} ---\")\n        try:\n            emulator = SandboxedEmulator(script_path, project_root)\n            result_data = emulator.run()\n            result_data['script_name'] = script_name\n            results.append(result_data)\n        except Exception as e:\n            print(f\"\\n[CRITICAL ENGINE FAILURE] Could not analyze {script_name}. Reason: {e}\\n\")\n        finally:\n            print(\"\\n\")\n    analyze_and_report(results, output_file)\n    print(f\"Successfully generated Text Flow Analysis Report at: {output_file}\")\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "sys",
      "subprocess",
      "importlib.util",
      "contextlib",
      "runpy",
      "io",
      "builtins",
      "unittest.mock",
      "pathlib",
      "hashlib",
      "tempfile",
      "shutil",
      "ast",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "create_advanced_voice_variations.py",
    "size": 9107,
    "lines": 199,
    "source": "import os\nimport subprocess\nimport time\n\n# Configuration\nLIZARD_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD\"\nINPUT_VOICE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VoiceTrack.wav\")\nVIDEO_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\nOUTPUT_DIR = os.path.join(LIZARD_DIR, \"VOICE_ADVANCED\")\n\n# Advanced voice variations with complex filters\nVARIATIONS = [\n    {\n        \"name\": \"DeepResonance\",\n        \"description\": \"Deep, cavernous resonance with multiple layers of bass frequencies\",\n        \"filters\": \"bass=g=6:f=70:w=0.6,equalizer=f=200:width_type=h:width=150:g=3,aecho=0.8:0.9:1000|1500:0.6|0.4,areverse,aecho=0.6:0.6:100|60:0.3|0.4,areverse,chorus=0.7:0.9:60|90|120:0.4|0.3|0.2:0.5|0.3|0.2:2|3|4,lowpass=f=2000,volume=1.8\",\n        \"output_suffix\": \"DeepResonance\",\n        \"output_name\": \"Oceanic Depths\"\n    },\n    {\n        \"name\": \"MultiLayeredChorus\",\n        \"description\": \"Rich, multi-layered chorus effect with shifting harmonics and voices\",\n        \"filters\": \"chorus=0.7:0.9:60|90|40|20:0.5|0.4|0.3|0.2:0.4|0.3|0.5|0.2:2|3|1.5|2.5,chorus=0.6:0.4:50|25|75:0.5|0.4|0.3:0.5|0.3|0.6:2|1.5|3,equalizer=f=700:width_type=h:width=200:g=2,aphaser=in_gain=0.6:out_gain=0.6:delay=3:decay=0.6:speed=1:type=t,volume=1.6\",\n        \"output_suffix\": \"MultiChorus\",\n        \"output_name\": \"Ancestral Choir\"\n    },\n    {\n        \"name\": \"TexturalMorphology\",\n        \"description\": \"Dynamic textural morphing that evolves throughout with organic granular effects\",\n        \"filters\": \"flanger=delay=1:depth=0.5:regen=0.4:speed=0.1:shape=sinusoidal,vibrato=f=4:d=0.2,tremolo=f=2:d=0.4,highpass=f=200,equalizer=f=800:width_type=h:width=400:g=3,bandpass=f=1200:width_type=h:width=600,apulsator=mode=sine:hz=0.08:width=0.8,volume=1.5\",\n        \"output_suffix\": \"TexturalMorph\",\n        \"output_name\": \"Quantum Fabric\"\n    },\n    {\n        \"name\": \"SequentialTransformations\",\n        \"description\": \"Sequential vocal transformations that change character throughout the timeline\",\n        \"filters\": \"volume=2,asplit=5[v1][v2][v3][v4][v5],[v1]atempo=0.9,asetpts=PTS-STARTPTS[a1],[v2]aphaser,asetpts=PTS-STARTPTS+10/TB[a2],[v3]flanger=delay=10:depth=5:regen=5:speed=2,asetpts=PTS-STARTPTS+20/TB[a3],[v4]aecho=0.8:0.9:1000|500:0.3|0.5,asetpts=PTS-STARTPTS+30/TB[a4],[v5]chorus=0.6:0.9:50|40:0.5|0.4:0.5|0.3:2|3,asetpts=PTS-STARTPTS+40/TB[a5],[a1][a2][a3][a4][a5]amix=5:longest,highpass=f=100,lowpass=f=8000\",\n        \"output_suffix\": \"SequentialTransform\",\n        \"output_name\": \"Temporal Evolution\"\n    },\n    {\n        \"name\": \"ResonantHarmonics\",\n        \"description\": \"Resonant harmonic structures that emphasize overtone patterns and spectral richness\",\n        \"filters\": \"bass=g=5:f=80:w=0.5,treble=g=4:f=6000:w=0.5,equalizer=f=300:width_type=h:width=200:g=3,equalizer=f=1000:width_type=h:width=300:g=2,equalizer=f=3000:width_type=h:width=500:g=4,aecho=0.8:0.5:40|80|120:0.4|0.3|0.2,chorus=0.7:0.8:60|30:0.5|0.3:0.4|0.3:2|3,aphaser=in_gain=0.7:out_gain=0.7:delay=3:decay=0.5,volume=1.6\",\n        \"output_suffix\": \"ResonantHarmonics\",\n        \"output_name\": \"Spectral Resonance\"\n    }\n]\n\ndef ensure_output_dir():\n    \"\"\"Ensure output directory exists.\"\"\"\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f\"Created output directory: {OUTPUT_DIR}\")\n\ndef create_voice_variation(variation, simplified=False):\n    \"\"\"Create a specific voice variation.\"\"\"\n    name = variation[\"name\"]\n    filters = variation[\"filters\"]\n    suffix = variation[\"output_suffix\"]\n    description = variation[\"description\"]\n    \n    output_audio = os.path.join(OUTPUT_DIR, f\"Voice_{suffix}.wav\")\n    output_video = os.path.join(OUTPUT_DIR, f\"WhereYouGoWhenYouLeave_{suffix}.mp4\")\n    \n    print(f\"\\nCreating {name} voice variation: {description}\")\n    \n    # Use simpler filters if the complex ones fail\n    if simplified:\n        print(f\"Using simplified filters for {name}...\")\n        if name == \"DeepResonance\":\n            filters = \"bass=g=6:f=70:w=0.6,aecho=0.8:0.9:1000:0.6,lowpass=f=2000,volume=1.8\"\n        elif name == \"MultiLayeredChorus\":\n            filters = \"chorus=0.7:0.9:60|90:0.5|0.4:0.4|0.3:2|3,equalizer=f=700:width_type=h:width=200:g=2,volume=1.6\"\n        elif name == \"TexturalMorphology\":\n            filters = \"flanger=delay=1:depth=0.5:regen=0.4:speed=0.1:shape=sinusoidal,vibrato=f=4:d=0.2,tremolo=f=2:d=0.4,volume=1.5\"\n        elif name == \"SequentialTransformations\":\n            filters = \"aphaser,flanger=delay=10:depth=5:regen=5:speed=2,aecho=0.8:0.9:1000:0.3,volume=1.5\"\n        elif name == \"ResonantHarmonics\":\n            filters = \"bass=g=5:f=80:w=0.5,treble=g=4:f=6000:w=0.5,aecho=0.8:0.5:40|80:0.4|0.3,volume=1.6\"\n    \n    # Apply audio effect\n    effect_cmd = [\n        'ffmpeg',\n        '-i', INPUT_VOICE,\n        '-af', filters,\n        '-y',\n        output_audio\n    ]\n    \n    print(f\"Applying {name} effect...\")\n    process = subprocess.run(effect_cmd)\n    \n    if not os.path.exists(output_audio) or os.path.getsize(output_audio) == 0:\n        if not simplified:\n            print(f\"Error with complex filters for {name}. Trying simplified version...\")\n            return create_voice_variation(variation, simplified=True)\n        print(f\"Error creating {name} voice variation.\")\n        return False\n    \n    # Get duration to verify\n    duration_cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        output_audio\n    ]\n    \n    try:\n        result = subprocess.run(duration_cmd, stdout=subprocess.PIPE, text=True)\n        duration_str = result.stdout.strip()\n        if duration_str:\n            duration = float(duration_str)\n            print(f\"{name} voice duration: {duration:.2f} seconds\")\n        else:\n            print(f\"Warning: Could not determine duration for {name} voice\")\n            duration = 0\n    except Exception as e:\n        print(f\"Error getting duration: {str(e)}\")\n        duration = 0\n    \n    # Combine with video\n    video_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-i', output_audio,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'copy',\n        '-c:a', 'aac',\n        '-shortest',\n        '-metadata', f'title=Where You Go When You Leave - {variation[\"output_name\"]}',\n        '-metadata', f'comment={description}',\n        '-y',\n        output_video\n    ]\n    \n    print(f\"Creating {name} video...\")\n    subprocess.run(video_cmd)\n    \n    if os.path.exists(output_video):\n        print(f\"\u2713 {name} variation complete: {output_video}\")\n        return True\n    \n    print(f\"Error creating {name} video.\")\n    return False\n\ndef create_all_variations():\n    \"\"\"Create all voice variations.\"\"\"\n    ensure_output_dir()\n    \n    # Create a markdown report\n    report_path = os.path.join(OUTPUT_DIR, \"Advanced_Voice_Variations_Guide.md\")\n    \n    with open(report_path, 'w') as report:\n        report.write(\"# WHERE YOU GO WHEN YOU LEAVE - ADVANCED VOICE VARIATIONS\\n\\n\")\n        report.write(\"*Deep resonant and textural transformations for the poetic journey*\\n\\n\")\n        report.write(\"## Description\\n\\n\")\n        report.write(\"These advanced voice variations explore deeper dimensions of your Afro-futurist aesthetic through \")\n        report.write(\"complex resonances, multi-layered choruses, and dynamic textural morphing that evolves throughout the timeline. \")\n        report.write(\"Each variation creates a unique sonic environment that recontextualizes the poetry within rich \")\n        report.write(\"acoustic spaces that extend beyond conventional voice processing.\\n\\n\")\n        report.write(\"## Advanced Variations\\n\\n\")\n        report.write(\"| Variation | Description | Audio File | Video File |\\n\")\n        report.write(\"|-----------|-------------|------------|------------|\\n\")\n        \n        # Process each variation\n        completed = []\n        for variation in VARIATIONS:\n            try:\n                success = create_voice_variation(variation)\n                if success:\n                    completed.append(variation)\n            except Exception as e:\n                print(f\"Error processing {variation['name']}: {str(e)}\")\n                continue\n            \n        # Add successful variations to report\n        for variation in completed:\n            name = variation[\"name\"]\n            description = variation[\"description\"]\n            suffix = variation[\"output_suffix\"]\n            output_name = variation[\"output_name\"]\n            \n            audio_file = f\"Voice_{suffix}.wav\"\n            video_file = f\"WhereYouGoWhenYouLeave_{suffix}.mp4\"\n            \n            report.write(f\"| **{output_name}** | {description} | `{audio_file}` | `{video_file}` |\\n\")\n    \n    print(f\"\\nAdvanced voice variations complete. Guide created at: {report_path}\")\n\nif __name__ == \"__main__\":\n    start_time = time.time()\n    print(\"Starting advanced voice variation creation...\")\n    create_all_variations()\n    end_time = time.time()\n    print(f\"\\nProcess completed in {(end_time - start_time):.2f} seconds.\")\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_VoiceTrack.wav",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "Voice_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "Voice_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "/Users/gaia/resurrecting atlantis/LIZARD",
      "volume=2,asplit=5[v1][v2][v3][v4][v5],[v1]atempo=0.9,asetpts=PTS-STARTPTS[a1],[v2]aphaser,asetpts=PTS-STARTPTS+10/TB[a2],[v3]flanger=delay=10:depth=5:regen=5:speed=2,asetpts=PTS-STARTPTS+20/TB[a3],[v4]aecho=0.8:0.9:1000|500:0.3|0.5,asetpts=PTS-STARTPTS+30/TB[a4],[v5]chorus=0.6:0.9:50|40:0.5|0.4:0.5|0.3:2|3,asetpts=PTS-STARTPTS+40/TB[a5],[a1][a2][a3][a4][a5]amix=5:longest,highpass=f=100,lowpass=f=8000"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "effect_cmd"
      },
      {
        "type": "run",
        "snippet": "duration_cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "video_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "time"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "add_audio_to_video.py",
    "size": 5537,
    "lines": 151,
    "source": "import os\nimport subprocess\nimport datetime\n\n# Configuration\nLIZARD_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD\"\nINPUT_VIDEO = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\nINPUT_AUDIO = os.path.join(LIZARD_DIR, \"ElevenLabs_2025-05-30T10_23_43_Maputo jazz mortician_gen_sp100_s50_sb75_se0_b_m2.mp3\")\nOUTPUT_VIDEO = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_WithAudio.mp4\")\nLOG_FILE = os.path.join(LIZARD_DIR, \"audio_overlay_log.txt\")\n\ndef check_files_exist():\n    \"\"\"Check if input files exist.\"\"\"\n    if not os.path.exists(INPUT_VIDEO):\n        return False, f\"Video file not found at {INPUT_VIDEO}\"\n    if not os.path.exists(INPUT_AUDIO):\n        return False, f\"Audio file not found at {INPUT_AUDIO}\"\n    return True, \"Files exist\"\n\ndef get_video_duration(video_path):\n    \"\"\"Get the duration of a video using ffprobe.\"\"\"\n    cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        video_path\n    ]\n    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n    return float(result.stdout.strip())\n\ndef get_audio_duration(audio_path):\n    \"\"\"Get the duration of an audio file using ffprobe.\"\"\"\n    cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        audio_path\n    ]\n    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n    return float(result.stdout.strip())\n\ndef add_audio_to_video():\n    \"\"\"Add audio to video using FFmpeg.\"\"\"\n    # Check if input files exist\n    files_exist, message = check_files_exist()\n    if not files_exist:\n        print(message)\n        return False\n    \n    # Get durations\n    video_duration = get_video_duration(INPUT_VIDEO)\n    audio_duration = get_audio_duration(INPUT_AUDIO)\n    \n    # Create log header\n    with open(LOG_FILE, 'w') as log:\n        log.write(\"# AUDIO OVERLAY LOG\\n\\n\")\n        log.write(f\"Process started at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n        log.write(f\"Input video: {INPUT_VIDEO}\\n\")\n        log.write(f\"Input audio: {INPUT_AUDIO}\\n\")\n        log.write(f\"Output file: {OUTPUT_VIDEO}\\n\\n\")\n        log.write(f\"Video duration: {video_duration:.2f} seconds\\n\")\n        log.write(f\"Audio duration: {audio_duration:.2f} seconds\\n\\n\")\n        \n        # Determine appropriate action based on durations\n        if audio_duration >= video_duration:\n            log.write(\"Audio is longer than or equal to video. Will trim audio to match video duration.\\n\\n\")\n            method = \"trim\"\n        else:\n            log.write(\"Audio is shorter than video. Will loop audio to fill video duration.\\n\\n\")\n            method = \"loop\"\n            \n        log.write(\"## FFmpeg Operation Log:\\n\\n\")\n    \n    # Prepare FFmpeg command based on method\n    if method == \"trim\":\n        # Trim audio to match video duration\n        ffmpeg_cmd = [\n            'ffmpeg',\n            '-i', INPUT_VIDEO,\n            '-i', INPUT_AUDIO,\n            '-map', '0:v',        # Select video stream from first input\n            '-map', '1:a',        # Select audio stream from second input\n            '-c:v', 'copy',       # Copy video codec (no re-encoding)\n            '-c:a', 'aac',        # Convert audio to AAC\n            '-shortest',          # End when the shortest input ends\n            OUTPUT_VIDEO\n        ]\n    else:\n        # Loop audio to match video duration\n        ffmpeg_cmd = [\n            'ffmpeg',\n            '-i', INPUT_VIDEO,\n            '-stream_loop', '-1', # Loop audio indefinitely\n            '-i', INPUT_AUDIO,\n            '-map', '0:v',        # Select video stream from first input\n            '-map', '1:a',        # Select audio stream from second input\n            '-c:v', 'copy',       # Copy video codec (no re-encoding)\n            '-c:a', 'aac',        # Convert audio to AAC\n            '-shortest',          # End when the shortest input ends\n            OUTPUT_VIDEO\n        ]\n    \n    # Run FFmpeg and capture output\n    process = subprocess.Popen(\n        ffmpeg_cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True\n    )\n    \n    # Write FFmpeg output to log\n    with open(LOG_FILE, 'a') as log:\n        for line in process.stdout:\n            log.write(line)\n    \n    # Wait for process to complete\n    return_code = process.wait()\n    \n    # Add completion status to log\n    with open(LOG_FILE, 'a') as log:\n        log.write(f\"\\n\\nProcess completed at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        log.write(f\"Return code: {return_code}\\n\")\n        \n        if return_code == 0:\n            log.write(\"Status: SUCCESS\\n\")\n            if os.path.exists(OUTPUT_VIDEO):\n                file_size = os.path.getsize(OUTPUT_VIDEO) / (1024 * 1024)  # Size in MB\n                log.write(f\"Output file size: {file_size:.2f} MB\\n\")\n            else:\n                log.write(\"WARNING: Output file not found despite successful return code.\\n\")\n        else:\n            log.write(\"Status: FAILED\\n\")\n    \n    return return_code == 0\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Starting audio overlay process...\")\n    success = add_audio_to_video()\n    \n    if success:\n        print(f\"Audio overlay complete! Output file: {OUTPUT_VIDEO}\")\n        print(f\"Log file: {LOG_FILE}\")\n    else:\n        print(f\"Audio overlay failed. Check the log for details: {LOG_FILE}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "ElevenLabs_2025-05-30T10_23_43_Maputo jazz mortician_gen_sp100_s50_sb75_se0_b_m2.mp3",
      "WhereYouGoWhenYouLeave_WithAudio.mp4",
      "/Users/gaia/resurrecting atlantis/LIZARD",
      ")\n            if os.path.exists(OUTPUT_VIDEO):\n                file_size = os.path.getsize(OUTPUT_VIDEO) / (1024 * 1024)  # Size in MB\n                log.write(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "Popen",
        "snippet": "\n        ffmpeg_cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True\n    "
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "format_json_packets.py",
    "size": 2223,
    "lines": 69,
    "source": "#!/usr/bin/env python3\nimport json\nimport re\nimport os\nimport sys\n\ndef main():\n    if len(sys.argv) < 2:\n        print(f\"Usage: {sys.argv[0]} <packet_file_path>\")\n        print(f\"Example: {sys.argv[0]} /Users/gaia/resurrecting\\\\ atlantis/TIGER/SH/SH_packets/SH_packet_01.json\")\n        sys.exit(1)\n    \n    input_file = sys.argv[1]\n    if not os.path.exists(input_file):\n        print(f\"Error: File {input_file} not found\")\n        sys.exit(1)\n    \n    dir_path = os.path.dirname(input_file)\n    file_name = os.path.basename(input_file)\n    output_file = os.path.join(dir_path, f\"{os.path.splitext(file_name)[0]}_formatted.json\")\n    \n    # Read the file content\n    with open(input_file, 'r') as f:\n        content = f.read()\n    \n    # Extract all packet blocks (enclosed in curly braces)\n    packet_blocks = re.findall(r'{[^}]*}', content)\n    \n    # Create a list to hold the structured JSON objects\n    formatted_packets = []\n    \n    for block in packet_blocks:\n        # Process each block\n        items = block.strip('{}').split(',')\n        packet_obj = {}\n        \n        for i, item in enumerate(items):\n            item = item.strip()\n            \n            # Extract the shot ID (e.g., SH001)\n            shot_id_match = re.search(r'([A-Z]{2}\\d{3})', item)\n            if shot_id_match:\n                shot_id = shot_id_match.group(1)\n                \n                # Extract the rest of the info\n                parts = item.split('\u00b7')\n                if len(parts) >= 4:\n                    syntagma_type = parts[1].strip()\n                    category = parts[2].strip()\n                    description = ' \u00b7 '.join(parts[3:]).strip()\n                    \n                    packet_obj[shot_id] = {\n                        \"syntagmaType\": syntagma_type,\n                        \"category\": category,\n                        \"description\": description\n                    }\n        \n        if packet_obj:\n            formatted_packets.append(packet_obj)\n    \n    # Write the formatted JSON to the output file\n    with open(output_file, 'w') as f:\n        json.dump(formatted_packets, f, indent=2)\n    \n    print(f\"Created formatted JSON file: {output_file}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "Example: {sys.argv[0]} /Users/gaia/resurrecting\\\\ atlantis/TIGER/SH/SH_packets/SH_packet_01.json",
      "{os.path.splitext(file_name)[0]}_formatted.json",
      "Example: {sys.argv[0]} /Users/gaia/resurrecting\\\\ atlantis/TIGER/SH/SH_packets/SH_packet_01.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "re",
      "os",
      "sys"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "organize_lizard_videos.py",
    "size": 6998,
    "lines": 164,
    "source": "import os\nimport re\nimport shutil\nfrom datetime import datetime, timedelta\n\n# Configuration\nLIZARD_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD\"\nOUTPUT_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD/ORDERED\"\nREPORT_FILE = \"/Users/gaia/resurrecting atlantis/LIZARD/video_sequence_report.md\"\n\n# Poem sequence with timestamps\nPOEM_SEQUENCE = [\n    {\"id\": \"01\", \"code\": \"SH\", \"title\": \"Out of Life\", \"timestamp\": \"000000\"},\n    {\"id\": \"02\", \"code\": \"FL\", \"title\": \"Flashing Lights\", \"timestamp\": \"021100\"},\n    {\"id\": \"03\", \"code\": \"HT\", \"title\": \"How to Break Off an Engagement\", \"timestamp\": \"042200\"},\n    {\"id\": \"04\", \"code\": \"NM\", \"title\": \"Nevermore\", \"timestamp\": \"063300\"},\n    {\"id\": \"05\", \"code\": \"BE\", \"title\": \"Bloodline\", \"timestamp\": \"084400\"},\n    {\"id\": \"06\", \"code\": \"AT\", \"title\": \"Resurrecting Atlantis\", \"timestamp\": \"105500\"},\n    {\"id\": \"07\", \"code\": \"DJ\", \"title\": \"DJ Turn Me Up\", \"timestamp\": \"130600\"},\n    {\"id\": \"08\", \"code\": \"NS\", \"title\": \"Newly Single\", \"timestamp\": \"151700\"},\n    {\"id\": \"09\", \"code\": \"YH\", \"title\": \"Yet Heard\", \"timestamp\": \"172800\"},\n    {\"id\": \"10\", \"code\": \"MR\", \"title\": \"Magic Ride\", \"timestamp\": \"193900\"},\n    {\"id\": \"12\", \"code\": \"RU\", \"title\": \"Reunion\", \"timestamp\": \"215000\"},\n    {\"id\": \"13\", \"code\": \"HW\", \"title\": \"How to Win My Heart\", \"timestamp\": \"240100\"},\n    {\"id\": \"14\", \"code\": \"HM\", \"title\": \"Hot Minute\", \"timestamp\": \"261200\"}\n]\n\ndef format_timestamp(timestamp_str):\n    \"\"\"Convert timestamp string 'HHMMSS' to 'HH:MM:SS' format.\"\"\"\n    if len(timestamp_str) != 6:\n        return timestamp_str\n    return f\"{timestamp_str[0:2]}:{timestamp_str[2:4]}:{timestamp_str[4:6]}\"\n\ndef extract_poem_id(filename):\n    \"\"\"Extract poem ID (e.g., 'AT031') from filename.\"\"\"\n    # Pattern for IDs like AT031, BE018, etc.\n    pattern1 = r'^([A-Z]{2})0*(\\d+)'\n    match = re.search(pattern1, filename)\n    \n    if match:\n        poem_code = match.group(1)\n        poem_num = int(match.group(2))\n        return poem_code, poem_num\n    \n    # Alternative pattern for different file naming\n    pattern2 = r'([A-Z]{2})_?(\\d+)'\n    match = re.search(pattern2, filename)\n    \n    if match:\n        poem_code = match.group(1)\n        poem_num = int(match.group(2))\n        return poem_code, poem_num\n    \n    return None, None\n\ndef get_poem_sequence_index(poem_code):\n    \"\"\"Get the sequence index for a poem code.\"\"\"\n    for i, poem in enumerate(POEM_SEQUENCE):\n        if poem[\"code\"] == poem_code:\n            return i\n    return -1  # Not found\n\ndef calculate_video_timestamp(poem_code, poem_num):\n    \"\"\"Calculate the timestamp for a specific video based on poem section and ID number.\"\"\"\n    sequence_index = get_poem_sequence_index(poem_code)\n    \n    if sequence_index == -1:\n        return \"Unknown\"\n    \n    # Get the start timestamp for this poem section\n    start_timestamp = POEM_SEQUENCE[sequence_index][\"timestamp\"]\n    \n    # Get the next section's start time (or use end of video if this is the last section)\n    if sequence_index < len(POEM_SEQUENCE) - 1:\n        end_timestamp = POEM_SEQUENCE[sequence_index + 1][\"timestamp\"]\n    else:\n        end_timestamp = \"282300\"  # 28:23:00 - end of video\n    \n    # Convert timestamps to seconds\n    start_seconds = int(start_timestamp[0:2]) * 3600 + int(start_timestamp[2:4]) * 60 + int(start_timestamp[4:6])\n    end_seconds = int(end_timestamp[0:2]) * 3600 + int(end_timestamp[2:4]) * 60 + int(end_timestamp[4:6])\n    \n    # Each poem section is 2:11 (131 seconds)\n    # Calculate position within section based on ID number\n    # Assume max 70 IDs per poem\n    relative_position = min(poem_num / 70.0, 1.0)\n    video_seconds = start_seconds + int((end_seconds - start_seconds) * relative_position)\n    \n    # Convert back to timestamp format\n    hours = video_seconds // 3600\n    minutes = (video_seconds % 3600) // 60\n    seconds = video_seconds % 60\n    \n    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n\ndef organize_lizard_videos():\n    \"\"\"Organize videos from LIZARD directory according to poem sequence.\"\"\"\n    # Create output directory if it doesn't exist\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n    \n    # Find all video files\n    video_extensions = ['.mp4', '.mov', '.avi', '.mkv', '.wmv']\n    video_files = []\n    \n    for filename in os.listdir(LIZARD_DIR):\n        file_path = os.path.join(LIZARD_DIR, filename)\n        if os.path.isfile(file_path) and any(filename.lower().endswith(ext) for ext in video_extensions):\n            poem_code, poem_num = extract_poem_id(filename)\n            \n            if poem_code and poem_num:\n                sequence_index = get_poem_sequence_index(poem_code)\n                if sequence_index != -1:\n                    timestamp = calculate_video_timestamp(poem_code, poem_num)\n                    \n                    video_files.append({\n                        \"filename\": filename,\n                        \"path\": file_path,\n                        \"poem_code\": poem_code,\n                        \"poem_num\": poem_num,\n                        \"sequence_index\": sequence_index,\n                        \"timestamp\": timestamp\n                    })\n    \n    # Sort videos by poem sequence and then by ID number\n    video_files.sort(key=lambda x: (x[\"sequence_index\"], x[\"poem_num\"]))\n    \n    # Generate a report\n    with open(REPORT_FILE, 'w') as report:\n        report.write(\"# LIZARD VIDEO SEQUENCE REPORT\\n\\n\")\n        report.write(\"Videos ordered by poem sequence and ID number\\n\\n\")\n        \n        current_section = None\n        \n        for i, video in enumerate(video_files):\n            poem_index = video[\"sequence_index\"]\n            \n            if poem_index != current_section:\n                current_section = poem_index\n                poem_info = POEM_SEQUENCE[poem_index]\n                report.write(f\"\\n## {poem_info['id']}_{poem_info['code']}_{poem_info['title'].replace(' ', '')}_{poem_info['timestamp']}\\n\\n\")\n            \n            # Create a new filename that includes ordering information\n            new_filename = f\"{i+1:03d}_{video['poem_code']}{video['poem_num']:03d}_{video['timestamp'].replace(':', '')}.mp4\"\n            new_path = os.path.join(OUTPUT_DIR, new_filename)\n            \n            # Write to report\n            report.write(f\"- [{video['timestamp']}] {video['poem_code']}{video['poem_num']:03d} - `{video['filename']}`\\n\")\n            report.write(f\"  - New name: `{new_filename}`\\n\")\n            \n            # Copy the file to the output directory with the new name\n            try:\n                shutil.copy2(video[\"path\"], new_path)\n                report.write(f\"  - \u2713 Copied to ordered directory\\n\")\n            except Exception as e:\n                report.write(f\"  - \u2717 Error: {str(e)}\\n\")\n    \n    print(f\"Organization complete. {len(video_files)} videos processed.\")\n    print(f\"Report generated at: {REPORT_FILE}\")\n    print(f\"Ordered videos copied to: {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    organize_lizard_videos()\n",
    "file_references": [
      ")}.mp4",
      "/Users/gaia/resurrecting atlantis/LIZARD",
      "/Users/gaia/resurrecting atlantis/LIZARD/ORDERED",
      "/Users/gaia/resurrecting atlantis/LIZARD/video_sequence_report.md",
      "  # 28:23:00 - end of video\n    \n    # Convert timestamps to seconds\n    start_seconds = int(start_timestamp[0:2]) * 3600 + int(start_timestamp[2:4]) * 60 + int(start_timestamp[4:6])\n    end_seconds = int(end_timestamp[0:2]) * 3600 + int(end_timestamp[2:4]) * 60 + int(end_timestamp[4:6])\n    \n    # Each poem section is 2:11 (131 seconds)\n    # Calculate position within section based on ID number\n    # Assume max 70 IDs per poem\n    relative_position = min(poem_num / 70.0, 1.0)\n    video_seconds = start_seconds + int((end_seconds - start_seconds) * relative_position)\n    \n    # Convert back to timestamp format\n    hours = video_seconds // 3600\n    minutes = (video_seconds % 3600) // 60\n    seconds = video_seconds % 60\n    \n    return f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "enhance_voice_professional.py",
    "size": 12594,
    "lines": 335,
    "source": "import os\nimport subprocess\nimport json\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_ENHANCED\")\nINPUT_VOICE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VoiceTrack.wav\")\nVIDEO_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n\n# Define professional voice enhancement presets\nVOICE_PRESETS = [\n    {\n        \"name\": \"CrystalClarity\",\n        \"description\": \"Extremely clear, crisp voice with enhanced articulation and presence\",\n        \"filters\": [\n            # Loudness normalization\n            \"loudnorm=I=-16:LRA=11:TP=-1.5\",\n            # Highpass to remove rumble\n            \"highpass=f=80\",\n            # De-essing to control sibilance\n            \"highshelf=f=6000:g=-4\",\n            # EQ for vocal clarity - cut mud, boost presence\n            \"equalizer=f=250:width_type=h:width=100:g=-3,equalizer=f=450:width_type=h:width=100:g=-2\",\n            \"equalizer=f=2500:width_type=h:width=1000:g=4,equalizer=f=5000:width_type=h:width=1000:g=2\",\n            # Multiband compression for dynamics control\n            \"compand=attacks=0.01:decays=0.5:points=-80/-105 -30/-30 0/-10:gain=3\",\n            # Final volume adjustment\n            \"volume=1.3\"\n        ],\n        \"output_suffix\": \"CrystalClarity\"\n    },\n    {\n        \"name\": \"ConfidentPresence\",\n        \"description\": \"Rich, confident voice with enhanced low-mids and powerful presence\",\n        \"filters\": [\n            # Loudness normalization\n            \"loudnorm=I=-14:LRA=9:TP=-1\",\n            # Highpass to remove unnecessary lows\n            \"highpass=f=100\",\n            # Boost low-mids for power\n            \"equalizer=f=180:width_type=h:width=100:g=2,equalizer=f=350:width_type=h:width=150:g=3\",\n            # Add presence and clarity\n            \"equalizer=f=2800:width_type=h:width=1000:g=4,equalizer=f=7000:width_type=h:width=2000:g=1\",\n            # Dynamic range compression for consistency\n            \"compand=attacks=0.02:decays=0.6:points=-80/-80 -30/-24 0/-8:gain=5\",\n            # Final volume\n            \"volume=1.25\"\n        ],\n        \"output_suffix\": \"ConfidentPresence\"\n    },\n    {\n        \"name\": \"ExecutiveVoice\",\n        \"description\": \"Authoritative, executive-style voice with perfect articulation and commanding presence\",\n        \"filters\": [\n            # Loudness normalization\n            \"loudnorm=I=-14:LRA=8:TP=-1\",\n            # High-pass filter to remove rumble\n            \"highpass=f=90\",\n            # De-essing (reduce harsh sibilance)\n            \"highshelf=f=8000:g=-3\",\n            # EQ for power and presence\n            \"equalizer=f=100:width_type=h:width=100:g=1.5\",\n            \"equalizer=f=240:width_type=h:width=100:g=2.5\",\n            \"equalizer=f=900:width_type=h:width=100:g=-2\",\n            \"equalizer=f=3000:width_type=h:width=1000:g=4.5\",\n            \"equalizer=f=10000:width_type=h:width=2000:g=1.5\",\n            # Multiband compression for consistency\n            \"compand=attacks=0.01:decays=0.5:points=-80/-90 -40/-40 -30/-24 0/-6:gain=4\",\n            # Final volume\n            \"volume=1.25\"\n        ],\n        \"output_suffix\": \"ExecutiveVoice\"\n    },\n    {\n        \"name\": \"BroadcastPerfection\",\n        \"description\": \"Professional broadcast-quality voice with perfect balance of warmth and clarity\",\n        \"filters\": [\n            # Loudness normalization\n            \"loudnorm=I=-16:LRA=11:TP=-1.5\",\n            # Highpass filter to remove rumble\n            \"highpass=f=85\",\n            # EQ for warmth and clarity\n            \"equalizer=f=120:width_type=h:width=100:g=2,equalizer=f=250:width_type=h:width=100:g=-2\",\n            \"equalizer=f=500:width_type=h:width=100:g=-1,equalizer=f=2500:width_type=h:width=1000:g=3\",\n            \"equalizer=f=5000:width_type=h:width=1000:g=1.5,equalizer=f=8000:width_type=h:width=2000:g=-1\",\n            # Dynamic range compression for consistency\n            \"compand=attacks=0.02:decays=0.6:points=-80/-80 -30/-24 0/-7:gain=4\",\n            # Final volume adjustment\n            \"volume=1.2\"\n        ],\n        \"output_suffix\": \"BroadcastPerfection\"\n    },\n    {\n        \"name\": \"UltraCrispDefinition\",\n        \"description\": \"Ultra-defined voice with enhanced articulation and microscopic detail\",\n        \"filters\": [\n            # Loudness normalization\n            \"loudnorm=I=-14:LRA=7:TP=-1\",\n            # Highpass to remove unnecessary lows\n            \"highpass=f=120\",\n            # Cut muddy frequencies, boost clarity\n            \"equalizer=f=300:width_type=h:width=100:g=-3,equalizer=f=500:width_type=h:width=100:g=-1.5\",\n            \"equalizer=f=1200:width_type=h:width=100:g=1,equalizer=f=3000:width_type=h:width=1000:g=4\",\n            \"equalizer=f=6000:width_type=h:width=1000:g=3,equalizer=f=10000:width_type=h:width=2000:g=1.5\",\n            # De-essing (reduce harsh sibilance)\n            \"highshelf=f=9000:g=-2\",\n            # Multiband compression for detail\n            \"compand=attacks=0.005:decays=0.3:points=-90/-90 -40/-35 -30/-25 0/-5:gain=4\",\n            # Final volume\n            \"volume=1.3\"\n        ],\n        \"output_suffix\": \"UltraCrispDefinition\"\n    }\n]\n\ndef ensure_temp_dir():\n    \"\"\"Ensure temporary directory exists.\"\"\"\n    if not os.path.exists(TEMP_DIR):\n        os.makedirs(TEMP_DIR)\n        print(f\"Created temporary directory: {TEMP_DIR}\")\n\ndef extract_original_audio():\n    \"\"\"Extract original audio from the assembled video.\"\"\"\n    combined_audio = os.path.join(TEMP_DIR, \"original_combined.wav\")\n    \n    # Extract audio directly from the final assembly video\n    extract_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-vn',\n        '-acodec', 'pcm_s16le',\n        '-y',\n        combined_audio\n    ]\n    \n    print(\"Extracting original audio from assembled video...\")\n    subprocess.run(extract_cmd)\n    \n    return combined_audio if os.path.exists(combined_audio) else None\n\ndef get_video_duration():\n    \"\"\"Get the exact duration of the video in seconds.\"\"\"\n    cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        VIDEO_FILE\n    ]\n    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n    return float(result.stdout.strip())\n\ndef enhance_voice(preset):\n    \"\"\"Enhance voice using professional preset.\"\"\"\n    name = preset[\"name\"]\n    description = preset[\"description\"]\n    filters = preset[\"filters\"]\n    suffix = preset[\"output_suffix\"]\n    \n    # Get exact video duration\n    video_duration = get_video_duration()\n    print(f\"Target video duration: {video_duration:.2f} seconds\")\n    \n    # Output files\n    enhanced_voice = os.path.join(TEMP_DIR, f\"voice_{suffix}.wav\")\n    enhanced_voice_fixed = os.path.join(TEMP_DIR, f\"voice_{suffix}_fixed.wav\")\n    final_mix = os.path.join(TEMP_DIR, f\"mix_{suffix}.wav\")\n    output_video = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{suffix}.mp4\")\n    \n    print(f\"\\nCreating {name} enhanced voice: {description}\")\n    \n    # Apply professional voice enhancement filters\n    filter_str = ','.join(filters)\n    voice_cmd = [\n        'ffmpeg',\n        '-i', INPUT_VOICE,\n        '-af', filter_str,\n        '-y',\n        enhanced_voice\n    ]\n    \n    print(f\"Enhancing voice with {name} preset...\")\n    subprocess.run(voice_cmd)\n    \n    if not os.path.exists(enhanced_voice):\n        print(f\"Error enhancing voice for {name}\")\n        return False\n    \n    # Get processed voice duration\n    duration_cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        enhanced_voice\n    ]\n    \n    result = subprocess.run(duration_cmd, stdout=subprocess.PIPE, text=True)\n    voice_duration = float(result.stdout.strip())\n    print(f\"Enhanced voice duration: {voice_duration:.2f} seconds\")\n    \n    # Calculate tempo adjustment to match video duration exactly\n    tempo_factor = voice_duration / video_duration\n    print(f\"Tempo adjustment factor: {tempo_factor:.4f}\")\n    \n    # Apply tempo adjustment to make voice track exactly match video duration\n    fix_cmd = [\n        'ffmpeg',\n        '-i', enhanced_voice,\n        '-af', f'atempo={tempo_factor}',\n        '-y',\n        enhanced_voice_fixed\n    ]\n    \n    print(f\"Adjusting voice timing to match video duration...\")\n    subprocess.run(fix_cmd)\n    \n    if not os.path.exists(enhanced_voice_fixed):\n        print(f\"Error fixing voice duration for {name}\")\n        return False\n    \n    # Extract original audio from video for subtle background\n    original_audio = extract_original_audio()\n    if not original_audio:\n        print(\"Error extracting original audio from video\")\n        return False\n    \n    # Mix enhanced voice with subtle background\n    mix_cmd = [\n        'ffmpeg',\n        '-i', enhanced_voice_fixed,\n        '-i', original_audio,\n        '-filter_complex', '[0:a][1:a]amix=inputs=2:weights=1 0.15[aout]',\n        '-map', '[aout]',\n        '-y',\n        final_mix\n    ]\n    \n    print(f\"Creating final audio mix for {name}...\")\n    subprocess.run(mix_cmd)\n    \n    if not os.path.exists(final_mix):\n        print(f\"Error creating final mix for {name}\")\n        return False\n    \n    # Combine with video\n    video_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-i', final_mix,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'copy',\n        '-c:a', 'aac',\n        '-shortest',\n        '-metadata', f'title=Where You Go When You Leave - {name}',\n        '-metadata', f'comment={description}',\n        '-y',\n        output_video\n    ]\n    \n    print(f\"Creating {name} video with enhanced voice...\")\n    subprocess.run(video_cmd)\n    \n    if os.path.exists(output_video):\n        print(f\"\u2713 {name} enhanced voice complete: {output_video}\")\n        return True\n    \n    print(f\"Error creating {name} video\")\n    return False\n\ndef create_all_enhancements():\n    \"\"\"Create all professional voice enhancements.\"\"\"\n    ensure_temp_dir()\n    \n    # Create a markdown report\n    report_path = os.path.join(LIZARD_DIR, \"Professional_Voice_Guide.md\")\n    \n    with open(report_path, 'w') as report:\n        report.write(\"# WHERE YOU GO WHEN YOU LEAVE - PROFESSIONAL VOICE ENHANCEMENTS\\n\\n\")\n        report.write(\"*Crisp, clear, and confident voice enhancements with professional audio engineering*\\n\\n\")\n        report.write(\"## Description\\n\\n\")\n        report.write(\"These professional voice enhancements apply broadcast-grade processing to create \")\n        report.write(\"crisp, clear, and confident voices with perfect articulation and presence. \")\n        report.write(\"Each variation uses professional audio engineering techniques including:\")\n        report.write(\"\\n\\n\")\n        report.write(\"- Multiband compression for consistent dynamics\\n\")\n        report.write(\"- Precision EQ for vocal clarity and presence\\n\")\n        report.write(\"- De-essing for sibilance control\\n\")\n        report.write(\"- Loudness normalization for broadcast standards\\n\")\n        report.write(\"- Exact timing synchronization with the video (1:40)\\n\")\n        report.write(\"\\n\\n\")\n        report.write(\"## Professional Voice Variations\\n\\n\")\n        report.write(\"| Variation | Description | Video File |\\n\")\n        report.write(\"|-----------|-------------|------------|\\n\")\n        \n        # Process each voice preset\n        completed = []\n        for preset in VOICE_PRESETS:\n            try:\n                success = enhance_voice(preset)\n                if success:\n                    completed.append(preset)\n            except Exception as e:\n                print(f\"Error processing {preset['name']}: {str(e)}\")\n                continue\n            \n        # Add successful variations to report\n        for preset in completed:\n            name = preset[\"name\"]\n            description = preset[\"description\"]\n            suffix = preset[\"output_suffix\"]\n            \n            video_file = f\"WhereYouGoWhenYouLeave_{suffix}.mp4\"\n            \n            report.write(f\"| **{name}** | {description} | `{video_file}` |\\n\")\n    \n    print(f\"\\nProfessional voice enhancements complete. Guide created at: {report_path}\")\n    return True\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Starting professional voice enhancement...\")\n    success = create_all_enhancements()\n    \n    if success:\n        print(\"Professional voice enhancement complete!\")\n    else:\n        print(\"Error creating professional voice enhancements\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_VoiceTrack.wav",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "original_combined.wav",
      "voice_{suffix}.wav",
      "voice_{suffix}_fixed.wav",
      "mix_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "/Users/gaia/resurrecting atlantis",
      "compand=attacks=0.01:decays=0.5:points=-80/-105 -30/-30 0/-10:gain=3",
      "compand=attacks=0.02:decays=0.6:points=-80/-80 -30/-24 0/-8:gain=5",
      "compand=attacks=0.01:decays=0.5:points=-80/-90 -40/-40 -30/-24 0/-6:gain=4",
      "compand=attacks=0.02:decays=0.6:points=-80/-80 -30/-24 0/-7:gain=4",
      "compand=attacks=0.005:decays=0.3:points=-90/-90 -40/-35 -30/-25 0/-5:gain=4",
      ")\n    \n    # Calculate tempo adjustment to match video duration exactly\n    tempo_factor = voice_duration / video_duration\n    print(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "extract_cmd"
      },
      {
        "type": "run",
        "snippet": "cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "voice_cmd"
      },
      {
        "type": "run",
        "snippet": "duration_cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "fix_cmd"
      },
      {
        "type": "run",
        "snippet": "mix_cmd"
      },
      {
        "type": "run",
        "snippet": "video_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "create_mixed_audio_video.py",
    "size": 8961,
    "lines": 258,
    "source": "import os\nimport subprocess\nimport json\nimport glob\nimport re\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nORDERED_DIR = os.path.join(LIZARD_DIR, \"ORDERED\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_AUDIO\")\nBACKGROUND_MUSIC = os.path.join(LIZARD_DIR, \"ElevenLabs_2025-05-30T10_18_59_Maputo jazz mortician_gen_sp100_s50_sb75_se70_b_m2.mp3\")\nADJUSTED_MUSIC = os.path.join(LIZARD_DIR, \"Maputo_jazz_mortician_adjusted_speed.mp3\")\nFINAL_OUTPUT = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_MixedAudio.mp4\")\nVIDEO_LIST = os.path.join(LIZARD_DIR, \"video_list.txt\")\nCONCAT_LIST = os.path.join(LIZARD_DIR, \"concat_list.txt\")\nLOG_FILE = os.path.join(LIZARD_DIR, \"mixed_audio_log.txt\")\n\ndef ensure_temp_dir():\n    \"\"\"Ensure the temporary directory exists.\"\"\"\n    if not os.path.exists(TEMP_DIR):\n        os.makedirs(TEMP_DIR)\n        print(f\"Created temporary directory: {TEMP_DIR}\")\n\ndef adjust_background_music():\n    \"\"\"Adjust the background music speed to match video length.\"\"\"\n    # Get the total duration of the assembled video\n    video_duration_cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    ]\n    video_duration = float(subprocess.run(video_duration_cmd, capture_output=True, text=True).stdout.strip())\n    \n    # Get the duration of the background music\n    music_duration_cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        BACKGROUND_MUSIC\n    ]\n    music_duration = float(subprocess.run(music_duration_cmd, capture_output=True, text=True).stdout.strip())\n    \n    # Calculate speed factor\n    speed_factor = music_duration / video_duration\n    \n    # Adjust music speed\n    speed_cmd = [\n        'ffmpeg',\n        '-i', BACKGROUND_MUSIC,\n        '-filter:a', f'atempo={speed_factor}',\n        '-y',\n        ADJUSTED_MUSIC\n    ]\n    \n    print(f\"Adjusting background music speed by factor of {speed_factor:.2f}...\")\n    subprocess.run(speed_cmd, capture_output=True)\n    \n    return os.path.exists(ADJUSTED_MUSIC)\n\ndef extract_audio_from_videos():\n    \"\"\"Extract audio from each video in the ORDERED directory.\"\"\"\n    videos = sorted(glob.glob(os.path.join(ORDERED_DIR, \"*.mp4\")))\n    extracted = []\n    \n    for i, video in enumerate(videos):\n        video_name = os.path.basename(video)\n        output_audio = os.path.join(TEMP_DIR, f\"{i+1:03d}_audio.wav\")\n        \n        # Extract audio from video\n        extract_cmd = [\n            'ffmpeg',\n            '-i', video,\n            '-vn',  # No video\n            '-acodec', 'pcm_s16le',  # PCM 16-bit LE format\n            '-y',\n            output_audio\n        ]\n        \n        print(f\"Extracting audio from {video_name}...\")\n        subprocess.run(extract_cmd, capture_output=True)\n        \n        # Get duration\n        duration_cmd = [\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'default=noprint_wrappers=1:nokey=1', \n            video\n        ]\n        duration = float(subprocess.run(duration_cmd, capture_output=True, text=True).stdout.strip())\n        \n        if os.path.exists(output_audio):\n            extracted.append({\n                'video': video,\n                'audio': output_audio,\n                'name': video_name,\n                'duration': duration\n            })\n    \n    return extracted\n\ndef create_mixed_video_segments(extracted_audio):\n    \"\"\"Create video segments with mixed audio.\"\"\"\n    mixed_segments = []\n    \n    # Get the total duration of the assembled video for calculating position\n    video_duration_cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    ]\n    total_duration = float(subprocess.run(video_duration_cmd, capture_output=True, text=True).stdout.strip())\n    \n    # Start position for tracking\n    current_pos = 0\n    \n    for i, item in enumerate(extracted_audio):\n        video = item['video']\n        audio_file = item['audio']\n        name = item['name']\n        duration = item['duration']\n        \n        # Output file for this segment\n        output_segment = os.path.join(TEMP_DIR, f\"{i+1:03d}_mixed.mp4\")\n        \n        # Calculate segment's position in the overall timeline (as fraction)\n        position = current_pos / total_duration\n        \n        # Mix original audio with background music at this position\n        mix_cmd = [\n            'ffmpeg',\n            '-i', video,  # Original video with its audio\n            '-i', ADJUSTED_MUSIC,  # Background music\n            '-filter_complex',\n            # Mix the original audio at 0.8 volume and the background music at 0.3 volume\n            # Also seek the background music to the correct position\n            f'[0:a]volume=0.8[a1];[1:a]volume=0.3,atrim=start={current_pos}:end={current_pos+duration}[a2];[a1][a2]amix=inputs=2:duration=first[aout]',\n            '-map', '0:v',  # Take video from input 0\n            '-map', '[aout]',  # Take the mixed audio\n            '-c:v', 'copy',  # Copy video codec\n            '-c:a', 'aac',  # AAC for audio\n            '-shortest',  # End when the shortest input ends\n            '-y',\n            output_segment\n        ]\n        \n        print(f\"Creating mixed segment for {name} (position: {position:.2f})...\")\n        subprocess.run(mix_cmd, capture_output=True)\n        \n        if os.path.exists(output_segment):\n            mixed_segments.append({\n                'file': output_segment,\n                'name': name,\n                'position': position,\n                'duration': duration\n            })\n            \n        # Update position for next segment\n        current_pos += duration\n    \n    return mixed_segments\n\ndef create_concat_file(mixed_segments):\n    \"\"\"Create a file list for concatenation.\"\"\"\n    with open(CONCAT_LIST, 'w') as f:\n        for segment in mixed_segments:\n            f.write(f\"file '{segment['file']}'\\n\")\n    \n    return os.path.exists(CONCAT_LIST)\n\ndef concatenate_segments():\n    \"\"\"Concatenate all mixed segments into the final video.\"\"\"\n    concat_cmd = [\n        'ffmpeg',\n        '-f', 'concat',\n        '-safe', '0',\n        '-i', CONCAT_LIST,\n        '-c', 'copy',\n        '-y',\n        FINAL_OUTPUT\n    ]\n    \n    print(\"Concatenating all segments into final video...\")\n    subprocess.run(concat_cmd, capture_output=True)\n    \n    return os.path.exists(FINAL_OUTPUT)\n\ndef create_mixed_audio_video():\n    \"\"\"Create a video with mixed audio.\"\"\"\n    # Create log file\n    with open(LOG_FILE, 'w') as log:\n        log.write(\"# MIXED AUDIO VIDEO CREATION LOG\\n\\n\")\n        log.write(f\"Process started at: {subprocess.run(['date'], capture_output=True, text=True).stdout.strip()}\\n\\n\")\n    \n    # Ensure temp directory exists\n    ensure_temp_dir()\n    \n    # Adjust background music\n    if not adjust_background_music():\n        print(\"Failed to adjust background music.\")\n        return False\n    \n    # Extract audio from each video\n    extracted_audio = extract_audio_from_videos()\n    if not extracted_audio:\n        print(\"Failed to extract audio from videos.\")\n        return False\n    \n    # Create mixed video segments\n    mixed_segments = create_mixed_video_segments(extracted_audio)\n    if not mixed_segments:\n        print(\"Failed to create mixed video segments.\")\n        return False\n    \n    # Create concat file\n    if not create_concat_file(mixed_segments):\n        print(\"Failed to create concatenation file.\")\n        return False\n    \n    # Concatenate segments\n    if not concatenate_segments():\n        print(\"Failed to concatenate segments.\")\n        return False\n    \n    # Update log file\n    with open(LOG_FILE, 'a') as log:\n        log.write(f\"\\nProcess completed at: {subprocess.run(['date'], capture_output=True, text=True).stdout.strip()}\\n\")\n        log.write(f\"Final output: {FINAL_OUTPUT}\\n\")\n        if os.path.exists(FINAL_OUTPUT):\n            file_size = os.path.getsize(FINAL_OUTPUT) / (1024 * 1024)\n            log.write(f\"Output file size: {file_size:.2f} MB\\n\")\n            log.write(\"Status: SUCCESS\\n\")\n        else:\n            log.write(\"Status: FAILED\\n\")\n    \n    return os.path.exists(FINAL_OUTPUT)\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Starting mixed audio video creation process...\")\n    success = create_mixed_audio_video()\n    \n    if success:\n        print(f\"Mixed audio video creation complete!\")\n        print(f\"Output file: {FINAL_OUTPUT}\")\n        print(f\"Log file: {LOG_FILE}\")\n    else:\n        print(f\"Process failed. Check the log for details: {LOG_FILE}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "ElevenLabs_2025-05-30T10_18_59_Maputo jazz mortician_gen_sp100_s50_sb75_se70_b_m2.mp3",
      "Maputo_jazz_mortician_adjusted_speed.mp3",
      "WhereYouGoWhenYouLeave_MixedAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "*.mp4",
      "{i+1:03d}_audio.wav",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "{i+1:03d}_mixed.mp4",
      "/Users/gaia/resurrecting atlantis",
      ", \n        BACKGROUND_MUSIC\n    ]\n    music_duration = float(subprocess.run(music_duration_cmd, capture_output=True, text=True).stdout.strip())\n    \n    # Calculate speed factor\n    speed_factor = music_duration / video_duration\n    \n    # Adjust music speed\n    speed_cmd = [\n        ",
      "s position in the overall timeline (as fraction)\n        position = current_pos / total_duration\n        \n        # Mix original audio with background music at this position\n        mix_cmd = [\n            ",
      ")\n        if os.path.exists(FINAL_OUTPUT):\n            file_size = os.path.getsize(FINAL_OUTPUT) / (1024 * 1024)\n            log.write(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "video_duration_cmd, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "music_duration_cmd, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "speed_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "extract_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "duration_cmd, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "video_duration_cmd, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "mix_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "concat_cmd, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "['date'], capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "['date'], capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "glob",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "create_immersive_soundscape_fixed.py",
    "size": 10221,
    "lines": 288,
    "source": "import os\nimport subprocess\nimport glob\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nORDERED_DIR = os.path.join(LIZARD_DIR, \"ORDERED\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_SOUND\")\nINPUT_VOICE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VoiceTrack.wav\")\nVIDEO_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n\n# Define soundscape variations\nVARIATIONS = [\n    {\n        \"name\": \"CathedralResonance\",\n        \"description\": \"Massive cathedral-like reverb with tuned resonance and subtle harmonic chimes\",\n        \"voice_filters\": \"aecho=0.8:0.9:1000|1800:0.3|0.25,equalizer=f=200:width_type=h:width=100:g=3,volume=1.4\",\n        \"voice_volume\": 1.0,\n        \"original_volume\": 0.5,\n        \"output_suffix\": \"CathedralResonance\"\n    },\n    {\n        \"name\": \"SpatialHarmonics\",\n        \"description\": \"Spatial harmonics with tuned resonators and subtle ambient drones\",\n        \"voice_filters\": \"aecho=0.8:0.7:300|400|500|600:0.4|0.3|0.2|0.1,chorus=0.7:0.9:60|90:0.5|0.4:0.4|0.3:2|3,volume=1.3\",\n        \"voice_volume\": 1.0,\n        \"original_volume\": 0.6,\n        \"output_suffix\": \"SpatialHarmonics\"\n    },\n    {\n        \"name\": \"ResonantChamber\",\n        \"description\": \"Deep resonant chamber with harmonically tuned reflections and subtle chimes\",\n        \"voice_filters\": \"bass=g=4:f=80:w=0.5,treble=g=2:f=8000:w=0.5,aecho=0.8:0.7:100|200|300:0.5|0.3|0.2,volume=1.5\",\n        \"voice_volume\": 1.0,\n        \"original_volume\": 0.5,\n        \"output_suffix\": \"ResonantChamber\"\n    },\n    {\n        \"name\": \"WetReverb\",\n        \"description\": \"Extremely wet reverb with long tail and subtle ambient textures\",\n        \"voice_filters\": \"aecho=0.9:0.9:1000|2000|3000:0.6|0.3|0.2,aecho=0.7:0.7:100|50:0.4|0.3,bass=g=3:f=100:w=0.5,volume=1.2\",\n        \"voice_volume\": 1.0,\n        \"original_volume\": 0.4,\n        \"output_suffix\": \"WetReverb\"\n    },\n    {\n        \"name\": \"AmbientHarmony\",\n        \"description\": \"Harmonically tuned voice with ambient chimes and atmospheric textures\",\n        \"voice_filters\": \"chorus=0.6:0.9:60|40:0.5|0.4:0.5|0.3:2|3,aecho=0.8:0.8:500|700:0.4|0.3,equalizer=f=300:width_type=h:width=200:g=3,volume=1.3\",\n        \"voice_volume\": 1.0,\n        \"original_volume\": 0.5,\n        \"output_suffix\": \"AmbientHarmony\"\n    }\n]\n\ndef ensure_temp_dir():\n    \"\"\"Ensure temporary directory exists.\"\"\"\n    if not os.path.exists(TEMP_DIR):\n        os.makedirs(TEMP_DIR)\n        print(f\"Created temporary directory: {TEMP_DIR}\")\n\ndef extract_audio_from_videos():\n    \"\"\"Extract combined audio from the ordered video directory.\"\"\"\n    combined_audio = os.path.join(TEMP_DIR, \"original_combined.wav\")\n    \n    # Extract audio directly from the final assembly video\n    extract_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-vn',\n        '-acodec', 'pcm_s16le',\n        '-y',\n        combined_audio\n    ]\n    \n    print(\"Extracting original audio from assembled video...\")\n    subprocess.run(extract_cmd)\n    \n    return combined_audio if os.path.exists(combined_audio) else None\n\ndef generate_ambient_elements():\n    \"\"\"Generate ambient elements like chimes and drones.\"\"\"\n    # Generate a soft drone\n    drone_file = os.path.join(TEMP_DIR, \"ambient_drone.wav\")\n    drone_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.1:c=brown:d=101',\n        '-af', 'lowpass=f=1000,volume=0.15',\n        '-y',\n        drone_file\n    ]\n    \n    print(\"Generating ambient drone...\")\n    subprocess.run(drone_cmd)\n    \n    # Generate subtle chimes\n    chimes_file = os.path.join(TEMP_DIR, \"ambient_chimes.wav\")\n    chimes_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.02:c=blue:d=101',\n        '-af', 'bandpass=f=2000:width_type=h:width=800,treble=g=5:f=3000:w=0.5,volume=0.1',\n        '-y',\n        chimes_file\n    ]\n    \n    print(\"Generating ambient chimes...\")\n    subprocess.run(chimes_cmd)\n    \n    return {\n        'drone': drone_file if os.path.exists(drone_file) else None,\n        'chimes': chimes_file if os.path.exists(chimes_file) else None\n    }\n\ndef create_immersive_variation(variation, original_audio, ambient_elements):\n    \"\"\"Create an immersive soundscape variation.\"\"\"\n    name = variation[\"name\"]\n    description = variation[\"description\"]\n    voice_filters = variation[\"voice_filters\"]\n    voice_volume = variation[\"voice_volume\"]\n    original_volume = variation[\"original_volume\"]\n    suffix = variation[\"output_suffix\"]\n    \n    # Output files\n    processed_voice = os.path.join(TEMP_DIR, f\"voice_{suffix}.wav\")\n    output_video = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{suffix}.mp4\")\n    final_mix = os.path.join(TEMP_DIR, f\"mix_{suffix}.wav\")\n    \n    print(f\"\\nCreating {name} immersive soundscape: {description}\")\n    \n    # Process voice with reverb and resonance\n    voice_cmd = [\n        'ffmpeg',\n        '-i', INPUT_VOICE,\n        '-af', voice_filters,\n        '-y',\n        processed_voice\n    ]\n    \n    print(f\"Processing voice with {name} effects...\")\n    subprocess.run(voice_cmd)\n    \n    if not os.path.exists(processed_voice):\n        print(f\"Error creating processed voice for {name}\")\n        return False\n    \n    # Create filter for mixing voice with original audio\n    filter_str = (f\"[0:a]volume={voice_volume}[voice];\"\n                  f\"[1:a]volume={original_volume}[orig];\"\n                  f\"[voice][orig]amix=inputs=2:duration=longest[mixed]\")\n    \n    # If ambient elements exist, add them to the mix\n    if ambient_elements['drone'] and os.path.exists(ambient_elements['drone']):\n        filter_str += (f\";[mixed][2:a]amix=inputs=2:duration=longest[with_drone]\")\n        if ambient_elements['chimes'] and os.path.exists(ambient_elements['chimes']):\n            filter_str += (f\";[with_drone][3:a]amix=inputs=2:duration=longest[aout]\")\n            map_out = \"[aout]\"\n            inputs = [\n                '-i', processed_voice,\n                '-i', original_audio,\n                '-i', ambient_elements['drone'],\n                '-i', ambient_elements['chimes']\n            ]\n        else:\n            map_out = \"[with_drone]\"\n            inputs = [\n                '-i', processed_voice,\n                '-i', original_audio,\n                '-i', ambient_elements['drone']\n            ]\n    else:\n        map_out = \"[mixed]\"\n        inputs = [\n            '-i', processed_voice,\n            '-i', original_audio\n        ]\n    \n    # Mix all audio elements\n    mix_cmd = [\n        'ffmpeg',\n        *inputs,\n        '-filter_complex', filter_str,\n        '-map', map_out,\n        '-y',\n        final_mix\n    ]\n    \n    print(f\"Creating final audio mix for {name}...\")\n    subprocess.run(mix_cmd)\n    \n    if not os.path.exists(final_mix):\n        print(f\"Error creating final mix for {name}\")\n        return False\n    \n    # Combine with video\n    video_cmd = [\n        'ffmpeg',\n        '-i', VIDEO_FILE,\n        '-i', final_mix,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'copy',\n        '-c:a', 'aac',\n        '-shortest',\n        '-metadata', f'title=Where You Go When You Leave - {name}',\n        '-metadata', f'comment={description}',\n        '-y',\n        output_video\n    ]\n    \n    print(f\"Creating {name} video with immersive audio...\")\n    subprocess.run(video_cmd)\n    \n    if os.path.exists(output_video):\n        print(f\"\u2713 {name} immersive soundscape complete: {output_video}\")\n        return True\n    \n    print(f\"Error creating {name} video\")\n    return False\n\ndef create_all_variations():\n    \"\"\"Create all immersive soundscape variations.\"\"\"\n    ensure_temp_dir()\n    \n    # Extract original audio\n    original_audio = extract_audio_from_videos()\n    if not original_audio:\n        print(\"Error extracting original audio from videos\")\n        return False\n    \n    # Generate ambient elements\n    ambient_elements = generate_ambient_elements()\n    \n    # Create a markdown report\n    report_path = os.path.join(LIZARD_DIR, \"Immersive_Soundscape_Guide.md\")\n    \n    with open(report_path, 'w') as report:\n        report.write(\"# WHERE YOU GO WHEN YOU LEAVE - IMMERSIVE SOUNDSCAPES\\n\\n\")\n        report.write(\"*Resonant blended soundscapes for the poetic journey*\\n\\n\")\n        report.write(\"## Description\\n\\n\")\n        report.write(\"These immersive soundscapes blend the voice with original audio from the videos, \")\n        report.write(\"creating deeply resonant environments with tuned reverberation, spatial harmonics, \")\n        report.write(\"and subtle ambient elements. Each variation explores different acoustic spaces \")\n        report.write(\"that recontextualize the poetry within rich, evocative sonic landscapes aligned with \")\n        report.write(\"the Afro-futurist aesthetic of the overall project.\\n\\n\")\n        report.write(\"## Immersive Variations\\n\\n\")\n        report.write(\"| Variation | Description | Video File |\\n\")\n        report.write(\"|-----------|-------------|------------|\\n\")\n        \n        # Process each variation\n        completed = []\n        for variation in VARIATIONS:\n            try:\n                success = create_immersive_variation(variation, original_audio, ambient_elements)\n                if success:\n                    completed.append(variation)\n            except Exception as e:\n                print(f\"Error processing {variation['name']}: {str(e)}\")\n                continue\n            \n        # Add successful variations to report\n        for variation in completed:\n            name = variation[\"name\"]\n            description = variation[\"description\"]\n            suffix = variation[\"output_suffix\"]\n            \n            video_file = f\"WhereYouGoWhenYouLeave_{suffix}.mp4\"\n            \n            report.write(f\"| **{name}** | {description} | `{video_file}` |\\n\")\n    \n    print(f\"\\nImmersive soundscapes complete. Guide created at: {report_path}\")\n    return True\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Starting immersive soundscape creation...\")\n    success = create_all_variations()\n    \n    if success:\n        print(\"Immersive soundscape creation complete!\")\n    else:\n        print(\"Error creating immersive soundscapes\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_VoiceTrack.wav",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "original_combined.wav",
      "ambient_drone.wav",
      "ambient_chimes.wav",
      "voice_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "mix_{suffix}.wav",
      "WhereYouGoWhenYouLeave_{suffix}.mp4",
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "extract_cmd"
      },
      {
        "type": "run",
        "snippet": "drone_cmd"
      },
      {
        "type": "run",
        "snippet": "chimes_cmd"
      },
      {
        "type": "run",
        "snippet": "voice_cmd"
      },
      {
        "type": "run",
        "snippet": "mix_cmd"
      },
      {
        "type": "run",
        "snippet": "video_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "glob"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "split_packets.py",
    "size": 2707,
    "lines": 89,
    "source": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nimport sys\n\ndef extract_packet_info(content):\n    \"\"\"\n    Extracts individual packets from the content of a _packet_01.json file\n    and returns a list of (packet_content, start_id, end_id) tuples.\n    \"\"\"\n    # Find all content within curly braces\n    packet_matches = re.findall(r'\\{([^{}]+)\\}', content)\n    packets = []\n    \n    for packet_str in packet_matches:\n        # Split by commas and clean up\n        prompts = [p.strip() for p in packet_str.split(',')]\n        \n        # Extract ID ranges\n        first_prompt = prompts[0]\n        last_prompt = prompts[-1]\n        \n        # Extract IDs using regex\n        first_id_match = re.search(r'([A-Z]{2}\\d{3})', first_prompt)\n        last_id_match = re.search(r'([A-Z]{2}\\d{3})', last_prompt)\n        \n        if first_id_match and last_id_match:\n            start_id = first_id_match.group(1)\n            end_id = last_id_match.group(1)\n            \n            # Combine all prompts into a JSON array format\n            packet_content = json.dumps(prompts, indent=2)\n            packets.append((packet_content, start_id, end_id))\n    \n    return packets\n\ndef process_packet_file(file_path):\n    \"\"\"\n    Processes a packet file and splits it into individual packet files.\n    \"\"\"\n    try:\n        # Read the content of the file\n        with open(file_path, 'r') as f:\n            content = f.read()\n        \n        # Extract packets\n        packets = extract_packet_info(content)\n        \n        # Create output directory\n        dir_path = os.path.dirname(file_path)\n        \n        # Process each packet\n        for packet_content, start_id, end_id in packets:\n            # Create filename\n            filename = f\"{start_id}-{end_id}.json\"\n            output_path = os.path.join(dir_path, filename)\n            \n            # Write to file\n            with open(output_path, 'w') as f:\n                f.write(packet_content)\n            \n            print(f\"Created packet file: {output_path}\")\n        \n        print(f\"Successfully processed {len(packets)} packets from {file_path}\")\n        return True\n    \n    except Exception as e:\n        print(f\"Error processing {file_path}: {str(e)}\")\n        return False\n\ndef main():\n    if len(sys.argv) < 2:\n        print(\"Usage: python split_packets.py <path_to_packet_file>\")\n        print(\"Example: python split_packets.py /Users/gaia/resurrecting atlantis/TIGER/SH/SH_packets/SH_packet_01.json\")\n        return\n    \n    file_path = sys.argv[1]\n    \n    if not os.path.exists(file_path):\n        print(f\"Error: File {file_path} not found.\")\n        return\n    \n    process_packet_file(file_path)\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "{start_id}-{end_id}.json",
      "Example: python split_packets.py /Users/gaia/resurrecting atlantis/TIGER/SH/SH_packets/SH_packet_01.json",
      "Example: python split_packets.py /Users/gaia/resurrecting atlantis/TIGER/SH/SH_packets/SH_packet_01.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "json",
      "sys"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "join_audio.py",
    "size": 2131,
    "lines": 62,
    "source": "import os\nimport subprocess\n\ndef join_audio_files(directory, output_filename):\n    \"\"\"\n    Joins audio files in a directory in alphanumeric order using ffmpeg.\n\n    Args:\n        directory (str): The directory containing the audio files.\n        output_filename (str): The name of the output file.\n    \"\"\"\n    # Get all .wav files in the directory\n    try:\n        files = sorted([f for f in os.listdir(directory) if f.endswith('.wav')])\n    except FileNotFoundError:\n        print(f\"Error: Directory not found at {directory}\")\n        return\n\n    if not files:\n        print(f\"No .wav files found in {directory}\")\n        return\n\n    # Create a temporary file list for ffmpeg\n    list_filename = os.path.join(directory, \"mylist.txt\")\n    with open(list_filename, 'w') as f:\n        for filename in files:\n            # FFmpeg requires special handling for certain characters in filenames, like single quotes.\n            safe_filename = filename.replace(\"'\", \"'\\\\''\")\n            f.write(f\"file '{safe_filename}'\\n\")\n\n    # Construct the ffmpeg command\n    output_path = os.path.join(directory, output_filename)\n    command = [\n        'ffmpeg',\n        '-y', # Overwrite output file if it exists\n        '-f', 'concat',\n        '-safe', '0',\n        '-i', list_filename,\n        '-c', 'copy',\n        output_path\n    ]\n\n    # Run the command\n    print(f\"Joining {len(files)} files into {output_path}...\")\n    try:\n        result = subprocess.run(command, check=True, capture_output=True, text=True)\n        print(\"Successfully joined audio files.\")\n        print(\"FFmpeg output:\", result.stdout)\n    except subprocess.CalledProcessError as e:\n        print(\"Error during ffmpeg execution:\")\n        print(\"FFmpeg stderr:\", e.stderr)\n    finally:\n        # Clean up the temporary file list\n        if os.path.exists(list_filename):\n            os.remove(list_filename)\n            print(f\"Removed temporary file: {list_filename}\")\n\nif __name__ == \"__main__\":\n    target_directory = \"/Users/gaia/resurrecting atlantis/ANT/VO_Mixdowns_RY\"\n    output_file = \"combined_mixdown.wav\"\n    join_audio_files(target_directory, output_file)\n",
    "file_references": [
      "combined_mixdown.wav",
      "/Users/gaia/resurrecting atlantis/ANT/VO_Mixdowns_RY"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "command, check=True, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "extract_poem_content.py",
    "size": 5535,
    "lines": 139,
    "source": "import os\nimport json\nimport re\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nSEQUENCE_REPORT = os.path.join(LIZARD_DIR, \"video_sequence_report.md\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_PoemContent.md\")\n\n# Poem sequence for reference\nPOEM_SEQUENCE = [\n    {\"id\": \"01\", \"code\": \"SH\", \"title\": \"Out of Life\", \"timestamp\": \"000000\"},\n    {\"id\": \"02\", \"code\": \"FL\", \"title\": \"Flashing Lights\", \"timestamp\": \"021100\"},\n    {\"id\": \"03\", \"code\": \"HT\", \"title\": \"How to Break Off an Engagement\", \"timestamp\": \"042200\"},\n    {\"id\": \"04\", \"code\": \"NM\", \"title\": \"Nevermore\", \"timestamp\": \"063300\"},\n    {\"id\": \"05\", \"code\": \"BE\", \"title\": \"Bloodline\", \"timestamp\": \"084400\"},\n    {\"id\": \"06\", \"code\": \"AT\", \"title\": \"Resurrecting Atlantis\", \"timestamp\": \"105500\"},\n    {\"id\": \"07\", \"code\": \"DJ\", \"title\": \"DJ Turn Me Up\", \"timestamp\": \"130600\"},\n    {\"id\": \"08\", \"code\": \"NS\", \"title\": \"Newly Single\", \"timestamp\": \"151700\"},\n    {\"id\": \"09\", \"code\": \"YH\", \"title\": \"Yet Heard\", \"timestamp\": \"172800\"},\n    {\"id\": \"10\", \"code\": \"MR\", \"title\": \"Magic Ride\", \"timestamp\": \"193900\"},\n    {\"id\": \"12\", \"code\": \"RU\", \"title\": \"Reunion\", \"timestamp\": \"215000\"},\n    {\"id\": \"13\", \"code\": \"HW\", \"title\": \"How to Win My Heart\", \"timestamp\": \"240100\"},\n    {\"id\": \"14\", \"code\": \"HM\", \"title\": \"Hot Minute\", \"timestamp\": \"261200\"}\n]\n\ndef parse_sequence_report():\n    \"\"\"Parse the video sequence report to extract video IDs.\"\"\"\n    if not os.path.exists(SEQUENCE_REPORT):\n        print(f\"Error: Sequence report not found at {SEQUENCE_REPORT}\")\n        return []\n        \n    video_ids = []\n    \n    with open(SEQUENCE_REPORT, 'r') as f:\n        lines = f.readlines()\n        \n    # Pattern to match video IDs in format: [HH:MM:SS] XX123 - `filename`\n    id_pattern = r'\\[(\\d{2}:\\d{2}:\\d{2})\\] ([A-Z]{2}\\d{3}) - `'\n    \n    for line in lines:\n        match = re.search(id_pattern, line)\n        if match:\n            timestamp = match.group(1)\n            video_id = match.group(2)\n            video_ids.append((timestamp, video_id))\n            \n    return video_ids\n\ndef get_poem_code_mapping():\n    \"\"\"Create a mapping of poem codes to full titles.\"\"\"\n    return {poem[\"code\"]: poem[\"title\"] for poem in POEM_SEQUENCE}\n\ndef find_content_in_assembly(video_id):\n    \"\"\"Find the content for a video ID in its assembly.json file.\"\"\"\n    poem_code = video_id[:2]  # Extract poem code (e.g., \"SH\" from \"SH023\")\n    \n    # Construct path to assembly.json file\n    assembly_path = os.path.join(TIGER_DIR, poem_code, f\"{poem_code}_assembly.json\")\n    \n    if not os.path.exists(assembly_path):\n        return {\"error\": f\"Assembly file not found: {assembly_path}\"}\n        \n    try:\n        with open(assembly_path, 'r') as f:\n            assembly_data = json.load(f)\n            \n        # Find entry with matching ID\n        for entry in assembly_data:\n            if entry.get(\"id\") == video_id:\n                return entry\n                \n        return {\"error\": f\"ID {video_id} not found in assembly file\"}\n    except Exception as e:\n        return {\"error\": f\"Error processing {assembly_path}: {str(e)}\"}\n\ndef extract_poem_content():\n    \"\"\"Extract poem content for each video ID from assembly files.\"\"\"\n    video_ids = parse_sequence_report()\n    poem_mapping = get_poem_code_mapping()\n    \n    if not video_ids:\n        print(\"No video IDs found in sequence report.\")\n        return\n        \n    with open(OUTPUT_FILE, 'w') as outfile:\n        outfile.write(\"# WHERE YOU GO WHEN YOU LEAVE - POEM CONTENT\\n\\n\")\n        outfile.write(\"*Content lines from assembly.json files for each video in sequence*\\n\\n\")\n        \n        current_poem = None\n        \n        for timestamp, video_id in video_ids:\n            poem_code = video_id[:2]\n            \n            # Start a new section if poem changes\n            if poem_code != current_poem:\n                current_poem = poem_code\n                poem_title = poem_mapping.get(poem_code, f\"Unknown ({poem_code})\")\n                \n                # Find poem sequence number\n                sequence_num = \"??\"\n                for poem in POEM_SEQUENCE:\n                    if poem[\"code\"] == poem_code:\n                        sequence_num = poem[\"id\"]\n                        break\n                        \n                outfile.write(f\"\\n## {sequence_num}_{poem_code}_{poem_title.replace(' ', '')}\\n\\n\")\n            \n            # Get content from assembly\n            entry = find_content_in_assembly(video_id)\n            \n            # Write entry details\n            outfile.write(f\"### {video_id} [{timestamp}]\\n\\n\")\n            \n            if \"error\" in entry:\n                outfile.write(f\"*{entry['error']}*\\n\\n\")\n            else:\n                # Extract key fields\n                content = entry.get(\"content\", \"\")\n                syntagma_type = entry.get(\"syntagmaType\", \"\")\n                ekphrasis = entry.get(\"operativeEkphrasis\", \"\")\n                \n                outfile.write(f\"**Content:** {content}\\n\\n\")\n                \n                if syntagma_type:\n                    outfile.write(f\"**Syntagma Type:** {syntagma_type}\\n\\n\")\n                    \n                if ekphrasis:\n                    outfile.write(f\"**Ekphrasis:** {ekphrasis}\\n\\n\")\n                    \n            outfile.write(\"---\\n\\n\")\n            \n    print(f\"Poem content extracted to: {OUTPUT_FILE}\")\n\nif __name__ == \"__main__\":\n    extract_poem_content()\n",
    "file_references": [
      "{poem_code}_assembly.json",
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "analyze_assembly_scripts.py",
    "size": 6847,
    "lines": 180,
    "source": "import ast\nimport os\nfrom pathlib import Path\n\nclass ScriptAnalyzer(ast.NodeVisitor):\n    \"\"\"\n    Analyzes a Python script using AST to extract key features.\n    \"\"\"\n    def __init__(self):\n        self.imports = set()\n        self.functions_called = set()\n        self.string_literals = set()\n\n    def visit_Import(self, node):\n        for alias in node.names:\n            self.imports.add(alias.name)\n        self.generic_visit(node)\n\n    def visit_ImportFrom(self, node):\n        if node.module:\n            self.imports.add(node.module)\n        self.generic_visit(node)\n\n    def get_full_call_name(self, node):\n        if isinstance(node, ast.Name):\n            return node.id\n        elif isinstance(node, ast.Attribute):\n            base = self.get_full_call_name(node.value)\n            if base:\n                return f\"{base}.{node.attr}\"\n            return node.attr\n        elif isinstance(node, ast.Call):\n            return self.get_full_call_name(node.func)\n        return None\n\n    def visit_Call(self, node):\n        func_name = self.get_full_call_name(node.func)\n        if func_name:\n            self.functions_called.add(func_name)\n        self.generic_visit(node)\n\n    def visit_Constant(self, node):\n        if isinstance(node.value, str) and node.value:\n            self.string_literals.add(node.value)\n        self.generic_visit(node)\n\n    def visit_Str(self, node):\n        if node.s:\n            self.string_literals.add(node.s)\n        self.generic_visit(node)\n\n    def analyze(self, script_path):\n        \"\"\"Parses the script and returns the extracted features.\"\"\"\n        with open(script_path, 'r', encoding='utf-8', errors='ignore') as f:\n            source = f.read()\n        try:\n            tree = ast.parse(source)\n            self.visit(tree)\n        except SyntaxError:\n            return {\"imports\": [], \"functions_called\": [], \"potential_paths\": []}\n\n        potential_paths = {\n            s for s in self.string_literals\n            if ('/' in s or '\\\\' in s or any(s.endswith(ext) for ext in ['.mp4', '.mp3', '.wav', '.json', '.txt', '.md']))\n            and not s.startswith('http')\n            and len(s.strip()) > 3\n        }\n        return {\n            \"imports\": sorted(list(self.imports)),\n            \"functions_called\": sorted(list(self.functions_called)),\n            \"potential_paths\": sorted(list(potential_paths))\n        }\n\ndef find_assembly_scripts(root_dir):\n    \"\"\"Finds all assembly scripts in the given directory, excluding itself.\"\"\"\n    root_path = Path(root_dir)\n    script_name = Path(__file__).name\n    return [p for p in root_path.rglob('*assembl*.py') if p.name != script_name]\n\ndef generate_analysis_report(scripts_analysis, output_file):\n    \"\"\"Generates a markdown report from the analysis data.\"\"\"\n    with open(output_file, 'w', encoding='utf-8') as f:\n        f.write(\"# Assembly Scripts Analysis Report\\n\\n\")\n        f.write(\"This report analyzes the key differences between the assembly scripts in this project, focusing on imports, function calls, and file paths to reveal their unique purpose.\\n\\n\")\n\n        for script_path, analysis in sorted(scripts_analysis.items()):\n            script_name = os.path.basename(script_path)\n            f.write(f\"## ` {script_name} `\\n\\n\")\n            f.write(f\"- **Full Path:** `{script_path}`\\n\")\n\n            f.write(\"### Key Imports\\n\")\n            if analysis['imports']:\n                top_level_imports = sorted({imp.split('.')[0] for imp in analysis['imports']})\n                for imp in top_level_imports:\n                    f.write(f\"- `{imp}`\\n\")\n            else:\n                f.write(\"- None\\n\")\n            f.write(\"\\n\")\n\n            f.write(\"### Core Function Calls\\n\")\n            if analysis['functions_called']:\n                core_keywords = [\n                    'VideoFileClip', 'ImageClip', 'AudioFileClip', 'concatenate',\n                    'CompositeVideoClip', 'write_videofile', 'write_audiofile',\n                    'fadein', 'fadeout', 'speedx', 'resize', 'crop', 'glob', 'pathlib',\n                    'subprocess.run', 'os.system', 'json.load', 'json.dump'\n                ]\n                core_funcs = set()\n                for func_call in analysis['functions_called']:\n                    for keyword in core_keywords:\n                        if keyword in func_call:\n                            core_funcs.add(func_call)\n                \n                if core_funcs:\n                    for func in sorted(list(core_funcs)):\n                        f.write(f\"- `{func}`\\n\")\n                else:\n                    f.write(\"- No core video/file operations detected.\\n\")\n            else:\n                f.write(\"- None\\n\")\n            f.write(\"\\n\")\n\n            f.write(\"### Potential Input/Output Paths\\n\")\n            if analysis['potential_paths']:\n                cleaned_paths = [p for p in analysis['potential_paths'] if not p.endswith(('.py', '.pyc')) and len(p.split()) < 4]\n                if cleaned_paths:\n                    for path in cleaned_paths:\n                        f.write(f\"- `{path}`\\n\")\n                else:\n                    f.write(\"- None detected\\n\")\n            else:\n                f.write(\"- None detected\\n\")\n            f.write(\"\\n---\\n\\n\")\n\n    print(f\"\\nSuccessfully generated analysis report at: {output_file}\")\n\ndef main():\n    \"\"\"Main function to run the analysis.\"\"\"\n    project_root = Path('/Users/gaia/resurrecting atlantis')\n    output_file = project_root / 'assembly_scripts_analysis.md'\n    \n    print(f\"Starting analysis of assembly scripts in: {project_root}\")\n    \n    scripts = find_assembly_scripts(project_root)\n    if not scripts:\n        print(\"No assembly scripts found.\")\n        return\n\n    print(f\"Found {len(scripts)} assembly scripts to analyze.\")\n    \n    all_analysis = {}\n    # Deduplicate scripts by content\n    unique_scripts = {}\n    for script_path in scripts:\n        try:\n            with open(script_path, 'r', encoding='utf-8', errors='ignore') as f:\n                content = f.read()\n            if content not in unique_scripts:\n                unique_scripts[content] = script_path\n        except Exception as e:\n            print(f\"    - ERROR reading {script_path.name} for deduplication: {e}\")\n\n    print(f\"Found {len(unique_scripts)} unique scripts to analyze.\")\n\n    for script_path in unique_scripts.values():\n        print(f\"  - Analyzing {script_path.name}...\")\n        analyzer = ScriptAnalyzer()\n        try:\n            analysis_data = analyzer.analyze(script_path)\n            all_analysis[str(script_path)] = analysis_data\n        except Exception as e:\n            print(f\"    - ERROR analyzing {script_path.name}: {e}\")\n\n    if all_analysis:\n        print(\"\\nGenerating analysis report...\")\n        generate_analysis_report(all_analysis, output_file)\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "- No core video/file operations detected.\\n",
      "### Potential Input/Output Paths\\n",
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [],
    "imports": [
      "ast",
      "os",
      "pathlib"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "adjust_audio_speed.py",
    "size": 4642,
    "lines": 132,
    "source": "import os\nimport subprocess\nimport datetime\n\n# Configuration\nLIZARD_DIR = \"/Users/gaia/resurrecting atlantis/LIZARD\"\nINPUT_AUDIO = \"/Users/gaia/resurrecting atlantis/LIZARD/ElevenLabs_2025-05-30T10_18_59_Maputo jazz mortician_gen_sp100_s50_sb75_se70_b_m2.mp3\"\nADJUSTED_AUDIO = os.path.join(LIZARD_DIR, \"Maputo_jazz_mortician_adjusted_speed.mp3\")\nINPUT_VIDEO = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\nOUTPUT_VIDEO = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalWithAudio.mp4\")\nLOG_FILE = os.path.join(LIZARD_DIR, \"audio_speed_adjustment_log.txt\")\n\ndef check_files_exist():\n    \"\"\"Check if input files exist.\"\"\"\n    if not os.path.exists(INPUT_AUDIO):\n        return False, f\"Audio file not found at {INPUT_AUDIO}\"\n    if not os.path.exists(INPUT_VIDEO):\n        return False, f\"Video file not found at {INPUT_VIDEO}\"\n    return True, \"Files exist\"\n\ndef get_duration(file_path):\n    \"\"\"Get duration of a media file using ffprobe.\"\"\"\n    cmd = [\n        'ffprobe', \n        '-v', 'error', \n        '-show_entries', 'format=duration', \n        '-of', 'default=noprint_wrappers=1:nokey=1', \n        file_path\n    ]\n    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n    return float(result.stdout.strip())\n\ndef adjust_audio_speed():\n    \"\"\"Adjust audio speed to match video duration and create new video with adjusted audio.\"\"\"\n    # Check if input files exist\n    files_exist, message = check_files_exist()\n    if not files_exist:\n        print(message)\n        return False\n    \n    # Get durations\n    video_duration = get_duration(INPUT_VIDEO)\n    audio_duration = get_duration(INPUT_AUDIO)\n    \n    # Calculate speed factor\n    speed_factor = audio_duration / video_duration\n    \n    # Create log header\n    with open(LOG_FILE, 'w') as log:\n        log.write(\"# AUDIO SPEED ADJUSTMENT LOG\\n\\n\")\n        log.write(f\"Process started at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n        log.write(f\"Input video: {INPUT_VIDEO}\\n\")\n        log.write(f\"Input audio: {INPUT_AUDIO}\\n\")\n        log.write(f\"Adjusted audio: {ADJUSTED_AUDIO}\\n\")\n        log.write(f\"Final output: {OUTPUT_VIDEO}\\n\\n\")\n        log.write(f\"Video duration: {video_duration:.2f} seconds\\n\")\n        log.write(f\"Audio duration: {audio_duration:.2f} seconds\\n\")\n        log.write(f\"Speed factor: {speed_factor:.2f}x\\n\\n\")\n        log.write(\"## FFmpeg Operation Log:\\n\\n\")\n    \n    # Step 1: Adjust audio speed\n    speed_cmd = [\n        'ffmpeg',\n        '-i', INPUT_AUDIO,\n        '-filter:a', f'atempo={speed_factor}',\n        '-y',\n        ADJUSTED_AUDIO\n    ]\n    \n    print(f\"Adjusting audio speed by factor of {speed_factor:.2f}...\")\n    process = subprocess.run(speed_cmd, capture_output=True, text=True)\n    \n    with open(LOG_FILE, 'a') as log:\n        log.write(\"=== AUDIO SPEED ADJUSTMENT ===\\n\")\n        log.write(process.stdout)\n        log.write(process.stderr)\n        log.write(\"\\n\\n\")\n    \n    if process.returncode != 0 or not os.path.exists(ADJUSTED_AUDIO):\n        print(\"Error adjusting audio speed.\")\n        return False\n    \n    # Step 2: Combine adjusted audio with video\n    combine_cmd = [\n        'ffmpeg',\n        '-i', INPUT_VIDEO,\n        '-i', ADJUSTED_AUDIO,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'copy',\n        '-c:a', 'aac',\n        '-shortest',\n        '-y',\n        OUTPUT_VIDEO\n    ]\n    \n    print(\"Combining adjusted audio with video...\")\n    process = subprocess.run(combine_cmd, capture_output=True, text=True)\n    \n    with open(LOG_FILE, 'a') as log:\n        log.write(\"=== COMBINING AUDIO WITH VIDEO ===\\n\")\n        log.write(process.stdout)\n        log.write(process.stderr)\n        log.write(\"\\n\\n\")\n        \n        # Write completion info\n        log.write(f\"Process completed at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        \n        if process.returncode == 0 and os.path.exists(OUTPUT_VIDEO):\n            log.write(\"Status: SUCCESS\\n\")\n            file_size = os.path.getsize(OUTPUT_VIDEO) / (1024 * 1024)  # Size in MB\n            log.write(f\"Output file size: {file_size:.2f} MB\\n\")\n        else:\n            log.write(\"Status: FAILED\\n\")\n    \n    return process.returncode == 0\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Starting audio speed adjustment process...\")\n    success = adjust_audio_speed()\n    \n    if success:\n        print(f\"Audio adjustment and video creation complete!\")\n        print(f\"Output file: {OUTPUT_VIDEO}\")\n        print(f\"Log file: {LOG_FILE}\")\n    else:\n        print(f\"Process failed. Check the log for details: {LOG_FILE}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/LIZARD/ElevenLabs_2025-05-30T10_18_59_Maputo jazz mortician_gen_sp100_s50_sb75_se70_b_m2.mp3",
      "Maputo_jazz_mortician_adjusted_speed.mp3",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "WhereYouGoWhenYouLeave_FinalWithAudio.mp4",
      "/Users/gaia/resurrecting atlantis/LIZARD",
      "/Users/gaia/resurrecting atlantis/LIZARD/ElevenLabs_2025-05-30T10_18_59_Maputo jazz mortician_gen_sp100_s50_sb75_se70_b_m2.mp3",
      "\n    # Check if input files exist\n    files_exist, message = check_files_exist()\n    if not files_exist:\n        print(message)\n        return False\n    \n    # Get durations\n    video_duration = get_duration(INPUT_VIDEO)\n    audio_duration = get_duration(INPUT_AUDIO)\n    \n    # Calculate speed factor\n    speed_factor = audio_duration / video_duration\n    \n    # Create log header\n    with open(LOG_FILE, ",
      ")\n            file_size = os.path.getsize(OUTPUT_VIDEO) / (1024 * 1024)  # Size in MB\n            log.write(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "speed_cmd, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "combine_cmd, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "generate_world_model.py",
    "size": 14136,
    "lines": 293,
    "source": "import ast\nimport os\nimport hashlib\nfrom pathlib import Path\nimport re\n\n# OlogGenerator is the foundational AST parser from our previous script.\nclass OlogGenerator(ast.NodeVisitor):\n    def __init__(self, script_path):\n        self.script_path = script_path\n        self.script_name = os.path.basename(script_path)\n        self.model = []\n        self.variables = {}\n        self.temp_dir_vars = set()\n        self.indent_level = 1\n\n    def add_morphism(self, description, level=None):\n        if level is None:\n            level = self.indent_level\n        self.model.append(f\"{'    ' * level}\u2192 {description}\")\n\n    def get_value(self, node):\n        if isinstance(node, ast.Constant):\n            return node.value\n        if isinstance(node, ast.Name):\n            return self.variables.get(node.id, f'`{node.id}`')\n        if isinstance(node, ast.JoinedStr):  # f-string\n            return \"\".join(str(self.get_value(v)) for v in node.values)\n        if isinstance(node, ast.FormattedValue):\n            return self.get_value(node.value)\n        if isinstance(node, ast.BinOp) and isinstance(node.op, ast.Add):\n            left = self.get_value(node.left)\n            right = self.get_value(node.right)\n            if left is not None and right is not None:\n                return str(left) + str(right)\n        if isinstance(node, ast.Call):\n            func_name = ast.unparse(node.func)\n            if 'os.path.join' in func_name:\n                args = [self.get_value(arg) for arg in node.args]\n                return os.path.join(*[a for a in args if a is not None])\n            if 'str' in func_name and node.args:\n                return str(self.get_value(node.args[0]))\n        if isinstance(node, ast.Attribute):\n            val = self.get_value(node.value)\n            if val and (not isinstance(val, str) or '`' not in val):\n                return f\"{val}.{node.attr}\"\n        # Fallback with variable substitution\n        unparsed = ast.unparse(node)\n        for var_name, var_value in self.variables.items():\n            if var_value and isinstance(var_value, str):\n                unparsed = re.sub(r'\\b' + re.escape(var_name) + r'\\b', var_value, unparsed)\n        return f\"`{unparsed}`\"\n\n    def visit_Assign(self, node):\n        if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):\n            target_name = node.targets[0].id\n            self.variables[target_name] = self.get_value(node.value)\n            if isinstance(node.value, ast.Call) and 'TemporaryDirectory' in ast.unparse(node.value.func):\n                self.temp_dir_vars.add(target_name)\n        self.generic_visit(node)\n\n    def visit_Call(self, node):\n        func_name = ast.unparse(node.func)\n        handled = False\n\n        if 'subprocess.run' in func_name or 'subprocess.Popen' in func_name:\n            self.handle_ffmpeg_call(node)\n            handled = True\n        elif 'write_videofile' in func_name:\n            output_file = self.get_value(node.args[0])\n            self.add_morphism(f\"generates \u2192 \u25fb Video File (`{output_file}`)\")\n            self.add_morphism(f\"utilizes \u2192 \u25fb moviepy\", level=self.indent_level + 1)\n            handled = True\n        elif 'write' in func_name: # Generic file write\n            if node.args:\n                written_content = self.get_value(node.args[0])\n                self.add_morphism(f\"writes content \u2192 {written_content}\")\n            else:\n                self.add_morphism(f\"performs generic write action\")\n            handled = True\n        elif 'glob' in func_name:\n            pattern = self.get_value(node.args[0])\n            self.add_morphism(f\"reads \u2192 \u25fb File Sequence (matching `{pattern}`)\")\n            handled = True\n        elif 'json.dump' in func_name:\n            output_file_obj = self.get_value(node.args[1])\n            self.add_morphism(f\"writes \u2192 \u25fb JSON Data (to `{output_file_obj}`)\")\n            handled = True\n        elif 'json.load' in func_name:\n            input_file_obj = self.get_value(node.args[0])\n            self.add_morphism(f\"reads \u2192 \u25fb JSON Data (from `{input_file_obj}`)\")\n            handled = True\n\n        if not handled:\n            self.generic_visit(node)\n\n    def visit_For(self, node):\n        iterable = self.get_value(node.iter)\n        self.add_morphism(f\"for each item in `{iterable}`:\")\n        self.indent_level += 1\n        for sub_node in node.body:\n            self.visit(sub_node)\n        self.indent_level -= 1\n\n    def visit_With(self, node):\n        for item in node.items:\n            context_expr = self.get_value(item.context_expr)\n            if isinstance(item.optional_vars, ast.Name) and item.optional_vars.id in self.temp_dir_vars:\n                self.add_morphism(f\"manages \u2192 \u25fb Temporary Directory (`{context_expr}`)\")\n        \n        self.indent_level += 1\n        for sub_node in node.body:\n            self.visit(sub_node)\n        self.indent_level -= 1\n\n    def handle_ffmpeg_call(self, node):\n        try:\n            cmd_list = []\n            cmd_str = \"\"\n            cmd_list_node = node.args[0]\n            if isinstance(cmd_list_node, (ast.List, ast.Tuple)):\n                cmd_list = [self.get_value(e) for e in cmd_list_node.elts]\n                cmd_str = ' '.join(map(str, filter(None, cmd_list)))\n            else:\n                cmd_str = self.get_value(cmd_list_node)\n                cmd_list = cmd_str.replace('`', '').replace('{', ' ').replace('}', ' ').split()\n\n            self.add_morphism(f\"executes \u2192 \u25fb ffmpeg command\")\n            if '-i' in cmd_list:\n                try:\n                    i_index = cmd_list.index('-i') + 1\n                    input_file = cmd_list[i_index]\n                    self.add_morphism(f\"reads \u2192 \u25fb Input (`{input_file}`)\", level=2)\n                except (ValueError, IndexError): pass\n            if '-f concat' in cmd_str: self.add_morphism(f\"performs \u2192 \u25fb Lossless Concatenation\", level=2)\n            if '-c copy' in cmd_str: self.add_morphism(f\"using codec \u2192 `copy` (no re-encoding)\", level=2)\n            if '-an' in cmd_str: self.add_morphism(f\"removes \u2192 \u25fb Audio Stream (-an)\", level=2)\n            if cmd_list: self.add_morphism(f\"generates \u2192 \u25fb Output File (`{cmd_list[-1]}`)\", level=2)\n        except (IndexError, AttributeError, UnboundLocalError): self.add_morphism(\"executes complex ffmpeg command\")\n\n    def analyze(self):\n        with open(self.script_path, 'r', encoding='utf-8', errors='ignore') as f:\n            source = f.read()\n        try:\n            tree = ast.parse(source)\n            self.visit(tree)\n        except Exception as e:\n            self.model.append(f\"    \u2192 ERROR: Could not analyze script: {e}\")\n        return self.model\n\n# WorldModelGenerator re-categorizes the raw olog and synthesizes a final trace.\nclass WorldModelGenerator:\n    def __init__(self, olog_generator):\n        self.generator = olog_generator\n        self.phenotype = self.classify_phenotype()\n        self.canvas = []\n        self.creation = []\n        self.sequencing = []\n        self.conditioning = []\n        self.process_model()\n        self.trace = self.generate_trace()\n\n    def classify_phenotype(self):\n        morphisms_str = ' '.join(self.generator.model)\n        has_ffmpeg = 'ffmpeg' in morphisms_str\n        has_ffprobe = 'ffprobe' in morphisms_str\n        has_moviepy = 'moviepy' in morphisms_str or 'ImageSequenceClip' in morphisms_str\n        has_json = 'json.load' in morphisms_str or 'json.dump' in morphisms_str\n        has_concat = 'Lossless Concatenation' in morphisms_str\n        if has_moviepy: return \"Generative Assembler\"\n        if has_ffmpeg and has_ffprobe: return \"Conditional Processor & Assembler\"\n        if has_ffmpeg and has_concat: return \"Sequential Assembler\"\n        if has_json and not has_ffmpeg and not has_moviepy: return \"Metadata Manager & Analyst\"\n        if has_ffmpeg: return \"Conditional Processor & Assembler\"\n        if 'analyze' in self.generator.script_name or 'index' in self.generator.script_name: return \"Utility & Analysis\"\n        return \"Composite\"\n\n    def process_model(self):\n        for item in self.generator.model:\n            item = item.strip()\n            if '\u2192 reads' in item or 'Input (`' in item or 'json.load' in item: self.canvas.append(item)\n            elif '\u2192 generates' in item or '\u2192 writes' in item or 'json.dump' in item: self.creation.append(item)\n            elif '\u2192 for each' in item or 'sort' in item: self.sequencing.append(item)\n            elif '\u2192 performs' in item or '\u2192 using codec' in item or '\u2192 removes' in item or '\u2192 manages' in item or '\u2192 executes' in item or '\u2192 utilizes' in item: self.conditioning.append(item)\n\n    def generate_trace(self):\n        trace = {\n            \"genesis\": \"Unknown Source\", \"engine\": \"Unknown Engine\",\n            \"flow\": \"\", \"creation\": \"Unknown Artifact\"\n        }\n        conditioning_str = ' '.join(self.conditioning)\n\n        # Find primary Genesis (input)\n        for item in self.canvas:\n            if 'File Sequence' in item or 'glob' in item:\n                trace['genesis'] = item.replace('\u2192 reads \u2192 \u25fb ', '')\n                break\n        if trace['genesis'] == 'Unknown Source' and self.canvas:\n            trace['genesis'] = self.canvas[0].replace('\u2192 reads \u2192 ', '').replace('\u2192 ', '')\n\n        # Find primary Creation (output)\n        for item in self.creation:\n            if 'Video File' in item or any(ext in item for ext in ['.mp4', '.mov']):\n                trace['creation'] = item.replace('\u2192 generates \u2192 \u25fb ', '')\n                break\n        if trace['creation'] == 'Unknown Artifact' and self.creation:\n            trace['creation'] = self.creation[-1].replace('\u2192 generates \u2192 ', '').replace('\u2192 writes content \u2192 ', '').replace('\u2192 ', '')\n\n        # Determine Engine\n        if 'ffmpeg' in conditioning_str: trace['engine'] = 'ffmpeg'\n        elif 'moviepy' in conditioning_str: trace['engine'] = 'moviepy'\n        elif 'json' in ' '.join(self.canvas) + ' '.join(self.creation): trace['engine'] = 'JSON Processor'\n        else: trace['engine'] = 'Custom Python Logic'\n\n        # Build a narrative flow\n        flow_parts = [f\"Takes {trace['genesis']}\"]\n        sequencing_actions = [s for s in ['sorted', 'iterated'] if s in ' '.join(self.sequencing)]\n        if sequencing_actions: flow_parts.append(f\"which is then {', '.join(set(sequencing_actions))}\")\n\n        conditioning_actions = []\n        if 'Lossless Concatenation' in conditioning_str: conditioning_actions.append(\"concatenated losslessly\")\n        if 'codec' in conditioning_str and 'copy' not in conditioning_str: conditioning_actions.append(\"re-encoded\")\n        if 'removes \u2192 \u25fb Audio' in conditioning_str: conditioning_actions.append(\"stripped of audio\")\n        if conditioning_actions: flow_parts.append(f\"and {' and '.join(conditioning_actions)}\")\n\n        flow_parts.append(f\"using the {trace['engine']} engine to produce {trace['creation']}.\")\n        trace['flow'] = ' '.join(flow_parts)\n\n        # Clean up for readability\n        for k, v in trace.items():\n            if isinstance(v, str):\n                trace[k] = v.replace('`', '').replace('\u25fb ', '')\n\n        return trace\n\n    def to_markdown(self):\n        md = []\n        md.append(f'### {self.generator.script_path}')\n        md.append(f'**Phenotype:** {self.phenotype}\\\\n')\n        md.append('*   **World Trace Model (Final Image):**')\n        md.append(f'    *   **Genesis (Source):** {self.trace[\"genesis\"]}')\n        md.append(f'    *   **Engine:** {self.trace[\"engine\"]}')\n        md.append(f'    *   **Material Flow:** {self.trace[\"flow\"]}')\n        md.append(f'    *   **Creation (Artifact):** {self.trace[\"creation\"]}\\\\n')\n        md.append('*   **Decomposed Analysis:**')\n        if self.canvas: md.extend([f'    *   \u25fb **Canvas (Inputs):**'] + [f'        *   {c}' for c in self.canvas])\n        if self.creation: md.extend([f'    *   \u25fb **Creation (Outputs):**'] + [f'        *   {c}' for c in self.creation])\n        if self.sequencing: md.extend([f'    *   \u25fb **Sequencing (The Narrative):**'] + [f'        *   {s}' for s in self.sequencing])\n        if self.conditioning: md.extend([f'    *   \u25fb **Conditioning (The Style):**'] + [f'        *   {c}' for c in self.conditioning])\n        return \"\\\\n\".join(md)\n\n# --- Main Execution ---\ndef get_script_hash(script_path):\n    with open(script_path, 'r', encoding='utf-8', errors='ignore') as f:\n        return hashlib.sha256(f.read().encode()).hexdigest()\n\ndef find_assembly_scripts(root_dir):\n    root_path = Path(root_dir)\n    return [str(p) for p in root_path.rglob('*assembl*.py')]\n\ndef main():\n    project_root = '/Users/gaia/resurrecting atlantis'\n    output_md_file = os.path.join(project_root, 'world_model.md')\n    print(f\"Generating World Models for assembly scripts in: {project_root}\")\n\n    assembly_scripts = find_assembly_scripts(project_root)\n    unique_scripts = {key: val for key, val in {(os.path.basename(p), get_script_hash(p)): p for p in assembly_scripts}.items()}\n    print(f\"Found {len(unique_scripts)} unique scripts to model.\")\n\n    world_models = []\n    for script_path in sorted(unique_scripts.values()):\n        print(f\"  - Modeling {os.path.basename(script_path)}...\")\n        try:\n            olog_generator = OlogGenerator(script_path)\n            olog_generator.analyze()\n            world_model = WorldModelGenerator(olog_generator)\n            world_models.append(world_model)\n        except Exception as e:\n            print(f\"    ERROR: Could not process {script_path}: {e}\")\n\n    with open(output_md_file, 'w', encoding='utf-8') as f:\n        f.write(\"# World Models of Assembly Scripts\\n\\n\")\n        f.write(\"This document provides a semantic, media-centric model of each Python assembly script, detailing its recipe of inputs, outputs, sequencing, and styling.\\n\\n\")\n        f.write(\"---\\n\\n\")\n        for model in sorted(world_models, key=lambda m: m.generator.script_path):\n            f.write(model.to_markdown())\n            f.write('\\n\\n---\\n')\n\n    print(f\"\\nSuccessfully generated World Models at: {output_md_file}\")\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [],
    "imports": [
      "ast",
      "os",
      "hashlib",
      "pathlib",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "CAT/update_image_paths.py",
    "size": 3391,
    "lines": 95,
    "source": "import os\nimport re\nimport shutil\n\n# --- Configuration ---\nJSON_FILE_PATH = '/Users/gaia/resurrecting atlantis/CAT/storyboard.json'\nIMAGE_DIR_PATH = '/Users/gaia/resurrecting atlantis/CAT/WHISKER'\nBACKUP_FILE_PATH = JSON_FILE_PATH + '.bak'\n\ndef create_image_map(image_dir):\n    \"\"\"Scans the image directory and creates a map from ID to filename.\"\"\"\n    image_map = {}\n    try:\n        for filename in os.listdir(image_dir):\n            match = re.match(r'(WGY\\d+)', filename)\n            if match:\n                image_id = match.group(1)\n                image_map[image_id] = filename\n    except FileNotFoundError:\n        print(f\"Error: Image directory not found at {image_dir}\")\n        return None\n    print(f\"Found {len(image_map)} images to map.\")\n    return image_map\n\ndef update_storyboard():\n    \"\"\"Safely updates the storyboard file by rebuilding it object by object.\"\"\"\n    # 1. Create a backup\n    try:\n        shutil.copy(JSON_FILE_PATH, BACKUP_FILE_PATH)\n        print(f\"Successfully created backup: {BACKUP_FILE_PATH}\")\n    except FileNotFoundError:\n        print(f\"Error: Cannot create backup. Source file not found at {JSON_FILE_PATH}\")\n        return\n\n    # 2. Create image map\n    image_map = create_image_map(IMAGE_DIR_PATH)\n    if not image_map:\n        return\n\n    # 3. Read the original file content\n    with open(JSON_FILE_PATH, 'r') as f:\n        content = f.read()\n\n    # 4. Find all individual storyboard objects (non-greedy match between braces)\n    storyboard_objects = re.findall(r'({.*?})', content, re.DOTALL)\n    print(f\"Found {len(storyboard_objects)} objects in the storyboard file.\")\n\n    if not storyboard_objects:\n        print(\"Error: Could not find any storyboard objects in the file. Aborting.\")\n        return\n\n    updated_objects = []\n    update_count = 0\n    \n    # 5. Process each object\n    for obj_string in storyboard_objects:\n        id_match = re.search(r'\"id\":\\s*\"(WGY\\d+)\"', obj_string)\n        if not id_match:\n            updated_objects.append(obj_string) # Keep object as-is if no ID\n            continue\n\n        shot_id = id_match.group(1)\n        \n        if shot_id in image_map:\n            new_image_path = os.path.join(IMAGE_DIR_PATH, image_map[shot_id]).replace('\\\\', '/')\n            \n            # Replace the image_path value for this object\n            path_pattern = re.compile(r'(\"image_path\":\\s*).*?(,)', re.DOTALL)\n            replacement = rf'\\g<1>\"{new_image_path}\"\\2'\n            modified_obj, count = path_pattern.subn(replacement, obj_string)\n            \n            if count > 0:\n                updated_objects.append(modified_obj)\n                update_count += 1\n            else:\n                updated_objects.append(obj_string)\n                print(f\"Warning: Found ID {shot_id} but could not replace image_path.\")\n        else:\n            updated_objects.append(obj_string)\n\n    # 6. Rebuild the file content (Corrected multi-line assignment)\n    new_content = ('[\\n' +\n                   ',\\n'.join(updated_objects) +\n                   '\\n]')\n    \n    # 7. Write the new content back to the original file\n    with open(JSON_FILE_PATH, 'w') as f:\n        f.write(new_content)\n\n    print(f\"\\nProcess complete. Total objects updated: {update_count}/{len(storyboard_objects)}\")\n    print(f\"Successfully saved updated storyboard to {JSON_FILE_PATH}\")\n\nif __name__ == '__main__':\n    update_storyboard()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/CAT/storyboard.json",
      "/Users/gaia/resurrecting atlantis/CAT/storyboard.json",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER",
      "\\nProcess complete. Total objects updated: {update_count}/{len(storyboard_objects)}"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "CAT/cat_leg_generator.py",
    "size": 5235,
    "lines": 153,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCAT LEG Generator - Processes storyboard.json to create image cards.\n- For each entry in the storyboard, generates an image card.\n- Displays genome (S, I, C lines) using appropriate fonts for symbols.\n- Outputs cards into a 'CARDS' directory within the CAT project.\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport unicodedata\nimport re\n\n# --- Configuration ---\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nOUTPUT_DIR = os.path.join(BASE_DIR, 'CARDS')\nTIMELINE_PATH = os.path.join(BASE_DIR, 'storyboard.json')\n# Use the genome data generated for the CAT project\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(BASE_DIR, 'symbolic_genome_data.json')\n\n# --- Font Configuration ---\nFONT_PATH_REGULAR = \"/System/Library/Fonts/Menlo.ttc\"\nFONT_PATH_BOLD = \"/System/Library/Fonts/Supplemental/Arial Bold.ttf\"\n# Font for special symbols\nFONT_PATH_SYMBOLS = \"/System/Library/Fonts/Apple Symbols.ttf\"\n\n# --- slugify function ---\ndef slugify(value, allow_unicode=False):\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n\n# --- Main Functions ---\ndef load_timeline(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {file_path}\")\n        return []\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}\")\n        return []\n\ndef load_genome_data(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Genome data file not found at {file_path}\")\n        return {}\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}\")\n        return {}\n\ndef get_genome_for_poem(poem_title, genome_data):\n    for poem_entry in genome_data:\n        if poem_entry.get('poem') == poem_title:\n            return poem_entry.get('genome', {})\n    return {}\n\ndef render_card(entry, genome_data, output_dir):\n    # --- Get data from entry ---\n    image_path = entry.get('image_path')\n    entry_id = entry.get('id', 'UnknownID')\n    poem_title = entry.get('poem', 'UnknownPoem')\n    \n    # --- Create output directory for the poem ---\n    poem_output_dir = os.path.join(output_dir, slugify(poem_title))\n    os.makedirs(poem_output_dir, exist_ok=True)\n\n    # --- Load Image ---\n    try:\n        base_image = Image.open(image_path).convert('RGBA')\n    except FileNotFoundError:\n        print(f\"Error: Image not found for entry {entry_id}. Path: {image_path}\")\n        return\n\n    # --- Create a new image for the card ---\n    card_width = base_image.width + 400\n    card_height = base_image.height\n    card = Image.new('RGBA', (card_width, card_height), (255, 255, 255, 255))\n    card.paste(base_image, (0, 0))\n\n    draw = ImageDraw.Draw(card)\n    \n    # --- Load Fonts ---\n    try:\n        font_regular = ImageFont.truetype(FONT_PATH_REGULAR, 24)\n        font_bold = ImageFont.truetype(FONT_PATH_BOLD, 32)\n        font_symbols = ImageFont.truetype(FONT_PATH_SYMBOLS, 24)\n    except IOError:\n        print(\"Error: A font file was not found. Using default fonts.\")\n        font_regular = ImageFont.load_default()\n        font_bold = ImageFont.load_default()\n        font_symbols = ImageFont.load_default()\n\n    # --- Draw Text ---\n    text_x = base_image.width + 30\n    text_y = 30\n\n    # Poem Title\n    draw.text((text_x, text_y), poem_title, font=font_bold, fill='black')\n    text_y += 60\n\n    # Entry ID\n    draw.text((text_x, text_y), f\"ID: {entry_id}\", font=font_regular, fill='black')\n    text_y += 40\n\n    # Genome Data\n    poem_genome = get_genome_for_poem(poem_title, genome_data)\n    if poem_genome:\n        for key, value in poem_genome.items():\n            key_text = f\"{key}: \"\n            # Draw key with regular font\n            draw.text((text_x, text_y), key_text, font=font_regular, fill='gray')\n            # Calculate width of key to position the symbols\n            key_width = draw.textlength(key_text, font=font_regular)\n            # Draw symbols with symbol font\n            draw.text((text_x + key_width, text_y), value, font=font_symbols, fill='black')\n            text_y += 30\n    \n    # --- Save Card ---\n    output_filename = f\"{slugify(entry_id)}.png\"\n    output_path = os.path.join(poem_output_dir, output_filename)\n    card.save(output_path)\n    print(f\"Rendered card for {entry_id} -> {output_path}\")\n\ndef main():\n    print(\"Starting CAT LEG Generator...\")\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    timeline_data = load_timeline(TIMELINE_PATH)\n    genome_data = load_genome_data(SYMBOLIC_GENOME_DATA_PATH)\n\n    if not timeline_data or not genome_data:\n        print(\"Aborting due to missing data.\")\n        return\n\n    for entry in timeline_data:\n        render_card(entry, genome_data, OUTPUT_DIR)\n\n    print(\"\\nProcessing complete.\")\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      "storyboard.json",
      "symbolic_genome_data.json",
      "{slugify(entry_id)}.png",
      "/System/Library/Fonts/Menlo.ttc",
      "/System/Library/Fonts/Supplemental/Arial Bold.ttf",
      "/System/Library/Fonts/Apple Symbols.ttf"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "unicodedata",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CAT LEG Generator - Processes storyboard.json to create image cards.\n- For each entry in the storyboard, generates an image card.\n- Displays genome (S, I, C lines) using appropriate fonts for symbols.\n- Outputs cards into a 'CARDS' directory within the CAT project."
  },
  {
    "path": "CAT/leg_generator_v1.py",
    "size": 32671,
    "lines": 619,
    "source": "#!/usr/bin/env python3\n\"\"\"\nLEG Generator - Processes a timeline JSON file (e.g., ORDERED-TIMELINE-FIXED.json).\n- For each poem in the timeline, generates image cards for its entries.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Outputs cards into poem-specific subdirectories under a main output folder.\n- Adapted from CLAPPER61PRIMEPLUS.\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\nimport unicodedata # Added for slugify\n\n# --- slugify function ---\ndef slugify(value, allow_unicode=False):\n    \"\"\"\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n    dashes to single dashes. Remove characters that aren't alphanumerics,\n    underscores, or hyphens. Convert to lowercase. Also strip leading and\n    trailing whitespace, dashes, and underscores.\n    \"\"\"\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n\n# --- get_flexible_glyph function ---\ndef get_flexible_glyph(data_string, glyph_dict, default_glyph=\"?\"):\n    \"\"\"Gets a glyph from a dictionary, trying exact match first, then flexible match (ignoring suffix in parentheses).\"\"\"\n    if not data_string:\n        return default_glyph\n\n    # 1. Try exact match first\n    glyph = glyph_dict.get(data_string)\n    if glyph:\n        return glyph.strip()\n\n    # 2. Try flexible match (strip suffix and whitespace)\n    #    e.g., \"Autonomous Syntagma\" from data vs \"Autonomous Syntagma (AS)\" in dict\n    data_string_base = re.sub(r'\\s*\\(.*\\)$', '', data_string).strip()\n    \n    for key, val in glyph_dict.items():\n        key_base = re.sub(r'\\s*\\(.*\\)$', '', key).strip()\n        if key_base.lower() == data_string_base.lower():\n            return val.strip()\n            \n    return default_glyph\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60\nHEADER_HEIGHT = 200\nIMAGE_DISPLAY_HEIGHT = 576\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230)\n\n# Glyph Definitions\nSYNTAGMA_GLYPHS = {\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (CS)\": \"\u2756\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n# Path to the ordered timeline JSON file\nTIMELINE_PATH = \"/Users/gaia/resurrecting atlantis/DOG/ORDERED-TIMELINE-FIXED.json\"\n\n# Path to the symbolic genome data (assumed to be in the same directory as the 'scripts' folder, i.e., DOG/LEG/)\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), \"symbolic_genome_data.json\")\n\n# Base directory for all poem-specific output folders (will be created if it doesn't exist)\nOUTPUT_DIR_BASE = os.path.join(os.path.dirname(BASE_DIR), \"output_poems_sequenced\")\n\n# Base directory for finding source images (e.g., SH/, BL/, MR_IMAGES/)\n# This should point to the root of your 'resurrecting atlantis' project.\n# BASE_DIR is the directory of this script. os.path.join(BASE_DIR, \"..\", \"..\", \"..\") navigates up three levels.\n# For /Users/gaia/resurrecting atlantis/DOG/LEG/scripts, this becomes /Users/gaia/resurrecting atlantis/TIGER/\nBASE_IMAGE_DIR = os.path.normpath(os.path.join(BASE_DIR, \"..\", \"..\", \"..\", \"TIGER\"))\n\n# Note: Poem-specific output directories and the OUTPUT_DIR_BASE will be created in main() as needed.\n\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n    try:\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                try:\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    relative_path = entry.get('image_path')\n    if not relative_path:\n        print(f\"Warning: 'image_path' missing in entry: {entry.get('id', 'Unknown ID')}\")\n        return None\n\n    # Assume relative_path from JSON is directly usable relative to BASE_IMAGE_DIR\n    # e.g., if image_path is \"SH_IMAGES/foo.png\", and BASE_IMAGE_DIR is \"/Users/gaia/resurrecting atlantis/\",\n    # it will look for \"/Users/gaia/resurrecting atlantis/SH_IMAGES/foo.png\".\n    adjusted_relative_path = relative_path\n    full_image_path = os.path.join(BASE_IMAGE_DIR, adjusted_relative_path)\n    \n    if os.path.exists(full_image_path):\n        return os.path.normpath(full_image_path)\n    else:\n        # Fallback if TIGER/MR path not found, try original MR/ path directly under BASE_IMAGE_DIR\n        if relative_path.startswith(\"MR/\"):\n            original_mr_path = os.path.join(BASE_IMAGE_DIR, relative_path)\n            if os.path.exists(original_mr_path):\n                return os.path.normpath(original_mr_path)\n        \n        # Broader fallback search using entry ID (adapted from original clapper61prime)\n        entry_id_val = entry.get('id', '')\n        if entry_id_val:\n            search_dirs = [BASE_IMAGE_DIR] # General search in base image dir\n            if relative_path.startswith(\"MR/\"):\n                 # If it was an MR path, also specifically check TIGER/MR and TIGER folders\n                search_dirs.insert(0, os.path.join(BASE_IMAGE_DIR, 'TIGER', 'MR'))\n                search_dirs.insert(1, os.path.join(BASE_IMAGE_DIR, 'TIGER'))\n\n            for search_dir_base in search_dirs:\n                if os.path.exists(search_dir_base):\n                    for root, _, files in os.walk(search_dir_base):\n                        for file_name in files:\n                            if file_name.startswith(entry_id_val + '__') and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                                return os.path.normpath(os.path.join(root, file_name))\n                            # Check for exact filename match from image_path as well\n                            if os.path.basename(relative_path) == file_name:\n                                return os.path.normpath(os.path.join(root, file_name))\n        \n        print(f\"Error: Image not found for entry {entry.get('id', 'Unknown ID')}. Path attempted: {full_image_path}\")\n        return None\n\ndef load_genome_data_from_json(file_path):\n    genome_map_dict = {}\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems_data_list = json.load(f)\n        if not isinstance(poems_data_list, list):\n            print(f\"Error: Genome data in {file_path} is not a list. Found {type(poems_data_list)}.\")\n            return {}\n        for item in poems_data_list:\n            if isinstance(item, dict) and 'title' in item:\n                poem_name = item['title'].strip().lower()\n                genome_map_dict[poem_name] = {\n                    's_line': item.get('s_line', ''),\n                    'i_line': item.get('i_line', ''),\n                    'c_line': item.get('c_line', '')\n                }\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}.\")\n    return genome_map_dict\n\ndef get_original_char_pos(text, non_space_target_idx):\n    \"\"\"Given a text string and a 0-indexed target for non-space characters,\n    returns the original character index in the text string.\n    Returns -1 if non_space_target_idx is out of bounds.\n    \"\"\"\n    if non_space_target_idx < 0:\n        return -1\n    \n    current_non_space_count = 0\n    for original_idx, char in enumerate(text):\n        if char != ' ':\n            if current_non_space_count == non_space_target_idx:\n                return original_idx\n            current_non_space_count += 1\n    \n    # Target index not found (e.g., non_space_target_idx >= total non-space chars in text)\n    return -1\n\ndef get_non_space_char_index(text, original_char_pos):\n    count = -1\n    if original_char_pos >= len(text):\n        return -1\n    for i in range(original_char_pos + 1):\n        if text[i] != ' ':\n            count += 1\n    return count\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10\n    value_start_x = 10 + narrow_label_area_width\n    current_y_offset = 4\n    genome_section_y_on_img = current_y_offset\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70)\n    genome_content_x1_on_img = 10\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n    i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A') if genome_data_entry else 'I_LINE_N/A'\n    s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A') if genome_data_entry else 'S_LINE_N/A'\n    c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A') if genome_data_entry else 'C_LINE_N/A'\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n    frame_position = entry.get('frameNumber', 0)\n    highlight_index = int(frame_position) - 1 if isinstance(frame_position, (int, float)) and frame_position > 0 else -1\n    \n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    if len(genome_lines_to_render) * chosen_genome_line_height > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        if len(genome_lines_to_render) * chosen_genome_line_height > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    if genome_y_start < genome_section_y_on_img + 4: genome_y_start = genome_section_y_on_img + 4\n\n    max_render_width = BASE_WIDTH - 10 # Allow a small margin on the right\n    avg_char_width = chosen_genome_font.getlength(\"X\") if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(\"X\")[2] - chosen_genome_font.getbbox(\"X\")[0]\n    if avg_char_width <= 0: avg_char_width = 1\n    placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR) if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[2] - chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[0]\n    if placeholder_char_width <= 0: placeholder_char_width = 1\n\n    for line_idx, original_line_text in enumerate(genome_lines_to_render):\n        current_x = genome_content_x1_on_img\n        chars_that_fit_total_approx = int(max_render_width // avg_char_width)\n        if chars_that_fit_total_approx <= 0: chars_that_fit_total_approx = 1 # Avoid division by zero or negative\n        target_highlight_offset_in_view = chars_that_fit_total_approx // 4\n\n        display_start_index = 0\n        scrolled_from_left = False\n\n        if highlight_index >= 0: # Ensure we have a valid highlight_index\n            if len(original_line_text) <= chars_that_fit_total_approx:\n                # Line fits entirely, no scrolling needed\n                display_start_index = 0\n                scrolled_from_left = False\n            else:\n                # Line is longer than available space, scrolling is needed\n                actual_pos_of_highlight_char_in_string = get_original_char_pos(original_line_text, highlight_index)\n\n                if actual_pos_of_highlight_char_in_string != -1:\n                    scrolled_from_left = True\n                    # Default start: try to position the highlighted character (at actual_pos_of_highlight_char_in_string)\n                    # such that it appears target_highlight_offset_in_view characters from the left of the display window.\n                    # This means the display window should start target_highlight_offset_in_view characters *before* actual_pos_of_highlight_char_in_string.\n                    # Note: target_highlight_offset_in_view is a count of characters, not a direct string index offset.\n                    # This subtraction is an approximation but works with the pinning logic below.\n                    tentative_display_start_index = max(0, actual_pos_of_highlight_char_in_string - target_highlight_offset_in_view)\n                    \n                    # Adjust if scrolling near the end of the line\n                    # Ensure the window doesn't go past the end of the string\n                    if tentative_display_start_index + chars_that_fit_total_approx > len(original_line_text):\n                        display_start_index = max(0, len(original_line_text) - chars_that_fit_total_approx)\n                    else:\n                        display_start_index = tentative_display_start_index\n                else:\n                    # Highlight character is NOT found (highlight_index is out of bounds for this line's non-space chars)\n                    # Check if the line, despite no highlight, is too wide in pixels and needs to scroll to the end.\n                    actual_pixel_width_of_line = chosen_genome_font.getlength(original_line_text)\n                    # max_render_width is already defined earlier (BASE_WIDTH - 10)\n                    if actual_pixel_width_of_line <= max_render_width:\n                        display_start_index = 0 # Line fits, show from beginning\n                        scrolled_from_left = False\n                    else:\n                        # Line is too wide in pixels, scroll to show the end\n                        display_start_index = max(0, len(original_line_text) - chars_that_fit_total_approx)\n                        scrolled_from_left = display_start_index > 0\n        else: # No valid highlight_index (e.g. -1 from frame_position calculation), just show from the beginning\n            display_start_index = 0\n            scrolled_from_left = False\n            \n        # Prefix display has been removed to maximize space for genome characters.\n        # The scrolled_from_left flag is still determined above to correctly set display_start_index.\n        for idx_in_original in range(display_start_index, len(original_line_text)):\n            char_to_draw = original_line_text[idx_in_original]\n            char_width = chosen_genome_font.getlength(char_to_draw) if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(char_to_draw)[2] - chosen_genome_font.getbbox(char_to_draw)[0]\n            if char_width <=0: char_width = avg_char_width # Fallback if char_width is bad\n            \n            # Check if the current character fits\n            if current_x + char_width > max_render_width:\n                break # Stop drawing this line if the character doesn't fit\n\n            # Determine default color based on line index\n            if line_idx == 0: # I-line (corresponds to Image Type)\n                default_char_color = IMAGE_TYPE_COLOR\n            elif line_idx == 1: # S-line (corresponds to Syntagma Type)\n                default_char_color = SYNTAGMA_COLOR\n            elif line_idx == 2: # C-line (corresponds to Cineosis Function)\n                default_char_color = CINEOSIS_COLOR\n            else: # Fallback, though should not happen with 3 lines\n                default_char_color = LIGHT_GRAY # Fallback to a neutral color\n\n            char_color = default_char_color\n            \n            # Highlighted character is white, overriding line color\n            if char_to_draw != ' ': # Only check for highlight if not a space\n                current_char_overall_non_space_idx = get_non_space_char_index(original_line_text, idx_in_original)\n                if current_char_overall_non_space_idx == highlight_index:\n                    char_color = AMBER\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n        drawn_width = current_x - genome_content_x1_on_img\n        remaining_line_width = max_render_width - drawn_width\n        if remaining_line_width > 0 and placeholder_char_width > 0:\n            num_padding_chars = int(remaining_line_width // placeholder_char_width)\n            if num_padding_chars > 0:\n                draw.text((current_x, genome_y_start), PLACEHOLDER_CHAR * num_padding_chars, font=chosen_genome_font, fill=PLACEHOLDER_COLOR)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 4\n    header_content_y = current_y_offset + 4\n    top_row_internal_height = 30\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position_val = entry.get('frameNumber', 0)\n    frame_total_val = entry.get('totalFrames', 0)\n    frame_text = f\"FRAME: ({frame_position_val}/{frame_total_val})\"\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name[:25]}\"\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    current_y_offset += top_row_internal_height\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8\n    initial_content_row_y = current_y_offset\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = get_flexible_glyph(image_type_str_raw, IMAGE_TYPE_GLYPHS)\n    line_content = entry.get('content', '---')\n    glyph_column_x = 10\n    draw.text((glyph_column_x, initial_content_row_y), image_glyph, font=chosen_genome_font, fill=AMBER)\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_text_width_bbox = text_font.getbbox(image_type_str_raw)\n    image_text_width = image_text_width_bbox[2] - image_text_width_bbox[0]\n    line_field_x_start = max(int(BASE_WIDTH * 0.35), min(value_start_x + image_text_width + 30, int(BASE_WIDTH * 0.50)))\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width_bbox = header_font.getbbox(line_prefix)\n    line_prefix_width = line_prefix_width_bbox[2] - line_prefix_width_bbox[0]\n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    ONE_TEXT_LINE_HEIGHT = (text_font.getbbox(\"Ay\")[3] - text_font.getbbox(\"Ay\")[1] + 4) if hasattr(text_font, 'getbbox') else 16\n    current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT) if line_content.strip() and line_content.strip() != '---' else initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    current_y_offset += 4\n    divider_start_x = glyph_column_x - 4 if glyph_column_x > 4 else 6\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = get_flexible_glyph(syntagma_type_str_raw, SYNTAGMA_GLYPHS)\n    draw.text((glyph_column_x, current_y_offset), syntagma_glyph, font=chosen_genome_font, fill=AMBER)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = get_flexible_glyph(cineosis_func_str_raw, CINEOSIS_FUNCTION_GLYPHS)\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=chosen_genome_font, fill=AMBER)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n    img.save(output_path)\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name:\n            poem_frames[poem_name] = poem_frames.get(poem_name, 0) + 1\n    processed_data = []\n    current_poem_entry_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        new_entry = entry.copy()\n        if poem_name:\n            current_poem_entry_count[poem_name] = current_poem_entry_count.get(poem_name, 0) + 1\n            new_entry['frameNumber'] = entry.get('frameNumber', current_poem_entry_count[poem_name]) # Use existing or assign\n            new_entry['totalFrames'] = entry.get('totalFrames', poem_frames.get(poem_name, 0))\n        else:\n            new_entry['frameNumber'] = entry.get('frameNumber', 0)\n            new_entry['totalFrames'] = entry.get('totalFrames', 0)\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting LEG Generator to process all poems from: {TIMELINE_PATH}\")\n    print(f\"Base output directory for poem sequences: {OUTPUT_DIR_BASE}\")\n    print(f\"Symbolic genome data from: {SYMBOLIC_GENOME_DATA_PATH}\")\n    print(f\"Base image directory: {BASE_IMAGE_DIR}\")\n\n    genome_map = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    if not genome_map:\n        print(f\"Warning: No genome data loaded from {SYMBOLIC_GENOME_DATA_PATH}. Proceeding without genome data on cards.\")\n        # genome_map will be an empty dict or None, render_card should handle this gracefully.\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16),\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12),\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data_raw = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    # calculate_frame_counts might add per-poem frame numbers if enhanced.\n    # For now, assume it's a pass-through or provides data main() can use.\n    all_entries_processed = calculate_frame_counts(timeline_data_raw)\n\n    if not all_entries_processed:\n        print(\"No timeline entries found or processed. Exiting.\")\n        return\n\n    total_cards_rendered_overall = 0\n    poems_processed_count = 0\n    \n    # Group entries by poem first to process them together\n    entries_by_poem = {}\n    for entry in all_entries_processed:\n        poem_name = entry.get('poem', 'UnknownPoem').strip()\n        if not poem_name: poem_name = 'UnknownPoem' # Ensure there's always a poem name\n        entries_by_poem.setdefault(poem_name, []).append(entry)\n\n    # Create the base output directory if it doesn't exist\n    os.makedirs(OUTPUT_DIR_BASE, exist_ok=True)\n\n    for poem_name, entries_for_poem in entries_by_poem.items():\n        poems_processed_count += 1\n        if poem_name != \"Bloodline\":\n            print(f\"Skipping poem: '{poem_name}' (not Bloodline)\") # Optional: for verbosity\n            continue\n        poem_slug = slugify(poem_name)\n        current_poem_output_dir = os.path.join(OUTPUT_DIR_BASE, poem_slug)\n        os.makedirs(current_poem_output_dir, exist_ok=True)\n\n        print(f\"\\nProcessing poem: '{poem_name}' ({len(entries_for_poem)} entries) -> Outputting to: {current_poem_output_dir}\")\n        \n        def get_sort_key(e):\n            pfn = e.get('poem_frame_number') # Per-poem frame number from calculate_frame_counts\n            fn = e.get('frameNumber') # Original frame number\n            if isinstance(pfn, (int, float)): return pfn\n            if isinstance(fn, (int, float)): return fn\n            if isinstance(fn, str) and fn.isdigit(): return int(fn)\n            return float('inf') # Entries with no valid frame number go last\n\n        entries_for_poem.sort(key=get_sort_key)\n        if entries_for_poem: # If there are entries for this poem\n            pass # Test mode logic was previously here\n            # entries_for_poem = entries_for_poem[:1] # Keep only the first one for the test run\n\n        cards_rendered_for_poem = 0\n        for i, entry_data in enumerate(entries_for_poem):\n            entry_id = entry_data.get('id', f'{poem_slug}_unknown-id_{i+1}')\n            \n            frame_num_val = entry_data.get('poem_frame_number')\n            if not isinstance(frame_num_val, (int, float)):\n                frame_num_val = entry_data.get('frameNumber')\n                if not (isinstance(frame_num_val, (int, float)) or (isinstance(frame_num_val, str) and frame_num_val.isdigit())):\n                    frame_num_val = i + 1 # Fallback to simple enumeration\n            \n            frame_num_str = str(int(frame_num_val) if isinstance(frame_num_val, (int,float)) else frame_num_val).zfill(4)\n\n            output_filename = f\"frame_{frame_num_str}.png\"\n            output_path = os.path.join(current_poem_output_dir, output_filename)\n            \n            print(f\"  Rendering card for: {entry_id} (Frame: {frame_num_str}) -> {output_filename}\")\n            try:\n                render_card(entry_data, output_path, fonts, genome_map)\n                cards_rendered_for_poem += 1\n            except Exception as e:\n                print(f\"    ERROR rendering card for {entry_id} (Path: {output_path}): {e}\")\n        \n        print(f\"  Finished poem '{poem_name}'. Cards rendered for this poem: {cards_rendered_for_poem}\")\n        total_cards_rendered_overall += cards_rendered_for_poem\n\n    print(f\"\\n--- Script Finished ---\")\n    print(f\"Total poems processed: {poems_processed_count}\")\n    print(f\"Total cards rendered overall: {total_cards_rendered_overall}\")\n    print(f\"Output saved in subdirectories under: {OUTPUT_DIR_BASE}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/DOG/ORDERED-TIMELINE-FIXED.json",
      "symbolic_genome_data.json",
      "SH_IMAGES/foo.png",
      "/Users/gaia/resurrecting atlantis/SH_IMAGES/foo.png",
      "frame_{frame_num_str}.png",
      "\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if ",
      "/Users/gaia/resurrecting atlantis/DOG/ORDERED-TIMELINE-FIXED.json",
      " folder, i.e., DOG/LEG/)\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), ",
      ")\n\n# Base directory for finding source images (e.g., SH/, BL/, MR_IMAGES/)\n# This should point to the root of your ",
      ") navigates up three levels.\n# For /Users/gaia/resurrecting atlantis/DOG/LEG/scripts, this becomes /Users/gaia/resurrecting atlantis/TIGER/\nBASE_IMAGE_DIR = os.path.normpath(os.path.join(BASE_DIR, ",
      "SH_IMAGES/foo.png",
      "/Users/gaia/resurrecting atlantis/",
      "/Users/gaia/resurrecting atlantis/SH_IMAGES/foo.png",
      "):\n                 # If it was an MR path, also specifically check TIGER/MR and TIGER folders\n                search_dirs.insert(0, os.path.join(BASE_IMAGE_DIR, ",
      "I_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    if genome_y_start < genome_section_y_on_img + 4: genome_y_start = genome_section_y_on_img + 4\n\n    max_render_width = BASE_WIDTH - 10 # Allow a small margin on the right\n    avg_char_width = chosen_genome_font.getlength(",
      ") else chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[2] - chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[0]\n    if placeholder_char_width <= 0: placeholder_char_width = 1\n\n    for line_idx, original_line_text in enumerate(genome_lines_to_render):\n        current_x = genome_content_x1_on_img\n        chars_that_fit_total_approx = int(max_render_width // avg_char_width)\n        if chars_that_fit_total_approx <= 0: chars_that_fit_total_approx = 1 # Avoid division by zero or negative\n        target_highlight_offset_in_view = chars_that_fit_total_approx // 4\n\n        display_start_index = 0\n        scrolled_from_left = False\n\n        if highlight_index >= 0: # Ensure we have a valid highlight_index\n            if len(original_line_text) <= chars_that_fit_total_approx:\n                # Line fits entirely, no scrolling needed\n                display_start_index = 0\n                scrolled_from_left = False\n            else:\n                # Line is longer than available space, scrolling is needed\n                actual_pos_of_highlight_char_in_string = get_original_char_pos(original_line_text, highlight_index)\n\n                if actual_pos_of_highlight_char_in_string != -1:\n                    scrolled_from_left = True\n                    # Default start: try to position the highlighted character (at actual_pos_of_highlight_char_in_string)\n                    # such that it appears target_highlight_offset_in_view characters from the left of the display window.\n                    # This means the display window should start target_highlight_offset_in_view characters *before* actual_pos_of_highlight_char_in_string.\n                    # Note: target_highlight_offset_in_view is a count of characters, not a direct string index offset.\n                    # This subtraction is an approximation but works with the pinning logic below.\n                    tentative_display_start_index = max(0, actual_pos_of_highlight_char_in_string - target_highlight_offset_in_view)\n                    \n                    # Adjust if scrolling near the end of the line\n                    # Ensure the window doesn",
      ": # Only check for highlight if not a space\n                current_char_overall_non_space_idx = get_non_space_char_index(original_line_text, idx_in_original)\n                if current_char_overall_non_space_idx == highlight_index:\n                    char_color = AMBER\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n        drawn_width = current_x - genome_content_x1_on_img\n        remaining_line_width = max_render_width - drawn_width\n        if remaining_line_width > 0 and placeholder_char_width > 0:\n            num_padding_chars = int(remaining_line_width // placeholder_char_width)\n            if num_padding_chars > 0:\n                draw.text((current_x, genome_y_start), PLACEHOLDER_CHAR * num_padding_chars, font=chosen_genome_font, fill=PLACEHOLDER_COLOR)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 4\n    header_content_y = current_y_offset + 4\n    top_row_internal_height = 30\n    id_label_text = ",
      "FRAME: ({frame_position_val}/{frame_total_val})",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random",
      "unicodedata"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "LEG Generator - Processes a timeline JSON file (e.g., ORDERED-TIMELINE-FIXED.json).\n- For each poem in the timeline, generates image cards for its entries.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Outputs cards into poem-specific subdirectories under a main output folder.\n- Adapted from CLAPPER61PRIMEPLUS."
  },
  {
    "path": "CAT/cutter.py",
    "size": 3596,
    "lines": 83,
    "source": "import fitz # PyMuPDF\nimport math\n\ndef generate_timing_analysis(pdf_path, x_threshold=280, wpm=150):\n    \"\"\"\n    Analyzes a two-column PDF to estimate the reading time for left-column text\n    associated with each right-column scene description (shot).\n\n    Args:\n        pdf_path (str): Path to the PDF file.\n        x_threshold (int): The x-coordinate separating left and right columns.\n        wpm (int): Words per minute, for calculating reading time.\n\n    Returns:\n        A list of strings, formatted in Markdown, ready to be written to a file.\n    \"\"\"\n    output_lines = []\n    try:\n        doc = fitz.open(pdf_path)\n        output_lines.append(f\"# Shot Timing Analysis for {pdf_path.split('/')[-1]}\\n\")\n        output_lines.append(f\"_Analysis based on a reading speed of {wpm} WPM._\\n\")\n\n        for page_num in range(doc.page_count):\n            output_lines.append(f\"\\n## ===== Page {page_num + 1} =====\\n\")\n            page = doc.load_page(page_num)\n            blocks = sorted(page.get_text(\"blocks\"), key=lambda b: (b[1], b[0]))\n\n            current_poetry = []\n            for block in blocks:\n                x0, _, _, _, text, _, _ = block\n                clean_text = text.strip()\n                if not clean_text:\n                    continue\n\n                if x0 < x_threshold: # It's poetry (left column)\n                    current_poetry.append(clean_text)\n                else: # It's a shot description (right column)\n                    # First, process any preceding poetry for this shot\n                    if current_poetry:\n                        poetry_text = \"\\n\".join(current_poetry)\n                        word_count = len(poetry_text.split())\n                        duration = (word_count / wpm) * 60\n                        \n                        output_lines.append(\"\\n---\")\n                        output_lines.append(\"**POETRY (VOICEOVER):**\")\n                        output_lines.append(f\"> {poetry_text.replace('\\n', ' ')}\")\n                        output_lines.append(f\"**ANALYSIS:** `{word_count} words | ~{duration:.1f} seconds`\")\n                        current_poetry = [] # Reset for the next group\n\n                    # Now, add the shot description\n                    output_lines.append(\"\\n**SHOT DESCRIPTION:**\")\n                    output_lines.append(f\"> {clean_text.replace('\\n', ' ')}\")\n            \n            # Handle any trailing poetry at the end of the page\n            if current_poetry:\n                poetry_text = \"\\n\".join(current_poetry)\n                word_count = len(poetry_text.split())\n                duration = (word_count / wpm) * 60\n                output_lines.append(\"\\n---\")\n                output_lines.append(\"**POETRY (VOICEOVER):**\")\n                output_lines.append(f\"> {poetry_text.replace('\\n', ' ')}\")\n                output_lines.append(f\"**ANALYSIS:** `{word_count} words | ~{duration:.1f} seconds`\")\n\n        doc.close()\n        return output_lines\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    pdf_file_path = \"Where You Go When You Leave (Assembly_ R5-26-25) (1).pdf\"\n    output_filename = \"shot_timing_analysis.md\"\n    \n    print(f\"Starting timing analysis for '{pdf_file_path}'...\")\n    analysis_results = generate_timing_analysis(pdf_file_path, wpm=150)\n\n    if analysis_results:\n        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"\\n\".join(analysis_results))\n        print(f\"\\nAnalysis complete. Results saved to {output_filename}\")\n    else:\n        print(\"\\nAnalysis failed.\")",
    "file_references": [
      ".join(current_poetry)\n                        word_count = len(poetry_text.split())\n                        duration = (word_count / wpm) * 60\n                        \n                        output_lines.append(",
      ".join(current_poetry)\n                word_count = len(poetry_text.split())\n                duration = (word_count / wpm) * 60\n                output_lines.append("
    ],
    "subprocess_calls": [],
    "imports": [
      "fitz",
      "math"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "CAT/generate_cat_genome.py",
    "size": 3094,
    "lines": 94,
    "source": "#!/usr/bin/env python3\n\"\"\"\nGenerates symbolic_genome_data.json for the CAT project using a specific glyph map.\n\nThis script reads storyboard.json, maps the syntagmaType, imageType, and\ncineosisFunction fields for each entry to their corresponding Unicode symbols,\nand then groups this genome data by poem.\n\"\"\"\n\nimport json\nimport os\nfrom collections import defaultdict\n\n# --- Configuration ---\nINPUT_FILE = '/Users/gaia/resurrecting atlantis/CAT/storyboard.json'\nOUTPUT_FILE = '/Users/gaia/resurrecting atlantis/CAT/symbolic_genome_data.json'\n\n# --- Symbol Mapping ---\nSYNTAGMA_MAP = {\n    'Autonomous Syntagma (AS)': '\u2591',\n    'Chronological Syntagma (CS)': '\u2592',\n    'Crystal Syntagma (XS)': '\u2756',\n    'Descriptive Syntagma (DS)': '\u259e',\n    'Flashback Syntagma (FS)': '\u2599',\n    'Thematic Montage (TM)': '\u2588',\n}\n\nIMAGE_TYPE_MAP = {\n    'Action-Image': '\u25ba',\n    'Affection-Image': '\u2639',\n    'Crystal-Image': '\u25c8',\n    'Descriptive Image': '\u263c',\n    'Opsign': '\u25ce',\n    'Perception-Image': '\u2691',\n    'Recollection-Image': '\u2302',\n    'Sonsign': '\u266c',\n    'Thematic Montage': '\u2263',\n}\n\nCINEOSIS_MAP = {\n    'Aural-Echo Extension': '\u266a',\n    'Causal Motion Trigger': '\u2794',\n    'Emotion Relay': '\u2765',\n    'Event Pause Invocation': '\u275a',\n    'Memory Storage Retrieval': '\u21bb',\n    'Mood Environment Stabilizer': '\u25ff',\n    'Narrative Modifier': '\u2726',\n    'Subjective Frame Recalibration': '\u25a1',\n    'Temporal Reflection Loop': '\u2318',\n}\n\ndef get_symbol(value, mapping):\n    \"\"\"Returns the corresponding symbol from the mapping, or '?' if not found.\"\"\"\n    return mapping.get(value, '?')\n\ndef generate_genome_data():\n    \"\"\"Generates the genome data from the storyboard file using symbol maps.\"\"\"\n    try:\n        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n            storyboard_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Input file not found at {INPUT_FILE}\")\n        return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {INPUT_FILE}\")\n        return\n\n    poems_genome = defaultdict(lambda: {'S': '', 'I': '', 'C': ''})\n\n    for entry in storyboard_data:\n        poem_title = entry.get('poem')\n        if not poem_title:\n            continue\n\n        syntagma_type = entry.get('syntagmaType', '')\n        image_type = entry.get('imageType', '')\n        cineosis_function = entry.get('cineosisFunction', '')\n\n        poems_genome[poem_title]['S'] += get_symbol(syntagma_type, SYNTAGMA_MAP)\n        poems_genome[poem_title]['I'] += get_symbol(image_type, IMAGE_TYPE_MAP)\n        poems_genome[poem_title]['C'] += get_symbol(cineosis_function, CINEOSIS_MAP)\n\n    output_data = [{'poem': poem, 'genome': genome} for poem, genome in poems_genome.items()]\n\n    try:\n        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n            json.dump(output_data, f, indent=4, ensure_ascii=False)\n        print(f\"Successfully generated symbolic genome data to {OUTPUT_FILE}\")\n    except IOError as e:\n        print(f\"Error writing to file {OUTPUT_FILE}: {e}\")\n\nif __name__ == '__main__':\n    generate_genome_data()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/CAT/storyboard.json",
      "/Users/gaia/resurrecting atlantis/CAT/symbolic_genome_data.json",
      "/Users/gaia/resurrecting atlantis/CAT/storyboard.json",
      "/Users/gaia/resurrecting atlantis/CAT/symbolic_genome_data.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "collections"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Generates symbolic_genome_data.json for the CAT project using a specific glyph map.\n\nThis script reads storyboard.json, maps the syntagmaType, imageType, and\ncineosisFunction fields for each entry to their corresponding Unicode symbols,\nand then groups this genome data by poem."
  },
  {
    "path": "CAT/prompt_formatter.py",
    "size": 1857,
    "lines": 51,
    "source": "import re\n\ndef format_prompts_individually(input_file, output_file):\n    \"\"\"\n    Reads a file containing Python-like dictionary structures (not valid JSON),\n    extracts 'id' and 'full_prompt' using regex, formats them,\n    and writes each prompt to a new line in a text file.\n    \"\"\"\n    try:\n        with open(input_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n    except FileNotFoundError:\n        print(f\"Error: Input file not found at '{input_file}'\")\n        return\n\n    # Regex to find the id and full_prompt fields\n    id_pattern = re.compile(r'\"id\":\\s*\"(.*?)\"')\n    prompt_pattern = re.compile(r'\"full_prompt\":\\s*\"(.*?)\"')\n\n    ids = id_pattern.findall(content)\n    full_prompts = prompt_pattern.findall(content)\n\n    if not ids or not full_prompts or len(ids) != len(full_prompts):\n        print(\"Error: Could not find matching 'id' and 'full_prompt' pairs.\")\n        print(f\"Found {len(ids)} IDs and {len(full_prompts)} prompts.\")\n        return\n\n    all_prompts = []\n    for i in range(len(ids)):\n        prompt_id = ids[i]\n        full_prompt = full_prompts[i]\n        \n        # Remove commas and create the final prompt string\n        cleaned_prompt = full_prompt.replace(',', '')\n        final_prompt = f\"{prompt_id} {cleaned_prompt} --ar 16:9\"\n        all_prompts.append(final_prompt)\n\n    # Write each prompt to a new line in the output file\n    with open(output_file, 'w', encoding='utf-8') as f:\n        for prompt in all_prompts:\n            f.write(prompt + '\\n')\n            \n    print(f\"Successfully formatted and saved {len(all_prompts)} individual prompts.\")\n    print(f\"Output saved to '{output_file}'\")\n\n\nif __name__ == \"__main__\":\n    input_pseudo_json_file = \"storyboard-sequence.json\"\n    output_txt_file = \"prompts_individual.txt\"\n    format_prompts_individually(input_pseudo_json_file, output_txt_file)\n",
    "file_references": [
      "storyboard-sequence.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "CAT/cat_leg_generator_v2.py",
    "size": 14706,
    "lines": 326,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCAT LEG Generator V3 - Final refinement pass.\nProcesses storyboard.json to create highly stylized image cards that are\nvisually identical to the DOG project's output.\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport re\n\n# --- Configuration ---\n# Directories\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nOUTPUT_DIR_BASE = os.path.join(BASE_DIR, \"CARDS_V3\") # New directory for this version\n\n# File Paths\nTIMELINE_PATH = os.path.join(BASE_DIR, \"storyboard.json\")\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(BASE_DIR, \"symbolic_genome_data.json\")\n\n# --- Card Layout and Style ---\n# Dimensions\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60\nHEADER_HEIGHT = 200 # Increased height to accommodate all fields\nIMAGE_DISPLAY_HEIGHT = 576\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors - Meticulously matched to the DOG project's style\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\nSYNTAGMA_COLOR = (100, 200, 255)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Light Blue\nCINEOSIS_COLOR = (230, 150, 255) # Light Purple\nSTYLE_COLOR = (150, 255, 180) # A light green for style conditioning\nLINE_COLOR = AMBER\nHIGHLIGHT_COLOR = (45, 45, 45) # Will be unused for genome highlighting\nDIVIDER_COLOR = (60, 60, 60)\nEKPHRASIS_COLOR = (255, 191, 0) # This is AMBER, used for highlighted genome glyph\nSTYLE_TEXT_COLOR = (150, 255, 180)\n\n# Genome Line Specific Colors (matching ELEPHANT)\nI_LINE_COLOR = (255, 100, 100) # Red-ish\nS_LINE_COLOR = (100, 100, 255) # Blue-ish\nC_LINE_COLOR = (100, 255, 100) # Green-ish\n\n# Fonts\ntry:\n    FONT_REGULAR = ImageFont.truetype(\"/System/Library/Fonts/Menlo.ttc\", 14)\n    # The DOG project uses a bold font for the type descriptions.\n    FONT_BOLD = ImageFont.truetype(\"/System/Library/Fonts/Menlo.ttc\", 14, index=1) # Attempt to get bold from TTC\n    FONT_SYMBOLS = ImageFont.truetype(\"Apple Symbols\", 20)\n    FONT_GENOME = ImageFont.truetype(\"Menlo\", 12)\n    FONT_STYLE_TEXT = ImageFont.truetype(\"Menlo\", 16)  # For styleConditioning, Ekphrasis, matching ELEPHANT\nexcept IOError:\n    # Fallback for bold font\n    try:\n        FONT_BOLD = ImageFont.truetype(\"/System/Library/Fonts/Supplemental/Arial Bold.ttf\", 14)\n    except IOError:\n        print(\"Error: Required fonts not found. Using regular for bold.\")\n        FONT_BOLD = FONT_REGULAR\n\n# Glyph Mappings\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\", \"Chronological Syntagma (CS)\": \"\u2592\", \"Crystal Syntagma (XS)\": \"\u2756\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\", \"Flashback Syntagma (FS)\": \"\u2599\", \"Thematic Montage (TM)\": \"\u2588\",\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\", \"Affection-Image\": \"\u2639\", \"Crystal-Image\": \"\u25c8\", \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\", \"Perception-Image\": \"\u2691\", \"Recollection-Image\": \"\u2302\", \"Sonsign\": \"\u266c\", \"Thematic Montage\": \"\u2263\",\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\", \"Causal Motion Trigger\": \"\u2794\", \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\", \"Memory Storage Retrieval\": \"\u21bb\", \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\", \"Subjective Frame Recalibration\": \"\u25a1\", \"Temporal Reflection Loop\": \"\u2318\",\n}\n\n# --- Helper Functions ---\ndef slugify(value):\n    value = re.sub(r'[^\\w\\s-]', '', value).strip().lower()\n    value = re.sub(r'[-\\s]+', '-', value)\n    return value\n\ndef get_flexible_glyph(data_string, glyph_dict, default_glyph=\"?\"):\n    if not data_string: return default_glyph\n    if data_string in glyph_dict: return glyph_dict[data_string].strip()\n    data_string_base = re.sub(r'\\s*\\(.*\\)$', '', data_string).strip()\n    for key, val in glyph_dict.items():\n        key_base = re.sub(r'\\s*\\(.*\\)$', '', key).strip()\n        if key_base.lower() == data_string_base.lower():\n            return val.strip()\n    return default_glyph\n\ndef draw_text_with_wrapping(draw, text, position, font, color, max_width):\n    if not text or not text.strip(): # Check if text is None, empty, or only whitespace\n        return position[1]\n\n    x, current_y_baseline = position # Assuming position is (x, baseline_y_for_first_line)\n    \n    try:\n        ascent, descent = font.getmetrics()\n        font_native_height = ascent + descent # Full height of the font's glyphs\n    except AttributeError: # Fallback for older Pillow versions\n        bbox = font.getbbox(\"Ag\") # Using \"Ag\" to capture typical ascent and descent\n        font_native_height = bbox[3] - bbox[1]\n        if font_native_height <= 0: # Further fallback if bbox is unusual\n            font_native_height = font.size # Approximate with specified font size\n\n    # line_spacing_from_prev_baseline is the extra space between the bottom of one line's glyphs and top of next, effectively leading\n    # For baseline-to-baseline, it's font_native_height + leading.\n    # ELEPHANT uses +4, which seems to be added to the full glyph height for next baseline.\n    leading = 4 \n    effective_baseline_to_baseline_height = font_native_height + leading\n\n    lines_to_draw = []\n    words = text.split(' ')\n    current_line_text_content = \"\"\n\n    for word in words:\n        if not current_line_text_content: # If current line is empty, start with this word\n            current_line_text_content = word\n        else:\n            # Try appending the new word with a space\n            potential_line = current_line_text_content + \" \" + word\n            if font.getlength(potential_line) <= max_width:\n                current_line_text_content = potential_line # Word fits, add to current line\n            else:\n                # Word does not fit. Finalize current line.\n                lines_to_draw.append(current_line_text_content)\n                # Start a new line with the current word.\n                current_line_text_content = word\n    \n    # Add the last accumulated line (if any)\n    if current_line_text_content:\n        lines_to_draw.append(current_line_text_content)\n\n    if not lines_to_draw: # If, after processing, no lines are formed (e.g., input was \" \")\n        return position[1]\n\n    # Draw the processed lines\n    for i, line_text in enumerate(lines_to_draw):\n        if i > 0: # For lines after the first, advance y baseline\n            current_y_baseline += effective_baseline_to_baseline_height\n        draw.text((x, current_y_baseline), line_text, font=font, fill=color)\n    \n    # Return the y-coordinate for the baseline of the *next* logical line of text\n    return current_y_baseline + effective_baseline_to_baseline_height\n\ndef render_card(entry, frame_num, total_frames, genome_data):\n    # --- Data Extraction ---\n    poem_title = entry.get(\"poem\", \"Untitled\")\n    entry_id = entry.get(\"id\", \"N/A\")\n    timestamp = entry.get(\"timestamp\", \"00:00:00\")\n    image_path = entry.get(\"image_path\")\n    content_line = entry.get(\"content\", \"\")\n    syntagma_type = entry.get(\"syntagmaType\", \"\")\n    image_type = entry.get(\"imageType\", \"\")\n    cineosis_function = entry.get(\"cineosisFunction\", \"\")\n    operative_ekphrasis = entry.get(\"operativeEkphrasis\", \"\")\n    style_conditioning = entry.get(\"styleConditioning\", \"\")\n\n    if not image_path or not os.path.exists(image_path):\n        print(f\"Skipping {entry_id}: Image path '{image_path}' not found.\")\n        return\n\n    # --- Load Image & Create Card ---\n    with Image.open(image_path) as img:\n        img = img.convert(\"RGB\").resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n\n    card = Image.new(\"RGB\", (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(card)\n\n    # --- 1. Genome Header ---\n    y_offset = 0\n    poem_genome = next((p for p in genome_data if p.get('title', '').lower() == poem_title.lower()), None)\n    if poem_genome:\n        draw.rectangle([0, 0, BASE_WIDTH, GENOME_REPORT_HEIGHT], fill=(10, 10, 10))\n        \n        i_line = poem_genome.get('i_line', '')\n        s_line = poem_genome.get('s_line', '')\n        c_line = poem_genome.get('c_line', '')\n\n        i_glyphs = i_line.split()\n        s_glyphs = s_line.split()\n        c_glyphs = c_line.split()\n        \n        # Desired order: I, S, C\n        genome_lines_ordered = [i_glyphs, s_glyphs, c_glyphs]\n        genome_line_base_colors = [I_LINE_COLOR, S_LINE_COLOR, C_LINE_COLOR]\n        \n        len_i, len_s, len_c = len(i_glyphs), len(s_glyphs), len(c_glyphs)\n        current_glyph_index = frame_num - 1 # 0-indexed glyph position across all lines\n        highlight_line_idx, highlight_glyph_idx_in_line = -1, -1\n\n        # Determine which glyph to highlight based on I, S, C order\n        if 0 <= current_glyph_index < len_i:\n            highlight_line_idx = 0 # I-Line is the 0th line in genome_lines_ordered\n            highlight_glyph_idx_in_line = current_glyph_index\n        elif len_i <= current_glyph_index < len_i + len_s:\n            highlight_line_idx = 1 # S-Line is the 1st line\n            highlight_glyph_idx_in_line = current_glyph_index - len_i\n        elif len_i + len_s <= current_glyph_index < len_i + len_s + len_c:\n            highlight_line_idx = 2 # C-Line is the 2nd line\n            highlight_glyph_idx_in_line = current_glyph_index - (len_i + len_s)\n\n        # Using original y-positioning logic for vertical centering and line height\n        line_height_from_font = FONT_GENOME.getbbox('A')[3] # Approx height of glyph from baseline\n        inter_line_spacing = 4 \n        effective_line_height = line_height_from_font + inter_line_spacing # Baseline to baseline distance\n        \n        total_text_block_height = (effective_line_height * 3) - inter_line_spacing # Total height of 3 lines\n        y_pos_current_line_top = (GENOME_REPORT_HEIGHT - total_text_block_height) / 2\n        # Ensure y_pos_current_line_top is not negative or too small, providing a small top margin\n        y_pos_current_line_top = max(y_pos_current_line_top, 2) # Minimum 2px margin from the top of the genome report area\n\n        for line_idx_in_ordered_list, glyphs_in_current_line in enumerate(genome_lines_ordered):\n            x_pos = 10 # Start x-position for glyphs on this line\n            space_width = FONT_GENOME.getlength(' ')\n            \n            current_line_base_color = genome_line_base_colors[line_idx_in_ordered_list]\n\n            for glyph_idx_in_current_line, glyph_char_value in enumerate(glyphs_in_current_line):\n                if not glyph_char_value.strip(): continue # Skip empty glyphs\n                \n                glyph_width = FONT_GENOME.getlength(glyph_char_value)\n                final_glyph_text_color = current_line_base_color # Default to base color for the line\n\n                # Check if this is the glyph to highlight\n                if line_idx_in_ordered_list == highlight_line_idx and glyph_idx_in_current_line == highlight_glyph_idx_in_line:\n                    final_glyph_text_color = EKPHRASIS_COLOR # Highlighted glyph text is Amber\n                \n                # Draw the glyph text with the determined color\n                # Assuming y_pos_current_line_top is the 'top' for draw.text\n                draw.text((x_pos, y_pos_current_line_top), glyph_char_value, font=FONT_GENOME, fill=final_glyph_text_color)\n                \n                x_pos += glyph_width + space_width # Advance x-position for the next glyph\n            \n            y_pos_current_line_top += effective_line_height # Advance y-position for the next line of glyphs\n\n        draw.line([0, GENOME_REPORT_HEIGHT - 1, BASE_WIDTH, GENOME_REPORT_HEIGHT - 1], fill=BLUE_BORDER, width=2)\n    \n    y_offset += GENOME_REPORT_HEIGHT\n\n    # --- 2. Main Header ---\n    meta_text = f\"ID:{entry_id}   POEM: {poem_title}   TIME: {timestamp}   FRAME: ({frame_num}/{total_frames})\"\n    draw.text((10, y_offset + 5), meta_text, font=FONT_REGULAR, fill=WHITE)\n    y_offset += 25\n\n    # --- 3. Type Information ---\n    def draw_type_line(y, glyph_dict, text, color):\n        if not text: return y\n        symbol = get_flexible_glyph(text, glyph_dict)\n        draw.text((20, y), symbol, font=FONT_SYMBOLS, fill=color)\n        draw.text((50, y), text, font=FONT_BOLD, fill=color)\n        return y + 22\n\n    current_y = y_offset + 15\n    if image_type:\n        symbol = get_flexible_glyph(image_type, IMAGE_TYPE_GLYPHS)\n        draw.text((20, current_y), symbol, font=FONT_SYMBOLS, fill=IMAGE_TYPE_COLOR)\n        draw.text((50, current_y), image_type, font=FONT_BOLD, fill=IMAGE_TYPE_COLOR)\n\n    if content_line:\n        line_x_pos = 350\n        line_text = f'LINE: \"{content_line}\"'\n        draw_text_with_wrapping(draw, line_text, (line_x_pos, current_y), FONT_REGULAR, LINE_COLOR, BASE_WIDTH - line_x_pos - 10)\n\n    y_offset = current_y + 25\n\n    # Divider line - ONLY HERE\n    y_offset += 5\n    draw.line([(10, y_offset), (BASE_WIDTH - 10, y_offset)], fill=DIVIDER_COLOR, width=1)\n    y_offset += 10\n    \n    y_offset = draw_type_line(y_offset, SYNTAGMA_GLYPHS, syntagma_type, SYNTAGMA_COLOR)\n    y_offset = draw_type_line(y_offset, CINEOSIS_FUNCTION_GLYPHS, cineosis_function, CINEOSIS_COLOR)\n    \n    # --- 4. Ekphrasis & Style ---\n    y_offset += 5\n    text_field_max_width = BASE_WIDTH - 20 - 10 # x=20, 10px right margin\n    y_offset = draw_text_with_wrapping(draw, operative_ekphrasis, (20, y_offset), FONT_STYLE_TEXT, EKPHRASIS_COLOR, text_field_max_width)\n    y_offset = draw_text_with_wrapping(draw, style_conditioning, (20, y_offset), FONT_STYLE_TEXT, STYLE_TEXT_COLOR, text_field_max_width)\n\n    # --- 5. Paste Image ---\n    card.paste(img, (0, GENOME_REPORT_HEIGHT + HEADER_HEIGHT))\n\n    # --- Save Card ---\n    poem_dir = os.path.join(OUTPUT_DIR_BASE, slugify(poem_title))\n    os.makedirs(poem_dir, exist_ok=True)\n    output_path = os.path.join(poem_dir, f\"{slugify(entry_id)}.png\")\n    card.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef main():\n    print(\"Starting CAT LEG Generator V3...\")\n    \n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline = json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError) as e:\n        print(f\"Fatal: Could not load timeline from {TIMELINE_PATH}. Error: {e}\")\n        return\n\n    try:\n        with open(SYMBOLIC_GENOME_DATA_PATH, 'r', encoding='utf-8') as f:\n            genome_data = json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError) as e:\n        print(f\"Fatal: Could not load genome data from {SYMBOLIC_GENOME_DATA_PATH}. Error: {e}\")\n        return\n\n    os.makedirs(OUTPUT_DIR_BASE, exist_ok=True)\n    \n    total_frames = len(timeline)\n    for i, entry in enumerate(timeline):\n        render_card(entry, i + 1, total_frames, genome_data)\n\n    print(f\"\\nProcessing complete. {total_frames} cards generated in {OUTPUT_DIR_BASE}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "storyboard.json",
      "symbolic_genome_data.json",
      "{slugify(entry_id)}.png",
      "/System/Library/Fonts/Menlo.ttc",
      "/System/Library/Fonts/Menlo.ttc",
      "/System/Library/Fonts/Supplemental/Arial Bold.ttf",
      "N/A",
      ")[3] # Approx height of glyph from baseline\n        inter_line_spacing = 4 \n        effective_line_height = line_height_from_font + inter_line_spacing # Baseline to baseline distance\n        \n        total_text_block_height = (effective_line_height * 3) - inter_line_spacing # Total height of 3 lines\n        y_pos_current_line_top = (GENOME_REPORT_HEIGHT - total_text_block_height) / 2\n        # Ensure y_pos_current_line_top is not negative or too small, providing a small top margin\n        y_pos_current_line_top = max(y_pos_current_line_top, 2) # Minimum 2px margin from the top of the genome report area\n\n        for line_idx_in_ordered_list, glyphs_in_current_line in enumerate(genome_lines_ordered):\n            x_pos = 10 # Start x-position for glyphs on this line\n            space_width = FONT_GENOME.getlength(",
      "ID:{entry_id}   POEM: {poem_title}   TIME: {timestamp}   FRAME: ({frame_num}/{total_frames})"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CAT LEG Generator V3 - Final refinement pass.\nProcesses storyboard.json to create highly stylized image cards that are\nvisually identical to the DOG project's output."
  },
  {
    "path": "ANT/update_paths.py",
    "size": 658,
    "lines": 23,
    "source": "import json\n\n# Define the path to the JSON file\njson_file_path = '/Users/gaia/resurrecting atlantis/ANT/total-cinome.json'\n\n# Define the prefix to add to the image paths\nprefix = '/Users/gaia/resurrecting atlantis/TIGER/'\n\n# Read the JSON file\nwith open(json_file_path, 'r') as f:\n    data = json.load(f)\n\n# Iterate over the list of dictionaries and update the image_path\nfor item in data:\n    if 'image_path' in item:\n        item['image_path'] = prefix + item['image_path']\n\n# Write the updated data back to the JSON file\nwith open(json_file_path, 'w') as f:\n    json.dump(data, f, indent=4)\n\nprint(f'Successfully updated image paths in {json_file_path}')\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/ANT/total-cinome.json",
      "/Users/gaia/resurrecting atlantis/ANT/total-cinome.json",
      "/Users/gaia/resurrecting atlantis/TIGER/"
    ],
    "subprocess_calls": [],
    "imports": [
      "json"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "ANT/copy_images.py",
    "size": 980,
    "lines": 33,
    "source": "import json\nimport os\nimport shutil\n\n# Define file paths\njson_file_path = '/Users/gaia/resurrecting atlantis/ANT/total-cinome.json'\ndestination_dir = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/MEDIA/'\n\n# Create the destination directory if it doesn't exist\nos.makedirs(destination_dir, exist_ok=True)\n\n# Read the JSON file\nwith open(json_file_path, 'r') as f:\n    data = json.load(f)\n\n# Iterate over the data and copy files\nfor i, item in enumerate(data):\n    source_path = item.get('image_path')\n    if not source_path:\n        print(f'Skipping item {i+1}: no image_path found.')\n        continue\n\n    if os.path.exists(source_path):\n        try:\n            shutil.copy(source_path, destination_dir)\n            print(f'Copied {os.path.basename(source_path)} to {destination_dir}')\n        except Exception as e:\n            print(f'Error copying {source_path}: {e}')\n    else:\n        print(f'Source file not found: {source_path}')\n\nprint('\\nImage copy process complete.')\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/ANT/total-cinome.json",
      "/Users/gaia/resurrecting atlantis/ANT/total-cinome.json",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/MEDIA/"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "shutil"
    ],
    "generates": [],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "ANT/update_json_paths.py",
    "size": 938,
    "lines": 30,
    "source": "import json\nimport os\n\n# Define file paths\noriginal_json_path = '/Users/gaia/resurrecting atlantis/ANT/total-cinome.json'\nnew_json_path = '/Users/gaia/resurrecting atlantis/ANT/total-cinome-media.json'\nmedia_dir = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/MEDIA/'\n\n# Read the original JSON file\nwith open(original_json_path, 'r') as f:\n    data = json.load(f)\n\n# Create a new list for the updated data\nnew_data = []\n\n# Iterate over the data and update the image_path\nfor item in data:\n    if 'image_path' in item:\n        # Get the base filename from the old path\n        filename = os.path.basename(item['image_path'])\n        # Create the new path\n        item['image_path'] = os.path.join(media_dir, filename)\n    new_data.append(item)\n\n# Write the updated data to the new JSON file\nwith open(new_json_path, 'w') as f:\n    json.dump(new_data, f, indent=4)\n\nprint(f'Successfully created {new_json_path} with updated image paths.')\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/ANT/total-cinome.json",
      "/Users/gaia/resurrecting atlantis/ANT/total-cinome-media.json",
      "/Users/gaia/resurrecting atlantis/ANT/total-cinome.json",
      "/Users/gaia/resurrecting atlantis/ANT/total-cinome-media.json",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/MEDIA/"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "ARKADU/analyze-provenance.py",
    "size": 8018,
    "lines": 209,
    "source": "#!/usr/bin/env python3\n\"\"\"\nProvenance Network Analysis - Find key nodes in circulation networks\n\"\"\"\n\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\n\ndef load_manifest(path='media-manifest.json'):\n    with open(path, 'r') as f:\n        return json.load(f)\n\ndef analyze_provenance_networks(manifest):\n    \"\"\"Analyze bidirectional provenance links\"\"\"\n    \n    all_artifacts = (manifest['artifacts']['images'] + \n                    manifest['artifacts']['videos'] + \n                    manifest['artifacts']['audio'])\n    \n    # Build network graph\n    artifact_degree = defaultdict(int)  # how many HTML files use this\n    html_degree = defaultdict(int)      # how many artifacts this HTML uses\n    \n    for artifact in all_artifacts:\n        if artifact.get('used_by'):\n            artifact_degree[artifact['path']] = len(artifact['used_by'])\n            for html in artifact['used_by']:\n                html_degree[html] += 1\n    \n    # Find hub artifacts (used by many HTML files)\n    hubs = sorted(artifact_degree.items(), key=lambda x: x[1], reverse=True)\n    \n    # Find hub HTML files (use many artifacts)\n    html_hubs = sorted(html_degree.items(), key=lambda x: x[1], reverse=True)\n    \n    return {\n        'artifact_hubs': hubs,\n        'html_hubs': html_hubs,\n        'total_edges': sum(artifact_degree.values())\n    }\n\ndef find_shared_artifacts(manifest):\n    \"\"\"Find artifacts shared across multiple HTML files (candidates for Git)\"\"\"\n    all_artifacts = (manifest['artifacts']['images'] + \n                    manifest['artifacts']['videos'] + \n                    manifest['artifacts']['audio'])\n    \n    shared = []\n    for artifact in all_artifacts:\n        if artifact.get('used_by') and len(artifact['used_by']) > 1:\n            shared.append({\n                'path': artifact['path'],\n                'size': artifact['size'],\n                'usage_count': len(artifact['used_by']),\n                'used_by': artifact['used_by']\n            })\n    \n    return sorted(shared, key=lambda x: x['usage_count'], reverse=True)\n\ndef find_critical_paths(manifest):\n    \"\"\"Find artifacts that are critical (only source for an HTML)\"\"\"\n    circulation = manifest['circulation']\n    \n    # For each HTML, find its unique dependencies\n    critical = []\n    for html, artifacts in circulation.items():\n        for artifact_path in artifacts:\n            # Check if this artifact is used elsewhere\n            all_artifacts = (manifest['artifacts']['images'] + \n                           manifest['artifacts']['videos'] + \n                           manifest['artifacts']['audio'])\n            \n            artifact_obj = next((a for a in all_artifacts if a['path'] == artifact_path), None)\n            if artifact_obj and artifact_obj.get('used_by') and len(artifact_obj['used_by']) == 1:\n                critical.append({\n                    'artifact': artifact_path,\n                    'html': html,\n                    'size': artifact_obj['size'],\n                    'type': artifact_obj['type']\n                })\n    \n    return critical\n\ndef recommend_git_subset(manifest):\n    \"\"\"Recommend which files should stay in Git\"\"\"\n    recommendations = {\n        'must_keep': [],    # Shared across files\n        'should_keep': [],  # Small and linked\n        'can_external': [], # Large but used\n        'can_archive': []   # Large and orphaned\n    }\n    \n    all_artifacts = (manifest['artifacts']['images'] + \n                    manifest['artifacts']['videos'] + \n                    manifest['artifacts']['audio'])\n    \n    for artifact in all_artifacts:\n        size = artifact['size']\n        linked = bool(artifact.get('used_by'))\n        usage_count = len(artifact.get('used_by', []))\n        \n        # Decision tree\n        if usage_count > 1:\n            recommendations['must_keep'].append(artifact)\n        elif linked and size < 10_000_000:  # <10 MB\n            recommendations['should_keep'].append(artifact)\n        elif linked and size >= 10_000_000:\n            recommendations['can_external'].append(artifact)\n        else:  # Not linked\n            if size > 50_000_000:  # >50 MB\n                recommendations['can_archive'].append(artifact)\n            # Small orphans can stay\n    \n    return recommendations\n\ndef print_report(manifest):\n    \"\"\"Print comprehensive provenance analysis\"\"\"\n    \n    print(\"=\"*70)\n    print(\"ARKADU PROVENANCE ANALYSIS\")\n    print(\"=\"*70)\n    \n    # Network analysis\n    networks = analyze_provenance_networks(manifest)\n    print(f\"\\n\ud83d\udd17 NETWORK TOPOLOGY\")\n    print(f\"  Total provenance links: {networks['total_edges']}\")\n    print(f\"  HTML hub nodes: {len(networks['html_hubs'])}\")\n    print(f\"  Artifact hub nodes: {len(networks['artifact_hubs'])}\")\n    \n    # Shared artifacts\n    shared = find_shared_artifacts(manifest)\n    print(f\"\\n\ud83c\udf1f SHARED ARTIFACTS (used by multiple HTML files)\")\n    if shared:\n        print(f\"  Found {len(shared)} shared artifacts\")\n        for i, item in enumerate(shared[:10], 1):\n            size_mb = round(item['size'] / 1_000_000, 2)\n            print(f\"  {i}. {item['path']}\")\n            print(f\"     Used by {item['usage_count']} files | {size_mb} MB\")\n    else:\n        print(\"  None found - no artifacts are reused across HTML files\")\n    \n    # HTML hubs\n    print(f\"\\n\ud83d\udcc4 HTML HUBS (files that use many artifacts)\")\n    for i, (html, count) in enumerate(networks['html_hubs'][:10], 1):\n        print(f\"  {i}. {html} \u2192 uses {count} artifacts\")\n    \n    # Recommendations\n    recs = recommend_git_subset(manifest)\n    print(f\"\\n\ud83c\udfaf GIT STORAGE RECOMMENDATIONS\")\n    \n    must_keep_size = sum(a['size'] for a in recs['must_keep']) / 1_000_000\n    should_keep_size = sum(a['size'] for a in recs['should_keep']) / 1_000_000\n    can_external_size = sum(a['size'] for a in recs['can_external']) / 1_000_000\n    can_archive_size = sum(a['size'] for a in recs['can_archive']) / 1_000_000\n    \n    print(f\"\\n  MUST KEEP (shared across files):\")\n    print(f\"    Count: {len(recs['must_keep'])} files\")\n    print(f\"    Size:  {must_keep_size:.2f} MB\")\n    print(f\"    Why:   Used by multiple HTML files - critical to keep\")\n    \n    print(f\"\\n  SHOULD KEEP (small & linked):\")\n    print(f\"    Count: {len(recs['should_keep'])} files\")\n    print(f\"    Size:  {should_keep_size:.2f} MB\")\n    print(f\"    Why:   Small enough (<10 MB) and actively used\")\n    \n    print(f\"\\n  CAN EXTERNALIZE (large & linked):\")\n    print(f\"    Count: {len(recs['can_external'])} files\")\n    print(f\"    Size:  {can_external_size:.2f} MB\")\n    print(f\"    Why:   Used by HTML but too large for Git\")\n    \n    print(f\"\\n  CAN ARCHIVE (large & orphaned):\")\n    print(f\"    Count: {len(recs['can_archive'])} files\")\n    print(f\"    Size:  {can_archive_size:.2f} MB\")\n    print(f\"    Why:   Not referenced, likely working files\")\n    \n    total_git_size = must_keep_size + should_keep_size\n    print(f\"\\n  \ud83d\udcbe TOTAL GIT SIZE: {total_git_size:.2f} MB\")\n    \n    if total_git_size < 1000:\n        print(f\"     \u2713 Under 1 GB - acceptable for GitHub\")\n    else:\n        print(f\"     \u26a0\ufe0f  Over 1 GB - consider Git LFS or further reduction\")\n    \n    print(\"\\n\" + \"=\"*70)\n\ndef export_recommendations(manifest, output_path='arkadu-recommendations.json'):\n    \"\"\"Export recommendations as JSON\"\"\"\n    recs = recommend_git_subset(manifest)\n    \n    # Convert to simple format\n    output = {\n        'must_keep': [a['path'] for a in recs['must_keep']],\n        'should_keep': [a['path'] for a in recs['should_keep']],\n        'can_external': [a['path'] for a in recs['can_external']],\n        'can_archive': [a['path'] for a in recs['can_archive']],\n        'shared_artifacts': [s['path'] for s in find_shared_artifacts(manifest)]\n    }\n    \n    with open(output_path, 'w') as f:\n        json.dump(output, f, indent=2)\n    \n    print(f\"\\n\ud83d\udcbe Recommendations exported to: {output_path}\")\n\nif __name__ == '__main__':\n    manifest = load_manifest()\n    print_report(manifest)\n    export_recommendations(manifest)\n",
    "file_references": [
      "media-manifest.json",
      "arkadu-recommendations.json",
      "] / 1_000_000, 2)\n            print(f",
      "]) / 1_000_000\n    should_keep_size = sum(a[",
      "]) / 1_000_000\n    can_external_size = sum(a[",
      "]) / 1_000_000\n    can_archive_size = sum(a[",
      "]) / 1_000_000\n    \n    print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "collections",
      "pathlib"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Provenance Network Analysis - Find key nodes in circulation networks"
  },
  {
    "path": "ARKADU/generate-gitignore.py",
    "size": 4925,
    "lines": 150,
    "source": "#!/usr/bin/env python3\n\"\"\"\nGenerate intelligent .gitignore based on ARKADU manifest analysis\n\"\"\"\n\nimport json\nfrom pathlib import Path\n\ndef load_manifest(path='media-manifest.json'):\n    with open(path, 'r') as f:\n        return json.load(f)\n\ndef generate_gitignore(manifest):\n    \"\"\"Generate .gitignore based on artifact analysis\"\"\"\n    \n    all_artifacts = (manifest['artifacts']['images'] + \n                    manifest['artifacts']['videos'] + \n                    manifest['artifacts']['audio'])\n    \n    # Find files to keep (linked or small)\n    keep_files = set()\n    keep_patterns = set()\n    \n    for artifact in all_artifacts:\n        # Keep if used by HTML files\n        if artifact.get('used_by'):\n            keep_files.add(artifact['path'])\n        \n        # Keep small header prompts\n        if 'header_prompt' in artifact['path'] and artifact['size'] < 20_000_000:\n            keep_patterns.add('*_header_prompt.mp4')\n        \n        # Keep clapper cards\n        if 'clapper' in artifact['path'].lower():\n            keep_patterns.add('*clapper*.png')\n    \n    # Generate ignore rules\n    rules = []\n    rules.append(\"# ARKADU-generated .gitignore\")\n    rules.append(\"# Media Archaeology Strategy: Selective Archival\")\n    rules.append(\"\")\n    \n    rules.append(\"# Exclude large video formats\")\n    rules.append(\"*.mp4\")\n    rules.append(\"*.mov\")\n    rules.append(\"*.avi\")\n    rules.append(\"*.webm\")\n    rules.append(\"*.mkv\")\n    rules.append(\"\")\n    \n    rules.append(\"# Exclude large audio formats\")\n    rules.append(\"*.wav\")\n    rules.append(\"!*.mp3  # MP3s are usually small enough\")\n    rules.append(\"\")\n    \n    rules.append(\"# But keep critical video artifacts\")\n    for pattern in sorted(keep_patterns):\n        rules.append(f\"!{pattern}\")\n    rules.append(\"\")\n    \n    rules.append(\"# Exclude animal working directories\")\n    animal_dirs = set()\n    for artifact in all_artifacts:\n        if '/MEDIA/' in artifact['path'] or '/TRUNK/' in artifact['path']:\n            parts = Path(artifact['path']).parts\n            if len(parts) > 2:\n                animal_dirs.add(f\"{parts[0]}/{parts[1]}/\")\n    \n    for animal_dir in sorted(animal_dirs):\n        rules.append(f\"{animal_dir}\")\n    rules.append(\"\")\n    \n    rules.append(\"# Exclude large compilations\")\n    for artifact in all_artifacts:\n        if artifact['size'] > 100_000_000:  # >100 MB\n            rules.append(f\"{artifact['path']}\")\n    rules.append(\"\")\n    \n    rules.append(\"# Keep archaeology tools and manifests\")\n    rules.append(\"!ARKADU/\")\n    rules.append(\"!media-manifest.json\")\n    rules.append(\"!*.html\")\n    rules.append(\"!*.py\")\n    rules.append(\"!*.md\")\n    rules.append(\"\")\n    \n    rules.append(\"# Standard exclusions\")\n    rules.append(\".DS_Store\")\n    rules.append(\"__pycache__/\")\n    rules.append(\"*.pyc\")\n    rules.append(\".venv/\")\n    rules.append(\"venv/\")\n    \n    return '\\n'.join(rules)\n\ndef analyze_savings(manifest):\n    \"\"\"Calculate how much space we'd save\"\"\"\n    all_artifacts = (manifest['artifacts']['images'] + \n                    manifest['artifacts']['videos'] + \n                    manifest['artifacts']['audio'])\n    \n    excluded_size = 0\n    kept_size = 0\n    \n    for artifact in all_artifacts:\n        is_video = artifact['path'].endswith(('.mp4', '.mov', '.avi', '.webm', '.mkv'))\n        is_large = artifact['size'] > 100_000_000\n        is_media_dir = '/MEDIA/' in artifact['path'] or '/TRUNK/' in artifact['path']\n        has_keep_pattern = ('header_prompt' in artifact['path'] or \n                           'clapper' in artifact['path'].lower())\n        is_linked = bool(artifact.get('used_by'))\n        \n        should_exclude = (is_video or is_large or is_media_dir) and not (has_keep_pattern or is_linked)\n        \n        if should_exclude:\n            excluded_size += artifact['size']\n        else:\n            kept_size += artifact['size']\n    \n    return {\n        'excluded_mb': round(excluded_size / 1_000_000, 2),\n        'kept_mb': round(kept_size / 1_000_000, 2),\n        'savings_percent': round((excluded_size / (excluded_size + kept_size)) * 100, 1)\n    }\n\nif __name__ == '__main__':\n    manifest = load_manifest()\n    \n    # Generate .gitignore\n    gitignore_content = generate_gitignore(manifest)\n    \n    # Write to file\n    output_path = Path('.gitignore.arkadu')\n    with open(output_path, 'w') as f:\n        f.write(gitignore_content)\n    \n    print(\"\u2713 Generated .gitignore.arkadu\")\n    print(f\"  Review and rename to .gitignore to use\")\n    print()\n    \n    # Show savings analysis\n    savings = analyze_savings(manifest)\n    print(\"\ud83d\udcca Storage Analysis:\")\n    print(f\"  Would exclude: {savings['excluded_mb']} MB\")\n    print(f\"  Would keep:    {savings['kept_mb']} MB\")\n    print(f\"  Space savings: {savings['savings_percent']}%\")\n    print()\n    print(\"\u26a0\ufe0f  Review .gitignore.arkadu before using!\")\n    print(\"   Make sure critical files aren't excluded.\")\n",
    "file_references": [
      "media-manifest.json",
      "*_header_prompt.mp4",
      "*clapper*.png",
      "*.mp4",
      "*.mov",
      "*.avi",
      "*.wav",
      "!media-manifest.json",
      "{parts[0]}/{parts[1]}/",
      ": round(excluded_size / 1_000_000, 2),\n        ",
      ": round(kept_size / 1_000_000, 2),\n        ",
      ": round((excluded_size / (excluded_size + kept_size)) * 100, 1)\n    }\n\nif __name__ == "
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "pathlib"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Generate intelligent .gitignore based on ARKADU manifest analysis"
  },
  {
    "path": "ARKADU/arkadu-scan.py",
    "size": 11217,
    "lines": 279,
    "source": "#!/usr/bin/env python3\n\"\"\"\nARKADU - Media Archaeology Engine Scanner\nNoah's Ark \u2297 Xanadu for Resurrecting Atlantis\n\nExcavates media artifacts, documents provenance, maps circulation networks.\n\"\"\"\n\nimport os\nimport json\nimport mimetypes\nfrom pathlib import Path\nfrom datetime import datetime\nfrom collections import defaultdict\nimport re\n\nclass MediaArchaeologist:\n    \"\"\"\n    A Media Archaeologist [investigates] <media> <artifacts> across <time>\n    \"\"\"\n    \n    def __init__(self, root_path):\n        self.root = Path(root_path)\n        self.artifacts = {\n            'images': [],\n            'videos': [],\n            'audio': []\n        }\n        self.provenance = defaultdict(list)  # file -> used_by mapping\n        self.strata = defaultdict(list)  # animal -> artifacts\n        self.circulation = defaultdict(list)  # html -> media mapping\n        \n    def excavate(self):\n        \"\"\"[excavate] all <technocultural strata>\"\"\"\n        print(\"\ud83d\udd0d EXCAVATING MEDIA STRATA...\")\n        \n        # Find all media files\n        media_extensions = {\n            'images': ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg'],\n            'videos': ['.mp4', '.mov', '.avi', '.webm', '.mkv'],\n            'audio': ['.mp3', '.wav', '.ogg', '.m4a']\n        }\n        \n        for artifact_type, extensions in media_extensions.items():\n            for ext in extensions:\n                files = self.root.rglob(f'*{ext}')\n                for file_path in files:\n                    self.catalog_artifact(file_path, artifact_type)\n        \n        # Find all HTML files and scan for references\n        html_files = self.root.rglob('*.html')\n        for html_file in html_files:\n            self.map_circulation(html_file)\n        \n        print(f\"\u2713 Discovered {len(self.artifacts['images'])} images\")\n        print(f\"\u2713 Discovered {len(self.artifacts['videos'])} videos\")\n        print(f\"\u2713 Discovered {len(self.artifacts['audio'])} audio files\")\n        print(f\"\u2713 Mapped {len(self.circulation)} HTML circulation nodes\")\n        \n    def catalog_artifact(self, file_path, artifact_type):\n        \"\"\"[document] <artifact> with <metadata>\"\"\"\n        rel_path = file_path.relative_to(self.root)\n        stat = file_path.stat()\n        \n        artifact = {\n            'path': str(rel_path),\n            'absolute_path': str(file_path),\n            'size': stat.st_size,\n            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),\n            'type': file_path.suffix.upper().strip('.'),\n            'animal': self.infer_animal(rel_path),\n            'stratum': self.infer_stratum(rel_path),\n            'used_by': []\n        }\n        \n        self.artifacts[artifact_type].append(artifact)\n        self.strata[artifact['animal']].append(artifact)\n        \n    def infer_animal(self, path):\n        \"\"\"Infer which animal kingdom this artifact belongs to\"\"\"\n        parts = path.parts\n        animals = ['CAT', 'DOG', 'HORSE', 'ELEPHANT', 'ANT', 'BOY', 'TIGER', \n                   'WHALE', 'SPIDER', 'MANTA', 'LIZARD', 'IMPALA', 'JELLYFISH',\n                   'KOALA', 'IBEX', 'HONEYBADGER', 'SHARK']\n        \n        for part in parts:\n            if part in animals:\n                return part\n        return 'UNKNOWN'\n    \n    def infer_stratum(self, path):\n        \"\"\"Infer technocultural stratum\"\"\"\n        path_str = str(path).upper()\n        \n        strata_markers = {\n            'WHISKER': 'WHISKER_LAYER',\n            'TRUNK': 'TRUNK_LAYER',\n            'SLIDESHOWS': 'SLIDESHOW_COMPILATION',\n            'HEADER_PROMPT': 'HEADER_PROMPT_LAYER',\n            'CLAPPER': 'CLAPPER_CARD_LAYER',\n            'TIMAEUS': 'TIMAEUS_PORTAL',\n            'MYTH': 'MYTH_TIME',\n            'FAL': 'FAL_SHEET'\n        }\n        \n        for marker, stratum in strata_markers.items():\n            if marker in path_str:\n                return stratum\n        \n        return 'ROOT_STRATUM'\n    \n    def map_circulation(self, html_path):\n        \"\"\"[document] <circulation> through <networks>\"\"\"\n        try:\n            with open(html_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n            \n            rel_html_path = html_path.relative_to(self.root)\n            \n            # Find all media references (src, href with media extensions)\n            media_pattern = r'(?:src|href)=[\"\\']([^\"\\']+\\.(?:jpg|jpeg|png|gif|webp|svg|mp4|mov|avi|webm|mp3|wav))[\"\\']'\n            matches = re.findall(media_pattern, content, re.IGNORECASE)\n            \n            for media_ref in matches:\n                # Normalize path\n                media_path = self.normalize_media_path(media_ref, html_path)\n                if media_path:\n                    self.circulation[str(rel_html_path)].append(media_path)\n                    self.provenance[media_path].append(str(rel_html_path))\n            \n        except Exception as e:\n            print(f\"\u26a0\ufe0f  Could not scan {html_path}: {e}\")\n    \n    def normalize_media_path(self, ref, from_html):\n        \"\"\"Convert relative references to project-relative paths\"\"\"\n        # Handle absolute paths, URLs, etc.\n        if ref.startswith('http') or ref.startswith('//'):\n            return None\n        \n        if ref.startswith('/'):\n            ref = ref.lstrip('/')\n        \n        # Resolve relative to HTML file location\n        html_dir = from_html.parent\n        try:\n            media_path = (html_dir / ref).resolve().relative_to(self.root)\n            return str(media_path)\n        except:\n            return ref\n    \n    def link_provenance(self):\n        \"\"\"Link provenance data back to artifacts\"\"\"\n        all_artifacts = (self.artifacts['images'] + \n                        self.artifacts['videos'] + \n                        self.artifacts['audio'])\n        \n        for artifact in all_artifacts:\n            path = artifact['path']\n            if path in self.provenance:\n                artifact['used_by'] = self.provenance[path]\n    \n    def analyze_github_strategy(self):\n        \"\"\"Analyze size constraints and recommend GitHub strategy\"\"\"\n        all_artifacts = (self.artifacts['images'] + \n                        self.artifacts['videos'] + \n                        self.artifacts['audio'])\n        \n        total_size = sum(a['size'] for a in all_artifacts)\n        video_size = sum(a['size'] for a in self.artifacts['videos'])\n        large_files = [a for a in all_artifacts if a['size'] > 100_000_000]\n        very_large_files = [a for a in all_artifacts if a['size'] > 50_000_000]\n        \n        analysis = {\n            'total_size': total_size,\n            'total_size_mb': round(total_size / 1_000_000, 2),\n            'video_size_mb': round(video_size / 1_000_000, 2),\n            'total_files': len(all_artifacts),\n            'large_files_over_100mb': len(large_files),\n            'large_files_over_50mb': len(very_large_files),\n            'github_limit_exceeded': len(large_files) > 0,\n            'recommendation': self.generate_recommendation(total_size, large_files, very_large_files)\n        }\n        \n        return analysis\n    \n    def generate_recommendation(self, total_size, large_files, very_large_files):\n        \"\"\"Generate storage strategy recommendation\"\"\"\n        if total_size < 100_000_000:  # < 100 MB\n            return \"PROJECT_SMALL: Safe to push all media to GitHub\"\n        elif total_size < 1_000_000_000 and not large_files:  # < 1 GB, no huge files\n            return \"USE_GIT_LFS: Use Git LFS for videos, keep images in repo\"\n        elif len(large_files) > 0:\n            return \"EXTERNAL_REQUIRED: Files >100MB detected. Must use external hosting or split files\"\n        else:\n            return \"HYBRID_APPROACH: Keep selective media in Git, host full collection externally\"\n    \n    def export_manifest(self, output_path='media-manifest.json'):\n        \"\"\"Export complete manifest as JSON\"\"\"\n        self.link_provenance()\n        \n        manifest = {\n            'generated': datetime.now().isoformat(),\n            'project': 'Resurrecting Atlantis - ARKADU Scan',\n            'artifacts': self.artifacts,\n            'strata': {k: len(v) for k, v in self.strata.items()},\n            'circulation': dict(self.circulation),\n            'provenance': dict(self.provenance),\n            'github_analysis': self.analyze_github_strategy(),\n            'stats': {\n                'total_images': len(self.artifacts['images']),\n                'total_videos': len(self.artifacts['videos']),\n                'total_audio': len(self.artifacts['audio']),\n                'html_files': len(self.circulation),\n                'orphaned_artifacts': len([a for artifacts in self.artifacts.values() \n                                          for a in artifacts if not a.get('used_by')])\n            }\n        }\n        \n        output_file = self.root / output_path\n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(manifest, f, indent=2)\n        \n        print(f\"\\n\ud83d\udcbe Manifest exported to: {output_file}\")\n        return manifest\n    \n    def print_report(self):\n        \"\"\"Print excavation report to console\"\"\"\n        self.link_provenance()\n        analysis = self.analyze_github_strategy()\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"ARKADU EXCAVATION REPORT\")\n        print(\"=\"*70)\n        \n        print(f\"\\n\ud83d\udcca STATISTICS\")\n        print(f\"  Images:       {len(self.artifacts['images'])}\")\n        print(f\"  Videos:       {len(self.artifacts['videos'])}\")\n        print(f\"  Audio:        {len(self.artifacts['audio'])}\")\n        print(f\"  Total Size:   {analysis['total_size_mb']} MB\")\n        print(f\"  HTML Files:   {len(self.circulation)}\")\n        \n        print(f\"\\n\ud83e\udd81 ANIMAL KINGDOMS (Strata)\")\n        for animal, artifacts in sorted(self.strata.items(), key=lambda x: len(x[1]), reverse=True):\n            if artifacts:\n                print(f\"  {animal:15} \u2192 {len(artifacts)} artifacts\")\n        \n        print(f\"\\n\ud83d\udd17 PROVENANCE\")\n        linked = len([a for artifacts in self.artifacts.values() \n                     for a in artifacts if a.get('used_by')])\n        orphaned = len([a for artifacts in self.artifacts.values() \n                       for a in artifacts if not a.get('used_by')])\n        print(f\"  Linked artifacts:    {linked}\")\n        print(f\"  Orphaned artifacts:  {orphaned}\")\n        \n        print(f\"\\n\u26a0\ufe0f  GITHUB ANALYSIS\")\n        print(f\"  Total size:          {analysis['total_size_mb']} MB\")\n        print(f\"  Video size:          {analysis['video_size_mb']} MB\")\n        print(f\"  Files over 50MB:     {analysis['large_files_over_50mb']}\")\n        print(f\"  Files over 100MB:    {analysis['large_files_over_100mb']}\")\n        print(f\"  Recommendation:      {analysis['recommendation']}\")\n        \n        print(\"\\n\" + \"=\"*70)\n\n\nif __name__ == '__main__':\n    import sys\n    \n    root_dir = sys.argv[1] if len(sys.argv) > 1 else '.'\n    \n    print(\"\ud83c\udfdb\ufe0f  ARKADU \u2014 Media Archaeology Engine\")\n    print(\"    Noah's Ark \u2297 Xanadu\\n\")\n    \n    archaeologist = MediaArchaeologist(root_dir)\n    archaeologist.excavate()\n    archaeologist.print_report()\n    archaeologist.export_manifest()\n    \n    print(\"\\n\u2713 Excavation complete. Review media-manifest.json for full details.\")\n",
    "file_references": [
      "media-manifest.json",
      ")\n        \n        # Resolve relative to HTML file location\n        html_dir = from_html.parent\n        try:\n            media_path = (html_dir / ref).resolve().relative_to(self.root)\n            return str(media_path)\n        except:\n            return ref\n    \n    def link_provenance(self):\n        ",
      ": round(total_size / 1_000_000, 2),\n            ",
      ": round(video_size / 1_000_000, 2),\n            ",
      ")])\n            }\n        }\n        \n        output_file = self.root / output_path\n        with open(output_file, "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "mimetypes",
      "pathlib",
      "datetime",
      "collections",
      "re"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "ARKADU - Media Archaeology Engine Scanner\nNoah's Ark \u2297 Xanadu for Resurrecting Atlantis\n\nExcavates media artifacts, documents provenance, maps circulation networks."
  },
  {
    "path": "DOG/reorder_timeline.py",
    "size": 3762,
    "lines": 101,
    "source": "import json\nfrom collections import defaultdict\nimport os\n\ninput_file_path = \"/Users/gaia/resurrecting atlantis/DOG/SEQUENCE-FIXED-TIMELINE.json\"\noutput_file_path = \"/Users/gaia/resurrecting atlantis/DOG/ORDERED-TIMELINE-FIXED.json\"\n\n# Desired order of poems (derived from user's list)\nordered_poem_names = [\n    \"Out of Life\",\n    \"Flashing Lights\",\n    \"How to break off an engagement\",  # Corrected capitalization\n    \"Nevermore\",\n    \"Bloodline\",\n    \"Resurrecting Atlantis\",\n    \"DJ Turn Me Up\",\n    \"Newly Single\",\n    \"Yet, Heard\",  # Corrected with comma and capitalization\n    \"Magic ride\",  # Corrected capitalization\n    \"Reunion\",\n    \"How To Win My Heart\",\n    \"Hot Minute\"\n]\n\ndef main():\n    print(f\"Starting timeline reordering script.\")\n    print(f\"Input file: {input_file_path}\")\n    print(f\"Output file: {output_file_path}\")\n\n    if not os.path.exists(input_file_path):\n        print(f\"Error: Input file not found at {input_file_path}\")\n        return\n\n    try:\n        with open(input_file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n    except json.JSONDecodeError as e:\n        print(f\"Error: Could not decode JSON from {input_file_path}. Details: {e}\")\n        return\n    except Exception as e:\n        print(f\"Error reading input file {input_file_path}. Details: {e}\")\n        return\n\n    if not isinstance(data, list):\n        print(f\"Error: Expected a JSON list in {input_file_path}, but got {type(data).__name__}\")\n        return\n\n    poems_data = defaultdict(list)\n    all_poem_names_in_file = set()\n    items_without_poem_key = []\n\n    for i, item in enumerate(data):\n        if not isinstance(item, dict):\n            print(f\"Warning: Skipping non-dictionary item at index {i}: {str(item)[:100]}...\")\n            continue\n        poem_name = item.get(\"poem\")\n        if poem_name:\n            poems_data[poem_name].append(item)\n            all_poem_names_in_file.add(poem_name)\n        else:\n            items_without_poem_key.append(item)\n\n    ordered_timeline = []\n    poems_added_to_timeline = set()\n\n    print(\"\\nProcessing poems in specified order:\")\n    for poem_name in ordered_poem_names:\n        if poem_name in poems_data:\n            ordered_timeline.extend(poems_data[poem_name])\n            poems_added_to_timeline.add(poem_name)\n            print(f\"  Added {len(poems_data[poem_name])} items for poem: {poem_name}\")\n        else:\n            print(f\"  Poem not found in input file: {poem_name}\")\n    \n    remaining_poem_names = sorted(list(all_poem_names_in_file - poems_added_to_timeline))\n    if remaining_poem_names:\n        print(\"\\nProcessing remaining poems (not in specified order, appended alphabetically):\")\n        for poem_name in remaining_poem_names:\n            if poem_name in poems_data: # Should always be true\n                ordered_timeline.extend(poems_data[poem_name])\n                print(f\"  Added {len(poems_data[poem_name])} items for poem: {poem_name}\")\n\n    if items_without_poem_key:\n        ordered_timeline.extend(items_without_poem_key)\n        print(f\"\\nWarning: {len(items_without_poem_key)} items without a 'poem' key were appended to the end.\")\n\n    try:\n        with open(output_file_path, 'w', encoding='utf-8') as f:\n            json.dump(ordered_timeline, f, indent=2, ensure_ascii=False)\n        print(f\"\\nSuccessfully reordered timeline written to {output_file_path}\")\n        print(f\"Total items in input file: {len(data)}\")\n        print(f\"Total items in new file: {len(ordered_timeline)}\")\n\n    except IOError as e:\n        print(f\"Error: Could not write to output file at {output_file_path}. Details: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred during writing. Details: {e}\")\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/DOG/SEQUENCE-FIXED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/DOG/ORDERED-TIMELINE-FIXED.json",
      "/Users/gaia/resurrecting atlantis/DOG/SEQUENCE-FIXED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/DOG/ORDERED-TIMELINE-FIXED.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "collections",
      "os"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "DOG/generate_bloodline_symbols.py",
    "size": 2726,
    "lines": 82,
    "source": "import json\n\n# Glyphs based on /Users/gaia/resurrecting atlantis/HONEYBADGER/NOTATION/glyphs.md\nSYNTAGMA_MAP = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2756\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n}\n\nIMAGE_TYPE_MAP = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n}\n\nCINEOSIS_FUNCTION_MAP = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n}\n\nTIMELINE_FILE_PATH = \"/Users/gaia/resurrecting atlantis/DOG/BL-TIMELINE-ADDENDUM.json\"\n\ndef generate_symbol_lines():\n    try:\n        with open(TIMELINE_FILE_PATH, 'r', encoding='utf-8') as f:\n            timeline_entries = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_FILE_PATH}\")\n        return None, None, None\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_FILE_PATH}\")\n        return None, None, None\n\n    s_line_chars = []\n    i_line_chars = []\n    c_line_chars = []\n\n    for i, entry in enumerate(timeline_entries):\n        syntagma_type = entry.get('syntagmaType')\n        image_type = entry.get('imageType')\n        cineosis_function = entry.get('cineosisFunction')\n\n        s_glyph = SYNTAGMA_MAP.get(syntagma_type, '?') # Default to '?' if not found\n        i_glyph = IMAGE_TYPE_MAP.get(image_type, '?')\n        c_glyph = CINEOSIS_FUNCTION_MAP.get(cineosis_function, '?')\n\n        if s_glyph == '?':\n            print(f\"Warning: Unknown syntagmaType '{syntagma_type}' at entry {i}\")\n        if i_glyph == '?':\n            print(f\"Warning: Unknown imageType '{image_type}' at entry {i}\")\n        if c_glyph == '?':\n            print(f\"Warning: Unknown cineosisFunction '{cineosis_function}' at entry {i}\")\n\n        s_line_chars.append(s_glyph)\n        i_line_chars.append(i_glyph)\n        c_line_chars.append(c_glyph)\n    \n    return \"\".join(s_line_chars), \"\".join(i_line_chars), \"\".join(c_line_chars)\n\nif __name__ == \"__main__\":\n    s_line, i_line, c_line = generate_symbol_lines()\n    if s_line and i_line and c_line:\n        print(f\"s_line: {s_line}\")\n        print(f\"i_line: {i_line}\")\n        print(f\"c_line: {c_line}\")\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/DOG/BL-TIMELINE-ADDENDUM.json",
      "/Users/gaia/resurrecting atlantis/DOG/BL-TIMELINE-ADDENDUM.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json"
    ],
    "generates": [],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "TIGER/compile_timeline_assemblage.py",
    "size": 4636,
    "lines": 133,
    "source": "import os\nimport json\nimport re\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nSEQUENCE_FILE = os.path.join(BASE_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\nOUTPUT_FILE = os.path.join(BASE_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json\")\n\ndef extract_assembly_data_for_id(folder_id, entry_id):\n    \"\"\"Extract assembly JSON data for a specific ID.\"\"\"\n    assembly_path = os.path.join(BASE_DIR, folder_id, f\"{folder_id}_assembly.json\")\n    \n    if not os.path.exists(assembly_path):\n        return None\n        \n    try:\n        with open(assembly_path, 'r') as f:\n            assembly_data = json.load(f)\n            \n        # Find the entry with matching ID\n        for entry in assembly_data:\n            if entry.get(\"id\") == entry_id:\n                return entry\n                \n    except Exception as e:\n        print(f\"Error processing {assembly_path}: {str(e)}\")\n        \n    return None\n\ndef parse_sequence_file():\n    \"\"\"Parse the simplified sequence file to extract entries with timestamps.\"\"\"\n    if not os.path.exists(SEQUENCE_FILE):\n        print(f\"Sequence file not found: {SEQUENCE_FILE}\")\n        return []\n        \n    entries = []\n    \n    with open(SEQUENCE_FILE, 'r') as f:\n        lines = f.readlines()\n        \n    # Regular expression to match entries\n    pattern = r\"([A-Z]{2}\\d{3}) \\[(\\d{2}:\\d{2}:\\d{2})\\] `(.*?)`\"\n    \n    for line in lines:\n        match = re.search(pattern, line)\n        if match:\n            entry_id = match.group(1)\n            timestamp = match.group(2)\n            image_path = match.group(3)\n            \n            folder_id = entry_id[:2]\n            \n            entries.append({\n                \"id\": entry_id,\n                \"timestamp\": timestamp,\n                \"image_path\": image_path,\n                \"folder_id\": folder_id\n            })\n            \n    return entries\n\ndef compile_timeline_assemblage():\n    \"\"\"Compile a timeline assemblage with timestamps and assembly data.\"\"\"\n    entries = parse_sequence_file()\n    \n    if not entries:\n        print(\"No entries found in sequence file.\")\n        return\n        \n    timeline_assemblage = []\n    \n    for entry in entries:\n        entry_id = entry[\"id\"]\n        folder_id = entry[\"folder_id\"]\n        \n        # Get assembly data\n        assembly_data = extract_assembly_data_for_id(folder_id, entry_id)\n        \n        if assembly_data:\n            # Create a new entry with timestamp and assembly data\n            timeline_entry = {\n                \"timestamp\": entry[\"timestamp\"],\n                \"image_path\": entry[\"image_path\"],\n                **assembly_data\n            }\n            \n            timeline_assemblage.append(timeline_entry)\n        else:\n            # Include entry with just timestamp and ID if no assembly data found\n            timeline_entry = {\n                \"timestamp\": entry[\"timestamp\"],\n                \"image_path\": entry[\"image_path\"],\n                \"id\": entry_id,\n                \"missing_assembly_data\": True\n            }\n            \n            timeline_assemblage.append(timeline_entry)\n    \n    # Write the timeline assemblage to file\n    with open(OUTPUT_FILE, 'w') as f:\n        json.dump(timeline_assemblage, f, indent=2)\n        \n    print(f\"Created timeline assemblage with {len(timeline_assemblage)} entries at: {OUTPUT_FILE}\")\n    \n    # Also create a markdown version for readability\n    markdown_file = OUTPUT_FILE.replace(\".json\", \".md\")\n    \n    with open(markdown_file, 'w') as f:\n        f.write(\"# WHERE YOU GO WHEN YOU LEAVE - TIMELINE ASSEMBLAGE\\n\\n\")\n        \n        for entry in timeline_assemblage:\n            f.write(f\"## {entry['id']} [{entry['timestamp']}]\\n\\n\")\n            \n            if \"missing_assembly_data\" in entry:\n                f.write(\"*Assembly data not found*\\n\\n\")\n                f.write(f\"**Image:** `{entry['image_path']}`\\n\\n\")\n                continue\n                \n            f.write(f\"**Poem:** {entry.get('poem', 'Unknown')}\\n\\n\")\n            f.write(f\"**Content:** {entry.get('content', '')}\\n\\n\")\n            f.write(f\"**Syntagma Type:** {entry.get('syntagmaType', '')}\\n\\n\")\n            f.write(f\"**Operative Ekphrasis:** {entry.get('operativeEkphrasis', '')}\\n\\n\")\n            f.write(f\"**Image Type:** {entry.get('imageType', '')}\\n\\n\")\n            f.write(f\"**Cineosis Function:** {entry.get('cineosisFunction', '')}\\n\\n\")\n            f.write(f\"**Image:** `{entry['image_path']}`\\n\\n\")\n            f.write(\"---\\n\\n\")\n            \n    print(f\"Created markdown version at: {markdown_file}\")\n\nif __name__ == \"__main__\":\n    compile_timeline_assemblage()\n",
    "file_references": [
      "COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json",
      "{folder_id}_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "TIGER/compile_image_sequence.py",
    "size": 6616,
    "lines": 167,
    "source": "import os\nimport re\nimport glob\nfrom datetime import datetime, timedelta\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nOUTPUT_FILE = os.path.join(BASE_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\n\n# Video duration constants\nTOTAL_VIDEO_DURATION = \"28:23:00\"  # 28 minutes and 23 seconds\nPOEM_DURATION = \"02:11:00\"  # 2 minutes and 11 seconds\n\n# Convert time string to seconds\ndef time_to_seconds(time_str):\n    parts = time_str.split(\":\")\n    return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])\n\n# Convert seconds to time string\ndef seconds_to_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    secs = seconds % 60\n    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n\n# Sequence of poem sections\nSEQUENCE = [\n    (\"01_SH\", \"SH\", \"Out of Life\"),\n    (\"02_FL\", \"FL\", \"Flashing Lights\"),\n    (\"03_HT\", \"HT\", \"How to Break Off an Engagement\"),\n    (\"04_NM\", \"NM\", \"Nevermore\"),\n    (\"05_BE\", \"BE\", \"Bloodline\"),\n    (\"06_AT\", \"AT\", \"Resurrecting Atlantis\"),\n    (\"07_DJ\", \"DJ\", \"DJ Turn Me Up\"),\n    (\"08_NS\", \"NS\", \"Newly Single\"),\n    (\"09_YH\", \"YH\", \"Yet Heard\"),\n    (\"10_MR\", \"MR\", \"Magic Ride\"),\n    (\"12_RU\", \"RU\", \"Reunion\"),\n    (\"13_HW\", \"HW\", \"How to Win My Heart\"),\n    (\"14_HM\", \"HM\", \"Hot Minute\")\n]\n\ndef parse_timestamp(timestamp_str):\n    \"\"\"Convert timestamp string 'HHMMSS' to seconds.\"\"\"\n    hours = int(timestamp_str[0:2])\n    minutes = int(timestamp_str[2:4])\n    seconds = int(timestamp_str[4:6])\n    return hours * 3600 + minutes * 60 + seconds\n\ndef format_timestamp(seconds):\n    \"\"\"Convert seconds to 'HH:MM:SS' format.\"\"\"\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    secs = seconds % 60\n    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n\ndef calculate_entry_timestamp(poem_start_seconds, poem_end_seconds, entry_id):\n    \"\"\"Calculate timestamp for a specific entry based on its ID number.\"\"\"\n    try:\n        # Extract entry number (e.g., 001 from 'SH001')\n        entry_num = int(re.search(r'\\d+', entry_id).group())\n        # Count total entries in this poem\n        total_duration = poem_end_seconds - poem_start_seconds\n        # Assume we have approximately 70 entries per poem on average\n        max_entries = 70\n        # Calculate individual entry time\n        entry_seconds = poem_start_seconds + (total_duration * entry_num / max_entries)\n        return format_timestamp(int(entry_seconds))\n    except (AttributeError, ValueError):\n        return \"00:00:00\"  # Default timestamp if parsing fails\n\ndef find_images_for_entry(folder_id, entry_id):\n    \"\"\"Find all PNG images matching an entry ID.\"\"\"\n    images = set()  # Use a set to prevent duplicates\n    \n    # Check for different possible folder structures\n    possible_paths = [\n        os.path.join(BASE_DIR, folder_id, f\"{entry_id}__*.png\"),\n        os.path.join(BASE_DIR, folder_id, f\"{folder_id}_shots\", f\"{entry_id}__*.png\"),\n        os.path.join(BASE_DIR, folder_id, f\"{folder_id}_shots\", f\"{entry_id}_*.png\"),\n        os.path.join(BASE_DIR, folder_id, \"shots\", f\"{entry_id}__*.png\"),\n        os.path.join(BASE_DIR, folder_id, f\"{entry_id}*.png\")\n    ]\n    \n    for path_pattern in possible_paths:\n        found_images = glob.glob(path_pattern)\n        if found_images:\n            for img in found_images:\n                images.add(img)  # Add to set to ensure uniqueness\n    \n    return sorted(list(images))\n\ndef calculate_timestamp_for_entry(poem_index, entry_num):\n    \"\"\"Calculate timestamp for an entry based on poem index and entry number.\"\"\"\n    # Each poem is exactly 2:11 (2 minutes and 11 seconds)\n    poem_seconds = time_to_seconds(POEM_DURATION)\n    \n    # Calculate starting time for this poem\n    poem_start_seconds = poem_index * poem_seconds\n    \n    # Calculate relative position within the poem (0.0 to 1.0)\n    # Assuming max 70 entries per poem\n    relative_position = entry_num / 70.0\n    \n    # Calculate absolute time for this entry\n    entry_seconds = poem_start_seconds + (relative_position * poem_seconds)\n    \n    return seconds_to_time(int(entry_seconds))\n\ndef compile_simplified_sequence():\n    with open(OUTPUT_FILE, \"w\") as outfile:\n        outfile.write(\"# WHERE YOU GO WHEN YOU LEAVE - SIMPLIFIED IMAGE SEQUENCE\\n\\n\")\n        outfile.write(\"*Format: ID [timestamp] filepath*\\n\\n\")\n        outfile.write(\"*Each poem section has exactly 2:11, shots distributed within each poem's time block*\\n\\n\")\n        \n        # Define the time block for each poem (2:11 = 131 seconds)\n        poem_block_seconds = time_to_seconds(POEM_DURATION)\n        \n        # Process each poem section separately\n        all_timestamped_entries = []\n        \n        for poem_index, (seq_id, folder_id, title) in enumerate(SEQUENCE):\n            # Calculate the start time for this poem section\n            poem_start_seconds = poem_index * poem_block_seconds\n            \n            # Collect all images for this poem\n            poem_images = []\n            \n            # Scan for entries from 001 to 070\n            for i in range(1, 71):\n                entry_id = f\"{folder_id}{i:03d}\"\n                \n                # Find images for this entry\n                images = find_images_for_entry(folder_id, entry_id)\n                \n                # Add each image with its ID\n                for img_path in images:\n                    rel_path = os.path.relpath(img_path, BASE_DIR)\n                    poem_images.append((entry_id, rel_path))\n            \n            # Skip empty poems\n            if not poem_images:\n                continue\n                \n            # Calculate time per shot for this poem\n            num_shots = len(poem_images)\n            seconds_per_shot = poem_block_seconds / num_shots\n            \n            # Generate timestamps for each shot in this poem\n            for shot_index, (entry_id, img_path) in enumerate(poem_images):\n                shot_seconds = poem_start_seconds + (shot_index * seconds_per_shot)\n                timestamp = seconds_to_time(int(shot_seconds))\n                \n                all_timestamped_entries.append((entry_id, timestamp, img_path))\n        \n        # Sort by timestamp to ensure proper sequence\n        all_timestamped_entries.sort(key=lambda x: x[1])\n        \n        # Write all entries\n        for entry_id, timestamp, img_path in all_timestamped_entries:\n            outfile.write(f\"{entry_id} [{timestamp}] `{img_path}`\\n\")\n        \n        print(f\"Created sequence with {len(all_timestamped_entries)} images, each poem allocated 2:11 at: {OUTPUT_FILE}\")\n\nif __name__ == \"__main__\":\n    compile_simplified_sequence()\n",
    "file_references": [
      "{entry_id}__*.png",
      "{entry_id}__*.png",
      "{entry_id}_*.png",
      "{entry_id}__*.png",
      "{entry_id}*.png",
      "/Users/gaia/resurrecting atlantis/TIGER",
      ")\n    return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])\n\n# Convert seconds to time string\ndef seconds_to_time(seconds):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    secs = seconds % 60\n    return f",
      "\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    secs = seconds % 60\n    return f",
      ", entry_id).group())\n        # Count total entries in this poem\n        total_duration = poem_end_seconds - poem_start_seconds\n        # Assume we have approximately 70 entries per poem on average\n        max_entries = 70\n        # Calculate individual entry time\n        entry_seconds = poem_start_seconds + (total_duration * entry_num / max_entries)\n        return format_timestamp(int(entry_seconds))\n    except (AttributeError, ValueError):\n        return ",
      "\n    # Each poem is exactly 2:11 (2 minutes and 11 seconds)\n    poem_seconds = time_to_seconds(POEM_DURATION)\n    \n    # Calculate starting time for this poem\n    poem_start_seconds = poem_index * poem_seconds\n    \n    # Calculate relative position within the poem (0.0 to 1.0)\n    # Assuming max 70 entries per poem\n    relative_position = entry_num / 70.0\n    \n    # Calculate absolute time for this entry\n    entry_seconds = poem_start_seconds + (relative_position * poem_seconds)\n    \n    return seconds_to_time(int(entry_seconds))\n\ndef compile_simplified_sequence():\n    with open(OUTPUT_FILE, ",
      "\n                \n                # Find images for this entry\n                images = find_images_for_entry(folder_id, entry_id)\n                \n                # Add each image with its ID\n                for img_path in images:\n                    rel_path = os.path.relpath(img_path, BASE_DIR)\n                    poem_images.append((entry_id, rel_path))\n            \n            # Skip empty poems\n            if not poem_images:\n                continue\n                \n            # Calculate time per shot for this poem\n            num_shots = len(poem_images)\n            seconds_per_shot = poem_block_seconds / num_shots\n            \n            # Generate timestamps for each shot in this poem\n            for shot_index, (entry_id, img_path) in enumerate(poem_images):\n                shot_seconds = poem_start_seconds + (shot_index * seconds_per_shot)\n                timestamp = seconds_to_time(int(shot_seconds))\n                \n                all_timestamped_entries.append((entry_id, timestamp, img_path))\n        \n        # Sort by timestamp to ensure proper sequence\n        all_timestamped_entries.sort(key=lambda x: x[1])\n        \n        # Write all entries\n        for entry_id, timestamp, img_path in all_timestamped_entries:\n            outfile.write(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "glob",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "TIGER/compile_collections.py",
    "size": 5755,
    "lines": 133,
    "source": "import os\nimport json\nimport re\nfrom datetime import datetime, timedelta\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nCOLLECTION_PREFIX = \"COLLECTION_WhereYouGoWhenYouLeave\"\n\n# Sequence with timestamps\nSEQUENCE = [\n    (\"01_SH\", \"SH\", \"Out of Life\", \"000000\", \"021100\"),\n    (\"02_FL\", \"FL\", \"Flashing Lights\", \"021100\", \"042200\"),\n    (\"03_HT\", \"HT\", \"How to Break Off an Engagement\", \"042200\", \"063300\"),\n    (\"04_NM\", \"NM\", \"Nevermore\", \"063300\", \"084400\"),\n    (\"05_BE\", \"BE\", \"Bloodline\", \"084400\", \"105500\"),\n    (\"06_AT\", \"AT\", \"Resurrecting Atlantis\", \"105500\", \"130600\"),\n    (\"07_DJ\", \"DJ\", \"DJ Turn Me Up\", \"130600\", \"151700\"),\n    (\"08_NS\", \"NS\", \"Newly Single\", \"151700\", \"172800\"),\n    (\"09_YH\", \"YH\", \"Yet Heard\", \"172800\", \"193900\"),\n    (\"10_MR\", \"MR\", \"Magic Ride\", \"193900\", \"215000\"),\n    (\"12_RU\", \"RU\", \"Reunion\", \"215000\", \"240100\"),\n    (\"13_HW\", \"HW\", \"How to Win My Heart\", \"240100\", \"261200\"),\n    (\"14_HM\", \"HM\", \"Hot Minute\", \"261200\", \"282300\")\n]\n\ndef parse_timestamp(timestamp_str):\n    \"\"\"Convert timestamp string 'HHMMSS' to seconds.\"\"\"\n    hours = int(timestamp_str[0:2])\n    minutes = int(timestamp_str[2:4])\n    seconds = int(timestamp_str[4:6])\n    return hours * 3600 + minutes * 60 + seconds\n\ndef format_timestamp(seconds):\n    \"\"\"Convert seconds to 'HH:MM:SS' format.\"\"\"\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    secs = seconds % 60\n    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n\ndef calculate_entry_timestamp(poem_start_seconds, poem_end_seconds, entry_id):\n    \"\"\"Calculate timestamp for a specific entry based on its ID number.\"\"\"\n    try:\n        # Extract entry number (e.g., 001 from 'SH001')\n        entry_num = int(re.search(r'\\d+', entry_id).group())\n        # Count total entries in this poem\n        total_duration = poem_end_seconds - poem_start_seconds\n        # Assume we have approximately 70 entries per poem on average\n        max_entries = 70\n        # Calculate individual entry time\n        entry_seconds = poem_start_seconds + (total_duration * entry_num / max_entries)\n        return format_timestamp(int(entry_seconds))\n    except (AttributeError, ValueError):\n        return \"00:00:00\"  # Default timestamp if parsing fails\n\nFILE_TYPES = [\n    {\n        \"suffix\": \"_poem.md\",\n        \"output\": f\"{COLLECTION_PREFIX}_poems.md\",\n        \"formatter\": lambda x: x,\n        \"process_timestamps\": False\n    },\n    {\n        \"suffix\": \"_prompts.md\",\n        \"output\": f\"{COLLECTION_PREFIX}_prompts.md\",\n        \"formatter\": lambda x: x,\n        \"process_timestamps\": True\n    },\n    {\n        \"suffix\": \"_assembly.json\",\n        \"output\": f\"{COLLECTION_PREFIX}_assemblies.md\",\n        \"formatter\": lambda x: f\"```json\\n{x}\\n```\",\n        \"process_timestamps\": False\n    }\n]\n\ndef compile_collections():\n    for ft in FILE_TYPES:\n        output_path = os.path.join(BASE_DIR, ft[\"output\"])\n        \n        with open(output_path, \"w\") as outfile:\n            outfile.write(f\"# {COLLECTION_PREFIX} - {ft['suffix'][1:].replace('_', ' ').title()}\\n\\n\")\n            \n            for seq_id, folder_id, title, start_ts, end_ts in SEQUENCE:\n                file_path = os.path.join(BASE_DIR, folder_id, f\"{folder_id}{ft['suffix']}\")\n                header = f\"## {seq_id} - {title} ({start_ts[:2]}:{start_ts[2:4]}:{start_ts[4:]} - {end_ts[:2]}:{end_ts[2:4]}:{end_ts[4:]})\\n\\n\"\n                \n                outfile.write(header)\n                \n                # Parse timestamp range\n                start_seconds = parse_timestamp(start_ts)\n                end_seconds = parse_timestamp(end_ts)\n                \n                if os.path.exists(file_path):\n                    try:\n                        with open(file_path, \"r\") as infile:\n                            content = infile.read()\n                            \n                            # Process timestamps for prompts\n                            if ft[\"process_timestamps\"] and ft[\"suffix\"] == \"_prompts.md\":\n                                # Find all ID patterns and add timestamps\n                                lines = content.split('\\n')\n                                processed_lines = []\n                                \n                                for line in lines:\n                                    # Match ID pattern at start of line (e.g., SH001, BE034)\n                                    id_match = re.match(r'^([A-Z]{2}\\d{3})', line)\n                                    if id_match:\n                                        entry_id = id_match.group(1)\n                                        timestamp = calculate_entry_timestamp(start_seconds, end_seconds, entry_id)\n                                        # Add timestamp after ID\n                                        line = re.sub(r'^([A-Z]{2}\\d{3})', f'\\\\1 [{timestamp}]', line)\n                                    processed_lines.append(line)\n                                \n                                content = '\\n'.join(processed_lines)\n                            \n                            # For JSON files\n                            if ft[\"suffix\"].endswith(\".json\"):\n                                content = json.dumps(json.loads(content), indent=2)\n                                \n                            outfile.write(ft[\"formatter\"](content))\n                    except Exception as e:\n                        outfile.write(f\"*Error processing {os.path.basename(file_path)}: {str(e)}*\")\n                else:\n                    outfile.write(f\"*File not found: {os.path.basename(file_path)}*\")\n                \n                outfile.write(\"\\n\\n---\\n\\n\")\n        \n        print(f\"Created collection: {ft['output']}\")\n\n\nif __name__ == \"__main__\":\n    compile_collections()",
    "file_references": [
      "_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER",
      "\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    secs = seconds % 60\n    return f",
      ", entry_id).group())\n        # Count total entries in this poem\n        total_duration = poem_end_seconds - poem_start_seconds\n        # Assume we have approximately 70 entries per poem on average\n        max_entries = 70\n        # Calculate individual entry time\n        entry_seconds = poem_start_seconds + (total_duration * entry_num / max_entries)\n        return format_timestamp(int(entry_seconds))\n    except (AttributeError, ValueError):\n        return "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "re",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "ROSETTA/rosetta_render_chang_e.py",
    "size": 9952,
    "lines": 240,
    "source": "#!/usr/bin/env python3\nimport argparse\nimport json\nimport html\nimport re\n\n\ndef esc(s: str) -> str:\n    return html.escape(s, quote=True)\n\n\ndef ensure_mermaid_style(code: str) -> str:\n    if not code:\n        return 'flowchart-elk LR\\n'\n    lines = [ln.rstrip() for ln in code.split('\\n')]\n    # Remove leading empty lines\n    while lines and not lines[0].strip():\n        lines.pop(0)\n    # Force first line to 'flowchart-elk LR'\n    if lines:\n        lines[0] = 'flowchart-elk LR'\n    else:\n        lines = ['flowchart-elk LR']\n    base = '\\n'.join(lines).strip()\n    # Append class definitions if missing\n    class_block = (\"classDef default fill:#fff,stroke:#000,stroke-width:1px,color:#000,rx:0,ry:0;\\n\"\n                   \"classDef rootNode fill:#fff,stroke:#000,stroke-width:2px,color:#000,rx:0,ry:0;\\n\"\n                   \"classDef categoryNode fill:#fff,stroke:#000,stroke-width:1.5px,color:#000,rx:0,ry:0;\\n\"\n                   \"classDef leafNode fill:#fff,stroke:#000,stroke-width:1px,color:#000,rx:0,ry:0;\\n\"\n                   \"class A rootNode;\\n\"\n                   \"class B1,B2,B3,B4,B5 categoryNode;\\n\"\n                   \"class C1,C2,D1,D2,E1,E2,F1,G1 leafNode;\")\n    if 'classDef default' not in base and 'class A rootNode' not in base:\n        base = base + \"\\n\" + class_block\n    return base\n\n\ndef render_html(data: dict, assets_prefix: str = '..') -> str:\n    title = esc(data.get('title'))\n    # Strip leading \"GOAL:\" if present in the normalized data to avoid duplication\n    raw_goal = (data.get('goal') or '').strip()\n    if raw_goal.upper().startswith('GOAL:'):\n        raw_goal = raw_goal[len('GOAL:'):].strip()\n    goal = esc(raw_goal)\n    diagram_code = data.get('diagram', {}).get('code') or ''\n    diagram_code = ensure_mermaid_style(diagram_code)\n    analysis = esc(data.get('analysis_definition') or '')\n    sections = data.get('sections', [])\n\n    # Build sections HTML\n    section_html_parts = []\n    arrow_pattern = re.compile(r\"\\s*(?:\\u2192|->|\u2192)\\s*\")\n\n    def normalize_mechanism_text(txt: str) -> str:\n        if not txt:\n            return ''\n        parts = [p.strip() for p in arrow_pattern.split(txt) if p.strip()]\n        if not parts:\n            normalized = txt.strip()\n        else:\n            normalized = '; '.join(parts)\n        if normalized and not normalized.endswith('.'):\n            normalized += '.'\n        return normalized\n\n    for sec in sections:\n        number = sec.get('number')\n        name = esc(sec.get('name') or '')\n        model_description = esc(sec.get('model_description') or '')\n        symbol_list_items = sec.get('symbol_list') or []\n        symbol_list_text = \"; \".join(esc(x) for x in symbol_list_items)\n        if symbol_list_text and not symbol_list_text.endswith('.'):\n            symbol_list_text += '.'\n        prescriptive_description = esc(sec.get('prescriptive_description') or '')\n        mechanism_text = normalize_mechanism_text(sec.get('mechanism_text') or '')\n        section_html_parts.append(f'''\n        <section class=\"content-card\" id=\"sec-{number}\">\n          <div class=\"section-header\"><div class=\"section-number\">{number}</div><h2 class=\"section-title\">{name}</h2></div>\n          <div class=\"model-description\">{model_description}</div>\n          <div class=\"symbol-list\">{symbol_list_text}</div>\n          <div class=\"prescriptive-description\">{prescriptive_description}</div>\n          <div class=\"mechanism-text\">{mechanism_text}</div>\n        </section>\n        ''')\n    sections_html = \"\\n\".join(section_html_parts)\n\n    # Mermaid: do NOT escape the code content, Mermaid needs raw text inside <pre class=\"mermaid\">\n    diagram_pre = f'''<pre class=\"mermaid\">{diagram_code}</pre>'''\n\n    # Note: We will use the same CSS/JS as the CHANG-E target document for full visual parity.\n\n    # Sidebar TOC links based on sections\n    toc_links = []\n    for sec in sorted(sections, key=lambda x: x.get('number') or 0):\n        number = esc(str(sec.get('number') or ''))\n        name = esc(sec.get('name') or '')\n        toc_links.append(f'<a href=\"#sec-{number}\" class=\"toc-item\">{number}. {name}</a>')\n    toc_html = \"\\n\".join(toc_links)\n\n    # Build intro Description/Prescription aggregated from ALL sections\n    intro_desc = esc(\" \".join([s.get('model_description') or '' for s in sections if s.get('model_description')]))\n    intro_pres = esc(\" \".join([s.get('prescriptive_description') or '' for s in sections if s.get('prescriptive_description')]))\n\n    # Build interactive analysis-definition using sections 1..5\n    def clause_text_for(idx: int) -> str:\n        sec = next((s for s in sections if (s.get('number') == idx)), None)\n        if not sec:\n            return ''\n        if idx == 1:\n            # Use symbol list when present\n            sym = sec.get('symbol_list') or []\n            sym_text = \"; \".join(esc(x) for x in sym)\n            if sym_text:\n                if not sym_text.endswith('.'):\n                    sym_text += ''\n                return f\"(1) a system of symbols ({sym_text})\"\n            # fallback to model description\n            return f\"(1) {esc(sec.get('model_description') or '')}\"\n        if idx == 2:\n            # Use symbol list as moods/motivations list when present\n            sym = sec.get('symbol_list') or []\n            sym_text = \"; \".join(esc(x) for x in sym)\n            base = f\"(2) establish powerful, pervasive, and long-lasting moods and motivations\"\n            if sym_text:\n                return f\"{base} ({sym_text})\"\n            return f\"(2) {esc(sec.get('model_description') or '')}\"\n        if idx in (3, 4, 5):\n            return f\"({idx}) {esc(sec.get('model_description') or '')}\"\n        return ''\n\n    span1 = f'<span data-target-section=\"1\" data-mermaid-node=\"B1\">{clause_text_for(1)}</span>'\n    span2 = f'<span data-target-section=\"2\" data-mermaid-node=\"B2\">{clause_text_for(2)}</span>'\n    span3 = f'<span data-target-section=\"3\" data-mermaid-node=\"B3\">{clause_text_for(3)}</span>'\n    span4 = f'<span data-target-section=\"4\" data-mermaid-node=\"B4\">{clause_text_for(4)}</span>'\n    span5 = f'<span data-target-section=\"5\" data-mermaid-node=\"B5\">{clause_text_for(5)}</span>'\n\n    analysis_interactive = (\n        f'<div class=\"analysis-definition\">{span1} which acts to {span2} in individuals by {span3} and {span4} that {span5}</div>'\n    )\n\n    html_doc = f'''<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n  <title>CULTURAL SYSTEM FOR POEMS \u2014 Out of Life (Rendered)</title>\n  <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Pro:ital,wght@0,400;0,600;1,400&display=swap\" rel=\"stylesheet\" />\n  <link rel=\"stylesheet\" href=\"{assets_prefix}/0_CHANG-E/poem.css\" />\n</head>\n<body>\n  <header class=\"header\">\n    <div class=\"container\">\n      <div class=\"header-content\">\n        <div class=\"logo\">CULTURAL SYSTEM FOR POEMS</div>\n        <div class=\"breadcrumb\"><a href=\"#\">Home</a> / <a href=\"#\">Poems</a> / Out of Life (Rendered)</div>\n      </div>\n    </div>\n  </header>\n\n  <div class=\"container\">\n    <div class=\"main-content\">\n      <main class=\"content-area\">\n        <div class=\"title-section\">\n          <h1 class=\"poem-title\">{title}</h1>\n          <div class=\"poem-goal\"><strong>GOAL:</strong> {goal}</div>\n        </div>\n\n        <div class=\"intro-single\">\n          <div class=\"intro-row\">\n            <div class=\"intro-label\">Description</div>\n            <div class=\"intro-text\">{intro_desc}</div>\n          </div>\n          <div class=\"intro-row\">\n            <div class=\"intro-label\">Prescription</div>\n            <div class=\"intro-text\">{intro_pres}</div>\n          </div>\n        </div>\n\n        <div class=\"diagram-section\">\n          <h3 class=\"diagram-title\">Structural Analysis Diagram</h3>\n          {diagram_pre}\n        </div>\n\n        {analysis_interactive}\n\n        {sections_html}\n      </main>\n\n      <aside class=\"sidebar\">\n        <div class=\"sidebar-section\">\n          <h3 class=\"sidebar-title\">Table of Contents</h3>\n          {toc_html}\n        </div>\n        <div class=\"sidebar-section\">\n          <h3 class=\"sidebar-title\">Document Metadata</h3>\n          <div class=\"metadata-item\"><span>Type</span> <span>Literary Analysis</span></div>\n          <div class=\"metadata-item\"><span>Method</span> <span>Geertzian Framework</span></div>\n          <div class=\"metadata-item\"><span>Status</span> <span>Rendered</span></div>\n          <div class=\"metadata-item\"><span>Version</span> <span>1.0</span></div>\n        </div>\n      </aside>\n    </div>\n  </div>\n\n  <nav class=\"navigation\">\n    <div class=\"container\"><a href=\"#\" class=\"nav-button\">\u2190 Back to Poem Index</a></div>\n  </nav>\n\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.1/mermaid.min.js\"></script>\n  <script src=\"{assets_prefix}/0_CHANG-E/poem.js\"></script>\n  <script>\n    if (window.mermaid) {{\n      mermaid.initialize({{ startOnLoad: true }});\n      // Explicitly trigger a run in case the content was injected before Mermaid loaded\n      if (mermaid.run) {{ mermaid.run(); }}\n    }}\n  </script>\n</body>\n</html>'''\n    return html_doc\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Render CHANG-E style HTML from normalized JSON')\n    parser.add_argument('--input-json', required=True, help='Path to normalized JSON produced by rosetta_extract.py')\n    parser.add_argument('--output-html', required=True, help='Where to write the CHANG-E style HTML file')\n    parser.add_argument('--assets-prefix', default='..', help='Relative prefix to CHANG-E assets directory (default ..)')\n    args = parser.parse_args()\n\n    with open(args.input_json, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    html_out = render_html(data, assets_prefix=args.assets_prefix)\n    with open(args.output_html, 'w', encoding='utf-8') as f:\n        f.write(html_out)\n\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      ">{number}</div><h2 class=",
      ">{name}</h2></div>\n          <div class=",
      ">{model_description}</div>\n          <div class=",
      ">{symbol_list_text}</div>\n          <div class=",
      ">{prescriptive_description}</div>\n          <div class=",
      ">{mechanism_text}</div>\n        </section>\n        ",
      ">{diagram_code}</pre>",
      "\n\n    # Note: We will use the same CSS/JS as the CHANG-E target document for full visual parity.\n\n    # Sidebar TOC links based on sections\n    toc_links = []\n    for sec in sorted(sections, key=lambda x: x.get(",
      ">{number}. {name}</a>",
      ".join(toc_links)\n\n    # Build intro Description/Prescription aggregated from ALL sections\n    intro_desc = esc(",
      "\n        if idx == 2:\n            # Use symbol list as moods/motivations list when present\n            sym = sec.get(",
      ">{clause_text_for(1)}</span>",
      ">{clause_text_for(2)}</span>",
      ">{clause_text_for(3)}</span>",
      ">{clause_text_for(4)}</span>",
      ">{clause_text_for(5)}</span>",
      ">{span1} which acts to {span2} in individuals by {span3} and {span4} that {span5}</div>",
      " />\n  <meta name=",
      " />\n  <title>CULTURAL SYSTEM FOR POEMS \u2014 Out of Life (Rendered)</title>\n  <link href=",
      " />\n  <link rel=",
      "{assets_prefix}/0_CHANG-E/poem.css",
      ">CULTURAL SYSTEM FOR POEMS</div>\n        <div class=",
      ">Home</a> / <a href=",
      ">Poems</a> / Out of Life (Rendered)</div>\n      </div>\n    </div>\n  </header>\n\n  <div class=",
      ">{title}</h1>\n          <div class=",
      "><strong>GOAL:</strong> {goal}</div>\n        </div>\n\n        <div class=",
      ">Description</div>\n            <div class=",
      ">{intro_desc}</div>\n          </div>\n          <div class=",
      ">Prescription</div>\n            <div class=",
      ">{intro_pres}</div>\n          </div>\n        </div>\n\n        <div class=",
      ">Structural Analysis Diagram</h3>\n          {diagram_pre}\n        </div>\n\n        {analysis_interactive}\n\n        {sections_html}\n      </main>\n\n      <aside class=",
      ">Table of Contents</h3>\n          {toc_html}\n        </div>\n        <div class=",
      ">Document Metadata</h3>\n          <div class=",
      "><span>Type</span> <span>Literary Analysis</span></div>\n          <div class=",
      "><span>Method</span> <span>Geertzian Framework</span></div>\n          <div class=",
      "><span>Status</span> <span>Rendered</span></div>\n          <div class=",
      "><span>Version</span> <span>1.0</span></div>\n        </div>\n      </aside>\n    </div>\n  </div>\n\n  <nav class=",
      ">\u2190 Back to Poem Index</a></div>\n  </nav>\n\n  <script src=",
      "></script>\n  <script src=",
      "></script>\n  <script>\n    if (window.mermaid) {{\n      mermaid.initialize({{ startOnLoad: true }});\n      // Explicitly trigger a run in case the content was injected before Mermaid loaded\n      if (mermaid.run) {{ mermaid.run(); }}\n    }}\n  </script>\n</body>\n</html>"
    ],
    "subprocess_calls": [],
    "imports": [
      "argparse",
      "json",
      "html",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "ROSETTA/rosetta_batch.py",
    "size": 2412,
    "lines": 69,
    "source": "#!/usr/bin/env python3\nimport argparse\nimport subprocess\nimport sys\nfrom pathlib import Path\n\n\ndef run(cmd, cwd):\n    proc = subprocess.run(cmd, cwd=cwd, text=True)\n    if proc.returncode != 0:\n        raise SystemExit(proc.returncode)\n\n\ndef main():\n    p = argparse.ArgumentParser(description=\"Batch extract + render poems using Rosetta pipeline\")\n    p.add_argument(\"--rosetta\", required=True, help=\"Path to Rosetta spec JSON\")\n    p.add_argument(\"--source-dir\", required=True, help=\"Directory to search for source HTML files\")\n    p.add_argument(\"--pattern\", default=\"poem_*.html\", help=\"Glob pattern for source files (default: poem_*.html)\")\n    p.add_argument(\"--source-id\", required=True, help=\"source_id key in spec mappings (e.g., alps_poem_1_out_of_life or a generic alps id)\")\n    p.add_argument(\"--out-dir\", required=True, help=\"Directory to write outputs\")\n    args = p.parse_args()\n\n    repo_root = Path(__file__).resolve().parents[1]\n    source_dir = (repo_root / args.source_dir).resolve()\n    out_dir = (repo_root / args.out_dir).resolve()\n    rosetta = (repo_root / args.rosetta).resolve()\n\n    if not source_dir.is_dir():\n        print(f\"Source dir not found: {source_dir}\", file=sys.stderr)\n        sys.exit(1)\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    files = sorted(source_dir.glob(args.pattern))\n    if not files:\n        print(f\"No files matched pattern {args.pattern} in {source_dir}\")\n        return\n\n    for f in files:\n        base = f.stem  # e.g., poem_1_out_of_life\n        json_out = out_dir / f\"out_{base}.json\"\n        html_out = out_dir / f\"out_{base}_chang_e.html\"\n\n        # Extract\n        extract_cmd = [\n            sys.executable,\n            str((repo_root / \"ROSETTA/rosetta_extract.py\").resolve()),\n            \"--rosetta\", str(rosetta),\n            \"--source-id\", args.source_id,\n            \"--input\", str(f.resolve()),\n            \"--output\", str(json_out),\n        ]\n        run(extract_cmd, cwd=str(repo_root))\n\n        # Render\n        render_cmd = [\n            sys.executable,\n            str((repo_root / \"ROSETTA/rosetta_render_chang_e.py\").resolve()),\n            \"--input-json\", str(json_out),\n            \"--output-html\", str(html_out),\n            \"--assets-prefix\", \"../..\",\n        ]\n        run(render_cmd, cwd=str(repo_root))\n\n        print(f\"Processed {f.name} -> {html_out.relative_to(repo_root)}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "out_{base}.json",
      ")\n    args = p.parse_args()\n\n    repo_root = Path(__file__).resolve().parents[1]\n    source_dir = (repo_root / args.source_dir).resolve()\n    out_dir = (repo_root / args.out_dir).resolve()\n    rosetta = (repo_root / args.rosetta).resolve()\n\n    if not source_dir.is_dir():\n        print(f",
      ")\n        return\n\n    for f in files:\n        base = f.stem  # e.g., poem_1_out_of_life\n        json_out = out_dir / f",
      "\n        html_out = out_dir / f",
      "\n\n        # Extract\n        extract_cmd = [\n            sys.executable,\n            str((repo_root / ",
      ", str(json_out),\n        ]\n        run(extract_cmd, cwd=str(repo_root))\n\n        # Render\n        render_cmd = [\n            sys.executable,\n            str((repo_root / ",
      "../.."
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd, cwd=cwd, text=True"
      }
    ],
    "imports": [
      "argparse",
      "subprocess",
      "sys",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "ROSETTA/rosetta_render_all.py",
    "size": 12467,
    "lines": 290,
    "source": "#!/usr/bin/env python3\nimport argparse\nimport json\nimport html\nimport re\nfrom pathlib import Path\n\n\ndef esc(s: str) -> str:\n    return html.escape(s or '', quote=True)\n\n\ndef ensure_mermaid_style(code: str) -> str:\n    if not code:\n        return 'flowchart-elk LR\\n'\n    lines = [ln.rstrip() for ln in code.split('\\n')]\n    while lines and not lines[0].strip():\n        lines.pop(0)\n    if lines:\n        lines[0] = 'flowchart-elk LR'\n    else:\n        lines = ['flowchart-elk LR']\n    base = '\\n'.join(lines).strip()\n    class_block = (\n        \"classDef default fill:#fff,stroke:#000,stroke-width:1px,color:#000,rx:0,ry:0;\\n\"\n        \"classDef rootNode fill:#fff,stroke:#000,stroke-width:2px,color:#000,rx:0,ry:0;\\n\"\n        \"classDef categoryNode fill:#fff,stroke:#000,stroke-width:1.5px,color:#000,rx:0,ry:0;\\n\"\n        \"classDef leafNode fill:#fff,stroke:#000,stroke-width:1px,color:#000,rx:0,ry:0;\\n\"\n        \"class A rootNode;\\n\"\n        \"class B1,B2,B3,B4,B5 categoryNode;\\n\"\n        \"class C1,C2,D1,D2,E1,E2,F1,G1 leafNode;\"\n    )\n    if 'classDef default' not in base and 'class A rootNode' not in base:\n        base = base + \"\\n\" + class_block\n    return base\n\n\ndef normalize_mechanism_text(txt: str) -> str:\n    if not txt:\n        return ''\n    arrow_pattern = re.compile(r\"\\s*(?:\\u2192|->|\u2192)\\s*\")\n    parts = [p.strip() for p in arrow_pattern.split(txt) if p.strip()]\n    normalized = '; '.join(parts) if parts else txt.strip()\n    if normalized and not normalized.endswith('.'):\n        normalized += '.'\n    return normalized\n\n\ndef section_block(poem_slug: str, sec: dict) -> str:\n    number = sec.get('number')\n    name = esc(sec.get('name') or '')\n    model_description = esc(sec.get('model_description') or '')\n    symbol_list_items = sec.get('symbol_list') or []\n    symbol_list_text = \"; \".join(esc(x) for x in symbol_list_items)\n    if symbol_list_text and not symbol_list_text.endswith('.'): symbol_list_text += '.'\n    prescriptive_description = esc(sec.get('prescriptive_description') or '')\n    mechanism_text = normalize_mechanism_text(sec.get('mechanism_text') or '')\n    sec_id = f\"{poem_slug}-{number}\"\n    return f'''\n        <section class=\"content-card\" id=\"{sec_id}\">\n          <div class=\"section-header\"><div class=\"section-number\">{number}</div><h2 class=\"section-title\">{name}</h2></div>\n          <div class=\"model-description\">{model_description}</div>\n          <div class=\"symbol-list\">{symbol_list_text}</div>\n          <div class=\"prescriptive-description\">{prescriptive_description}</div>\n          <div class=\"mechanism-text\">{mechanism_text}</div>\n        </section>\n    '''\n\n\ndef analysis_interactive_block(poem_slug: str, sections: list) -> str:\n    def clause(idx: int) -> str:\n        sec = next((s for s in sections if (s.get('number') == idx)), None)\n        if not sec: return ''\n        if idx == 1:\n            sym = sec.get('symbol_list') or []\n            sym_text = \"; \".join(esc(x) for x in sym)\n            return f\"(1) a system of symbols ({sym_text})\" if sym_text else f\"(1) {esc(sec.get('model_description') or '')}\"\n        if idx == 2:\n            sym = sec.get('symbol_list') or []\n            sym_text = \"; \".join(esc(x) for x in sym)\n            base = \"(2) establish powerful, pervasive, and long-lasting moods and motivations\"\n            return f\"{base} ({sym_text})\" if sym_text else f\"(2) {esc(sec.get('model_description') or '')}\"\n        return f\"({idx}) {esc(sec.get('model_description') or '')}\"\n\n    spans = []\n    for idx, node in [(1,'B1'),(2,'B2'),(3,'B3'),(4,'B4'),(5,'B5')]:\n        # Use data-target-id to match exact section id and be compatible with poem.js\n        target_id = f\"{poem_slug}-{idx}\"\n        spans.append(f'<span data-target-id=\"{target_id}\" data-mermaid-node=\"{node}\">{clause(idx)}</span>')\n    return f'<div class=\"analysis-definition\">{spans[0]} which acts to {spans[1]} in individuals by {spans[2]} and {spans[3]} that {spans[4]}</div>'\n\n\ndef poem_block(data: dict, assets_prefix: str) -> str:\n    title = esc(data.get('title') or '')\n    poem_id = data.get('poem_id') or re.sub(r\"\\W+\",\"-\", title.lower()).strip('-')\n    goal_raw = (data.get('goal') or '').strip()\n    if goal_raw.upper().startswith('GOAL:'):\n        goal_raw = goal_raw[len('GOAL:'):].strip()\n    goal = esc(goal_raw)\n    diagram_code = ensure_mermaid_style((data.get('diagram') or {}).get('code') or '')\n    sections = data.get('sections') or []\n\n    # Intro aggregation\n    intro_desc = esc(\" \".join([s.get('model_description') or '' for s in sections if s.get('model_description')]))\n    intro_pres = esc(\" \".join([s.get('prescriptive_description') or '' for s in sections if s.get('prescriptive_description')]))\n\n    # TOC per-poem\n    toc_links = '\\n'.join([f'<a href=\"#{poem_id}-{s.get(\"number\")}\" class=\"toc-item\">{s.get(\"number\")}. {esc(s.get(\"name\") or \"\")}</a>' for s in sections])\n\n    sections_html = '\\n'.join([section_block(poem_id, s) for s in sections])\n    analysis_html = analysis_interactive_block(poem_id, sections)\n\n    return f'''\n    <section class=\"poem-block\" data-poem=\"{poem_id}\">\n      <div class=\"title-section\" id=\"{poem_id}\">\n        <h1 class=\"poem-title\">{title}</h1>\n        <div class=\"poem-goal\"><strong>GOAL:</strong> {goal}</div>\n      </div>\n\n      <div class=\"intro-single\">\n        <div class=\"intro-row\"><div class=\"intro-label\">Description</div><div class=\"intro-text\">{intro_desc}</div></div>\n        <div class=\"intro-row\"><div class=\"intro-label\">Prescription</div><div class=\"intro-text\">{intro_pres}</div></div>\n      </div>\n\n      <div class=\"diagram-section\">\n        <h3 class=\"diagram-title\">Structural Analysis Diagram</h3>\n        <pre class=\"mermaid\">{diagram_code}</pre>\n      </div>\n\n      {analysis_html}\n\n      {sections_html}\n      <hr style=\"margin:28px 0; border:none; border-top:1px solid var(--border);\" />\n    </section>\n    '''\n\n\ndef main():\n    ap = argparse.ArgumentParser(description='Render a single page with all poems')\n    ap.add_argument('--inputs-dir', required=True, help='Directory containing normalized JSON files')\n    ap.add_argument('--pattern', default='out_poem_*.json', help='Glob pattern for inputs')\n    ap.add_argument('--output-html', required=True)\n    ap.add_argument('--assets-prefix', default='..')\n    args = ap.parse_args()\n\n    root = Path(__file__).resolve().parents[1]\n    inputs_dir = (root / args.inputs_dir).resolve()\n    files = sorted(\n        inputs_dir.glob(args.pattern),\n        key=lambda p: int(re.search(r'out_poem_(\\d+)_', p.name).group(1))\n    )\n    if not files:\n        raise SystemExit(f'No inputs matching {args.pattern} in {inputs_dir}')\n\n    poems = []\n    for f in files:\n        with open(f, 'r', encoding='utf-8') as fh:\n            data = json.load(fh)\n        m = re.search(r'out_poem_(\\d+)_', f.name)\n        idx = int(m.group(1)) if m else 9999\n        data['__order'] = idx\n        poems.append(data)\n    poems.sort(key=lambda d: d.get('__order', 9999))\n\n    # Build sidebar with all poems and their sections\n    # Build collapsible sidebar groups: compact poem number as parent; children are section links\n    toc_groups = []\n    for data in poems:\n        raw_title = data.get('title') or ''\n        title = esc(raw_title)\n        poem_id = data.get('poem_id') or re.sub(r\"\\W+\",\"-\", raw_title.lower()).strip('-')\n        num_match = re.match(r\"^(\\d+)\", html.unescape(raw_title))\n        num_label = num_match.group(1) if num_match else str(data.get('__order', ''))\n        # Strip leading number, punctuation, and whitespace from display title to avoid duplicate number\n        display_title = re.sub(r\"^\\s*\\d+\\s*[\\.:\\)\\-]*\\s*\", \"\", raw_title).strip()\n        display_title = esc(display_title)\n        children = '\\n'.join([f'<a href=\"#{poem_id}-{s.get(\"number\")}\" class=\"toc-item\">{s.get(\"number\")}. {esc(s.get(\"name\") or \"\")}</a>' for s in (data.get('sections') or [])])\n        group_html = f'''<div class=\"toc-group\" data-poem=\"{poem_id}\">\n  <a href=\"#{poem_id}\" class=\"toc-item toc-parent\" aria-expanded=\"false\" data-poem=\"{poem_id}\"><span class=\"toc-number\">{num_label}</span><span class=\"toc-title\">{display_title}</span></a>\n  <div class=\"toc-children\" hidden>\n    <a href=\"#{poem_id}\" class=\"toc-item\">\u2014 {title} (overview)</a>\n    {children}\n  </div>\n</div>'''\n        toc_groups.append(group_html)\n    toc_html = '\\n'.join(toc_groups)\n\n    # Compose page\n    blocks = '\\n'.join([poem_block(d, args.assets_prefix) for d in poems])\n\n    html_doc = f'''<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\" />\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n  <title>CULTURAL SYSTEM FOR POEMS \u2014 All Poems</title>\n  <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Pro:ital,wght@0,400;0,600;1,400&display=swap\" rel=\"stylesheet\" />\n  <link rel=\"stylesheet\" href=\"{args.assets_prefix}/0_CHANG-E/poem.css\" />\n</head>\n<body>\n  <header class=\"header\">\n    <div class=\"container\">\n      <div class=\"header-content\">\n        <div class=\"logo\">CULTURAL SYSTEM FOR POEMS</div>\n        <div class=\"breadcrumb\"><a href=\"#\">Home</a> / <a href=\"#\">Poems</a> / All (Rendered)</div>\n      </div>\n    </div>\n  </header>\n\n  <div class=\"container\">\n    <div class=\"main-content\">\n      <main class=\"content-area\">\n{blocks}\n      </main>\n\n      <aside class=\"sidebar\">\n        <div class=\"sidebar-section\">\n          <h3 class=\"sidebar-title\">Table of Contents</h3>\n{toc_html}\n        </div>\n        <div class=\"sidebar-section\">\n          <h3 class=\"sidebar-title\">Document Metadata</h3>\n          <div class=\"metadata-item\"><span>Type</span> <span>Literary Analysis</span></div>\n          <div class=\"metadata-item\"><span>Method</span> <span>Geertzian Framework</span></div>\n          <div class=\"metadata-item\"><span>Status</span> <span>Rendered</span></div>\n          <div class=\"metadata-item\"><span>Version</span> <span>1.0</span></div>\n        </div>\n      </aside>\n    </div>\n  </div>\n\n  <nav class=\"navigation\">\n    <div class=\"container\"><a href=\"#\" class=\"nav-button\">\u2190 Back to Poem Index</a></div>\n  </nav>\n\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.1/mermaid.min.js\"></script>\n  <script src=\"{args.assets_prefix}/0_CHANG-E/poem.js\"></script>\n  <script>\n    (function(){{\n      function nearestPoem(el){{ return el && el.closest ? el.closest('.poem-block') : null; }}\n      // Scope diagram node clicks to local poem\n      document.querySelectorAll('.diagram-section .mermaid').forEach(diagram => {{\n        const obs = new MutationObserver(() => {{\n          const svg = diagram.querySelector('svg'); if (!svg) return;\n          svg.querySelectorAll('.node').forEach(node => {{\n            if (node.__boundLocal) return; node.__boundLocal = true;\n            const id = (node.dataset && node.dataset.id) || '';\n            if (!/^B\\d+$/.test(id)) return;\n            node.addEventListener('click', () => {{\n              const num = id.replace('B','');\n              const container = nearestPoem(diagram);\n              if (!container) return;\n              const target = container.querySelector(`.content-card[id$='-${{num}}']`);\n              if (target) target.scrollIntoView({{ behavior: 'smooth', block: 'start' }});\n            }});\n          }});\n        }});\n        obs.observe(diagram, {{ childList: true, subtree: true }});\n      }});\n      // Ensure analysis-definition spans scroll to exact target id (and not a different poem)\n      document.querySelectorAll('.analysis-definition span[data-target-id]').forEach(span => {{\n        if (span.__boundLocal) return; span.__boundLocal = true;\n        span.addEventListener('click', (e) => {{\n          const id = span.getAttribute('data-target-id');\n          const el = document.getElementById(id);\n          if (el) {{ e.preventDefault && e.preventDefault(); el.scrollIntoView({{ behavior: 'smooth', block: 'start' }}); try {{ history.replaceState(null,'','#'+id); }} catch(_) {{ location.hash = id; }} }}\n        }});\n      }});\n    }})();\n  </script>\n  <script>\n    if (window.mermaid) {{\n      mermaid.initialize({{ startOnLoad: true }});\n      if (mermaid.run) {{ mermaid.run(); }}\n    }}\n  </script>\n</body>\n</html>'''\n\n    out_path = (root / args.output_html).resolve()\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(out_path, 'w', encoding='utf-8') as f:\n        f.write(html_doc)\n\n    print(f\"Wrote {out_path}\")\n\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      "out_poem_*.json",
      ">{number}</div><h2 class=",
      ">{name}</h2></div>\n          <div class=",
      ">{model_description}</div>\n          <div class=",
      ">{symbol_list_text}</div>\n          <div class=",
      ">{prescriptive_description}</div>\n          <div class=",
      ">{mechanism_text}</div>\n        </section>\n    ",
      ">{clause(idx)}</span>",
      ">{spans[0]} which acts to {spans[1]} in individuals by {spans[2]} and {spans[3]} that {spans[4]}</div>",
      ")}</a>",
      ">{title}</h1>\n        <div class=",
      "><strong>GOAL:</strong> {goal}</div>\n      </div>\n\n      <div class=",
      ">Description</div><div class=",
      ">{intro_desc}</div></div>\n        <div class=",
      ">Prescription</div><div class=",
      ">{intro_pres}</div></div>\n      </div>\n\n      <div class=",
      ">Structural Analysis Diagram</h3>\n        <pre class=",
      ">{diagram_code}</pre>\n      </div>\n\n      {analysis_html}\n\n      {sections_html}\n      <hr style=",
      " />\n    </section>\n    ",
      ")\n    args = ap.parse_args()\n\n    root = Path(__file__).resolve().parents[1]\n    inputs_dir = (root / args.inputs_dir).resolve()\n    files = sorted(\n        inputs_dir.glob(args.pattern),\n        key=lambda p: int(re.search(r",
      ")}</a>",
      ">{num_label}</span><span class=",
      ">{display_title}</span></a>\n  <div class=",
      ">\u2014 {title} (overview)</a>\n    {children}\n  </div>\n</div>",
      " />\n  <meta name=",
      " />\n  <title>CULTURAL SYSTEM FOR POEMS \u2014 All Poems</title>\n  <link href=",
      " />\n  <link rel=",
      "{args.assets_prefix}/0_CHANG-E/poem.css",
      ">CULTURAL SYSTEM FOR POEMS</div>\n        <div class=",
      ">Home</a> / <a href=",
      ">Poems</a> / All (Rendered)</div>\n      </div>\n    </div>\n  </header>\n\n  <div class=",
      ">\n{blocks}\n      </main>\n\n      <aside class=",
      ">Table of Contents</h3>\n{toc_html}\n        </div>\n        <div class=",
      ">Document Metadata</h3>\n          <div class=",
      "><span>Type</span> <span>Literary Analysis</span></div>\n          <div class=",
      "><span>Method</span> <span>Geertzian Framework</span></div>\n          <div class=",
      "><span>Status</span> <span>Rendered</span></div>\n          <div class=",
      "><span>Version</span> <span>1.0</span></div>\n        </div>\n      </aside>\n    </div>\n  </div>\n\n  <nav class=",
      ">\u2190 Back to Poem Index</a></div>\n  </nav>\n\n  <script src=",
      "></script>\n  <script src=",
      "></script>\n  <script>\n    (function(){{\n      function nearestPoem(el){{ return el && el.closest ? el.closest(",
      ") : null; }}\n      // Scope diagram node clicks to local poem\n      document.querySelectorAll(",
      ";\n            if (!/^B\\d+$/.test(id)) return;\n            node.addEventListener(",
      " }});\n            }});\n          }});\n        }});\n        obs.observe(diagram, {{ childList: true, subtree: true }});\n      }});\n      // Ensure analysis-definition spans scroll to exact target id (and not a different poem)\n      document.querySelectorAll(",
      "+id); }} catch(_) {{ location.hash = id; }} }}\n        }});\n      }});\n    }})();\n  </script>\n  <script>\n    if (window.mermaid) {{\n      mermaid.initialize({{ startOnLoad: true }});\n      if (mermaid.run) {{ mermaid.run(); }}\n    }}\n  </script>\n</body>\n</html>",
      "\n\n    out_path = (root / args.output_html).resolve()\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(out_path, "
    ],
    "subprocess_calls": [],
    "imports": [
      "argparse",
      "json",
      "html",
      "re",
      "pathlib"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "ROSETTA/rosetta_extract.py",
    "size": 8632,
    "lines": 235,
    "source": "#!/usr/bin/env python3\nimport argparse\nimport json\nimport os\nimport re\nfrom bs4 import BeautifulSoup\n\n\ndef read_file(path: str):\n    with open(path, 'r', encoding='utf-8') as f:\n        return f.read()\n\n\ndef collapse_ws(s: str) -> str:\n    return re.sub(r\"\\s+\", \" \", s).strip()\n\n\ndef strip_prefix(text: str, prefix: str) -> str:\n    t = text.strip()\n    if t.startswith(prefix):\n        return t[len(prefix):].strip()\n    return t\n\n\ndef apply_postprocess(text: str, steps, rules):\n    if text is None:\n        return None\n    out = text\n    if isinstance(steps, list):\n        for step in steps:\n            if step == 'split_symbols':\n                # handled elsewhere for lists\n                pass\n            elif step == 'trim':\n                out = out.strip()\n            elif step == 'collapse_whitespace':\n                out = collapse_ws(out)\n            elif step.startswith('strip_prefix:'):\n                pref = step.split(':', 1)[1]\n                out = strip_prefix(out, pref)\n            else:\n                # unknown step noop\n                pass\n    # global rules\n    if rules.get('trim', False):\n        out = out.strip()\n    if rules.get('collapse_whitespace', False):\n        out = collapse_ws(out)\n    # strip common leading symbols\n    for pref in rules.get('strip_prefixes', []):\n        out = out.lstrip().lstrip(pref).lstrip()\n    # remove surrounding quotes if configured\n    if rules.get('strip_quotes', False):\n        out = out.strip()\n        if (out.startswith('\"') and out.endswith('\"')) or (out.startswith(\"'\") and out.endswith(\"'\")):\n            out = out[1:-1].strip()\n    return out\n\n\ndef split_symbols(text: str, rules) -> list:\n    if text is None:\n        return []\n    s = apply_postprocess(text, [], rules)\n    parts = [s]\n    for rule in rules.get('split_symbols', []):\n        delim = rule.get('delimiter')\n        if not delim:\n            continue\n        new_parts = []\n        for p in parts:\n            new_parts.extend(p.split(delim))\n        parts = new_parts\n    cleaned = []\n    for p in parts:\n        t = p.strip()\n        if not t:\n            continue\n        # remove leading bullets/arrows leftover\n        t = re.sub(r'^[\\-\u2022\u2192><\\s]+', '', t).strip()\n        # remove trailing separators\n        t = t.strip(';,' )\n        if t:\n            cleaned.append(t)\n    return cleaned\n\n\ndef get_text_by_selector(soup, selector):\n    el = soup.select_one(selector)\n    return el.get_text(separator=' ', strip=True) if el else None\n\ndef get_raw_text_preserve_newlines(el):\n    if not el:\n        return None\n    # Use .get_text with no strip and a newline separator to preserve Mermaid formatting\n    return el.get_text(\"\\n\")\n\n\ndef extract_chang_e(html, mapping, rules):\n    soup = BeautifulSoup(html, 'html.parser')\n    data = {\n        'poem_id': 'poem-1-out-of-life',\n        'title': apply_postprocess(get_text_by_selector(soup, mapping['title']), [], rules),\n        'goal': apply_postprocess(get_text_by_selector(soup, mapping['goal']['selector']), ['strip_prefix:GOAL:'], rules),\n        'diagram': {\n            'format': 'mermaid',\n            # Preserve newlines for Mermaid code\n            'code': get_raw_text_preserve_newlines(soup.select_one(mapping['diagram']['code']))\n        },\n        'analysis_definition': apply_postprocess(get_text_by_selector(soup, mapping['analysis_definition']), [], rules),\n        'sections': [],\n        'metadata': {\n            'source_id': 'chang-e_olo-fl-00-good'\n        }\n    }\n    for sec in soup.select(mapping['sections']['container']):\n        num = get_text_by_selector(sec, mapping['sections']['items'][0]['number'])\n        name = get_text_by_selector(sec, mapping['sections']['items'][0]['name'])\n        model = get_text_by_selector(sec, mapping['sections']['items'][0]['model_description'])\n        symbols_raw = get_text_by_selector(sec, mapping['sections']['items'][0]['symbol_list']['selector'])\n        symbols = split_symbols(symbols_raw, rules)\n        pres = get_text_by_selector(sec, mapping['sections']['items'][0]['prescriptive_description'])\n        mech = get_text_by_selector(sec, mapping['sections']['items'][0]['mechanism_text'])\n        data['sections'].append({\n            'number': int(re.sub(r'[^0-9]', '', num) or '0'),\n            'name': name,\n            'model_description': apply_postprocess(model, [], rules),\n            'symbol_list': symbols,\n            'prescriptive_description': apply_postprocess(pres, [], rules),\n            'mechanism_text': apply_postprocess(mech, [], rules)\n        })\n    data['sections'].sort(key=lambda x: x['number'])\n    return data\n\n\ndef extract_alps(html, mapping, rules):\n    soup = BeautifulSoup(html, 'html.parser')\n    data = {\n        'poem_id': 'poem-1-out-of-life',\n        'title': apply_postprocess(get_text_by_selector(soup, mapping['title']), [], rules),\n        'goal': apply_postprocess(get_text_by_selector(soup, mapping['goal']), [], rules),\n        'diagram': {\n            'format': 'mermaid',\n            # Preserve newlines for Mermaid code\n            'code': get_raw_text_preserve_newlines(soup.select_one(mapping['diagram']['code']))\n        },\n        'analysis_definition': None,\n        'sections': [],\n        'metadata': {\n            'source_id': 'alps_poem_1_out_of_life'\n        }\n    }\n    # analysis_definition: first paragraph after mermaid block\n    mermaid = soup.select_one(mapping['diagram']['code'])\n    if mermaid:\n        p = mermaid.find_next('p')\n        if p:\n            data['analysis_definition'] = apply_postprocess(p.get_text(separator=' ', strip=True), [], rules)\n    # sections by heading groups\n    for grp in mapping['sections']['groups']:\n        # find heading with text contains\n        heading = None\n        for h in soup.find_all(['h2','h3','h1']):\n            if grp['heading_text_contains'] in h.get_text(\" \", strip=True):\n                heading = h\n                break\n        if not heading:\n            continue\n        # gather following block paragraphs until next heading\n        p1 = heading.find_next('p')\n        p2 = p1.find_next('p') if p1 else None\n        p3 = p2.find_next('p') if p2 else None\n        p4 = p3.find_next('p') if p3 else None\n        model = None\n        pres = None\n        mech = None\n        symbols_raw = None\n        # model_description (teal code)\n        if p1:\n            code = p1.select_one('mark.highlight-teal > code')\n            model = code.get_text(\" \", strip=True) if code else p1.get_text(\" \", strip=True)\n        # symbol list\n        if p2:\n            symbols_raw = p2.get_text(\" \", strip=True)\n        # prescriptive (blue code)\n        if p3:\n            codeb = p3.select_one('mark.highlight-blue > code')\n            pres = codeb.get_text(\" \", strip=True) if codeb else p3.get_text(\" \", strip=True)\n        # mechanism\n        if p4:\n            mech = p4.get_text(\" \", strip=True)\n        data['sections'].append({\n            'number': grp.get('number'),\n            'name': grp.get('name').replace('literal:', '') if isinstance(grp.get('name'), str) else None,\n            'model_description': apply_postprocess(model or '', [], rules),\n            'symbol_list': split_symbols(symbols_raw or '', rules),\n            'prescriptive_description': apply_postprocess(pres or '', [], rules),\n            'mechanism_text': apply_postprocess(mech or '', [], rules)\n        })\n    data['sections'].sort(key=lambda x: x['number'] or 0)\n    return data\n\n\ndef main():\n    ap = argparse.ArgumentParser(description='Rosetta translation engine for poem documents')\n    ap.add_argument('--rosetta', required=True, help='Path to rosetta JSON mapping')\n    ap.add_argument('--source-id', required=True, choices=['chang-e_olo-fl-00-good','alps_poem_1_out_of_life'])\n    ap.add_argument('--input', required=True, help='Path to source HTML file')\n    ap.add_argument('--output', required=False, help='Path to write normalized JSON (default: stdout)')\n    args = ap.parse_args()\n\n    spec = json.loads(read_file(args.rosetta))\n    rules = spec.get('normalization_rules', {})\n    mapping = spec['mappings'][args.source_id]\n\n    html = read_file(args.input)\n\n    if args.source_id == 'chang-e_olo-fl-00-good':\n        data = extract_chang_e(html, mapping, rules)\n    else:\n        data = extract_alps(html, mapping, rules)\n\n    # attach metadata\n    data['metadata']['source_path'] = os.path.abspath(args.input)\n\n    out = json.dumps(data, ensure_ascii=False, indent=2)\n    if args.output:\n        with open(args.output, 'w', encoding='utf-8') as f:\n            f.write(out)\n    else:\n        print(out)\n\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      ")\n        if not delim:\n            continue\n        new_parts = []\n        for p in parts:\n            new_parts.extend(p.split(delim))\n        parts = new_parts\n    cleaned = []\n    for p in parts:\n        t = p.strip()\n        if not t:\n            continue\n        # remove leading bullets/arrows leftover\n        t = re.sub(r"
    ],
    "subprocess_calls": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "bs4"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/analyze_assemblies.py",
    "size": 10584,
    "lines": 279,
    "source": "#!/usr/bin/env python3\n\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom pathlib import Path\nimport re\nimport numpy as np\n\ndef load_data_from_markdown_robust(filepath):\n    \"\"\"\n    Attempts to load JSON data from a markdown file.\n    It looks for JSON objects and constructs an array from them.\n    \"\"\"\n    print(f\"Reading file: {filepath}\")\n    try:\n        with open(filepath, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        # Extract JSON objects from the markdown file\n        # This pattern matches individual JSON objects with some flexibility\n        pattern = r'[\\s\\n]*\\{\\s*\"id\"\\s*:\\s*\"[^\"]+\"\\s*,[\\s\\S]*?\\}\\s*(?=,\\s*\\{|$)'\n        json_objects = re.findall(pattern, content)\n        \n        if not json_objects:\n            print(\"No JSON objects found in the file.\")\n            return None\n        \n        print(f\"Found {len(json_objects)} potential JSON objects.\")\n        \n        # Parse each JSON object and collect valid ones\n        entries = []\n        for obj_str in json_objects:\n            try:\n                # Clean up the JSON string (remove trailing commas which are invalid in JSON)\n                cleaned_str = re.sub(r',\\s*}', '}', obj_str)\n                # Parse the JSON object\n                entry = json.loads(cleaned_str)\n                entries.append(entry)\n            except json.JSONDecodeError as e:\n                print(f\"Failed to parse a JSON object: {e}\")\n                print(f\"Problematic JSON string: {obj_str[:100]}...\")\n                continue\n        \n        print(f\"Successfully parsed {len(entries)} valid JSON objects.\")\n        return entries\n    except FileNotFoundError:\n        print(f\"Error: File not found at {filepath}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return None\n\ndef generate_stats_and_charts(df, column_name, output_dir, prefix=\"\"):\n    \"\"\"\n    Calculates statistics and generates charts for a given column.\n    \"\"\"\n    if column_name not in df.columns:\n        print(f\"Column '{column_name}' not found in the data. Skipping.\")\n        return\n\n    print(f\"\\n--- Statistics for {column_name} ---\")\n\n    # Overall statistics\n    overall_counts = df[column_name].value_counts()\n    overall_percentages = df[column_name].value_counts(normalize=True) * 100\n    print(\"\\nOverall Distribution:\")\n    overall_stats_df = pd.DataFrame({\n        'Count': overall_counts,\n        'Percentage (%)': overall_percentages.round(2)\n    }).sort_values(by='Count', ascending=False)\n    print(overall_stats_df)\n\n    # Plot overall distribution\n    if not overall_percentages.empty:\n        plt.figure(figsize=(max(12, len(overall_percentages) * 0.8), 8))\n        chart = sns.barplot(x=overall_stats_df.index, \n                           y=overall_stats_df['Percentage (%)'].values, \n                           palette=\"viridis\")\n        plt.title(f'Overall Distribution of {column_name}')\n        plt.xlabel(column_name)\n        plt.ylabel('Percentage (%)')\n        plt.xticks(rotation=45, ha=\"right\")\n        \n        # Add percentage labels on top of bars\n        for i, p in enumerate(chart.patches):\n            chart.annotate(f\"{p.get_height():.1f}%\", \n                          (p.get_x() + p.get_width() / 2., p.get_height()),\n                          ha = 'center', va = 'bottom',\n                          xytext = (0, 5), \n                          textcoords = 'offset points')\n        \n        plt.tight_layout()\n        chart_path = output_dir / f\"{prefix}overall_{column_name.lower().replace(' ', '_')}_distribution.png\"\n        plt.savefig(chart_path)\n        print(f\"Saved overall chart to {chart_path}\")\n        plt.close()\n    else:\n        print(f\"No data to plot for overall distribution of {column_name}.\")\n\n    # Statistics by poem\n    if 'poem' not in df.columns:\n        print(\"Column 'poem' not found. Skipping by-poem statistics.\")\n        return\n    \n    # Get unique poems and their item counts for sorting\n    poem_counts = df['poem'].value_counts().sort_values(ascending=False)\n    print(\"\\nPoem Counts:\")\n    print(poem_counts)\n    \n    # Take top 10 poems for readability in charts\n    top_poems = poem_counts.index[:10].tolist()\n    \n    print(f\"\\nDistribution by Poem for {column_name} (Top 10 Poems):\")\n    \n    # Filter for top poems\n    top_poems_df = df[df['poem'].isin(top_poems)]\n    \n    # Pivot table for heatmap\n    pivot = pd.crosstab(\n        top_poems_df['poem'], \n        top_poems_df[column_name], \n        normalize='index'\n    ) * 100\n    \n    print(\"\\nPercentages by Poem (Top 10):\")\n    print(pivot.round(2))\n    \n    # Plot heatmap for top poems\n    if not pivot.empty:\n        plt.figure(figsize=(max(12, len(pivot.columns) * 0.8), max(8, len(top_poems) * 0.6)))\n        sns.heatmap(pivot, annot=True, cmap=\"YlGnBu\", fmt='.1f', \n                   cbar_kws={'label': 'Percentage (%)'})\n        plt.title(f'Distribution of {column_name} by Poem (Top 10)')\n        plt.ylabel('Poem')\n        plt.xlabel(column_name)\n        plt.tight_layout()\n        chart_path = output_dir / f\"{prefix}heatmap_{column_name.lower().replace(' ', '_')}_by_poem.png\"\n        plt.savefig(chart_path)\n        print(f\"Saved heatmap to {chart_path}\")\n        plt.close()\n        \n        # Stacked bar chart for top poems\n        pivot_unstacked = pivot.stack().reset_index()\n        pivot_unstacked.columns = ['Poem', column_name, 'Percentage']\n        \n        plt.figure(figsize=(max(12, len(top_poems) * 1.2), 8))\n        chart = sns.barplot(x='Poem', y='Percentage', hue=column_name, \n                           data=pivot_unstacked, palette='viridis')\n        plt.title(f'{column_name} Distribution by Poem (Top 10)')\n        plt.ylabel('Percentage (%)')\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.legend(title=column_name, bbox_to_anchor=(1.05, 1), loc='upper left')\n        plt.tight_layout()\n        chart_path = output_dir / f\"{prefix}stacked_bar_{column_name.lower().replace(' ', '_')}_by_poem.png\"\n        plt.savefig(chart_path)\n        print(f\"Saved stacked bar chart to {chart_path}\")\n        plt.close()\n    else:\n        print(f\"No data to plot for {column_name} by poem.\")\n\ndef analyze_combinations(df, output_dir):\n    \"\"\"\n    Analyzes combinations of syntagmaType, cineosisFunction, and imageType.\n    \"\"\"\n    columns = ['syntagmaType', 'cineosisFunction', 'imageType']\n    if not all(col in df.columns for col in columns):\n        print(\"Missing required columns for combination analysis.\")\n        return\n    \n    print(\"\\n--- Combination Analysis ---\")\n    \n    # Count combinations\n    combinations = df.groupby(columns).size().reset_index(name='count')\n    combinations = combinations.sort_values('count', ascending=False)\n    \n    # Calculate percentages\n    total = combinations['count'].sum()\n    combinations['percentage'] = (combinations['count'] / total * 100).round(2)\n    \n    print(\"\\nTop 20 Most Common Combinations:\")\n    print(combinations.head(20))\n    \n    # Save combinations to CSV\n    csv_path = output_dir / \"combinations_analysis.csv\"\n    combinations.to_csv(csv_path, index=False)\n    print(f\"Saved all combinations to {csv_path}\")\n    \n    # Create visualization for top combinations\n    top_combinations = combinations.head(15)  # Top 15 for readability\n    \n    plt.figure(figsize=(14, 10))\n    combo_labels = [f\"{row['syntagmaType']}\\n{row['imageType']}\\n{row['cineosisFunction']}\" \n                   for _, row in top_combinations.iterrows()]\n    \n    chart = sns.barplot(x=np.arange(len(top_combinations)), \n                       y=top_combinations['percentage'].values,\n                       palette=\"viridis\")\n    \n    plt.title('Top 15 Most Common Combinations of Syntagma, Image Type, and Cineosis Function')\n    plt.xlabel('Combination')\n    plt.ylabel('Percentage (%)')\n    plt.xticks(np.arange(len(top_combinations)), combo_labels, rotation=90)\n    \n    # Add percentage labels\n    for i, p in enumerate(chart.patches):\n        chart.annotate(f\"{p.get_height():.1f}%\", \n                      (p.get_x() + p.get_width() / 2., p.get_height()),\n                      ha = 'center', va = 'bottom',\n                      xytext = (0, 5), \n                      textcoords = 'offset points')\n    \n    plt.tight_layout()\n    chart_path = output_dir / \"top_combinations.png\"\n    plt.savefig(chart_path)\n    print(f\"Saved top combinations chart to {chart_path}\")\n    plt.close()\n\ndef main():\n    filepath = \"/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_assemblies.md\"\n    output_dir = Path(\"/Users/gaia/resurrecting atlantis/TIGER/assemblies_analysis\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    print(f\"Analysis output will be saved to: {output_dir}\")\n    print(f\"Attempting to load data from: {filepath}\")\n    \n    entries = load_data_from_markdown_robust(filepath)\n    \n    if not entries:\n        print(\"Failed to load data. Exiting.\")\n        return\n    \n    print(f\"Successfully loaded {len(entries)} entries.\")\n    df = pd.DataFrame(entries)\n    \n    # Save summary info\n    print(f\"\\nDataFrame columns: {df.columns.tolist()}\")\n    \n    if 'poem' in df.columns:\n        print(f\"Unique poems found: {df['poem'].nunique()}\")\n        print(f\"Poems: {sorted(df['poem'].unique())}\")\n    else:\n        print(\"Warning: 'poem' column not found. By-poem statistics will be unavailable.\")\n    \n    # Save basic dataset summary\n    summary_path = output_dir / \"dataset_summary.txt\"\n    with open(summary_path, 'w') as f:\n        f.write(f\"Dataset Summary for {filepath}\\n\")\n        f.write(\"=\" * 50 + \"\\n\\n\")\n        f.write(f\"Total entries: {len(df)}\\n\")\n        f.write(f\"Columns: {', '.join(df.columns)}\\n\\n\")\n        \n        if 'poem' in df.columns:\n            f.write(f\"Unique poems: {df['poem'].nunique()}\\n\")\n            f.write(\"Poem counts:\\n\")\n            for poem, count in df['poem'].value_counts().items():\n                f.write(f\"- {poem}: {count}\\n\")\n    \n    print(f\"Saved dataset summary to {summary_path}\")\n    \n    # Columns to analyze\n    columns_to_analyze = ['syntagmaType', 'cineosisFunction', 'imageType']\n    \n    for col in columns_to_analyze:\n        generate_stats_and_charts(df, col, output_dir)\n    \n    # Analyze combinations\n    analyze_combinations(df, output_dir)\n    \n    # Save full processed dataset\n    df.to_csv(output_dir / \"processed_assemblies_data.csv\", index=False)\n    \n    print(f\"\\nAll statistics and charts have been generated in '{output_dir}'.\")\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      ")}_distribution.png",
      ")}_by_poem.png",
      ")}_by_poem.png",
      "top_combinations.png",
      ", \n                          (p.get_x() + p.get_width() / 2., p.get_height()),\n                          ha = ",
      ")\n        \n        plt.tight_layout()\n        chart_path = output_dir / f",
      ")\n        plt.xlabel(column_name)\n        plt.tight_layout()\n        chart_path = output_dir / f",
      ")\n        plt.tight_layout()\n        chart_path = output_dir / f",
      "] / total * 100).round(2)\n    \n    print(",
      ")\n    print(combinations.head(20))\n    \n    # Save combinations to CSV\n    csv_path = output_dir / ",
      ", \n                      (p.get_x() + p.get_width() / 2., p.get_height()),\n                      ha = ",
      ")\n    \n    plt.tight_layout()\n    chart_path = output_dir / ",
      "/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_assemblies.md",
      "/Users/gaia/resurrecting atlantis/TIGER/assemblies_analysis",
      ")\n    \n    # Save basic dataset summary\n    summary_path = output_dir / ",
      "]\n    \n    for col in columns_to_analyze:\n        generate_stats_and_charts(df, col, output_dir)\n    \n    # Analyze combinations\n    analyze_combinations(df, output_dir)\n    \n    # Save full processed dataset\n    df.to_csv(output_dir / "
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "pandas",
      "matplotlib.pyplot",
      "seaborn",
      "os",
      "pathlib",
      "re",
      "numpy"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/poem_section_codex_generator.py",
    "size": 7950,
    "lines": 228,
    "source": "#!/usr/bin/env python3\n\"\"\"\nPoem Section Codex Generator\n\nA generalizable script that generates properly formatted Codex entries for any poem section\nby combining data from:\n1. The Simplified sequence file (for all image variants with timestamps)\n2. The section's assembly JSON (for metadata)\n3. The section's prompts markdown (for prompt text)\n\nUsage: python3 poem_section_codex_generator.py PREFIX\nExample: python3 poem_section_codex_generator.py RU\n\"\"\"\n\nimport os\nimport re\nimport json\nimport sys\n\n# Base directories\nTIGER_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nSIMPLIFIED_PATH = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\nCODEX_PATH = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Codex.md\")\n\ndef load_assembly_data(prefix):\n    \"\"\"Load the assembly JSON data for the specified prefix.\"\"\"\n    section_dir = os.path.join(TIGER_DIR, prefix)\n    assembly_path = os.path.join(section_dir, f\"{prefix}_assembly.json\")\n    \n    if not os.path.exists(assembly_path):\n        print(f\"Error: Assembly file not found at {assembly_path}\")\n        return {}\n        \n    with open(assembly_path, 'r') as f:\n        return {item[\"id\"]: item for item in json.load(f)}\n\ndef load_prompts_data(prefix):\n    \"\"\"Load the prompts markdown data for the specified prefix.\"\"\"\n    section_dir = os.path.join(TIGER_DIR, prefix)\n    prompts_path = os.path.join(section_dir, f\"{prefix}_prompts.md\")\n    \n    if not os.path.exists(prompts_path):\n        print(f\"Error: Prompts file not found at {prompts_path}\")\n        return {}\n        \n    with open(prompts_path, 'r') as f:\n        prompts_text = f.read()\n    \n    # Extract prompts with shot IDs\n    prompts = {}\n    for line in prompts_text.splitlines():\n        line = line.strip()\n        if not line or line.startswith('#'):\n            continue\n            \n        # Match shot IDs at the beginning of lines\n        match = re.match(f'({prefix}\\\\d+)\\\\s*\u00b7\\\\s*(.*)', line)\n        if match:\n            shot_id, prompt_text = match.groups()\n            prompts[shot_id] = prompt_text\n            \n    return prompts\n\ndef extract_entries_from_simplified(prefix):\n    \"\"\"Extract all entries with timestamps from the Simplified markdown file for the specified prefix.\"\"\"\n    with open(SIMPLIFIED_PATH, 'r') as f:\n        simplified_text = f.read()\n    \n    # Extract entries with timestamps and image paths for the prefix\n    pattern = f'({prefix}\\\\d+)\\\\s+\\\\[([^\\\\]]+)\\\\]\\\\s+`([^`]+)`'\n    matches = re.findall(pattern, simplified_text)\n    \n    entries = []\n    for shot_id, timestamp, image_path in matches:\n        entries.append({\n            \"shot_id\": shot_id,\n            \"timestamp\": timestamp,\n            \"image_path\": image_path\n        })\n    \n    return entries\n\ndef get_poem_title(prefix, assembly_data):\n    \"\"\"Get the poem title for the specified prefix from assembly data.\"\"\"\n    # Try to get from first entry in assembly data\n    if assembly_data and len(assembly_data) > 0:\n        first_key = next(iter(assembly_data))\n        if \"poem\" in assembly_data[first_key]:\n            return assembly_data[first_key][\"poem\"]\n    \n    # Fallback mappings for poem titles\n    title_map = {\n        \"SH\": \"Out of Life\",\n        \"FL\": \"Flashing Lights\",\n        \"HT\": \"How To Win My Heart\",\n        \"RU\": \"Reunion\",\n        \"NM\": \"Nevermore\",\n        \"BE\": \"Bloodline\",\n        \"AT\": \"Resurrecting Atlantis\",\n        \"DJ\": \"DJ Turn Me Up\",\n        \"NS\": \"Newly Single\",\n        \"YH\": \"Yet Heard\",\n        \"MR\": \"Magic Ride\",\n        \"HW\": \"How to Win My Heart\",\n        \"HM\": \"Hot Minute\"\n    }\n    \n    return title_map.get(prefix, prefix)\n\ndef update_codex_file(prefix, codex_entries):\n    \"\"\"Update the main Codex file with the generated entries.\"\"\"\n    with open(CODEX_PATH, 'r') as f:\n        codex_content = f.read()\n    \n    # Check if section already exists\n    poem_title = get_poem_title(prefix, {})\n    section_header = f\"# {prefix} ({poem_title}) Codex Entries\"\n    \n    if section_header in codex_content:\n        print(f\"Section {prefix} already exists in Codex file. Skipping update.\")\n        return False\n        \n    # Append the new entries\n    with open(CODEX_PATH, 'a') as f:\n        f.write(\"\\n\\n\" + codex_entries)\n        \n    print(f\"Updated main Codex file with {prefix} entries\")\n    return True\n\ndef generate_codex_entries(prefix):\n    \"\"\"Generate formatted codex entries for all images for the specified prefix.\"\"\"\n    assembly_data = load_assembly_data(prefix)\n    prompts_data = load_prompts_data(prefix)\n    simplified_entries = extract_entries_from_simplified(prefix)\n    \n    if not assembly_data or not prompts_data or not simplified_entries:\n        print(f\"Error: Missing data for {prefix}. Cannot generate Codex entries.\")\n        print(f\"  Assembly data: {len(assembly_data)} items\")\n        print(f\"  Prompts data: {len(prompts_data)} items\")\n        print(f\"  Simplified entries: {len(simplified_entries)} items\")\n        return None\n    \n    # Get poem title\n    poem_title = get_poem_title(prefix, assembly_data)\n    \n    # Start output with a header\n    output_text = f\"# {prefix} ({poem_title}) Codex Entries - Complete Sequence\\n\\n\"\n    \n    # Generate formatted entries for each image\n    entries_count = 0\n    for entry in simplified_entries:\n        shot_id = entry[\"shot_id\"]\n        timestamp = entry[\"timestamp\"]\n        image_path = entry[\"image_path\"]\n        \n        # Skip if we don't have assembly data for this shot ID\n        if shot_id not in assembly_data:\n            print(f\"Warning: No assembly data found for {shot_id}\")\n            continue\n            \n        # Skip if we don't have prompt data for this shot ID\n        if shot_id not in prompts_data:\n            print(f\"Warning: No prompt data found for {shot_id}\")\n            continue\n        \n        # Format the codex entry\n        entry_text = f\"### {shot_id} [{timestamp}]\\n\\n\"\n        entry_text += f\"**Image:** `{image_path}`\\n\\n\"\n        entry_text += \"**Assembly Source:**\\n```json\\n\"\n        entry_text += json.dumps(assembly_data[shot_id], indent=2)\n        entry_text += \"\\n```\\n\\n\"\n        entry_text += f\"**Prompt:** {prompts_data[shot_id]}\\n\\n---\\n\\n\"\n        \n        output_text += entry_text\n        entries_count += 1\n    \n    output_path = os.path.join(TIGER_DIR, f\"{prefix}_complete_codex_entries.md\")\n    \n    # Write to output file\n    with open(output_path, 'w') as f:\n        f.write(output_text)\n    \n    print(f\"Generated complete codex entries written to {output_path}\")\n    print(f\"Total entries: {entries_count}\")\n    \n    return output_text\n\ndef generate_video(prefix):\n    \"\"\"Generate the header_prompt.mp4 video for the specified prefix.\"\"\"\n    import subprocess\n    \n    cmd = [\n        \"source\", \"~/venv_video/bin/activate\", \"&&\",\n        \"cd\", f'\"{os.path.join(os.path.dirname(TIGER_DIR), \"JELLYFISH\")}\"', \"&&\",\n        \"python3\", \"fl_video_generator_header_prompt.py\", f\"--prefix {prefix}\"\n    ]\n    \n    print(f\"Generating video for {prefix}...\")\n    print(f\"Run command: {' '.join(cmd)}\")\n    print(\"This should be run manually in the terminal after the Codex entries are generated.\")\n\ndef main():\n    if len(sys.argv) != 2:\n        print(\"Usage: python3 poem_section_codex_generator.py PREFIX\")\n        print(\"Example: python3 poem_section_codex_generator.py RU\")\n        sys.exit(1)\n        \n    prefix = sys.argv[1].upper()\n    \n    # Check if the prefix directory exists\n    prefix_dir = os.path.join(TIGER_DIR, prefix)\n    if not os.path.exists(prefix_dir):\n        print(f\"Error: Directory for {prefix} not found at {prefix_dir}\")\n        sys.exit(1)\n        \n    # Generate codex entries\n    codex_content = generate_codex_entries(prefix)\n    \n    if codex_content:\n        # Update the main Codex file\n        update_codex_file(prefix, codex_content)\n        \n        # Provide instructions for generating the video\n        generate_video(prefix)\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "{prefix}_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER",
      "~/venv_video/bin/activate"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "json",
      "sys"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Poem Section Codex Generator\n\nA generalizable script that generates properly formatted Codex entries for any poem section\nby combining data from:\n1. The Simplified sequence file (for all image variants with timestamps)\n2. The section's assembly JSON (for metadata)\n3. The section's prompts markdown (for prompt text)\n\nUsage: python3 poem_section_codex_generator.py PREFIX\nExample: python3 poem_section_codex_generator.py RU"
  },
  {
    "path": "JELLYFISH/normalize_final_files.py",
    "size": 2100,
    "lines": 67,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\n# Directory containing files to normalize\nIMPALA_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA')\n\n# Create backup directory\nBACKUP_DIR = IMPALA_DIR / 'original_files_backup'\nBACKUP_DIR.mkdir(exist_ok=True)\n\n# Final specific mappings\nFINAL_MAPPINGS = {\n    \"07_DJ_DJTurnMeUp_130, M 5.mp4\": \"07_DJ_DJTurnMeUp_130600_M 5.mp4\",\n    \"08_NS_NewlySingle_15, M 5.mp4\": \"08_NS_NewlySingle_151700_M 5.mp4\",\n    \"2966551071,  CinePrompt \\\"NEVERM, image-prompt, M 5.mp4\": \"04_NM_Nevermore_063300_CinePrompt_NEVERM_image-prompt_M5.mp4\"\n}\n\ndef normalize_final_files():\n    print(\"Starting to normalize final files in IMPALA directory...\")\n    \n    # Track statistics\n    total_files = 0\n    renamed_files = 0\n    skipped_files = 0\n    \n    # Process each specific file\n    for old_name, new_name in FINAL_MAPPINGS.items():\n        file_path = IMPALA_DIR / old_name\n        \n        # Skip if file doesn't exist\n        if not file_path.exists():\n            print(f\"File not found: {old_name}\")\n            skipped_files += 1\n            continue\n            \n        total_files += 1\n        \n        new_file_path = file_path.parent / new_name\n        \n        # Check if new filename already exists\n        if new_file_path.exists():\n            print(f\"Skipping (destination exists): {file_path.name}\")\n            skipped_files += 1\n            continue\n        \n        # Create backup\n        backup_path = BACKUP_DIR / file_path.name\n        print(f\"Backing up: {file_path.name} -> {backup_path.name}\")\n        shutil.copy2(file_path, backup_path)\n        \n        # Rename the file\n        print(f\"Renaming: {file_path.name} -> {new_name}\")\n        os.rename(file_path, new_file_path)\n        renamed_files += 1\n    \n    print(\"\\nFinal Normalization Complete!\")\n    print(f\"Total files processed: {total_files}\")\n    print(f\"Files renamed: {renamed_files}\")\n    print(f\"Files skipped: {skipped_files}\")\n    print(f\"Original files backed up to: {BACKUP_DIR}\")\n\nif __name__ == \"__main__\":\n    normalize_final_files()\n",
    "file_references": [
      "07_DJ_DJTurnMeUp_130, M 5.mp4",
      "07_DJ_DJTurnMeUp_130600_M 5.mp4",
      "08_NS_NewlySingle_15, M 5.mp4",
      "08_NS_NewlySingle_151700_M 5.mp4",
      "NEVERM, image-prompt, M 5.mp4",
      "04_NM_Nevermore_063300_CinePrompt_NEVERM_image-prompt_M5.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA",
      ")\n    \n    # Track statistics\n    total_files = 0\n    renamed_files = 0\n    skipped_files = 0\n    \n    # Process each specific file\n    for old_name, new_name in FINAL_MAPPINGS.items():\n        file_path = IMPALA_DIR / old_name\n        \n        # Skip if file doesn",
      ")\n            skipped_files += 1\n            continue\n            \n        total_files += 1\n        \n        new_file_path = file_path.parent / new_name\n        \n        # Check if new filename already exists\n        if new_file_path.exists():\n            print(f",
      ")\n            skipped_files += 1\n            continue\n        \n        # Create backup\n        backup_path = BACKUP_DIR / file_path.name\n        print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/mr_ordered_video_generator.py",
    "size": 10744,
    "lines": 286,
    "source": "#!/usr/bin/env python3\n\"\"\"\nMR Ordered Video Generator\nThis script generates the MR (Magic Ride) video using the existing images from the \nCOLLECTION_WhereYouGoWhenYouLeave_ImageSequence.md and following the official shot order.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport shutil\nimport subprocess\nfrom datetime import datetime\n\n# Paths\nJELLYFISH_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(JELLYFISH_DIR)\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nSIMPLIFIED_SEQUENCE = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\nIMAGE_SEQUENCE = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_ImageSequence.md\")\nMR_PROMPTS_FILE = os.path.join(TIGER_DIR, \"MR\", \"MR_prompts.md\")\nOUTPUT_FILE = os.path.join(TIGER_DIR, \"MR_ordered_codex_entries.md\")\nAUDIO_DIR = os.path.join(BASE_DIR, \"MANTA\", \"audio\")\nMR_AUDIO_FILE = os.path.join(AUDIO_DIR, \"MR_audio.wav\")\nVIDEO_OUTPUT_DIR = os.path.join(JELLYFISH_DIR, \"video_output\")\n\ndef extract_mr_shot_order():\n    \"\"\"Extract MR shots in exact order from MR prompts file.\"\"\"\n    shots = []\n    \n    with open(MR_PROMPTS_FILE, 'r') as file:\n        content = file.read()\n        \n        # Extract each shot line by line\n        lines = content.split('\\n')\n        for line in lines:\n            # Match lines like: MR001 \u00b7 DS \u00b7 Mood Environment Stabilizer \u00b7 Title arcs across night highway in neon...\n            match = re.match(r'(MR\\d+)\\s*\u00b7\\s*([A-Z]+)\\s*\u00b7\\s*([^\u00b7]+)\u00b7\\s*(.+)', line.strip())\n            if match:\n                shot_id, category, function, description = match.groups()\n                shots.append({\n                    'id': shot_id.strip(),\n                    'category': category.strip(),\n                    'function': function.strip(),\n                    'description': description.strip()\n                })\n    \n    print(f\"Extracted {len(shots)} MR shots from prompts in official order\")\n    return shots\n\ndef extract_existing_mr_images():\n    \"\"\"Extract the list of existing MR images from the ImageSequence file.\"\"\"\n    images = {}\n    \n    with open(IMAGE_SEQUENCE, 'r') as file:\n        content = file.read()\n        \n        # Find MR section in the sequence file\n        mr_section_match = re.search(r'### MR\\d+.*?(?=###|\\Z)', content, re.DOTALL)\n        if mr_section_match:\n            mr_section = mr_section_match.group(0)\n            \n            # Extract image paths\n            current_shot_id = None\n            for line in mr_section.split('\\n'):\n                # Match the shot ID line\n                shot_match = re.match(r'### (MR\\d+)', line)\n                if shot_match:\n                    current_shot_id = shot_match.group(1)\n                \n                # Match image paths\n                image_match = re.match(r'- `(MR/.+?\\.\\w+)`', line)\n                if image_match and current_shot_id:\n                    if current_shot_id not in images:\n                        images[current_shot_id] = []\n                    images[current_shot_id].append(image_match.group(1))\n    \n    # Also check the MR directory for any images not in the sequence file\n    mr_dir = os.path.join(TIGER_DIR, \"MR\")\n    if os.path.exists(mr_dir):\n        for filename in os.listdir(mr_dir):\n            if filename.endswith(('.png', '.jpg', '.jpeg')):\n                shot_match = re.match(r'(MR\\d+)', filename)\n                if shot_match:\n                    shot_id = shot_match.group(1)\n                    image_path = f\"MR/{filename}\"\n                    if shot_id not in images:\n                        images[shot_id] = []\n                    if image_path not in images[shot_id]:\n                        images[shot_id].append(image_path)\n    \n    return images\n\ndef calculate_timestamps(shot_count):\n    \"\"\"Calculate evenly distributed timestamps for the MR section (2:11 duration).\"\"\"\n    total_seconds = 2 * 60 + 11  # 2:11 in seconds\n    interval = total_seconds / shot_count\n    \n    timestamps = []\n    for i in range(shot_count):\n        seconds = i * interval\n        minutes = int(seconds / 60)\n        remaining_seconds = seconds % 60\n        timestamp = f\"{19:02d}:{minutes:02d}:{remaining_seconds:06.3f}\"\n        timestamps.append(timestamp)\n    \n    return timestamps\n\ndef generate_ordered_codex_entries(shots, images):\n    \"\"\"Generate ordered Codex entries for MR shots with timestamps.\"\"\"\n    entries = []\n    shots_with_images = 0\n    shots_without_images = 0\n    used_shots = []\n    \n    # Calculate timestamps\n    timestamps = calculate_timestamps(len(shots))\n    \n    for i, shot in enumerate(shots):\n        shot_id = shot['id']\n        shot_number = shot_id[2:]  # Extract number part (e.g., \"001\" from \"MR001\")\n        \n        # Format timestamp exactly as expected: hours:minutes:seconds (no milliseconds)\n        seconds = i * (2 * 60 + 11) / len(shots)\n        hours = 19  # Fixed at 19 hours for MR section based on simplified file\n        minutes = int(seconds / 60)\n        remaining_seconds = int(seconds % 60)  # Convert to integer, no milliseconds\n        timestamp = f\"{hours:02d}:{minutes:02d}:{remaining_seconds:02d}\"\n        \n        # Check if we have an image for this shot\n        if shot_id in images and images[shot_id]:\n            image_path = images[shot_id][0]  # Use the first available image\n            shots_with_images += 1\n            \n            # Generate the assembly source JSON with expected structure\n            assembly_source = {\n                \"poem\": \"Magic Ride\",\n                \"content\": shot['description'][:30] + \"...\",  # First part of description as content\n                \"syntagmaType\": f\"{shot['category']}\",\n                \"cineosisFunction\": f\"{shot['function']}\"\n            }\n            \n            # Generate the codex entry with EXACT format as expected by the parser\n            entry = f\"### MR{shot_number} [{timestamp}]\\n\\n\"\n            entry += f\"**Image:** `{image_path}`\\n\\n\"\n            entry += \"**Assembly Source:**\\n```json\\n\"\n            entry += json.dumps(assembly_source, indent=2)\n            entry += \"\\n```\\n\\n\"\n            entry += f\"**Prompt:** {shot['category']} \u00b7 {shot['function']} \u00b7 {shot['description']}\"\n            entry += \"\\n\\n---\\n\\n\"\n            \n            entries.append(entry)\n            used_shots.append(shot_id)\n        else:\n            print(f\"Warning: No image found for {shot_id}, skipping this shot\")\n            shots_without_images += 1\n    \n    print(f\"Total entries: {len(entries)}\")\n    print(f\"Shots with images: {shots_with_images}\")\n    print(f\"Shots without images: {shots_without_images}\")\n    print(f\"Used shots in order: {', '.join(used_shots)}\")\n    \n    return entries\n\ndef write_to_file(entries):\n    \"\"\"Write entries to output file.\"\"\"\n    with open(OUTPUT_FILE, 'w') as file:\n        file.write(\"# MR Magic Ride - Ordered Codex Entries\\n\\n\")\n        file.write(\"*Generated on \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"*\\n\\n\")\n        file.write(\"These entries are in EXACT shot order from the prompts markdown.\\n\\n\")\n        file.write(\"\".join(entries))\n    \n    print(f\"Generated ordered codex entries written to {OUTPUT_FILE}\")\n\ndef backup_existing_videos():\n    \"\"\"Backup existing MR videos with MISFIT suffix.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Backup video without audio\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt.mp4\")\n    if os.path.exists(video_file):\n        backup_file = os.path.join(VIDEO_OUTPUT_DIR, f\"MR_header_prompt_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_file, backup_file)\n        print(f\"Backing up existing MR video to {backup_file}\")\n    \n    # Backup video with audio\n    video_audio_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt_with_audio.mp4\")\n    if os.path.exists(video_audio_file):\n        backup_audio_file = os.path.join(VIDEO_OUTPUT_DIR, f\"MR_header_prompt_with_audio_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_audio_file, backup_audio_file)\n        print(f\"Backing up existing MR video with audio to {backup_audio_file}\")\n\ndef generate_mr_video():\n    \"\"\"Generate MR video with ordered shot sequence.\"\"\"\n    print(\"Generating MR video with ordered shot sequence...\")\n    \n    # Check if ordered codex file exists\n    if not os.path.exists(OUTPUT_FILE):\n        print(f\"Error: Ordered codex file {OUTPUT_FILE} not found.\")\n        return False\n    \n    # Run the FL video generator with MR prefix and ordered codex\n    cmd = [\n        \"python3\",\n        os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\"),\n        \"--prefix\", \"MR\",\n        \"--ordered-codex\", OUTPUT_FILE\n    ]\n    \n    print(f\"Using ordered codex file: {OUTPUT_FILE}\")\n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        print(\"Error: Failed to generate MR video.\")\n        return False\n    \n    return True\n\ndef add_audio_to_video():\n    \"\"\"Add MR audio to the generated video.\"\"\"\n    print(\"Adding audio to MR video...\")\n    \n    # Check if video file exists\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt.mp4\")\n    if not os.path.exists(video_file):\n        print(f\"Error: Video file {video_file} not found.\")\n        return False\n    \n    # Check if audio file exists\n    audio_file = MR_AUDIO_FILE\n    if not os.path.exists(audio_file):\n        print(f\"Error: Audio file {audio_file} not found.\")\n        return False\n    \n    # Output file path\n    output_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt_with_audio.mp4\")\n    \n    # FFmpeg command to merge video and audio\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-map\", \"0:v\", \"-map\", \"1:a\",\n        \"-c:v\", \"copy\", \"-c:a\", \"aac\",\n        \"-shortest\",\n        output_file\n    ]\n    \n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        print(\"Error: Failed to add audio to MR video.\")\n        return False\n    \n    print(f\"Successfully created MR video with audio: {output_file}\")\n    return True\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"=== MR Ordered Video Generator (FIXED CORRECT VERSION) ===\")\n    \n    # Backup existing videos\n    backup_existing_videos()\n    \n    # Extract MR shot order from prompts\n    shots = extract_mr_shot_order()\n    \n    # Extract existing MR images\n    images = extract_existing_mr_images()\n    \n    # Generate ordered entries\n    entries = generate_ordered_codex_entries(shots, images)\n    \n    # Write to file\n    write_to_file(entries)\n    \n    # Generate MR video\n    if generate_mr_video():\n        # Add audio to video\n        add_audio_to_video()\n    \n    print(\"DONE! Generated MR video with exact shot order from prompts using existing images.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "MR_audio.wav",
      "MR_header_prompt.mp4",
      "MR_header_prompt_MISFIT_{timestamp}.mp4",
      "MR_header_prompt_with_audio.mp4",
      "MR_header_prompt_with_audio_MISFIT_{timestamp}.mp4",
      "MR_header_prompt.mp4",
      "MR_header_prompt_with_audio.mp4",
      "- `(MR/.+?\\.\\w+)`",
      "MR/{filename}",
      "\n    total_seconds = 2 * 60 + 11  # 2:11 in seconds\n    interval = total_seconds / shot_count\n    \n    timestamps = []\n    for i in range(shot_count):\n        seconds = i * interval\n        minutes = int(seconds / 60)\n        remaining_seconds = seconds % 60\n        timestamp = f",
      ")\n        \n        # Format timestamp exactly as expected: hours:minutes:seconds (no milliseconds)\n        seconds = i * (2 * 60 + 11) / len(shots)\n        hours = 19  # Fixed at 19 hours for MR section based on simplified file\n        minutes = int(seconds / 60)\n        remaining_seconds = int(seconds % 60)  # Convert to integer, no milliseconds\n        timestamp = f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "re",
      "json",
      "shutil",
      "subprocess",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "MR Ordered Video Generator\nThis script generates the MR (Magic Ride) video using the existing images from the \nCOLLECTION_WhereYouGoWhenYouLeave_ImageSequence.md and following the official shot order."
  },
  {
    "path": "JELLYFISH/codex_overlay_aligned.py",
    "size": 6393,
    "lines": 155,
    "source": "#!/usr/bin/env python3\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nimport textwrap\nimport random\n\ndef create_codex_overlay(output_path, shot_id=\"FL012\", timestamp=\"02:38:34\"):\n    \"\"\"Create a mockup with two-line header and prompt in footer, with left alignment\"\"\"\n    # Create a black canvas (simulating video frame)\n    width, height = 1280, 720\n    image = Image.new('RGB', (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Load font\n    try:\n        header_font = ImageFont.truetype(\"Arial.ttf\", 14)\n        footer_font = ImageFont.truetype(\"Arial.ttf\", 18)\n        title_font = ImageFont.truetype(\"Arial.ttf\", 16)\n    except IOError:\n        try:\n            header_font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n            footer_font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n            title_font = ImageFont.truetype(\"DejaVuSans.ttf\", 16)\n        except IOError:\n            header_font = ImageFont.load_default()\n            footer_font = ImageFont.load_default()\n            title_font = ImageFont.load_default()\n    \n    # Define colors\n    cyan = (0, 255, 255)\n    amber = (255, 191, 0)\n    white = (255, 255, 255)\n    green = (80, 255, 80)\n    \n    # Sample assembly.json data\n    assembly_data = {\n        \"id\": \"FL012\",\n        \"poem\": \"Flashing Lights\",\n        \"content\": \"a concussion,\",\n        \"syntagmaType\": \"Perception-Image\",\n        \"operativeEkphrasis\": \"Stars explode behind eyelids--fireworks seen from inside a skull.\",\n        \"imageType\": \"Perception-Image\",\n        \"cineosisFunction\": \"Subjective Frame Recalibration\"\n    }\n    \n    # Create top header bar with assembly data - now two-line high\n    header_height = 60  # Height increased for two lines\n    draw.rectangle([(0, 0), (width, header_height)], fill=(0, 0, 0, 180))\n    draw.line([(0, header_height), (width, header_height)], fill=cyan, width=1)\n    \n    # Add scanlines to header\n    for y in range(0, header_height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n    \n    # Define left padding for alignment\n    padding = 20\n    \n    # Add text to header - first line\n    draw.text((padding, 9), f\"{assembly_data['id']}\", fill=cyan, font=title_font)\n    draw.text((padding + 70, 9), f\"[{timestamp}]\", fill=white, font=header_font)\n    draw.text((padding + 170, 9), f\"{assembly_data['poem']}\", fill=amber, font=title_font)\n    draw.text((padding + 320, 9), f\"\\\"{assembly_data['content']}\\\"\", fill=white, font=header_font)\n    \n    # Add frame counter to far right of first line\n    frame_num = random.randint(1000, 9000)\n    draw.text((width-90, 9), f\"F:{frame_num}\", fill=cyan, font=header_font)\n    \n    # Second line - left aligned with consistent spacing\n    # TYPE starts at the same position as FL012\n    draw.text((padding, 33), f\"TYPE: {assembly_data['imageType']}\", fill=green, font=header_font)\n    draw.text((padding + 240, 33), f\"FUNC: {assembly_data['cineosisFunction']}\", fill=green, font=header_font)\n    draw.text((padding + 480, 33), f\"SYNT: {assembly_data['syntagmaType']}\", fill=green, font=header_font)\n    \n    # Create the footer console overlay\n    footer_height = 80  # Height of bottom overlay\n    footer_y = height - footer_height\n    \n    # Draw semi-transparent black background for footer\n    draw.rectangle([(0, footer_y), (width, height)], fill=(0, 0, 0, 180))\n    draw.line([(0, footer_y), (width, footer_y)], fill=cyan, width=1)\n    \n    # Add scanline effect to footer\n    for y in range(footer_y, height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n    \n    # Add prompt text to footer\n    prompt_text = \"PROMPT: Stars explode behind eyelids--fireworks seen from inside a skull \u00b7 POV macro; eyelids closed, inner nebula bursts saffron-teal sparks, sub-bass rumble.\"\n    \n    # Wrap the prompt text across multiple lines if needed\n    wrapped_prompt = textwrap.wrap(prompt_text, width=110)\n    y_text = footer_y + 15\n    \n    # Add a small label for the prompt\n    draw.text((padding, y_text), \"PROMPT:\", fill=cyan, font=title_font)\n    \n    # Print the wrapped prompt text\n    for i, line in enumerate(wrapped_prompt):\n        if i == 0:\n            # For the first line, skip the \"PROMPT:\" label that we manually added\n            if line.startswith(\"PROMPT: \"):\n                line = line[8:]\n            draw.text((padding + 80, y_text), line, fill=white, font=footer_font)\n        else:\n            draw.text((padding, y_text + (i * 22)), line, fill=white, font=footer_font)\n    \n    # Add a timeline indicator at the bottom of the footer\n    timeline_y = height - 15\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 4 if i % 3 == 0 else 2\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f\"Codex overlay mockup created and saved to: {output_path}\")\n\ndef main():\n    output_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mockups\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create codex overlay mockup\n    output_path = os.path.join(output_dir, \"codex_overlay_aligned.png\")\n    create_codex_overlay(output_path)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "codex_overlay_aligned.png",
      ", (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Load font\n    try:\n        header_font = ImageFont.truetype(",
      "):\n                line = line[8:]\n            draw.text((padding + 80, y_text), line, fill=white, font=footer_font)\n        else:\n            draw.text((padding, y_text + (i * 22)), line, fill=white, font=footer_font)\n    \n    # Add a timeline indicator at the bottom of the footer\n    timeline_y = height - 15\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 4 if i % 3 == 0 else 2\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mockups"
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os",
      "textwrap",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/create_advanced_composites.py",
    "size": 20666,
    "lines": 488,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport random\nfrom pathlib import Path\nfrom PIL import Image, ImageFilter, ImageEnhance, ImageOps, ImageChops\nimport numpy as np\nfrom collections import defaultdict\n\n# Paths to source directories\nMPRE_RUN_TITLES_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run/backup_titles')\nMPOST_JOURNEY_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey')\nMIDJOURNEY_BG_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]')\n\n# Output directory\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/advanced_composites')\nOUTPUT_DIR.mkdir(exist_ok=True)\nLOGS_DIR = OUTPUT_DIR / \"logs\"\nLOGS_DIR.mkdir(exist_ok=True)\n\n# Track mapping for consistent naming\nTRACK_MAPPING = {\n    \"Out of Life\": {\"id\": \"01\", \"code\": \"SH\", \"timecode\": \"000000\"},\n    \"Flashing Lights\": {\"id\": \"02\", \"code\": \"FL\", \"timecode\": \"021100\"},\n    \"How to Break Off an Engagement\": {\"id\": \"03\", \"code\": \"HT\", \"timecode\": \"042200\"},\n    \"Nevermore\": {\"id\": \"04\", \"code\": \"NM\", \"timecode\": \"063300\"},\n    \"Bloodline\": {\"id\": \"05\", \"code\": \"BE\", \"timecode\": \"084400\"},\n    \"Resurrecting Atlantis\": {\"id\": \"06\", \"code\": \"AT\", \"timecode\": \"105500\"},\n    \"DJ Turn Me Up\": {\"id\": \"07\", \"code\": \"DJ\", \"timecode\": \"130600\"},\n    \"Newly Single\": {\"id\": \"08\", \"code\": \"NS\", \"timecode\": \"151700\"},\n    \"Yet Heard\": {\"id\": \"09\", \"code\": \"YH\", \"timecode\": \"172800\"},\n    \"Magic Ride\": {\"id\": \"10\", \"code\": \"MR\", \"timecode\": \"193900\"},\n    \"Reunion\": {\"id\": \"12\", \"code\": \"RU\", \"timecode\": \"215000\"},\n    \"How to Win My Heart\": {\"id\": \"13\", \"code\": \"HW\", \"timecode\": \"240100\"},\n    \"Hot Minute\": {\"id\": \"14\", \"code\": \"HM\", \"timecode\": \"261200\"},\n}\n\n# Enhanced blend modes with Afro-futurist aesthetic focus\nBLEND_MODES = [\n    \"neon_fracture\", \"digital_echo\", \"quantum_drift\", \"spectral_weave\", \n    \"time_collapse\", \"dream_resonance\", \"shadow_projection\", \"void_emergence\",\n    \"memory_trace\", \"data_corruption\", \"ancestral_echo\", \"urban_frequency\"\n]\n\n# Helper functions for track name and categorization\ndef find_track_name(filename):\n    for track_name in TRACK_MAPPING.keys():\n        track_variants = [\n            track_name, track_name.lower(), \n            track_name.replace(\" \", \"\"), track_name.lower().replace(\" \", \"\")\n        ]\n        for variant in track_variants:\n            if variant in filename.replace(\" \", \"\"):\n                return track_name\n    return None\n\ndef categorize_title_image(filename):\n    if \"Title Card\" in filename or \"Prompt Set\" in filename:\n        return \"TitleCard\"\n    elif \"Visual Concept\" in filename:\n        return \"VisualConcept\"\n    elif \"Poetic Ekphrasis\" in filename or \"Ekphrasis\" in filename:\n        return \"Ekphrasis\"\n    elif \"Duration\" in filename or \"Segment\" in filename:\n        return \"Segment\"\n    else:\n        return \"Other\"\n\n# Advanced blending functions - expanded with more techniques\ndef apply_neon_fracture(base, layer1, layer2=None, intensity=0.7):\n    \"\"\"Creates a neon fracture effect with multiple layers\"\"\"\n    # Enhance base with glow\n    base_glow = base.filter(ImageFilter.GaussianBlur(radius=15))\n    base_glow = ImageEnhance.Brightness(base_glow).enhance(1.8)\n    base_glow = ImageEnhance.Color(base_glow).enhance(2.2)\n    \n    # Split layer1 channels and shift them\n    r, g, b = layer1.split()\n    r_shift = Image.new('L', layer1.size)\n    r_shift.paste(r, (7, 0))\n    b_shift = Image.new('L', layer1.size)\n    b_shift.paste(b, (-7, 0))\n    \n    # Recombine with shifted channels\n    shifted = Image.merge('RGB', (r_shift, g, b_shift))\n    \n    # Create the fracture effect by overlaying the shifted layer\n    result = ImageChops.soft_light(base_glow, shifted)\n    \n    # Add third layer if provided\n    if layer2 is not None:\n        # Add some distortion to layer2\n        layer2 = layer2.filter(ImageFilter.EDGE_ENHANCE)\n        layer2 = ImageEnhance.Contrast(layer2).enhance(1.5)\n        \n        # Blend it in\n        result = Image.blend(result, layer2, 0.3)\n    \n    return result\n\ndef apply_digital_echo(base, layer1, layer2=None, intensity=0.7):\n    \"\"\"Creates an echo/ghost effect with multiple layers\"\"\"\n    # Create multiple echo layers with different opacities\n    echo_layers = []\n    for offset in [5, 10, 15, 20]:\n        # Create echo layer\n        echo = Image.new('RGB', base.size)\n        echo.paste(layer1, (offset, offset))\n        echo_layers.append(echo)\n    \n    # Start with base image\n    result = base.copy()\n    \n    # Blend in each echo layer with decreasing opacity\n    opacity = 0.4\n    for echo in echo_layers:\n        result = Image.blend(result, echo, opacity)\n        opacity -= 0.1\n    \n    # Add third layer if provided\n    if layer2 is not None:\n        # Add some glow to layer2\n        layer2 = layer2.filter(ImageFilter.GaussianBlur(radius=2))\n        layer2 = ImageEnhance.Brightness(layer2).enhance(1.2)\n        \n        # Blend it in with screen mode simulation\n        layer2_arr = np.array(layer2).astype(float)\n        result_arr = np.array(result).astype(float)\n        blend_arr = 255 - ((255 - layer2_arr) * (255 - result_arr) / 255)\n        result = Image.fromarray(np.uint8(blend_arr))\n    \n    return result\n\ndef apply_quantum_drift(base, layer1, layer2=None, intensity=0.7):\n    \"\"\"Creates a quantum-like drift/probability effect with multiple layers\"\"\"\n    # Split the image into horizontal bands\n    height = base.height\n    band_height = height // 25\n    \n    # Create a new image\n    result = Image.new('RGB', base.size)\n    \n    # Process in bands with quantum-like drift\n    for i in range(0, height, band_height):\n        # Determine band position\n        y_pos = i\n        height_slice = min(band_height, height - i)\n        \n        # Randomly decide which image to use for this band\n        rand = random.random()\n        if rand < 0.4:  # 40% base\n            src_img = base\n            x_offset = random.randint(-10, 10)\n        elif rand < 0.8:  # 40% layer1\n            src_img = layer1\n            x_offset = random.randint(-8, 8)\n        else:  # 20% layer2 if available, otherwise layer1\n            src_img = layer2 if layer2 is not None else layer1\n            x_offset = random.randint(-5, 5)\n        \n        # Get the band from the selected image\n        band = src_img.crop((0, y_pos, base.width, y_pos + height_slice))\n        \n        # Apply a small x-offset to create the drift effect\n        result.paste(band, (x_offset, y_pos))\n    \n    # Apply final enhancements\n    result = result.filter(ImageFilter.EDGE_ENHANCE_MORE)\n    result = ImageEnhance.Contrast(result).enhance(1.2)\n    \n    return result\n\ndef apply_spectral_weave(base, layer1, layer2=None, intensity=0.7):\n    \"\"\"Creates a spectral weaving pattern across layers\"\"\"\n    # Create a weave pattern\n    result = base.copy()\n    width, height = base.size\n    \n    # Vertical weave stripes\n    stripe_width = width // 40\n    for x in range(0, width, stripe_width * 2):\n        # Create vertical stripe\n        if x + stripe_width <= width:\n            stripe = layer1.crop((x, 0, x + stripe_width, height))\n            result.paste(stripe, (x, 0))\n    \n    # Horizontal weave stripes from layer2 if available\n    if layer2 is not None:\n        stripe_height = height // 40\n        for y in range(stripe_height, height, stripe_height * 2):\n            # Create horizontal stripe\n            if y + stripe_height <= height:\n                stripe = layer2.crop((0, y, width, y + stripe_height))\n                # Blend the stripe in rather than straight paste\n                for x in range(0, width, stripe_width * 2):\n                    if x + stripe_width <= width:\n                        # Skip where vertical stripes are to create weave\n                        continue\n                    # Blend in horizontal stripes between vertical ones\n                    section = stripe.crop((x, 0, x + stripe_width, stripe_height))\n                    result_section = result.crop((x, y, x + stripe_width, y + stripe_height))\n                    blended = Image.blend(result_section, section, 0.7)\n                    result.paste(blended, (x, y))\n    \n    # Apply final adjustments\n    result = ImageEnhance.Contrast(result).enhance(1.3)\n    \n    return result\n\ndef apply_time_collapse(base, layer1, layer2=None, intensity=0.7):\n    \"\"\"Creates a temporal collapse effect with layer stacking\"\"\"\n    # Prepare layers with different time 'distortions'\n    base_distort = base.filter(ImageFilter.GaussianBlur(radius=3))\n    layer1_distort = layer1.filter(ImageFilter.FIND_EDGES)\n    layer1_distort = ImageEnhance.Contrast(layer1_distort).enhance(2.0)\n    \n    # Create a radial gradient mask for blending\n    mask = Image.new('L', base.size, 0)\n    center_x, center_y = base.width // 2, base.height // 2\n    max_radius = min(center_x, center_y)\n    \n    # Fill the mask with radial gradient\n    for y in range(base.height):\n        for x in range(base.width):\n            # Calculate distance from center\n            distance = ((x - center_x)**2 + (y - center_y)**2)**0.5\n            # Normalize to 0-1 range and invert\n            value = max(0, min(255, int(255 * (1 - distance / max_radius))))\n            # Apply to mask\n            if x < mask.width and y < mask.height:  # Ensure within bounds\n                mask.putpixel((x, y), value)\n    \n    # Composite with mask\n    result = Image.composite(layer1_distort, base_distort, mask)\n    \n    # Add third layer if provided\n    if layer2 is not None:\n        # Apply a different distortion to layer2\n        layer2_distort = layer2.filter(ImageFilter.CONTOUR)\n        layer2_distort = ImageEnhance.Brightness(layer2_distort).enhance(1.5)\n        \n        # Create second mask - inverted from first\n        mask2 = ImageOps.invert(mask)\n        \n        # Composite with second mask\n        result = Image.composite(layer2_distort, result, mask2)\n    \n    return result\n\ndef apply_dream_resonance(base, layer1, layer2=None, intensity=0.7):\n    \"\"\"Creates dreamlike resonance patterns between layers\"\"\"\n    # Apply dreamy blur to base\n    base_blur = base.filter(ImageFilter.GaussianBlur(radius=4))\n    \n    # Apply color enhancement to layer1\n    layer1_enhanced = ImageEnhance.Color(layer1).enhance(1.8)\n    \n    # Blend with soft light\n    base_arr = np.array(base_blur).astype(float)\n    layer1_arr = np.array(layer1_enhanced).astype(float)\n    \n    # Soft light blend formula\n    result_arr = np.where(\n        layer1_arr < 128,\n        (base_arr * layer1_arr / 128),\n        255 - ((255 - base_arr) * (255 - layer1_arr) / 128)\n    )\n    result = Image.fromarray(np.uint8(np.clip(result_arr, 0, 255)))\n    \n    # Add layer2 if available with a motion effect\n    if layer2 is not None:\n        # Apply motion blur\n        layer2_motion = layer2.filter(ImageFilter.GaussianBlur(radius=3))\n        \n        # Create directional offset version\n        layer2_offset = Image.new('RGB', layer2.size)\n        layer2_offset.paste(layer2_motion, (int(intensity * 15), 0))\n        \n        # Blend motion layer with lower opacity\n        result = Image.blend(result, layer2_offset, 0.35)\n    \n    return result\n\n# Function to apply a specific blend mode\ndef apply_blend_mode(base, layer1, layer2=None, mode=None, intensity=0.7):\n    \"\"\"Apply a specific blend mode with multiple layers\"\"\"\n    # Resize all images to match base\n    layer1 = layer1.resize(base.size, Image.LANCZOS)\n    if layer2 is not None:\n        layer2 = layer2.resize(base.size, Image.LANCZOS)\n    \n    # Convert all to RGB mode if needed\n    if base.mode != 'RGB':\n        base = base.convert('RGB')\n    if layer1.mode != 'RGB':\n        layer1 = layer1.convert('RGB')\n    if layer2 is not None and layer2.mode != 'RGB':\n        layer2 = layer2.convert('RGB')\n    \n    # Select blend mode if not specified\n    if mode is None or mode not in BLEND_MODES:\n        mode = random.choice(BLEND_MODES)\n    \n    # Apply selected mode\n    if mode == \"neon_fracture\":\n        return apply_neon_fracture(base, layer1, layer2, intensity)\n    elif mode == \"digital_echo\":\n        return apply_digital_echo(base, layer1, layer2, intensity)\n    elif mode == \"quantum_drift\":\n        return apply_quantum_drift(base, layer1, layer2, intensity)\n    elif mode == \"spectral_weave\":\n        return apply_spectral_weave(base, layer1, layer2, intensity)\n    elif mode == \"time_collapse\":\n        return apply_time_collapse(base, layer1, layer2, intensity)\n    elif mode == \"dream_resonance\":\n        return apply_dream_resonance(base, layer1, layer2, intensity)\n    elif mode == \"shadow_projection\":\n        # Simplified shadow projection implementation\n        base_dark = ImageEnhance.Brightness(base).enhance(0.7)\n        layer1_edges = layer1.filter(ImageFilter.FIND_EDGES)\n        return Image.blend(base_dark, layer1_edges, 0.6)\n    elif mode == \"void_emergence\":\n        # Simplified void emergence implementation\n        base_neg = ImageOps.invert(base)\n        layer1_neg = ImageOps.invert(layer1)\n        return Image.blend(base_neg, layer1_neg, 0.5)\n    elif mode == \"memory_trace\":\n        # Simplified memory trace implementation\n        base_blur = base.filter(ImageFilter.GaussianBlur(radius=2))\n        layer1_sharp = ImageEnhance.Sharpness(layer1).enhance(2.0)\n        return Image.blend(base_blur, layer1_sharp, 0.7)\n    elif mode == \"data_corruption\":\n        # Simplified data corruption implementation\n        result = Image.blend(base, layer1, 0.5)\n        return result.filter(ImageFilter.FIND_EDGES)\n    elif mode == \"ancestral_echo\":\n        # Simplified ancestral echo implementation\n        sepia = ImageOps.colorize(ImageOps.grayscale(base), \"#704214\", \"#C0A080\")\n        return Image.blend(sepia, layer1, 0.6)\n    elif mode == \"urban_frequency\":\n        # Simplified urban frequency implementation\n        contrast = ImageEnhance.Contrast(base).enhance(1.5)\n        return Image.blend(contrast, layer1, 0.5)\n    else:\n        # Default to a simple blend if mode not implemented\n        result = Image.blend(base, layer1, 0.6)\n        if layer2 is not None:\n            result = Image.blend(result, layer2, 0.4)\n        return result\n\n# Organize files by track\ndef organize_files():\n    \"\"\"Organize all source files by track name and type\"\"\"\n    organized_files = defaultdict(lambda: defaultdict(list))\n    \n    # Process title/visual files from mpre-run/backup_titles\n    for file in MPRE_RUN_TITLES_DIR.glob(\"*.png\"):\n        track_name = find_track_name(file.name)\n        if track_name:\n            file_type = categorize_title_image(file.name)\n            organized_files[track_name][file_type].append(str(file))\n    \n    # Process mpost-journey files\n    for file in MPOST_JOURNEY_DIR.glob(\"*.png\"):\n        if \"backup\" not in file.name.lower():\n            track_id = file.name.split('_')[0]\n            track_code = file.name.split('_')[1]\n            \n            # Find corresponding track name\n            for track_name, info in TRACK_MAPPING.items():\n                if info[\"id\"] == track_id and info[\"code\"] == track_code:\n                    organized_files[track_name][\"Journey\"].append(str(file))\n                    break\n    \n    # Process midjourney background files\n    for file in MIDJOURNEY_BG_DIR.glob(\"*.png\"):\n        if \"backup\" not in file.name.lower() and \"BG\" in file.name:\n            track_id = file.name.split('_')[0]\n            if track_id.isdigit() or (len(track_id) == 2 and track_id[:2].isdigit()):\n                # Find corresponding track name\n                for track_name, info in TRACK_MAPPING.items():\n                    if info[\"id\"] == track_id:\n                        organized_files[track_name][\"Background\"].append(str(file))\n                        break\n    \n    return organized_files\n\n# Main function to create advanced composites\ndef create_advanced_composites():\n    \"\"\"Create more sophisticated composites using three-layer blending\"\"\"\n    organized_files = organize_files()\n    \n    # Log file for combinations\n    log_file = LOGS_DIR / \"advanced_combinations.txt\"\n    with open(log_file, \"w\") as log:\n        log.write(\"ADVANCED COMPOSITE COMBINATIONS\\n\")\n        log.write(\"===============================\\n\\n\")\n        \n        # Process each track\n        for track_name, file_types in organized_files.items():\n            track_info = TRACK_MAPPING.get(track_name)\n            if not track_info:\n                print(f\"Warning: No track mapping found for {track_name}\")\n                continue\n                \n            track_id = track_info[\"id\"]\n            track_code = track_info[\"code\"]\n            track_timecode = track_info[\"timecode\"]\n            \n            # Base filename format\n            base_filename = f\"{track_id}_{track_code}_{track_name.replace(' ', '')}_{track_timecode}\"\n            \n            # Log track info\n            log.write(f\"\\n{track_name} ({track_id}_{track_code}_{track_timecode})\\n\")\n            log.write(\"-\" * 50 + \"\\n\")\n            \n            # Generate composites with different layer combinations\n            bg_images = file_types.get(\"Background\", [])\n            journey_images = file_types.get(\"Journey\", [])\n            \n            # Process each type of title/concept image\n            for title_type in [\"TitleCard\", \"VisualConcept\", \"Ekphrasis\", \"Segment\"]:\n                title_images = file_types.get(title_type, [])\n                \n                if not title_images:\n                    continue\n                \n                # For each title image, create multiple advanced composites\n                for title_img_path in title_images:\n                    # Choose background image\n                    if bg_images:\n                        bg_img_path = random.choice(bg_images)\n                    else:\n                        # Find background from another track if needed\n                        all_bg_images = []\n                        for other_track, other_types in organized_files.items():\n                            if other_track != track_name:\n                                all_bg_images.extend(other_types.get(\"Background\", []))\n                        \n                        bg_img_path = random.choice(all_bg_images) if all_bg_images else None\n                    \n                    # Choose journey image\n                    journey_img_path = random.choice(journey_images) if journey_images else None\n                    \n                    # Skip if we don't have enough images\n                    if not bg_img_path:\n                        print(f\"Warning: No background image found for {track_name} - {title_type}\")\n                        continue\n                    \n                    # Load images\n                    title_img = Image.open(title_img_path)\n                    bg_img = Image.open(bg_img_path)\n                    journey_img = Image.open(journey_img_path) if journey_img_path else None\n                    \n                    # Generate multiple variations with different blend modes\n                    for i in range(3):  # Create 3 variations\n                        # Select blend mode\n                        blend_mode = random.choice(BLEND_MODES)\n                        variant_name = f\"Advanced_{blend_mode.capitalize()}_v{i+1}\"\n                        \n                        # Create composite\n                        composite = apply_blend_mode(bg_img, title_img, journey_img, blend_mode)\n                        \n                        # Create output filename\n                        output_filename = f\"{base_filename}_{title_type}_{variant_name}.png\"\n                        output_path = OUTPUT_DIR / output_filename\n                        \n                        # Save the composite\n                        composite.save(output_path)\n                        print(f\"Created: {output_filename}\")\n                        \n                        # Log combination\n                        log.write(f\"Created: {output_filename}\\n\")\n                        log.write(f\"  - Base: {Path(bg_img_path).name}\\n\")\n                        log.write(f\"  - Layer 1: {Path(title_img_path).name}\\n\")\n                        if journey_img_path:\n                            log.write(f\"  - Layer 2: {Path(journey_img_path).name}\\n\")\n                        log.write(f\"  - Blend Mode: {blend_mode}\\n\\n\")\n\n# Execute the script\nif __name__ == \"__main__\":\n    print(\"Starting to create advanced composite images...\")\n    create_advanced_composites()\n    print(\"Done! Check the output directory for the new composite images.\")\n    print(f\"Composites saved to: {OUTPUT_DIR}\")\n    print(f\"Combination log saved to: {LOGS_DIR / 'advanced_combinations.txt'}\")\n\n",
    "file_references": [
      "*.png",
      "*.png",
      "*.png",
      "{base_filename}_{title_type}_{variant_name}.png",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run/backup_titles",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/advanced_composites",
      "Creates an echo/ghost effect with multiple layers",
      ", base.size)\n        echo.paste(layer1, (offset, offset))\n        echo_layers.append(echo)\n    \n    # Start with base image\n    result = base.copy()\n    \n    # Blend in each echo layer with decreasing opacity\n    opacity = 0.4\n    for echo in echo_layers:\n        result = Image.blend(result, echo, opacity)\n        opacity -= 0.1\n    \n    # Add third layer if provided\n    if layer2 is not None:\n        # Add some glow to layer2\n        layer2 = layer2.filter(ImageFilter.GaussianBlur(radius=2))\n        layer2 = ImageEnhance.Brightness(layer2).enhance(1.2)\n        \n        # Blend it in with screen mode simulation\n        layer2_arr = np.array(layer2).astype(float)\n        result_arr = np.array(result).astype(float)\n        blend_arr = 255 - ((255 - layer2_arr) * (255 - result_arr) / 255)\n        result = Image.fromarray(np.uint8(blend_arr))\n    \n    return result\n\ndef apply_quantum_drift(base, layer1, layer2=None, intensity=0.7):\n    ",
      "Creates a quantum-like drift/probability effect with multiple layers",
      "\n    # Split the image into horizontal bands\n    height = base.height\n    band_height = height // 25\n    \n    # Create a new image\n    result = Image.new(",
      "\n    # Create a weave pattern\n    result = base.copy()\n    width, height = base.size\n    \n    # Vertical weave stripes\n    stripe_width = width // 40\n    for x in range(0, width, stripe_width * 2):\n        # Create vertical stripe\n        if x + stripe_width <= width:\n            stripe = layer1.crop((x, 0, x + stripe_width, height))\n            result.paste(stripe, (x, 0))\n    \n    # Horizontal weave stripes from layer2 if available\n    if layer2 is not None:\n        stripe_height = height // 40\n        for y in range(stripe_height, height, stripe_height * 2):\n            # Create horizontal stripe\n            if y + stripe_height <= height:\n                stripe = layer2.crop((0, y, width, y + stripe_height))\n                # Blend the stripe in rather than straight paste\n                for x in range(0, width, stripe_width * 2):\n                    if x + stripe_width <= width:\n                        # Skip where vertical stripes are to create weave\n                        continue\n                    # Blend in horizontal stripes between vertical ones\n                    section = stripe.crop((x, 0, x + stripe_width, stripe_height))\n                    result_section = result.crop((x, y, x + stripe_width, y + stripe_height))\n                    blended = Image.blend(result_section, section, 0.7)\n                    result.paste(blended, (x, y))\n    \n    # Apply final adjustments\n    result = ImageEnhance.Contrast(result).enhance(1.3)\n    \n    return result\n\ndef apply_time_collapse(base, layer1, layer2=None, intensity=0.7):\n    ",
      ", base.size, 0)\n    center_x, center_y = base.width // 2, base.height // 2\n    max_radius = min(center_x, center_y)\n    \n    # Fill the mask with radial gradient\n    for y in range(base.height):\n        for x in range(base.width):\n            # Calculate distance from center\n            distance = ((x - center_x)**2 + (y - center_y)**2)**0.5\n            # Normalize to 0-1 range and invert\n            value = max(0, min(255, int(255 * (1 - distance / max_radius))))\n            # Apply to mask\n            if x < mask.width and y < mask.height:  # Ensure within bounds\n                mask.putpixel((x, y), value)\n    \n    # Composite with mask\n    result = Image.composite(layer1_distort, base_distort, mask)\n    \n    # Add third layer if provided\n    if layer2 is not None:\n        # Apply a different distortion to layer2\n        layer2_distort = layer2.filter(ImageFilter.CONTOUR)\n        layer2_distort = ImageEnhance.Brightness(layer2_distort).enhance(1.5)\n        \n        # Create second mask - inverted from first\n        mask2 = ImageOps.invert(mask)\n        \n        # Composite with second mask\n        result = Image.composite(layer2_distort, result, mask2)\n    \n    return result\n\ndef apply_dream_resonance(base, layer1, layer2=None, intensity=0.7):\n    ",
      "\n    # Apply dreamy blur to base\n    base_blur = base.filter(ImageFilter.GaussianBlur(radius=4))\n    \n    # Apply color enhancement to layer1\n    layer1_enhanced = ImageEnhance.Color(layer1).enhance(1.8)\n    \n    # Blend with soft light\n    base_arr = np.array(base_blur).astype(float)\n    layer1_arr = np.array(layer1_enhanced).astype(float)\n    \n    # Soft light blend formula\n    result_arr = np.where(\n        layer1_arr < 128,\n        (base_arr * layer1_arr / 128),\n        255 - ((255 - base_arr) * (255 - layer1_arr) / 128)\n    )\n    result = Image.fromarray(np.uint8(np.clip(result_arr, 0, 255)))\n    \n    # Add layer2 if available with a motion effect\n    if layer2 is not None:\n        # Apply motion blur\n        layer2_motion = layer2.filter(ImageFilter.GaussianBlur(radius=3))\n        \n        # Create directional offset version\n        layer2_offset = Image.new(",
      "\n    organized_files = defaultdict(lambda: defaultdict(list))\n    \n    # Process title/visual files from mpre-run/backup_titles\n    for file in MPRE_RUN_TITLES_DIR.glob(",
      "\n    organized_files = organize_files()\n    \n    # Log file for combinations\n    log_file = LOGS_DIR / ",
      ", [])\n            \n            # Process each type of title/concept image\n            for title_type in [",
      "\n                        output_path = OUTPUT_DIR / output_filename\n                        \n                        # Save the composite\n                        composite.save(output_path)\n                        print(f",
      "Combination log saved to: {LOGS_DIR / "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "random",
      "pathlib",
      "PIL",
      "numpy",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/fl_video_generator_header_prompt.py",
    "size": 16017,
    "lines": 356,
    "source": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nimport textwrap\nfrom PIL import Image, ImageDraw, ImageFont\nimport subprocess\nimport tempfile\nimport shutil\n\nclass CodexVideoGenerator:\n    def __init__(self, codex_path, output_dir, tiger_dir):\n        self.codex_path = codex_path\n        self.output_dir = output_dir\n        self.tiger_dir = tiger_dir\n        self.temp_dir = os.path.join(output_dir, \"temp_frames\")\n        \n        # Cache for parsed codex data to avoid re-parsing\n        self.poem_entries_cache = {}\n        \n        # Ensure output directories exist\n        os.makedirs(self.output_dir, exist_ok=True)\n        os.makedirs(self.temp_dir, exist_ok=True)\n        \n        # Define dimensions for 3:2 aspect ratio\n        self.width = 1280  # Must be divisible by 2\n        self.height = 852  # 853 (from width*2/3) rounded down to ensure even number\n        \n        # Define heights for header components (all ensure even numbers)\n        self.metadata_header_height = 64  # Top section with ID, timestamp, etc.\n        self.prompt_header_height = 100   # Section for the prompt text\n        self.header_total = self.metadata_header_height + self.prompt_header_height\n        \n        # No footer needed - video controls will have clean space at bottom\n        self.footer_height = 0\n        \n        # Calculate total canvas height\n        self.canvas_height = self.height + self.header_total + self.footer_height\n        \n        # Font setup - will fall back to default if specified fonts aren't available\n        self.setup_fonts()\n        \n        # Define colors\n        self.cyan = (0, 255, 255)\n        self.amber = (255, 191, 0)\n        self.white = (255, 255, 255)\n        self.green = (80, 255, 80)\n        self.magenta = (255, 80, 255)\n        self.black = (0, 0, 0)\n        \n    def setup_fonts(self):\n        \"\"\"Setup fonts for the overlay\"\"\"\n        try:\n            self.header_font = ImageFont.truetype(\"Arial.ttf\", 14)\n            self.footer_font = ImageFont.truetype(\"Arial.ttf\", 18)\n            self.title_font = ImageFont.truetype(\"Arial.ttf\", 16)\n        except IOError:\n            try:\n                self.header_font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n                self.footer_font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n                self.title_font = ImageFont.truetype(\"DejaVuSans.ttf\", 16)\n            except IOError:\n                self.header_font = ImageFont.load_default()\n                self.footer_font = ImageFont.load_default()\n                self.title_font = ImageFont.load_default()\n    \n    def parse_codex(self, prefix=\"FL\"):\n        \"\"\"Parse codex data for entries with specified prefix, using cache if available\"\"\"\n        # Return cached entries if available\n        if prefix in self.poem_entries_cache:\n            return self.poem_entries_cache[prefix]\n        \n        print(f\"Parsing codex data for {prefix} entries...\")\n        entries = []\n        \n        with open(self.codex_path, 'r') as f:\n            content = f.read()\n        \n        # Find all entries with the specified prefix\n        pattern = r'### ' + prefix + r'(\\d+) \\[([\\d:]+)\\]\\s*\\*\\*Image:\\*\\* `([^`]+)`\\s*\\*\\*Assembly Source:\\*\\*\\s*```json\\s*({[\\s\\S]*?})\\s*```\\s*\\*\\*Prompt:\\*\\* (.*?)(?=\\n---|\\Z)'\n        matches = re.findall(pattern, content, re.DOTALL)\n        \n        for match in matches:\n            entry_id = prefix + match[0]\n            timestamp = match[1]\n            image_path = match[2]\n            assembly_json = match[3]\n            prompt = match[4].strip()\n            \n            # Parse the assembly JSON\n            try:\n                assembly_data = json.loads(assembly_json)\n            except json.JSONDecodeError:\n                print(f\"Error parsing JSON for {entry_id}\")\n                continue\n            \n            entries.append({\n                'id': entry_id,\n                'timestamp': timestamp,\n                'image_path': image_path,\n                'assembly': assembly_data,\n                'prompt': prompt\n            })\n        \n        # Store in cache\n        self.poem_entries_cache[prefix] = entries\n        print(f\"Found {len(entries)} entries for {prefix}\")\n        return entries\n    \n    def apply_overlay_header_prompt(self, image_path, entry, output_path):\n        \"\"\"Apply the overlay to an image and save it to output path\"\"\"\n        try:\n            # Try to load and process the image\n            if entry['id'].startswith('HT'):\n                # Special case for HT images which are in HT_shots subdirectory\n                # Try multiple possible paths for HT images\n                possible_paths = [\n                    os.path.join(self.tiger_dir, image_path),  # Standard path\n                    os.path.join(self.tiger_dir, 'HT', image_path),  # In HT directory\n                    os.path.join(self.tiger_dir, 'HT', 'HT_shots', image_path),  # In HT_shots subdirectory\n                    os.path.join(self.tiger_dir, 'HT', 'HT_shots', os.path.basename(image_path))  # Just the filename in HT_shots\n                ]\n                \n                # Try each path until we find the image\n                abs_image_path = None\n                for path in possible_paths:\n                    if os.path.exists(path):\n                        abs_image_path = path\n                        break\n                        \n                if abs_image_path is None:\n                    print(f\"Image not found for HT shot: {image_path}\")\n                    return False\n            else:\n                # Standard path for other prefixes\n                abs_image_path = os.path.join(self.tiger_dir, image_path)\n                if not os.path.exists(abs_image_path):\n                    print(f\"Image not found: {abs_image_path}\")\n                    return False\n            \n            original_image = Image.open(abs_image_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error opening image {image_path}: {e}\")\n            return None\n        \n        # Get original dimensions\n        orig_width, orig_height = original_image.size\n        \n        # Create a new canvas with expanded header for prompt\n        canvas = Image.new('RGB', (self.width, self.canvas_height), self.black)\n        \n        # Resize original to match our target width while maintaining aspect ratio\n        if orig_width != self.width or orig_height != self.height:\n            scale = min(self.width / orig_width, self.height / orig_height)\n            new_width = int(orig_width * scale)\n            new_height = int(orig_height * scale)\n            resized_image = original_image.resize((new_width, new_height), Image.LANCZOS)\n            \n            # Calculate position to center the image\n            left = (self.width - new_width) // 2\n            top = self.header_total + (self.height - new_height) // 2\n            \n            # Paste the resized image\n            canvas.paste(resized_image, (left, top))\n        else:\n            # Paste at exact position if already right size\n            canvas.paste(original_image, (0, self.header_total))\n        \n        draw = ImageDraw.Draw(canvas)\n        \n        # Extract data from entry\n        shot_id = entry['id']\n        timestamp = entry['timestamp']\n        assembly = entry['assembly']\n        \n        # ====== Draw metadata header section ======\n        draw.rectangle([(0, 0), (self.width, self.metadata_header_height)], fill=self.black)\n        \n        # Add scanlines to metadata header\n        for y in range(0, self.metadata_header_height, 2):\n            draw.line([(0, y), (self.width, y)], fill=(0, 0, 0, 70), width=1)\n        \n        # Define left padding for alignment\n        padding = 20\n        \n        # Add text to header - first line\n        draw.text((padding, 9), f\"{shot_id}\", fill=self.cyan, font=self.title_font)\n        draw.text((padding + 70, 9), f\"[{timestamp}]\", fill=self.white, font=self.header_font)\n        draw.text((padding + 170, 9), f\"{assembly.get('poem', '')}\", fill=self.amber, font=self.title_font)\n        draw.text((padding + 320, 9), f\"\\\"{assembly.get('content', '')}\\\"\", fill=self.white, font=self.header_font)\n        \n        # Get poem prefix and shot number\n        poem_prefix = shot_id[:2]\n        shot_num = int(shot_id[2:])\n        \n        # Get total frames for this poem\n        total_frames = len(self.parse_codex(prefix=poem_prefix))\n        \n        # Format as current/total for frame counter\n        frame_text = f\"F:{shot_num}/{total_frames}\"\n        draw.text((self.width-110, 9), frame_text, fill=self.cyan, font=self.header_font)\n        \n        # Second line - TYPE and FUNC left aligned, SYNT right aligned\n        draw.text((padding, 38), f\"TYPE: {assembly.get('imageType', '')}\", fill=self.green, font=self.header_font)\n        draw.text((padding + 240, 38), f\"FUNC: {assembly.get('cineosisFunction', '')}\", fill=self.green, font=self.header_font)\n        \n        # SYNT on the right side with different color\n        synt_text = f\"SYNT: {assembly.get('syntagmaType', '')}\"\n        synt_width = draw.textlength(synt_text, font=self.header_font)\n        draw.text((self.width - padding - synt_width - 90, 38), synt_text, fill=self.magenta, font=self.header_font)\n        \n        # Draw prominent divider line between metadata and prompt sections\n        # First a thicker white line\n        draw.line([(0, self.metadata_header_height), (self.width, self.metadata_header_height)], fill=self.white, width=2)\n        # Then an offset cyan line below it for a stylized double-line effect\n        draw.line([(0, self.metadata_header_height + 2), (self.width, self.metadata_header_height + 2)], fill=self.cyan, width=1)\n        \n        # ====== Draw prompt header section ======\n        prompt_start_y = self.metadata_header_height\n        draw.rectangle([(0, prompt_start_y), (self.width, self.header_total)], fill=self.black)\n        \n        # Add scanlines to prompt header\n        for y in range(prompt_start_y, self.header_total, 2):\n            draw.line([(0, y), (self.width, y)], fill=(0, 0, 0, 70), width=1)\n        \n        # Add prompt text to expanded header area\n        prompt_text = f\"PROMPT: {entry['prompt']}\"\n        \n        # Wrap the prompt text across multiple lines\n        wrapped_prompt = textwrap.wrap(prompt_text, width=110)\n        \n        # Position text right after the dividing line\n        y_text = prompt_start_y + 5\n        \n        # Add a small label for the prompt\n        draw.text((padding, y_text), \"PROMPT:\", fill=self.cyan, font=self.title_font)\n        \n        # Print the wrapped prompt text\n        for i, line in enumerate(wrapped_prompt):\n            if i == 0:\n                # For first line, skip the \"PROMPT:\" label that we manually added\n                if line.startswith(\"PROMPT: \"):\n                    line = line[8:]\n                draw.text((padding + 80, y_text), line, fill=self.white, font=self.footer_font)\n            else:\n                # Compress line spacing for multiple lines\n                draw.text((padding, y_text + (i * 18)), line, fill=self.white, font=self.footer_font)\n        \n        # Draw divider line between prompt header and image\n        draw.line([(0, self.header_total), (self.width, self.header_total)], fill=self.cyan, width=1)\n        \n        # Save the image with overlay\n        canvas.save(output_path)\n        return output_path\n\n    def calculate_frame_duration(self, total_frames, target_duration=131):\n        \"\"\"Calculate frame duration to achieve target video duration (default 2:11 = 131 seconds)\"\"\"\n        return target_duration / total_frames\n        \n    def generate_video(self, prefix=\"FL\", duration_per_frame=None, max_frames=None, target_duration=131):\n        \"\"\"Generate a video from parsed codex entries with target duration of 2:11 (131 seconds)\"\"\"\n        entries = self.parse_codex(prefix)\n        \n        if max_frames:\n            entries = entries[:max_frames]\n            # For test videos, still calculate correct timing based on full poem\n            full_count = len(self.parse_codex(prefix))\n            duration_per_frame = self.calculate_frame_duration(full_count, target_duration)\n        else:\n            # Calculate frame duration to achieve exactly 2:11 (131 seconds)\n            duration_per_frame = self.calculate_frame_duration(len(entries), target_duration)\n        \n        print(f\"Using {duration_per_frame:.2f} seconds per frame to achieve {target_duration} seconds total\")\n        \n        if not entries:\n            print(f\"No entries found for {prefix}\")\n            return None\n        \n        # Clear temp directory\n        for f in os.listdir(self.temp_dir):\n            os.remove(os.path.join(self.temp_dir, f))\n        \n        # Process each frame\n        frame_paths = []\n        for i, entry in enumerate(entries):\n            print(f\"Processing frame {i+1}/{len(entries)}: {entry['id']}\")\n            output_path = os.path.join(self.temp_dir, f\"{i:04d}.png\")\n            result = self.apply_overlay_header_prompt(entry['image_path'], entry, output_path)\n            if result:\n                frame_paths.append(result)\n        \n        if not frame_paths:\n            print(\"No frames were generated successfully\")\n            return None\n        \n        # Create a video from the frames using ffmpeg\n        output_video = os.path.join(self.output_dir, f\"{prefix}_header_prompt.mp4\")\n        try:\n            cmd = [\n                'ffmpeg',\n                '-y',  # Overwrite output file if it exists\n                '-framerate', f'1/{duration_per_frame}',  # Each frame lasts for duration_per_frame seconds\n                '-i', os.path.join(self.temp_dir, '%04d.png'),\n                '-c:v', 'libx264',\n                '-pix_fmt', 'yuv420p',\n                '-crf', '23',  # Higher quality\n                output_video\n            ]\n            print(f\"Running command: {' '.join(cmd)}\")\n            subprocess.run(cmd, check=True)\n            print(f\"Video generated: {output_video}\")\n            return output_video\n        except subprocess.CalledProcessError as e:\n            print(f\"Error generating video: {e}\")\n            return None\n        except Exception as e:\n            print(f\"Unexpected error: {e}\")\n            return None\n\ndef main():\n    import argparse\n    \n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description=\"Generate video with prompt in header\")\n    parser.add_argument(\"--prefix\", type=str, default=\"FL\", help=\"Poem prefix (FL, SH, BE, HT, NM)\")\n    parser.add_argument(\"--target-duration\", type=int, default=131, help=\"Target video duration in seconds (default 2:11=131s)\")\n    parser.add_argument(\"--max-frames\", type=int, help=\"Maximum number of frames to process\")\n    parser.add_argument(\"--test\", action=\"store_true\", help=\"Generate test video with only 10 frames\")\n    parser.add_argument(\"--ordered-codex\", type=str, help=\"Path to an ordered codex file to use instead of the main codex\")\n    args = parser.parse_args()\n    \n    # Set up paths\n    codex_path = \"/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_Codex.md\"\n    if args.ordered_codex and os.path.exists(args.ordered_codex):\n        print(f\"Using ordered codex file: {args.ordered_codex}\")\n        codex_path = args.ordered_codex\n    \n    tiger_dir = \"/Users/gaia/resurrecting atlantis/TIGER\"\n    output_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/video_output\"\n    \n    # Create the generator\n    generator = CodexVideoGenerator(codex_path, output_dir, tiger_dir)\n    \n    # Determine max frames\n    max_frames = args.max_frames\n    if args.test:\n        max_frames = 10\n    \n    # Generate video\n    print(f\"Generating video for {args.prefix} with {max_frames if max_frames else 'all'} frames, targeting {args.target_duration}s total duration\")\n    generator.generate_video(prefix=args.prefix, max_frames=max_frames, target_duration=args.target_duration)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "{i:04d}.png",
      "{prefix}_header_prompt.mp4",
      "%04d.png",
      ")\n        \n        # Cache for parsed codex data to avoid re-parsing\n        self.poem_entries_cache = {}\n        \n        # Ensure output directories exist\n        os.makedirs(self.output_dir, exist_ok=True)\n        os.makedirs(self.temp_dir, exist_ok=True)\n        \n        # Define dimensions for 3:2 aspect ratio\n        self.width = 1280  # Must be divisible by 2\n        self.height = 852  # 853 (from width*2/3) rounded down to ensure even number\n        \n        # Define heights for header components (all ensure even numbers)\n        self.metadata_header_height = 64  # Top section with ID, timestamp, etc.\n        self.prompt_header_height = 100   # Section for the prompt text\n        self.header_total = self.metadata_header_height + self.prompt_header_height\n        \n        # No footer needed - video controls will have clean space at bottom\n        self.footer_height = 0\n        \n        # Calculate total canvas height\n        self.canvas_height = self.height + self.header_total + self.footer_height\n        \n        # Font setup - will fall back to default if specified fonts aren",
      ", (self.width, self.canvas_height), self.black)\n        \n        # Resize original to match our target width while maintaining aspect ratio\n        if orig_width != self.width or orig_height != self.height:\n            scale = min(self.width / orig_width, self.height / orig_height)\n            new_width = int(orig_width * scale)\n            new_height = int(orig_height * scale)\n            resized_image = original_image.resize((new_width, new_height), Image.LANCZOS)\n            \n            # Calculate position to center the image\n            left = (self.width - new_width) // 2\n            top = self.header_total + (self.height - new_height) // 2\n            \n            # Paste the resized image\n            canvas.paste(resized_image, (left, top))\n        else:\n            # Paste at exact position if already right size\n            canvas.paste(original_image, (0, self.header_total))\n        \n        draw = ImageDraw.Draw(canvas)\n        \n        # Extract data from entry\n        shot_id = entry[",
      ", fill=self.white, font=self.header_font)\n        \n        # Get poem prefix and shot number\n        poem_prefix = shot_id[:2]\n        shot_num = int(shot_id[2:])\n        \n        # Get total frames for this poem\n        total_frames = len(self.parse_codex(prefix=poem_prefix))\n        \n        # Format as current/total for frame counter\n        frame_text = f",
      "\n        return target_duration / total_frames\n        \n    def generate_video(self, prefix=",
      "Processing frame {i+1}/{len(entries)}: {entry[",
      "1/{duration_per_frame}",
      "/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_Codex.md",
      "/Users/gaia/resurrecting atlantis/TIGER",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/video_output"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd, check=True"
      }
    ],
    "imports": [
      "os",
      "re",
      "json",
      "textwrap",
      "PIL",
      "subprocess",
      "tempfile",
      "shutil"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/ht_complete_codex_generator.py",
    "size": 3876,
    "lines": 115,
    "source": "#!/usr/bin/env python3\n\"\"\"\nHT Complete Codex Generator\n\nThis script generates properly formatted Codex entries for the HT (How To Win My Heart) poem section\nby combining data from:\n1. The Simplified sequence file (for all image variants with timestamps)\n2. The HT assembly JSON (for metadata)\n3. The HT prompts markdown (for prompt text)\n\nOutput is a markdown file ready to be added to the Codex.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport sys\n\n# Paths\nTIGER_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nHT_DIR = os.path.join(TIGER_DIR, \"HT\")\nSIMPLIFIED_PATH = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\nASSEMBLY_PATH = os.path.join(HT_DIR, \"HT_assembly.json\")\nPROMPTS_PATH = os.path.join(HT_DIR, \"HT_prompts.md\")\nOUTPUT_PATH = os.path.join(TIGER_DIR, \"HT_complete_codex_entries.md\")\n\ndef load_assembly_data():\n    \"\"\"Load the HT assembly JSON data.\"\"\"\n    with open(ASSEMBLY_PATH, 'r') as f:\n        return {item[\"id\"]: item for item in json.load(f)}\n\ndef load_prompts_data():\n    \"\"\"Load the HT prompts markdown data.\"\"\"\n    with open(PROMPTS_PATH, 'r') as f:\n        prompts_text = f.read()\n    \n    # Extract prompts with shot IDs\n    prompts = {}\n    for line in prompts_text.splitlines():\n        line = line.strip()\n        if not line or line.startswith('#'):\n            continue\n            \n        # Match HT shot IDs at the beginning of lines\n        match = re.match(r'(HT\\d+)\\s*\u00b7\\s*(.*)', line)\n        if match:\n            shot_id, prompt_text = match.groups()\n            prompts[shot_id] = prompt_text\n            \n    return prompts\n\ndef extract_ht_entries_from_simplified():\n    \"\"\"Extract all HT entries with timestamps from the Simplified markdown file.\"\"\"\n    with open(SIMPLIFIED_PATH, 'r') as f:\n        simplified_text = f.read()\n    \n    # Extract HT entries with timestamps and image paths\n    pattern = r'(HT\\d+)\\s+\\[([^\\]]+)\\]\\s+`([^`]+)`'\n    matches = re.findall(pattern, simplified_text)\n    \n    entries = []\n    for shot_id, timestamp, image_path in matches:\n        entries.append({\n            \"shot_id\": shot_id,\n            \"timestamp\": timestamp,\n            \"image_path\": image_path\n        })\n    \n    return entries\n\ndef generate_codex_entries():\n    \"\"\"Generate formatted codex entries for all HT images.\"\"\"\n    assembly_data = load_assembly_data()\n    prompts_data = load_prompts_data()\n    simplified_entries = extract_ht_entries_from_simplified()\n    \n    # Start output with a header\n    output_text = \"# HT (How To Win My Heart) Codex Entries - Complete Sequence\\n\\n\"\n    \n    # Generate formatted entries for each image\n    for entry in simplified_entries:\n        shot_id = entry[\"shot_id\"]\n        timestamp = entry[\"timestamp\"]\n        image_path = entry[\"image_path\"]\n        \n        # Skip if we don't have assembly data for this shot ID\n        if shot_id not in assembly_data:\n            print(f\"Warning: No assembly data found for {shot_id}\")\n            continue\n            \n        # Skip if we don't have prompt data for this shot ID\n        if shot_id not in prompts_data:\n            print(f\"Warning: No prompt data found for {shot_id}\")\n            continue\n        \n        # Format the codex entry\n        entry_text = f\"### {shot_id} [{timestamp}]\\n\\n\"\n        entry_text += f\"**Image:** `{image_path}`\\n\\n\"\n        entry_text += \"**Assembly Source:**\\n```json\\n\"\n        entry_text += json.dumps(assembly_data[shot_id], indent=2)\n        entry_text += \"\\n```\\n\\n\"\n        entry_text += f\"**Prompt:** {prompts_data[shot_id]}\\n\\n---\\n\\n\"\n        \n        output_text += entry_text\n    \n    # Write to output file\n    with open(OUTPUT_PATH, 'w') as f:\n        f.write(output_text)\n    \n    print(f\"Generated complete codex entries written to {OUTPUT_PATH}\")\n    print(f\"Total entries: {len(simplified_entries)}\")\n\nif __name__ == \"__main__\":\n    generate_codex_entries()\n",
    "file_references": [
      "HT_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "json",
      "sys"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "HT Complete Codex Generator\n\nThis script generates properly formatted Codex entries for the HT (How To Win My Heart) poem section\nby combining data from:\n1. The Simplified sequence file (for all image variants with timestamps)\n2. The HT assembly JSON (for metadata)\n3. The HT prompts markdown (for prompt text)\n\nOutput is a markdown file ready to be added to the Codex."
  },
  {
    "path": "JELLYFISH/assemble_ibex_fixed.py",
    "size": 9518,
    "lines": 246,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport re\n\n# Directory containing video files\nIBEX_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/IBEX')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Track order specified\nTRACK_ORDER = [\n    \"01_SH_OutOfLife_000000\",\n    \"02_FL_FlashingLights_021100\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\",\n    \"04_NM_Nevermore_063300\",\n    \"05_BE_Bloodline_084400\",\n    \"06_AT_ResurrectingAtlantis_105500\",\n    \"07_DJ_DJTurnMeUp_130600\",\n    \"08_NS_NewlySingle_151700\",\n    \"09_YH_YetHeard_172800\",\n    \"10_MR_MagicRide_193900\",\n    \"12_RU_Reunion_215000\",\n    \"13_HW_HowToWinMyHeart_240100\",\n    \"14_HM_HotMinute_261200\"\n]\n\ndef get_video_info(video_path):\n    \"\"\"Get the duration and other info of a video file using FFprobe.\"\"\"\n    try:\n        # Get duration\n        result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        duration = float(data['format']['duration'])\n        \n        # Get resolution and bitrate\n        result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-select_streams', 'v:0', \n            '-show_entries', 'stream=width,height,bit_rate', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        stream_data = json.loads(result.stdout)\n        if 'streams' in stream_data and stream_data['streams']:\n            width = stream_data['streams'][0].get('width', 'unknown')\n            height = stream_data['streams'][0].get('height', 'unknown')\n            bitrate = stream_data['streams'][0].get('bit_rate', 'unknown')\n        else:\n            width, height, bitrate = 'unknown', 'unknown', 'unknown'\n        \n        return {\n            'duration': duration,\n            'width': width, \n            'height': height,\n            'bitrate': bitrate\n        }\n    except (subprocess.CalledProcessError, KeyError, json.JSONDecodeError) as e:\n        print(f\"Error getting info for {video_path}: {e}\")\n        return {'duration': 0, 'width': 'unknown', 'height': 'unknown', 'bitrate': 'unknown'}\n\ndef find_video_files():\n    \"\"\"Find all video files in the IBEX directory.\"\"\"\n    video_files = []\n    for file in IBEX_DIR.glob('**/*.mp4'):\n        video_files.append(file)\n    return video_files\n\ndef sort_videos_by_track_order(video_files):\n    \"\"\"Sort video files according to the track order.\"\"\"\n    # Create a dictionary to map track prefix to its position in TRACK_ORDER\n    track_positions = {track: i for i, track in enumerate(TRACK_ORDER)}\n    \n    # Define a function to get the track prefix of a file\n    def get_track_prefix(file):\n        for track in TRACK_ORDER:\n            if file.name.startswith(track):\n                return track\n        return None\n    \n    # Sort the video files based on track order\n    sorted_videos = sorted(\n        video_files,\n        key=lambda file: track_positions.get(get_track_prefix(file), float('inf'))\n    )\n    \n    return sorted_videos\n\ndef trim_videos_and_assemble():\n    \"\"\"Check durations, trim if needed, and assemble all videos.\"\"\"\n    print(\"Finding video files in IBEX directory...\")\n    all_videos = find_video_files()\n    print(f\"Found {len(all_videos)} video files.\")\n    \n    if not all_videos:\n        print(\"No videos found in the IBEX directory.\")\n        return\n    \n    # Sort videos according to track order\n    print(\"Sorting videos by track order...\")\n    sorted_videos = sort_videos_by_track_order(all_videos)\n    \n    # Create a temporary directory for processed videos\n    temp_dir = OUTPUT_DIR / \"temp_videos\"\n    temp_dir.mkdir(exist_ok=True)\n    \n    # Process each video - get info and trim if necessary\n    processed_videos = []\n    total_expected_duration = 0\n    \n    print(\"\\nChecking video durations and processing videos:\")\n    print(\"-----------------------------------------------\")\n    \n    for i, video in enumerate(sorted_videos, 1):\n        # Get video info\n        info = get_video_info(video)\n        duration = info['duration']\n        \n        # Print original information\n        print(f\"\\n{i}. {video.name}\")\n        print(f\"   Duration: {duration:.2f} seconds\")\n        print(f\"   Resolution: {info['width']}x{info['height']}\")\n        \n        # Check if video is too long (over 10 seconds)\n        if duration > 10:\n            # Create trimmed version (take first 5 seconds)\n            trim_duration = 5.0\n            output_path = temp_dir / f\"trimmed_{video.name}\"\n            \n            print(f\"   \u26a0\ufe0f Video too long! Trimming to {trim_duration} seconds\")\n            \n            try:\n                subprocess.run([\n                    'ffmpeg',\n                    '-i', str(video),\n                    '-t', str(trim_duration),\n                    '-c', 'copy',\n                    '-y',\n                    str(output_path)\n                ], check=True, capture_output=True)\n                \n                # Add trimmed video to the list\n                processed_videos.append(output_path)\n                total_expected_duration += trim_duration\n                print(f\"   \u2713 Created trimmed version: {output_path.name}\")\n                \n            except subprocess.CalledProcessError as e:\n                print(f\"   \u274c Error trimming video: {e}\")\n                # If trimming fails, use the original\n                processed_videos.append(video)\n                total_expected_duration += duration\n                \n        else:\n            # Use original video\n            processed_videos.append(video)\n            total_expected_duration += duration\n            print(f\"   \u2713 Using original (duration OK)\")\n    \n    # Ensure we have all tracks, especially checking for 14_HM\n    have_14_hm = any(video.name.startswith(\"14_HM\") for video in processed_videos)\n    if not have_14_hm:\n        print(\"\\n\u26a0\ufe0f Warning: 14_HM_HotMinute track not found in the processed videos!\")\n        # Look for 14_HM in original directory\n        hm_videos = list(IBEX_DIR.glob(\"14_HM*.mp4\"))\n        if hm_videos:\n            print(f\"Found {len(hm_videos)} 14_HM videos in directory. Adding to sequence.\")\n            for hm_video in hm_videos:\n                info = get_video_info(hm_video)\n                print(f\"Adding: {hm_video.name} ({info['duration']:.2f} seconds)\")\n                processed_videos.append(hm_video)\n                total_expected_duration += info['duration']\n    \n    # Create a concat file for FFmpeg\n    concat_file = OUTPUT_DIR / \"ibex_fixed_concat_list.txt\"\n    print(f\"\\nCreating concatenation file at {concat_file}...\")\n    \n    with open(concat_file, 'w') as f:\n        for video in processed_videos:\n            # Escape single quotes in the path\n            escaped_path = str(video).replace(\"'\", \"'\\\\''\")\n            f.write(f\"file '{escaped_path}'\\n\")\n    \n    # Output filename\n    output_file = OUTPUT_DIR / \"ResurrectingAtlantis_IBEX_Fixed.mp4\"\n    \n    # Use FFmpeg to concatenate the videos\n    print(f\"Concatenating videos into {output_file}...\")\n    print(f\"Expected duration: {total_expected_duration:.2f} seconds ({total_expected_duration/60:.2f} minutes)\")\n    \n    try:\n        result = subprocess.run([\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file),\n            '-c', 'copy',\n            '-y',  # Overwrite output file if it exists\n            str(output_file)\n        ], check=True, capture_output=True)\n        \n        print(\"\\nIBEX video sequence assembled successfully!\")\n        print(f\"Output file: {output_file}\")\n        \n        # Create a text file with the list of videos used\n        video_list_file = OUTPUT_DIR / \"ibex_fixed_sequence_details.txt\"\n        with open(video_list_file, 'w') as f:\n            f.write(\"RESURRECTING ATLANTIS - IBEX FIXED SEQUENCE\\n\")\n            f.write(\"=========================================\\n\\n\")\n            f.write(f\"Total videos used: {len(processed_videos)}\\n\")\n            f.write(f\"Expected duration: {total_expected_duration:.2f} seconds ({total_expected_duration/60:.2f} minutes)\\n\")\n            f.write(f\"Output file: {output_file}\\n\\n\")\n            f.write(\"TRACK SEQUENCE:\\n\")\n            \n            for i, video in enumerate(processed_videos, 1):\n                # Find the track ID for this video\n                track_id = next((track for track in TRACK_ORDER if video.name.startswith(track) or \n                                 Path(video.name).name.startswith(track)), \"Unknown\")\n                duration = get_video_info(video)['duration']\n                \n                # Write video details\n                f.write(f\"{i}. {track_id}\\n\")\n                f.write(f\"   File: {video.name}\\n\")\n                f.write(f\"   Duration: {duration:.2f} seconds\\n\\n\")\n                \n        print(f\"IBEX sequence details saved to {video_list_file}\")\n        \n    except subprocess.CalledProcessError as e:\n        print(\"Error during video concatenation:\")\n        print(f\"FFmpeg stdout: {e.stdout.decode()}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode()}\")\n        print(\"Make sure FFmpeg is installed and all videos are valid.\")\n\nif __name__ == \"__main__\":\n    trim_videos_and_assemble()\n",
    "file_references": [
      "**/*.mp4",
      "14_HM*.mp4",
      "ResurrectingAtlantis_IBEX_Fixed.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA/IBEX",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence",
      "**/*.mp4",
      ")\n    sorted_videos = sort_videos_by_track_order(all_videos)\n    \n    # Create a temporary directory for processed videos\n    temp_dir = OUTPUT_DIR / ",
      ")\n        \n        # Check if video is too long (over 10 seconds)\n        if duration > 10:\n            # Create trimmed version (take first 5 seconds)\n            trim_duration = 5.0\n            output_path = temp_dir / f",
      "]\n    \n    # Create a concat file for FFmpeg\n    concat_file = OUTPUT_DIR / ",
      ")\n    \n    # Output filename\n    output_file = OUTPUT_DIR / ",
      "Expected duration: {total_expected_duration:.2f} seconds ({total_expected_duration/60:.2f} minutes)",
      ")\n        \n        # Create a text file with the list of videos used\n        video_list_file = OUTPUT_DIR / ",
      "Expected duration: {total_expected_duration:.2f} seconds ({total_expected_duration/60:.2f} minutes)\\n"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-select_streams', 'v:0', \n            '-show_entries', 'stream=width,height,bit_rate', \n            '-of', 'json', \n            str(v"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-i', str(video"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "pathlib",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/codex_legend_generator.py",
    "size": 15336,
    "lines": 322,
    "source": "#!/usr/bin/env python3\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont, ImageColor\nimport textwrap\n\nclass CodexLegendGenerator:\n    def __init__(self, tiger_dir, output_dir):\n        self.tiger_dir = tiger_dir\n        self.output_dir = output_dir\n        os.makedirs(self.output_dir, exist_ok=True)\n        \n        # Define canvas dimensions matching our video format\n        self.width = 1280\n        self.height = 1016  # Matches our video height\n        \n        # Define colors\n        self.cyan = (0, 255, 255)\n        self.amber = (255, 191, 0)\n        self.white = (255, 255, 255)\n        self.green = (80, 255, 80)\n        self.magenta = (255, 80, 255)\n        self.black = (0, 0, 0)\n        self.dark_blue = (16, 24, 32)\n        \n        # Setup fonts\n        self.setup_fonts()\n    \n    def setup_fonts(self):\n        \"\"\"Setup fonts for the legend\"\"\"\n        try:\n            self.header_font = ImageFont.truetype(\"Arial.ttf\", 14)\n            self.section_font = ImageFont.truetype(\"Arial.ttf\", 22)\n            self.title_font = ImageFont.truetype(\"Arial.ttf\", 26)\n            self.footer_font = ImageFont.truetype(\"Arial.ttf\", 18)\n            self.small_font = ImageFont.truetype(\"Arial.ttf\", 13)\n            self.bold_font = ImageFont.truetype(\"Arial Bold.ttf\", 16)\n        except IOError:\n            try:\n                self.header_font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n                self.section_font = ImageFont.truetype(\"DejaVuSans.ttf\", 22)\n                self.title_font = ImageFont.truetype(\"DejaVuSans.ttf\", 26)\n                self.footer_font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n                self.small_font = ImageFont.truetype(\"DejaVuSans.ttf\", 13)\n                self.bold_font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", 16)\n            except IOError:\n                self.header_font = ImageFont.load_default()\n                self.section_font = ImageFont.load_default()\n                self.title_font = ImageFont.load_default()\n                self.footer_font = ImageFont.load_default()\n                self.small_font = ImageFont.load_default()\n                self.bold_font = ImageFont.load_default()\n    \n    def create_legend(self, cover_image_path, output_path):\n        \"\"\"Create a legend image explaining all codex terminology\"\"\"\n        # Load the cover image\n        try:\n            cover_img = Image.open(cover_image_path).convert('RGB')\n            # Resize while maintaining aspect ratio\n            cover_img.thumbnail((self.width, self.height), Image.LANCZOS)\n        except Exception as e:\n            print(f\"Error opening cover image: {e}\")\n            # Create a plain dark blue background as fallback\n            cover_img = Image.new('RGB', (self.width, self.height), self.dark_blue)\n        \n        # Create new canvas sized to our video dimensions\n        canvas = Image.new('RGB', (self.width, self.height), self.dark_blue)\n        \n        # Center the cover image on canvas\n        offset_x = (self.width - cover_img.width) // 2\n        offset_y = (self.height - cover_img.height) // 2\n        canvas.paste(cover_img, (offset_x, offset_y))\n        \n        # Add a semi-transparent overlay to improve text readability\n        overlay = Image.new('RGBA', (self.width, self.height), (0, 0, 0, 180))\n        canvas = Image.alpha_composite(canvas.convert('RGBA'), overlay)\n        canvas = canvas.convert('RGB')\n        \n        draw = ImageDraw.Draw(canvas)\n        \n        # Add main title at the top\n        title_text = \"WHERE YOU GO WHEN YOU LEAVE: CODEX LEGEND\"\n        title_width = draw.textlength(title_text, font=self.title_font)\n        draw.text(((self.width - title_width) // 2, 40), title_text, fill=self.amber, font=self.title_font)\n        \n        # Add subtitle\n        subtitle_text = \"Reference Guide to Retrofuture Codex Overlay Terminology\"\n        subtitle_width = draw.textlength(subtitle_text, font=self.section_font)\n        draw.text(((self.width - subtitle_width) // 2, 80), subtitle_text, fill=self.cyan, font=self.section_font)\n        \n        # Draw dividing line\n        draw.line([(40, 120), (self.width-40, 120)], fill=self.white, width=2)\n        \n        # Start y position for content sections\n        y_pos = 150\n        left_margin = 60\n        \n        # Section 1: Header Elements\n        draw.text((left_margin, y_pos), \"HEADER ELEMENTS\", fill=self.amber, font=self.section_font)\n        y_pos += 40\n        \n        # Mock header example\n        header_y = y_pos\n        draw.rectangle([(left_margin, header_y), (self.width-left_margin, header_y+64)], outline=self.cyan, width=1)\n        \n        # First row of header\n        draw.text((left_margin+10, header_y+9), \"FL004\", fill=self.cyan, font=self.title_font)\n        draw.text((left_margin+80, header_y+9), \"[02:17:53]\", fill=self.white, font=self.header_font)\n        draw.text((left_margin+180, header_y+9), \"Flashing Lights\", fill=self.amber, font=self.title_font)\n        draw.text((left_margin+330, header_y+9), \"\\\"with my\\\"\", fill=self.white, font=self.header_font)\n        draw.text((self.width-left_margin-110, header_y+9), \"F:4/57\", fill=self.cyan, font=self.header_font)\n        \n        # Second row of header\n        draw.text((left_margin+10, header_y+38), \"TYPE: Crystal-Image\", fill=self.green, font=self.header_font)\n        draw.text((left_margin+250, header_y+38), \"FUNC: Temporal Reflection Loop\", fill=self.green, font=self.header_font)\n        draw.text((self.width-left_margin-300, header_y+38), \"SYNT: Crystal Syntagma (CS)\", fill=self.magenta, font=self.header_font)\n        \n        y_pos += 74\n        \n        # Header field explanations\n        explanations = [\n            (\"Shot ID (cyan)\", \"Unique identifier for each frame (e.g. FL004)\"),\n            (\"Timestamp (white)\", \"Timestamp in the poem sequence [mm:ss:ff]\"),\n            (\"Poem Title (amber)\", \"Title of the poem (e.g. Flashing Lights)\"),\n            (\"Content Fragment (white)\", \"Text fragment from the poem\"),\n            (\"Frame Counter (cyan)\", \"Current frame / total frames in poem\"),\n            (\"TYPE (green)\", \"Image Type classification (e.g. Crystal-Image, Narrative-Image)\"),\n            (\"FUNC (green)\", \"Cineosis Function (e.g. Temporal Reflection Loop)\"),\n            (\"SYNT (magenta)\", \"Syntagma Type (e.g. Crystal Syntagma (CS))\")\n        ]\n        \n        # Draw explanation of header elements in two columns\n        col_width = (self.width - 2*left_margin) // 2 - 20\n        for i, (term, desc) in enumerate(explanations):\n            x = left_margin + 10 + (col_width + 40) * (i // 4)\n            y = y_pos + 30 * (i % 4)\n            draw.text((x, y), term + \":\", fill=self.white, font=self.bold_font)\n            draw.text((x + draw.textlength(term + \": \", font=self.bold_font), y), \n                      desc, fill=self.white, font=self.header_font)\n        \n        # Move down for next section\n        y_pos += 150\n        \n        # Section 2: Assembly JSON Fields\n        draw.text((left_margin, y_pos), \"ASSEMBLY JSON FIELDS\", fill=self.amber, font=self.section_font)\n        y_pos += 40\n        \n        json_fields = [\n            (\"poem\", \"The poem identifier (e.g. 'Flashing Lights')\"),\n            (\"content\", \"Text fragment from the poem\"),\n            (\"syntagmaType\", \"Classification of narrative structure (CS, MS, BS, LS)\"),\n            (\"imageType\", \"Image classification (Crystal-Image, Movement-Image, etc.)\"),\n            (\"cineosisFunction\", \"Function in poem's visual narrative\")\n        ]\n        \n        # Draw JSON fields explanation\n        for i, (field, desc) in enumerate(json_fields):\n            draw.text((left_margin+10, y_pos+i*30), f\"\\\"{field}\\\":\", fill=self.cyan, font=self.bold_font)\n            draw.text((left_margin+150, y_pos+i*30), desc, fill=self.white, font=self.header_font)\n        \n        # Move down for next section\n        y_pos += 180\n        \n        # Section 3: Prompt Structure\n        draw.text((left_margin, y_pos), \"PROMPT STRUCTURE\", fill=self.amber, font=self.section_font)\n        y_pos += 40\n        \n        # Mock prompt example\n        prompt_y = y_pos\n        draw.rectangle([(left_margin, prompt_y), (self.width-left_margin, prompt_y+60)], outline=self.cyan, width=1)\n        wrapped_prompt = textwrap.wrap(\"CS : Temporal Reflection Loop : Time pauses on an upside-down silhouette, pockets spilling tiny Gravity-flip alley: silhouetted figure frozen mid-fall, miniature car-headlights pour like\", width=110)\n        draw.text((left_margin+10, prompt_y+5), \"PROMPT:\", fill=self.cyan, font=self.title_font)\n        for i, line in enumerate(wrapped_prompt):\n            y = prompt_y + 5 + (i * 25)\n            if i == 0:\n                draw.text((left_margin+80, y), line, fill=self.white, font=self.footer_font)\n            else:\n                draw.text((left_margin+10, y), line, fill=self.white, font=self.footer_font)\n                \n        y_pos += 70\n        \n        prompt_parts = [\n            (\"SYNTAGMA TYPE\", \"First element in prompt (CS, MS, BS, LS)\"),\n            (\"CINEOSIS FUNCTION\", \"Second element describing visual function\"),\n            (\"CONTENT DESCRIPTION\", \"Poetic description derived from poem content\")\n        ]\n        \n        # Draw prompt structure explanation\n        for i, (part, desc) in enumerate(prompt_parts):\n            draw.text((left_margin+10, y_pos+i*30), part + \":\", fill=self.cyan, font=self.bold_font)\n            draw.text((left_margin+210, y_pos+i*30), desc, fill=self.white, font=self.header_font)\n        \n        # Move down for final section\n        y_pos += 120\n        \n        # Section 4: Type Definitions\n        draw.text((left_margin, y_pos), \"TYPE DEFINITIONS\", fill=self.amber, font=self.section_font)\n        y_pos += 40\n        \n        # Two-column layout for type definitions\n        types_col1 = [\n            (\"CS\", \"Crystal Syntagma\", \"Time-focused, reflective, crystallized moments\"),\n            (\"MS\", \"Movement Syntagma\", \"Action-oriented, dynamic sequences\"),\n            (\"CI\", \"Crystal-Image\", \"Temporal reflection, memory-based imagery\")\n        ]\n        \n        types_col2 = [\n            (\"BS\", \"Binary Syntagma\", \"Contrasting or parallel elements\"),\n            (\"LS\", \"Linear Syntagma\", \"Sequential, progressive narrative\"),\n            (\"MI\", \"Movement-Image\", \"Action-focused, kinetic visual narrative\")\n        ]\n        \n        # Draw type definitions\n        col_width = (self.width - 2*left_margin) // 2 - 20\n        for i, (code, name, desc) in enumerate(types_col1):\n            draw.text((left_margin+10, y_pos+i*40), code + \":\", fill=self.magenta, font=self.bold_font)\n            draw.text((left_margin+50, y_pos+i*40), name, fill=self.cyan, font=self.bold_font)\n            wrapped = textwrap.wrap(desc, width=45)\n            for j, line in enumerate(wrapped):\n                draw.text((left_margin+50, y_pos+i*40+20+j*20), line, fill=self.white, font=self.small_font)\n        \n        # Second column\n        for i, (code, name, desc) in enumerate(types_col2):\n            x = left_margin + col_width + 40\n            draw.text((x+10, y_pos+i*40), code + \":\", fill=self.magenta, font=self.bold_font)\n            draw.text((x+50, y_pos+i*40), name, fill=self.cyan, font=self.bold_font)\n            wrapped = textwrap.wrap(desc, width=45)\n            for j, line in enumerate(wrapped):\n                draw.text((x+50, y_pos+i*40+20+j*20), line, fill=self.white, font=self.small_font)\n        \n        # Add footer with credits\n        footer_y = self.height - 40\n        credit_text = \"WHERE YOU GO WHEN YOU LEAVE \u2022 Mark Anthony Thomas \u2022 Retrofuture Codex Overlay\"\n        credit_width = draw.textlength(credit_text, font=self.footer_font)\n        draw.text(((self.width - credit_width) // 2, footer_y), credit_text, fill=self.amber, font=self.footer_font)\n        \n        # Save the image\n        canvas.save(output_path)\n        print(f\"Legend created: {output_path}\")\n        return output_path\n    \n    def combine_with_video(self, legend_path, video_path, output_path):\n        \"\"\"Combine the legend image with a video, adding it as a starting frame\"\"\"\n        try:\n            # Create a temporary video from the legend image (5 second duration)\n            temp_dir = os.path.join(self.output_dir, \"temp_legend\")\n            os.makedirs(temp_dir, exist_ok=True)\n            legend_vid_path = os.path.join(temp_dir, \"legend.mp4\")\n            \n            # Create 5 second video from single image - use subprocess instead of os.system\n            import subprocess\n            \n            # Create legend video from image (5 seconds)\n            ffmpeg_cmd = [\n                'ffmpeg',\n                '-y',\n                '-loop', '1',\n                '-i', legend_path,\n                '-c:v', 'libx264',\n                '-t', '5',\n                '-pix_fmt', 'yuv420p',\n                legend_vid_path\n            ]\n            subprocess.run(ffmpeg_cmd, check=True)\n            print(f\"Created legend video: {legend_vid_path}\")\n            \n            # Combine legend video with main video\n            concat_file = os.path.join(temp_dir, \"concat.txt\")\n            with open(concat_file, 'w') as f:\n                # Use absolute paths with escaped single quotes for ffmpeg concat\n                f.write(f\"file '{legend_vid_path.replace(\"'\", \"'\\\\''\")}\\n\")\n                f.write(f\"file '{video_path.replace(\"'\", \"'\\\\''\")}\\n\")\n            \n            # Concatenate the videos\n            concat_cmd = [\n                'ffmpeg',\n                '-y',\n                '-f', 'concat',\n                '-safe', '0',\n                '-i', concat_file,\n                '-c', 'copy',\n                output_path\n            ]\n            subprocess.run(concat_cmd, check=True)\n            print(f\"Combined video created: {output_path}\")\n            \n            # Clean up temp files\n            os.remove(concat_file)\n        except Exception as e:\n            print(f\"Error combining legend with video: {e}\")\n            import traceback\n            traceback.print_exc()\n\ndef main():\n    import argparse\n    \n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description=\"Generate codex legend image\")\n    parser.add_argument(\"--cover\", type=str, default=\"/Users/gaia/resurrecting atlantis/TIGER/cover.jpg\", help=\"Path to cover image\")\n    parser.add_argument(\"--video\", type=str, help=\"Path to video to combine with legend (optional)\")\n    parser.add_argument(\"--output\", type=str, default=\"/Users/gaia/resurrecting atlantis/JELLYFISH/video_output\", help=\"Output directory\")\n    parser.add_argument(\"--prefix\", type=str, default=\"FL\", help=\"Poem prefix (for output naming)\")\n    args = parser.parse_args()\n    \n    tiger_dir = \"/Users/gaia/resurrecting atlantis/TIGER\"\n    output_dir = args.output\n    \n    generator = CodexLegendGenerator(tiger_dir, output_dir)\n    \n    # Generate the legend image\n    legend_path = os.path.join(output_dir, f\"{args.prefix}_legend.png\")\n    generator.create_legend(args.cover, legend_path)\n    \n    # If a video is provided, combine the legend with it\n    if args.video:\n        combined_path = os.path.join(output_dir, f\"{args.prefix}_with_legend.mp4\")\n        generator.combine_with_video(legend_path, args.video, combined_path)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "legend.mp4",
      "/Users/gaia/resurrecting atlantis/TIGER/cover.jpg",
      "{args.prefix}_legend.png",
      "{args.prefix}_with_legend.mp4",
      ", (self.width, self.height), self.dark_blue)\n        \n        # Center the cover image on canvas\n        offset_x = (self.width - cover_img.width) // 2\n        offset_y = (self.height - cover_img.height) // 2\n        canvas.paste(cover_img, (offset_x, offset_y))\n        \n        # Add a semi-transparent overlay to improve text readability\n        overlay = Image.new(",
      "\n        title_width = draw.textlength(title_text, font=self.title_font)\n        draw.text(((self.width - title_width) // 2, 40), title_text, fill=self.amber, font=self.title_font)\n        \n        # Add subtitle\n        subtitle_text = ",
      "\n        subtitle_width = draw.textlength(subtitle_text, font=self.section_font)\n        draw.text(((self.width - subtitle_width) // 2, 80), subtitle_text, fill=self.cyan, font=self.section_font)\n        \n        # Draw dividing line\n        draw.line([(40, 120), (self.width-40, 120)], fill=self.white, width=2)\n        \n        # Start y position for content sections\n        y_pos = 150\n        left_margin = 60\n        \n        # Section 1: Header Elements\n        draw.text((left_margin, y_pos), ",
      "F:4/57",
      "Current frame / total frames in poem",
      ")\n        ]\n        \n        # Draw explanation of header elements in two columns\n        col_width = (self.width - 2*left_margin) // 2 - 20\n        for i, (term, desc) in enumerate(explanations):\n            x = left_margin + 10 + (col_width + 40) * (i // 4)\n            y = y_pos + 30 * (i % 4)\n            draw.text((x, y), term + ",
      ")\n        ]\n        \n        # Draw type definitions\n        col_width = (self.width - 2*left_margin) // 2 - 20\n        for i, (code, name, desc) in enumerate(types_col1):\n            draw.text((left_margin+10, y_pos+i*40), code + ",
      "\n        credit_width = draw.textlength(credit_text, font=self.footer_font)\n        draw.text(((self.width - credit_width) // 2, footer_y), credit_text, fill=self.amber, font=self.footer_font)\n        \n        # Save the image\n        canvas.save(output_path)\n        print(f",
      "/Users/gaia/resurrecting atlantis/TIGER/cover.jpg",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/video_output",
      "/Users/gaia/resurrecting atlantis/TIGER"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "ffmpeg_cmd, check=True"
      },
      {
        "type": "run",
        "snippet": "concat_cmd, check=True"
      }
    ],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/rename_mpre_run_files.py",
    "size": 2214,
    "lines": 61,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\n# Path to the mpre-run directory\nmpre_run_dir = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run')\n\n# Create a backup directory\nbackup_dir = mpre_run_dir / 'backup'\nbackup_dir.mkdir(exist_ok=True)\n\n# Dictionary to hold files and their metadata for sorting\nfile_metadata = {}\n\n# Process all files in the directory\nfor file_path in mpre_run_dir.glob('*'):\n    if file_path.is_file() and file_path.name.startswith('Gen4 ') and file_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n        # Create backup\n        backup_file = backup_dir / file_path.name\n        shutil.copy2(file_path, backup_file)\n        \n        # Create new name by removing \"Gen4 \" prefix\n        new_name = file_path.name[5:]  # Remove the first 5 characters (\"Gen4 \")\n        \n        # Extract key information for sorting\n        # Extract track name and timestamp if available\n        timestamp_match = re.search(r'(\\d{6} - \\d{6})', new_name)\n        timestamp = timestamp_match.group(1) if timestamp_match else \"\"\n        \n        # Extract track name (first part before any special characters or \"Prompt\")\n        track_name = new_name.split('Prompt')[0].split('Duration')[0].split('Visual')[0].strip()\n        \n        # Store metadata\n        file_metadata[file_path] = {\n            'new_name': new_name,\n            'track_name': track_name,\n            'timestamp': timestamp\n        }\n\n# Sort files by track name and then by timestamp\nsorted_files = sorted(file_metadata.keys(), \n                     key=lambda x: (file_metadata[x]['track_name'], file_metadata[x]['timestamp']))\n\n# Rename files with sequential numbering\nfor index, file_path in enumerate(sorted_files, 1):\n    metadata = file_metadata[file_path]\n    # Format new name with sequence number\n    sequence_prefix = f\"{index:02d}_\"\n    sequenced_name = sequence_prefix + metadata['new_name']\n    \n    # Create the new file path\n    new_file_path = file_path.parent / sequenced_name\n    \n    # Rename file\n    os.rename(file_path, new_file_path)\n    print(f\"Renamed: {file_path.name} -> {sequenced_name}\")\n\nprint(f\"\\nAll files processed. Backup copies saved in {backup_dir}\")\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run",
      "]:\n        # Create backup\n        backup_file = backup_dir / file_path.name\n        shutil.copy2(file_path, backup_file)\n        \n        # Create new name by removing ",
      "]\n    \n    # Create the new file path\n    new_file_path = file_path.parent / sequenced_name\n    \n    # Rename file\n    os.rename(file_path, new_file_path)\n    print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/codex_overlay_refined.py",
    "size": 6159,
    "lines": 151,
    "source": "#!/usr/bin/env python3\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nimport textwrap\nimport random\n\ndef create_codex_overlay(output_path, shot_id=\"FL012\", timestamp=\"02:38:34\"):\n    \"\"\"Create a mockup with assembly.json data in the header and prompt in footer\"\"\"\n    # Create a black canvas (simulating video frame)\n    width, height = 1280, 720\n    image = Image.new('RGB', (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Load font\n    try:\n        header_font = ImageFont.truetype(\"Arial.ttf\", 14)\n        footer_font = ImageFont.truetype(\"Arial.ttf\", 18)\n        title_font = ImageFont.truetype(\"Arial.ttf\", 16)\n    except IOError:\n        try:\n            header_font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n            footer_font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n            title_font = ImageFont.truetype(\"DejaVuSans.ttf\", 16)\n        except IOError:\n            header_font = ImageFont.load_default()\n            footer_font = ImageFont.load_default()\n            title_font = ImageFont.load_default()\n    \n    # Define colors\n    cyan = (0, 255, 255)\n    amber = (255, 191, 0)\n    white = (255, 255, 255)\n    green = (80, 255, 80)\n    \n    # Sample assembly.json data\n    assembly_data = {\n        \"id\": \"FL012\",\n        \"poem\": \"Flashing Lights\",\n        \"content\": \"a concussion,\",\n        \"syntagmaType\": \"Perception-Image\",\n        \"operativeEkphrasis\": \"Stars explode behind eyelids--fireworks seen from inside a skull.\",\n        \"imageType\": \"Perception-Image\",\n        \"cineosisFunction\": \"Subjective Frame Recalibration\"\n    }\n    \n    # Create top header bar with assembly data\n    header_height = 35\n    draw.rectangle([(0, 0), (width, header_height)], fill=(0, 0, 0, 180))\n    draw.line([(0, header_height), (width, header_height)], fill=cyan, width=1)\n    \n    # Add scanlines to header\n    for y in range(0, header_height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n    \n    # Add text to header - important assembly data\n    padding = 10\n    draw.text((padding, 9), f\"{assembly_data['id']}\", fill=cyan, font=title_font)\n    draw.text((padding + 60, 9), f\"[{timestamp}]\", fill=white, font=header_font)\n    draw.text((padding + 140, 9), f\"{assembly_data['poem']}\", fill=amber, font=header_font)\n    draw.text((padding + 240, 9), f\"\\\"{assembly_data['content']}\\\"\", fill=white, font=header_font)\n    \n    # Add syntagma and other key metadata fields from assembly.json\n    draw.text((padding + 350, 9), f\"SYNTAGMA: {assembly_data['syntagmaType']}\", fill=green, font=header_font)\n    draw.text((padding + 520, 9), f\"FUNC: {assembly_data['cineosisFunction']}\", fill=green, font=header_font)\n    \n    # Add frame counter to right side\n    frame_num = random.randint(1000, 9000)\n    draw.text((width-100, 9), f\"F:{frame_num}\", fill=cyan, font=header_font)\n    \n    # Create the footer console overlay\n    footer_height = 80  # Height of bottom overlay\n    footer_y = height - footer_height\n    \n    # Draw semi-transparent black background for footer\n    draw.rectangle([(0, footer_y), (width, height)], fill=(0, 0, 0, 180))\n    draw.line([(0, footer_y), (width, footer_y)], fill=cyan, width=1)\n    \n    # Add scanline effect to footer\n    for y in range(footer_y, height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n    \n    # Add prompt text to footer\n    prompt_text = \"PROMPT: Stars explode behind eyelids--fireworks seen from inside a skull \u00b7 POV macro; eyelids closed, inner nebula bursts saffron-teal sparks, sub-bass rumble.\"\n    \n    # Wrap the prompt text across multiple lines if needed\n    wrapped_prompt = textwrap.wrap(prompt_text, width=110)\n    y_text = footer_y + 15\n    \n    # Add a small label for the prompt\n    draw.text((padding, y_text), \"PROMPT:\", fill=cyan, font=title_font)\n    \n    # Print the wrapped prompt text\n    for i, line in enumerate(wrapped_prompt):\n        if i == 0:\n            # For the first line, skip the \"PROMPT:\" label that we manually added\n            if line.startswith(\"PROMPT: \"):\n                line = line[8:]\n            draw.text((padding + 80, y_text), line, fill=white, font=footer_font)\n        else:\n            draw.text((padding, y_text + (i * 22)), line, fill=white, font=footer_font)\n    \n    # Add a timeline indicator at the bottom of the footer\n    timeline_y = height - 15\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 4 if i % 3 == 0 else 2\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f\"Codex overlay mockup created and saved to: {output_path}\")\n\ndef main():\n    output_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mockups\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create codex overlay mockup\n    output_path = os.path.join(output_dir, \"codex_overlay_refined.png\")\n    create_codex_overlay(output_path)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "codex_overlay_refined.png",
      ", (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Load font\n    try:\n        header_font = ImageFont.truetype(",
      "):\n                line = line[8:]\n            draw.text((padding + 80, y_text), line, fill=white, font=footer_font)\n        else:\n            draw.text((padding, y_text + (i * 22)), line, fill=white, font=footer_font)\n    \n    # Add a timeline indicator at the bottom of the footer\n    timeline_y = height - 15\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 4 if i % 3 == 0 else 2\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mockups"
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os",
      "textwrap",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/generate_mr_ordered_video.py",
    "size": 4257,
    "lines": 127,
    "source": "#!/usr/bin/env python3\n\"\"\"\nGenerate MR Ordered Video\nThis script generates the MR (Magic Ride) video with correct shot ordering and audio integration.\n\"\"\"\n\nimport os\nimport sys\nimport shutil\nimport subprocess\nimport datetime\nimport argparse\n\n# Paths\nJELLYFISH_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(JELLYFISH_DIR)\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nVIDEO_OUTPUT_DIR = os.path.join(JELLYFISH_DIR, \"video_output\")\nMR_ORDERED_CODEX = os.path.join(TIGER_DIR, \"MR_ordered_codex_entries.md\")\nAUDIO_DIR = os.path.join(BASE_DIR, \"MANTA\", \"audio\")\nMR_AUDIO_FILE = os.path.join(AUDIO_DIR, \"MR_audio.wav\")\n\ndef backup_existing_videos():\n    \"\"\"Backup existing MR videos with MISFIT suffix.\"\"\"\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Backup video without audio\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt.mp4\")\n    if os.path.exists(video_file):\n        backup_file = os.path.join(VIDEO_OUTPUT_DIR, f\"MR_header_prompt_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_file, backup_file)\n        print(f\"Backing up existing MR video to {backup_file}\")\n    \n    # Backup video with audio\n    video_audio_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt_with_audio.mp4\")\n    if os.path.exists(video_audio_file):\n        backup_audio_file = os.path.join(VIDEO_OUTPUT_DIR, f\"MR_header_prompt_with_audio_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_audio_file, backup_audio_file)\n        print(f\"Backing up existing MR video with audio to {backup_audio_file}\")\n\ndef generate_mr_video():\n    \"\"\"Generate MR video with ordered shot sequence.\"\"\"\n    print(\"Generating MR video with ordered shot sequence...\")\n    \n    # Check if ordered codex file exists\n    if not os.path.exists(MR_ORDERED_CODEX):\n        print(f\"Error: Ordered codex file {MR_ORDERED_CODEX} not found.\")\n        print(\"Run the mr_ordered_sequence_generator.py script first.\")\n        return False\n    \n    # Run the FL video generator with MR prefix and ordered codex\n    cmd = [\n        \"python3\",\n        os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\"),\n        \"--prefix\", \"MR\",\n        \"--ordered-codex\", MR_ORDERED_CODEX\n    ]\n    \n    print(f\"Using ordered codex file: {MR_ORDERED_CODEX}\")\n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        print(\"Error: Failed to generate MR video.\")\n        return False\n    \n    return True\n\ndef add_audio_to_video():\n    \"\"\"Add MR audio to the generated video.\"\"\"\n    print(\"Adding audio to MR video...\")\n    \n    # Check if video file exists\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt.mp4\")\n    if not os.path.exists(video_file):\n        print(f\"Error: Video file {video_file} not found.\")\n        return False\n    \n    # Check if audio file exists\n    if not os.path.exists(MR_AUDIO_FILE):\n        print(f\"Error: Audio file {MR_AUDIO_FILE} not found.\")\n        print(\"Using a placeholder silent audio track instead.\")\n        # Generate a silent audio track for demo purposes\n        silence_cmd = [\n            \"ffmpeg\", \"-y\", \"-f\", \"lavfi\", \"-i\", \"anullsrc=r=44100:cl=stereo\", \"-t\", \"131\",\n            \"-c:a\", \"pcm_s16le\", \"/tmp/silent_audio_mr.wav\"\n        ]\n        subprocess.run(silence_cmd)\n        MR_AUDIO_FILE = \"/tmp/silent_audio_mr.wav\"\n    \n    # Output file path\n    output_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt_with_audio.mp4\")\n    \n    # FFmpeg command to merge video and audio\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", video_file,\n        \"-i\", MR_AUDIO_FILE,\n        \"-map\", \"0:v\", \"-map\", \"1:a\",\n        \"-c:v\", \"copy\", \"-c:a\", \"aac\",\n        \"-shortest\",\n        output_file\n    ]\n    \n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        print(\"Error: Failed to add audio to MR video.\")\n        return False\n    \n    print(f\"Successfully created MR video with audio: {output_file}\")\n    return True\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"=== MR Ordered Video Generator ===\")\n    \n    # Backup existing videos\n    backup_existing_videos()\n    \n    # Generate MR video\n    if generate_mr_video():\n        # Add audio to video\n        add_audio_to_video()\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "MR_audio.wav",
      "MR_header_prompt.mp4",
      "MR_header_prompt_MISFIT_{timestamp}.mp4",
      "MR_header_prompt_with_audio.mp4",
      "MR_header_prompt_with_audio_MISFIT_{timestamp}.mp4",
      "MR_header_prompt.mp4",
      "/tmp/silent_audio_mr.wav",
      "/tmp/silent_audio_mr.wav",
      "MR_header_prompt_with_audio.mp4",
      "/tmp/silent_audio_mr.wav",
      "/tmp/silent_audio_mr.wav"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "silence_cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "sys",
      "shutil",
      "subprocess",
      "datetime",
      "argparse"
    ],
    "generates": [],
    "reads": [],
    "docstring": "Generate MR Ordered Video\nThis script generates the MR (Magic Ride) video with correct shot ordering and audio integration."
  },
  {
    "path": "JELLYFISH/fl_video_generator.py",
    "size": 10251,
    "lines": 250,
    "source": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nimport textwrap\nfrom PIL import Image, ImageDraw, ImageFont\nimport subprocess\nimport tempfile\nimport shutil\n\nclass CodexVideoGenerator:\n    def __init__(self, codex_path, output_dir, tiger_dir):\n        self.codex_path = codex_path\n        self.output_dir = output_dir\n        self.tiger_dir = tiger_dir\n        self.temp_dir = os.path.join(output_dir, \"temp_frames\")\n        \n        # Ensure output directories exist\n        os.makedirs(self.output_dir, exist_ok=True)\n        os.makedirs(self.temp_dir, exist_ok=True)\n        \n        # Font setup - will fall back to default if specified fonts aren't available\n        self.setup_fonts()\n        \n        # Define colors\n        self.cyan = (0, 255, 255)\n        self.amber = (255, 191, 0)\n        self.white = (255, 255, 255)\n        self.green = (80, 255, 80)\n        self.magenta = (255, 80, 255)\n        \n    def setup_fonts(self):\n        \"\"\"Setup fonts for the overlay\"\"\"\n        try:\n            self.header_font = ImageFont.truetype(\"Arial.ttf\", 14)\n            self.footer_font = ImageFont.truetype(\"Arial.ttf\", 18)\n            self.title_font = ImageFont.truetype(\"Arial.ttf\", 16)\n        except IOError:\n            try:\n                self.header_font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n                self.footer_font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n                self.title_font = ImageFont.truetype(\"DejaVuSans.ttf\", 16)\n            except IOError:\n                self.header_font = ImageFont.load_default()\n                self.footer_font = ImageFont.load_default()\n                self.title_font = ImageFont.load_default()\n    \n    def parse_codex(self, prefix=\"FL\"):\n        \"\"\"Parse codex data for entries with specified prefix\"\"\"\n        print(f\"Parsing codex data for {prefix} entries...\")\n        entries = []\n        \n        with open(self.codex_path, 'r') as f:\n            content = f.read()\n        \n        # Find all entries with the FL prefix\n        pattern = r'### ' + prefix + r'(\\d+) \\[([\\d:]+)\\]\\s*\\*\\*Image:\\*\\* `([^`]+)`\\s*\\*\\*Assembly Source:\\*\\*\\s*```json\\s*({[\\s\\S]*?})\\s*```\\s*\\*\\*Prompt:\\*\\* (.*?)(?=\\n---|\\Z)'\n        matches = re.findall(pattern, content, re.DOTALL)\n        \n        for match in matches:\n            entry_id = prefix + match[0]\n            timestamp = match[1]\n            image_path = match[2]\n            assembly_json = match[3]\n            prompt = match[4].strip()\n            \n            # Parse the assembly JSON\n            try:\n                assembly_data = json.loads(assembly_json)\n            except json.JSONDecodeError:\n                print(f\"Error parsing JSON for {entry_id}\")\n                continue\n            \n            entries.append({\n                'id': entry_id,\n                'timestamp': timestamp,\n                'image_path': image_path,\n                'assembly': assembly_data,\n                'prompt': prompt\n            })\n        \n        print(f\"Found {len(entries)} entries for {prefix}\")\n        return entries\n    \n    def apply_overlay(self, image_path, entry, output_path):\n        \"\"\"Apply the codex overlay to an image\"\"\"\n        # Load the image\n        try:\n            abs_image_path = os.path.join(self.tiger_dir, image_path)\n            if not os.path.exists(abs_image_path):\n                print(f\"Image not found: {abs_image_path}\")\n                return None\n            \n            image = Image.open(abs_image_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error opening image {image_path}: {e}\")\n            return None\n        \n        # If image size is not standard, resize to 1280x720\n        if image.size != (1280, 720):\n            image = image.resize((1280, 720), Image.LANCZOS)\n        \n        # Get dimensions\n        width, height = image.size\n        draw = ImageDraw.Draw(image)\n        \n        # Create top header bar with assembly data - two-line high\n        header_height = 60\n        draw.rectangle([(0, 0), (width, header_height)], fill=(0, 0, 0, 180))\n        draw.line([(0, header_height), (width, header_height)], fill=self.cyan, width=1)\n        \n        # Add scanlines to header\n        for y in range(0, header_height, 2):\n            draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n        \n        # Define left padding for alignment\n        padding = 20\n        \n        # Extract data from entry\n        shot_id = entry['id']\n        timestamp = entry['timestamp']\n        assembly = entry['assembly']\n        \n        # Add text to header - first line\n        draw.text((padding, 9), f\"{shot_id}\", fill=self.cyan, font=self.title_font)\n        draw.text((padding + 70, 9), f\"[{timestamp}]\", fill=self.white, font=self.header_font)\n        draw.text((padding + 170, 9), f\"{assembly.get('poem', '')}\", fill=self.amber, font=self.title_font)\n        draw.text((padding + 320, 9), f\"\\\"{assembly.get('content', '')}\\\"\", fill=self.white, font=self.header_font)\n        \n        # Add frame counter to far right of first line\n        frame_num = int(re.search(r'(\\d+).png$', image_path).group(1)) if re.search(r'(\\d+).png$', image_path) else 0\n        draw.text((width-90, 9), f\"F:{frame_num}\", fill=self.cyan, font=self.header_font)\n        \n        # Second line - TYPE and FUNC left aligned, SYNT right aligned\n        draw.text((padding, 33), f\"TYPE: {assembly.get('imageType', '')}\", fill=self.green, font=self.header_font)\n        draw.text((padding + 240, 33), f\"FUNC: {assembly.get('cineosisFunction', '')}\", fill=self.green, font=self.header_font)\n        \n        # SYNT on the right side with different color\n        synt_text = f\"SYNT: {assembly.get('syntagmaType', '')}\"\n        synt_width = draw.textlength(synt_text, font=self.header_font)\n        draw.text((width - padding - synt_width - 90, 33), synt_text, fill=self.magenta, font=self.header_font)\n        \n        # Create the footer console overlay\n        footer_height = 80\n        footer_y = height - footer_height\n        \n        # Draw semi-transparent black background for footer\n        draw.rectangle([(0, footer_y), (width, height)], fill=(0, 0, 0, 180))\n        draw.line([(0, footer_y), (width, footer_y)], fill=self.cyan, width=1)\n        \n        # Add scanline effect to footer\n        for y in range(footer_y, height, 2):\n            draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n        \n        # Add prompt text to footer\n        prompt_text = f\"PROMPT: {entry['prompt']}\"\n        \n        # Wrap the prompt text across multiple lines if needed\n        wrapped_prompt = textwrap.wrap(prompt_text, width=110)\n        y_text = footer_y + 15\n        \n        # Add a small label for the prompt\n        draw.text((padding, y_text), \"PROMPT:\", fill=self.cyan, font=self.title_font)\n        \n        # Print the wrapped prompt text\n        for i, line in enumerate(wrapped_prompt):\n            if i == 0:\n                # For the first line, skip the \"PROMPT:\" label that we manually added\n                if line.startswith(\"PROMPT: \"):\n                    line = line[8:]\n                draw.text((padding + 80, y_text), line, fill=self.white, font=self.footer_font)\n            else:\n                draw.text((padding, y_text + (i * 22)), line, fill=self.white, font=self.footer_font)\n        \n        # Add a timeline indicator at the bottom of the footer\n        timeline_y = height - 15\n        draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n        \n        # Save the image with overlay\n        image.save(output_path)\n        return output_path\n\n    def generate_video(self, prefix=\"FL\", duration_per_frame=3, max_frames=None):\n        \"\"\"Generate a video from parsed codex entries\"\"\"\n        entries = self.parse_codex(prefix)\n        \n        if max_frames:\n            entries = entries[:max_frames]\n        \n        if not entries:\n            print(f\"No entries found for {prefix}\")\n            return None\n        \n        # Clear temp directory\n        for f in os.listdir(self.temp_dir):\n            os.remove(os.path.join(self.temp_dir, f))\n        \n        # Process each frame\n        frame_paths = []\n        for i, entry in enumerate(entries):\n            print(f\"Processing frame {i+1}/{len(entries)}: {entry['id']}\")\n            output_path = os.path.join(self.temp_dir, f\"{i:04d}.png\")\n            result = self.apply_overlay(entry['image_path'], entry, output_path)\n            if result:\n                frame_paths.append(result)\n        \n        if not frame_paths:\n            print(\"No frames were generated successfully\")\n            return None\n        \n        # Create a video from the frames using ffmpeg\n        output_video = os.path.join(self.output_dir, f\"{prefix}_codex_overlay.mp4\")\n        try:\n            cmd = [\n                'ffmpeg',\n                '-y',  # Overwrite output file if it exists\n                '-framerate', f'1/{duration_per_frame}',  # Each frame lasts for duration_per_frame seconds\n                '-i', os.path.join(self.temp_dir, '%04d.png'),\n                '-c:v', 'libx264',\n                '-pix_fmt', 'yuv420p',\n                '-crf', '23',  # Higher quality\n                output_video\n            ]\n            print(f\"Running command: {' '.join(cmd)}\")\n            subprocess.run(cmd, check=True)\n            print(f\"Video generated: {output_video}\")\n            return output_video\n        except subprocess.CalledProcessError as e:\n            print(f\"Error generating video: {e}\")\n            return None\n        except Exception as e:\n            print(f\"Unexpected error: {e}\")\n            return None\n\ndef main():\n    # Set up paths\n    codex_path = \"/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_Codex.md\"\n    tiger_dir = \"/Users/gaia/resurrecting atlantis/TIGER\"\n    output_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/video_output\"\n    \n    # Create the generator\n    generator = CodexVideoGenerator(codex_path, output_dir, tiger_dir)\n    \n    # Generate video for FL with each frame lasting 3 seconds, up to 10 frames for testing\n    generator.generate_video(prefix=\"FL\", duration_per_frame=3, max_frames=10)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "{i:04d}.png",
      "{prefix}_codex_overlay.mp4",
      "%04d.png",
      "Processing frame {i+1}/{len(entries)}: {entry[",
      "1/{duration_per_frame}",
      "/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_Codex.md",
      "/Users/gaia/resurrecting atlantis/TIGER",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/video_output"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd, check=True"
      }
    ],
    "imports": [
      "os",
      "re",
      "json",
      "textwrap",
      "PIL",
      "subprocess",
      "tempfile",
      "shutil"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/add_music_to_impala.py",
    "size": 8208,
    "lines": 208,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\n# Source directories and files\nVIDEO_FILE = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ResurrectingAtlantis_OnePerTrack.mp4')\nAUDIO_DIR = Path('/Users/gaia/resurrecting atlantis/MANTA/audio')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/music_versions')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\ndef get_video_duration(video_path):\n    \"\"\"Get the duration of a video file using FFprobe.\"\"\"\n    try:\n        result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        duration = float(data['format']['duration'])\n        return duration\n    except (subprocess.CalledProcessError, KeyError, json.JSONDecodeError) as e:\n        print(f\"Error getting duration for {video_path}: {e}\")\n        return 0\n\ndef find_audio_files():\n    \"\"\"Find all audio files in the MANTA/audio directory.\"\"\"\n    audio_extensions = ['.mp3', '.wav', '.m4a', '.aac', '.ogg']\n    audio_files = []\n    \n    for ext in audio_extensions:\n        audio_files.extend(list(AUDIO_DIR.glob(f'*{ext}')))\n    \n    return audio_files\n\ndef create_video_with_audio(video_path, audio_path, output_path, video_duration):\n    \"\"\"Create a new video with the given audio track, maintaining the original video duration.\"\"\"\n    try:\n        # Create a temporary directory for intermediate files\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir_path = Path(temp_dir)\n            \n            # Process audio file (trim or loop to match video duration)\n            audio_processed_path = temp_dir_path / f\"processed_{audio_path.name}\"\n            \n            # Get audio duration\n            audio_info_result = subprocess.run([\n                'ffprobe', \n                '-v', 'error', \n                '-show_entries', 'format=duration', \n                '-of', 'json', \n                str(audio_path)\n            ], capture_output=True, text=True, check=True)\n            \n            audio_info = json.loads(audio_info_result.stdout)\n            audio_duration = float(audio_info['format']['duration'])\n            \n            print(f\"  Audio duration: {audio_duration:.2f}s, Video duration: {video_duration:.2f}s\")\n            \n            # Process audio based on relative durations\n            if abs(audio_duration - video_duration) < 1.0:\n                # If durations are close, just trim exactly to video length\n                subprocess.run([\n                    'ffmpeg',\n                    '-i', str(audio_path),\n                    '-t', str(video_duration),\n                    '-y',\n                    str(audio_processed_path)\n                ], check=True, capture_output=True)\n                \n            elif audio_duration > video_duration:\n                # If audio is longer, trim it\n                print(\"  Trimming audio to match video length\")\n                subprocess.run([\n                    'ffmpeg',\n                    '-i', str(audio_path),\n                    '-t', str(video_duration),\n                    '-y',\n                    str(audio_processed_path)\n                ], check=True, capture_output=True)\n                \n            else:\n                # If audio is shorter, loop it to fill the video duration\n                print(\"  Looping audio to match video length\")\n                \n                # Calculate how many loops we need\n                loops_needed = int(video_duration / audio_duration) + 1\n                \n                # Create a file with the input repeated\n                concat_file = temp_dir_path / \"concat.txt\"\n                with open(concat_file, 'w') as f:\n                    for _ in range(loops_needed):\n                        f.write(f\"file '{audio_path}'\\n\")\n                \n                # Concatenate the audio files\n                looped_audio = temp_dir_path / \"looped_audio.mp3\"\n                subprocess.run([\n                    'ffmpeg',\n                    '-f', 'concat',\n                    '-safe', '0',\n                    '-i', str(concat_file),\n                    '-c', 'copy',\n                    '-y',\n                    str(looped_audio)\n                ], check=True, capture_output=True)\n                \n                # Trim the looped audio to the exact video duration\n                subprocess.run([\n                    'ffmpeg',\n                    '-i', str(looped_audio),\n                    '-t', str(video_duration),\n                    '-y',\n                    str(audio_processed_path)\n                ], check=True, capture_output=True)\n            \n            # Combine video with processed audio\n            subprocess.run([\n                'ffmpeg',\n                '-i', str(video_path),\n                '-i', str(audio_processed_path),\n                '-c:v', 'copy',\n                '-c:a', 'aac',\n                '-map', '0:v:0',\n                '-map', '1:a:0',\n                '-shortest',\n                '-y',\n                str(output_path)\n            ], check=True, capture_output=True)\n            \n            # Verify the output file\n            output_duration = get_video_duration(output_path)\n            print(f\"  Output duration: {output_duration:.2f}s\")\n            \n            if abs(output_duration - video_duration) > 1.0:\n                print(f\"  Warning: Output duration ({output_duration:.2f}s) differs from target ({video_duration:.2f}s)\")\n            \n            return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error creating video with audio: {e}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode() if hasattr(e, 'stderr') else 'No stderr available'}\")\n        return False\n\ndef main():\n    print(\"=== ADDING MUSIC TO IMPALA VIDEOS ===\\n\")\n    \n    # Get video duration\n    video_duration = get_video_duration(VIDEO_FILE)\n    print(f\"Source video: {VIDEO_FILE}\")\n    print(f\"Video duration: {video_duration:.2f} seconds\")\n    \n    if video_duration == 0:\n        print(\"Error: Could not determine video duration.\")\n        return\n    \n    # Find all audio files\n    audio_files = find_audio_files()\n    print(f\"Found {len(audio_files)} audio files in {AUDIO_DIR}\\n\")\n    \n    if not audio_files:\n        print(\"No audio files found.\")\n        return\n    \n    # Create a new video for each audio file\n    successful_videos = []\n    \n    for i, audio_file in enumerate(audio_files, 1):\n        # Create a clean filename for the output\n        audio_name = audio_file.stem.replace(' ', '_')\n        output_file = OUTPUT_DIR / f\"ResurrectingAtlantis_Music_{audio_name}.mp4\"\n        \n        print(f\"\\n{i}/{len(audio_files)} Processing: {audio_file.name}\")\n        print(f\"Creating: {output_file}\")\n        \n        if create_video_with_audio(VIDEO_FILE, audio_file, output_file, video_duration):\n            print(f\"\u2713 Successfully created {output_file}\")\n            successful_videos.append(output_file)\n        else:\n            print(f\"\u2717 Failed to create {output_file}\")\n    \n    # Create a summary file\n    summary_file = OUTPUT_DIR / \"music_versions_summary.txt\"\n    with open(summary_file, 'w') as f:\n        f.write(\"RESURRECTING ATLANTIS - MUSIC VERSIONS\\n\")\n        f.write(\"====================================\\n\\n\")\n        f.write(f\"Original video: {VIDEO_FILE}\\n\")\n        f.write(f\"Video duration: {video_duration:.2f} seconds\\n\")\n        f.write(f\"Total music versions created: {len(successful_videos)}/{len(audio_files)}\\n\\n\")\n        \n        f.write(\"MUSIC VERSIONS:\\n\")\n        for i, video in enumerate(successful_videos, 1):\n            output_duration = get_video_duration(video)\n            f.write(f\"{i}. {video.name}\\n\")\n            f.write(f\"   Duration: {output_duration:.2f} seconds\\n\\n\")\n    \n    print(f\"\\nCreated {len(successful_videos)}/{len(audio_files)} music versions.\")\n    print(f\"Output directory: {OUTPUT_DIR}\")\n    print(f\"Summary: {summary_file}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ResurrectingAtlantis_OnePerTrack.mp4",
      "looped_audio.mp3",
      "ResurrectingAtlantis_Music_{audio_name}.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ResurrectingAtlantis_OnePerTrack.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/music_versions",
      "Find all audio files in the MANTA/audio directory.",
      "\n    try:\n        # Create a temporary directory for intermediate files\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir_path = Path(temp_dir)\n            \n            # Process audio file (trim or loop to match video duration)\n            audio_processed_path = temp_dir_path / f",
      ")\n                \n                # Calculate how many loops we need\n                loops_needed = int(video_duration / audio_duration) + 1\n                \n                # Create a file with the input repeated\n                concat_file = temp_dir_path / ",
      ")\n                \n                # Concatenate the audio files\n                looped_audio = temp_dir_path / ",
      ")\n        output_file = OUTPUT_DIR / f",
      "\\n{i}/{len(audio_files)} Processing: {audio_file.name}",
      ")\n    \n    # Create a summary file\n    summary_file = OUTPUT_DIR / ",
      "Total music versions created: {len(successful_videos)}/{len(audio_files)}\\n\\n",
      "\\nCreated {len(successful_videos)}/{len(audio_files)} music versions."
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n                'ffprobe', \n                '-v', 'error', \n                '-show_entries', 'format=duration', \n                '-of', 'json', \n                str(audio_path"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-i', str(audio_path"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-i', str(audio_path"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-f', 'concat',\n                    '-safe', '0',\n                    '-i', str(concat_file"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-i', str(looped_audio"
      },
      {
        "type": "run",
        "snippet": "[\n                'ffmpeg',\n                '-i', str(video_path"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "pathlib",
      "tempfile",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/codex_overlay_final_design.py",
    "size": 6584,
    "lines": 159,
    "source": "#!/usr/bin/env python3\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nimport textwrap\nimport random\n\ndef create_codex_overlay(output_path, shot_id=\"FL012\", timestamp=\"02:38:34\"):\n    \"\"\"Create a mockup with two-line header and prompt in footer, with SYNT on the right side\"\"\"\n    # Create a black canvas (simulating video frame)\n    width, height = 1280, 720\n    image = Image.new('RGB', (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Load font\n    try:\n        header_font = ImageFont.truetype(\"Arial.ttf\", 14)\n        footer_font = ImageFont.truetype(\"Arial.ttf\", 18)\n        title_font = ImageFont.truetype(\"Arial.ttf\", 16)\n    except IOError:\n        try:\n            header_font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n            footer_font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n            title_font = ImageFont.truetype(\"DejaVuSans.ttf\", 16)\n        except IOError:\n            header_font = ImageFont.load_default()\n            footer_font = ImageFont.load_default()\n            title_font = ImageFont.load_default()\n    \n    # Define colors\n    cyan = (0, 255, 255)\n    amber = (255, 191, 0)\n    white = (255, 255, 255)\n    green = (80, 255, 80)\n    magenta = (255, 80, 255)  # New color for SYNT\n    \n    # Sample assembly.json data\n    assembly_data = {\n        \"id\": \"FL012\",\n        \"poem\": \"Flashing Lights\",\n        \"content\": \"a concussion,\",\n        \"syntagmaType\": \"Perception-Image\",\n        \"operativeEkphrasis\": \"Stars explode behind eyelids--fireworks seen from inside a skull.\",\n        \"imageType\": \"Perception-Image\",\n        \"cineosisFunction\": \"Subjective Frame Recalibration\"\n    }\n    \n    # Create top header bar with assembly data - now two-line high\n    header_height = 60  # Height increased for two lines\n    draw.rectangle([(0, 0), (width, header_height)], fill=(0, 0, 0, 180))\n    draw.line([(0, header_height), (width, header_height)], fill=cyan, width=1)\n    \n    # Add scanlines to header\n    for y in range(0, header_height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n    \n    # Define left padding for alignment\n    padding = 20\n    \n    # Add text to header - first line\n    draw.text((padding, 9), f\"{assembly_data['id']}\", fill=cyan, font=title_font)\n    draw.text((padding + 70, 9), f\"[{timestamp}]\", fill=white, font=header_font)\n    draw.text((padding + 170, 9), f\"{assembly_data['poem']}\", fill=amber, font=title_font)\n    draw.text((padding + 320, 9), f\"\\\"{assembly_data['content']}\\\"\", fill=white, font=header_font)\n    \n    # Add frame counter to far right of first line\n    frame_num = random.randint(1000, 9000)\n    draw.text((width-90, 9), f\"F:{frame_num}\", fill=cyan, font=header_font)\n    \n    # Second line - TYPE and FUNC left aligned, SYNT right aligned\n    draw.text((padding, 33), f\"TYPE: {assembly_data['imageType']}\", fill=green, font=header_font)\n    draw.text((padding + 240, 33), f\"FUNC: {assembly_data['cineosisFunction']}\", fill=green, font=header_font)\n    \n    # SYNT on the right side with different color\n    synt_text = f\"SYNT: {assembly_data['syntagmaType']}\"\n    synt_width = draw.textlength(synt_text, font=header_font)\n    draw.text((width - padding - synt_width - 90, 33), synt_text, fill=magenta, font=header_font)\n    \n    # Create the footer console overlay\n    footer_height = 80  # Height of bottom overlay\n    footer_y = height - footer_height\n    \n    # Draw semi-transparent black background for footer\n    draw.rectangle([(0, footer_y), (width, height)], fill=(0, 0, 0, 180))\n    draw.line([(0, footer_y), (width, footer_y)], fill=cyan, width=1)\n    \n    # Add scanline effect to footer\n    for y in range(footer_y, height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n    \n    # Add prompt text to footer\n    prompt_text = \"PROMPT: Stars explode behind eyelids--fireworks seen from inside a skull \u00b7 POV macro; eyelids closed, inner nebula bursts saffron-teal sparks, sub-bass rumble.\"\n    \n    # Wrap the prompt text across multiple lines if needed\n    wrapped_prompt = textwrap.wrap(prompt_text, width=110)\n    y_text = footer_y + 15\n    \n    # Add a small label for the prompt\n    draw.text((padding, y_text), \"PROMPT:\", fill=cyan, font=title_font)\n    \n    # Print the wrapped prompt text\n    for i, line in enumerate(wrapped_prompt):\n        if i == 0:\n            # For the first line, skip the \"PROMPT:\" label that we manually added\n            if line.startswith(\"PROMPT: \"):\n                line = line[8:]\n            draw.text((padding + 80, y_text), line, fill=white, font=footer_font)\n        else:\n            draw.text((padding, y_text + (i * 22)), line, fill=white, font=footer_font)\n    \n    # Add a timeline indicator at the bottom of the footer\n    timeline_y = height - 15\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 4 if i % 3 == 0 else 2\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f\"Codex overlay mockup created and saved to: {output_path}\")\n\ndef main():\n    output_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mockups\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create codex overlay mockup\n    output_path = os.path.join(output_dir, \"codex_overlay_final_design.png\")\n    create_codex_overlay(output_path)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "codex_overlay_final_design.png",
      ", (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Load font\n    try:\n        header_font = ImageFont.truetype(",
      "):\n                line = line[8:]\n            draw.text((padding + 80, y_text), line, fill=white, font=footer_font)\n        else:\n            draw.text((padding, y_text + (i * 22)), line, fill=white, font=footer_font)\n    \n    # Add a timeline indicator at the bottom of the footer\n    timeline_y = height - 15\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 4 if i % 3 == 0 else 2\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mockups"
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os",
      "textwrap",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/generate_mr_ordered_video_fixed.py",
    "size": 6894,
    "lines": 192,
    "source": "#!/usr/bin/env python3\n\"\"\"\nGenerate MR Ordered Video with Placeholder Images for Missing Shots\nThis script solves the problem of missing frames terminating FFmpeg processing.\n\"\"\"\n\nimport os\nimport sys\nimport shutil\nimport subprocess\nimport datetime\nimport argparse\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Paths\nJELLYFISH_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(JELLYFISH_DIR)\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nVIDEO_OUTPUT_DIR = os.path.join(JELLYFISH_DIR, \"video_output\")\nMR_ORDERED_CODEX = os.path.join(TIGER_DIR, \"MR_ordered_codex_entries.md\")\nTEMP_FRAMES_DIR = os.path.join(VIDEO_OUTPUT_DIR, \"temp_frames\")\nAUDIO_DIR = os.path.join(BASE_DIR, \"MANTA\", \"audio\")\nMR_AUDIO_FILE = os.path.join(AUDIO_DIR, \"MR_audio.wav\")\n\ndef backup_existing_videos():\n    \"\"\"Backup existing MR videos with MISFIT suffix.\"\"\"\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Backup video without audio\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt.mp4\")\n    if os.path.exists(video_file):\n        backup_file = os.path.join(VIDEO_OUTPUT_DIR, f\"MR_header_prompt_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_file, backup_file)\n        print(f\"Backing up existing MR video to {backup_file}\")\n    \n    # Backup video with audio\n    video_audio_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt_with_audio.mp4\")\n    if os.path.exists(video_audio_file):\n        backup_audio_file = os.path.join(VIDEO_OUTPUT_DIR, f\"MR_header_prompt_with_audio_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_audio_file, backup_audio_file)\n        print(f\"Backing up existing MR video with audio to {backup_audio_file}\")\n\ndef create_placeholder_image(shot_id):\n    \"\"\"Create a placeholder image for missing shots.\"\"\"\n    print(f\"Creating placeholder image for {shot_id}...\")\n    \n    # Create directory structure if it doesn't exist\n    os.makedirs(os.path.join(TIGER_DIR, \"MR\"), exist_ok=True)\n    \n    # Create a blank image with text\n    img = Image.new('RGB', (1280, 1016), color=(40, 40, 40))\n    d = ImageDraw.Draw(img)\n    \n    try:\n        # Try to load a font, fallback to default if not available\n        font = ImageFont.truetype(\"Arial.ttf\", 60)\n    except IOError:\n        font = ImageFont.load_default()\n    \n    # Add text to the image\n    text = f\"PLACEHOLDER IMAGE\\n{shot_id}\"\n    text_width, text_height = d.textbbox((0, 0), text, font=font)[2:4]\n    position = ((1280 - text_width) // 2, (1016 - text_height) // 2)\n    \n    # Draw the text\n    d.text(position, text, fill=(200, 200, 200), font=font)\n    \n    # Save the image\n    image_path = os.path.join(TIGER_DIR, \"MR\", f\"{shot_id}__missing_image.png\")\n    img.save(image_path)\n    \n    return image_path\n\ndef extract_missing_shots_from_log(log_file):\n    \"\"\"Extract missing shots from the log file.\"\"\"\n    missing_shots = []\n    if os.path.exists(log_file):\n        with open(log_file, 'r') as f:\n            for line in f:\n                if \"Image not found\" in line and \"MR\" in line:\n                    # Extract the shot ID (e.g., MR004) from the line\n                    parts = line.split('/')\n                    for part in parts:\n                        if part.startswith(\"MR\") and \"__missing_image.png\" in part:\n                            shot_id = part.split('__')[0]\n                            missing_shots.append(shot_id)\n    return missing_shots\n\ndef create_placeholders_for_missing_shots():\n    \"\"\"Create placeholder images for all missing shots.\"\"\"\n    # Get list of known missing shots\n    missing_shots = [\"MR004\", \"MR068\", \"MR071\"]  # From the MR ordered sequence generator output\n    \n    # Create placeholders for all missing shots\n    for shot_id in missing_shots:\n        create_placeholder_image(shot_id)\n    \n    print(f\"Created {len(missing_shots)} placeholder images for missing shots\")\n\ndef generate_mr_video():\n    \"\"\"Generate MR video with ordered shot sequence.\"\"\"\n    print(\"Generating MR video with ordered shot sequence...\")\n    \n    # First, create placeholders for missing shots\n    create_placeholders_for_missing_shots()\n    \n    # Check if ordered codex file exists\n    if not os.path.exists(MR_ORDERED_CODEX):\n        print(f\"Error: Ordered codex file {MR_ORDERED_CODEX} not found.\")\n        print(\"Run the mr_ordered_sequence_generator.py script first.\")\n        return False\n    \n    # Run the FL video generator with MR prefix and ordered codex\n    cmd = [\n        \"python3\",\n        os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\"),\n        \"--prefix\", \"MR\",\n        \"--ordered-codex\", MR_ORDERED_CODEX\n    ]\n    \n    print(f\"Using ordered codex file: {MR_ORDERED_CODEX}\")\n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        print(\"Error: Failed to generate MR video.\")\n        return False\n    \n    return True\n\ndef add_audio_to_video():\n    \"\"\"Add MR audio to the generated video.\"\"\"\n    print(\"Adding audio to MR video...\")\n    \n    # Check if video file exists\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt.mp4\")\n    if not os.path.exists(video_file):\n        print(f\"Error: Video file {video_file} not found.\")\n        return False\n    \n    # Check if audio file exists\n    audio_file = MR_AUDIO_FILE  # Make a local copy to avoid reference issues\n    if not os.path.exists(audio_file):\n        print(f\"Error: Audio file {audio_file} not found.\")\n        print(\"Using a placeholder silent audio track instead.\")\n        # Generate a silent audio track for demo purposes\n        silence_cmd = [\n            \"ffmpeg\", \"-y\", \"-f\", \"lavfi\", \"-i\", \"anullsrc=r=44100:cl=stereo\", \"-t\", \"131\",\n            \"-c:a\", \"pcm_s16le\", \"/tmp/silent_audio_mr.wav\"\n        ]\n        subprocess.run(silence_cmd)\n        audio_file = \"/tmp/silent_audio_mr.wav\"\n    \n    # Output file path\n    output_file = os.path.join(VIDEO_OUTPUT_DIR, \"MR_header_prompt_with_audio.mp4\")\n    \n    # FFmpeg command to merge video and audio\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-map\", \"0:v\", \"-map\", \"1:a\",\n        \"-c:v\", \"copy\", \"-c:a\", \"aac\",\n        \"-shortest\",\n        output_file\n    ]\n    \n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        print(\"Error: Failed to add audio to MR video.\")\n        return False\n    \n    print(f\"Successfully created MR video with audio: {output_file}\")\n    return True\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"=== MR Ordered Video Generator (FIXED VERSION) ===\")\n    \n    # Backup existing videos\n    backup_existing_videos()\n    \n    # Generate MR video with placeholders for missing shots\n    if generate_mr_video():\n        # Add audio to video\n        add_audio_to_video()\n    \n    print(\"DONE! This fixed version uses placeholder images for missing shots to prevent FFmpeg termination.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "MR_audio.wav",
      "MR_header_prompt.mp4",
      "MR_header_prompt_MISFIT_{timestamp}.mp4",
      "MR_header_prompt_with_audio.mp4",
      "MR_header_prompt_with_audio_MISFIT_{timestamp}.mp4",
      "{shot_id}__missing_image.png",
      "__missing_image.png",
      "MR_header_prompt.mp4",
      "/tmp/silent_audio_mr.wav",
      "/tmp/silent_audio_mr.wav",
      "MR_header_prompt_with_audio.mp4",
      "\n    text_width, text_height = d.textbbox((0, 0), text, font=font)[2:4]\n    position = ((1280 - text_width) // 2, (1016 - text_height) // 2)\n    \n    # Draw the text\n    d.text(position, text, fill=(200, 200, 200), font=font)\n    \n    # Save the image\n    image_path = os.path.join(TIGER_DIR, ",
      "/tmp/silent_audio_mr.wav",
      "/tmp/silent_audio_mr.wav"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "silence_cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "sys",
      "shutil",
      "subprocess",
      "datetime",
      "argparse",
      "PIL"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files"
    ],
    "docstring": "Generate MR Ordered Video with Placeholder Images for Missing Shots\nThis script solves the problem of missing frames terminating FFmpeg processing."
  },
  {
    "path": "JELLYFISH/ru_complete_codex_generator.py",
    "size": 3034,
    "lines": 96,
    "source": "#!/usr/bin/env python3\n\nimport json\nimport os\nimport re\n\n# Source files for RU section\nru_prompts_file = \"/Users/gaia/resurrecting atlantis/TIGER/RU/RU_prompts.md\"\nru_assembly_file = \"/Users/gaia/resurrecting atlantis/TIGER/RU/RU_assembly.json\"\nsimplified_file = \"/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\"\nru_image_dir = \"/Users/gaia/resurrecting atlantis/TIGER/RU\"\n\n# Output file \noutput_file = \"/Users/gaia/resurrecting atlantis/TIGER/RU_complete_codex_entries.md\"\n\n# Load RU assembly data\nwith open(ru_assembly_file, 'r') as f:\n    assembly_data = json.load(f)\n\n# Create assembly data lookup by ID\nassembly_lookup = {}\nfor entry in assembly_data:\n    assembly_lookup[entry[\"id\"]] = entry\n\n# Load RU prompts data\nwith open(ru_prompts_file, 'r') as f:\n    prompts_lines = f.readlines()\n\n# Create a dictionary of prompts by ID\nprompts = {}\nfor line in prompts_lines:\n    parts = line.strip().split(' ', 1)\n    if len(parts) >= 2:\n        shot_id = parts[0].strip()\n        prompt_text = line.strip()\n        prompts[shot_id] = prompt_text\n\n# Load simplified file to get all image variations with timestamps\nwith open(simplified_file, 'r') as f:\n    simplified_lines = f.readlines()\n\n# Extract RU entries from simplified file\nru_entries = []\npattern = r'^(RU\\d+)\\s+\\[([\\d:]+)\\]\\s+`(RU/.+\\.png)`'\nfor line in simplified_lines:\n    match = re.match(pattern, line.strip())\n    if match:\n        shot_id = match.group(1)\n        timestamp = match.group(2)\n        image_path = match.group(3)\n        \n        # Only include if we have assembly data for this ID\n        if shot_id in assembly_lookup:\n            ru_entries.append({\n                \"id\": shot_id,\n                \"timestamp\": timestamp,\n                \"image_path\": image_path\n            })\n\n# Sort entries by timestamp to maintain sequence\nru_entries.sort(key=lambda x: x[\"timestamp\"])\n\n# Generate codex entries\nwith open(output_file, 'w') as f:\n    f.write(\"# RU (Reunion) Codex Entries - Complete Sequence\\n\\n\")\n    \n    for entry in ru_entries:\n        shot_id = entry[\"id\"]\n        timestamp = entry[\"timestamp\"]\n        image_path = entry[\"image_path\"]\n        \n        # Skip if we don't have assembly data or prompt\n        if shot_id not in assembly_lookup or shot_id not in prompts:\n            continue\n        \n        # Get assembly data for this shot\n        assembly = assembly_lookup[shot_id]\n        \n        # Write the entry header\n        f.write(f\"### {shot_id} [{timestamp}]\\n\\n\")\n        \n        # Write the image path\n        f.write(f\"**Image:** `{image_path}`\\n\\n\")\n        \n        # Write the assembly source\n        f.write(\"**Assembly Source:**\\n```json\\n\")\n        f.write(json.dumps(assembly, indent=2))\n        f.write(\"\\n```\\n\\n\")\n        \n        # Write the prompt\n        f.write(f\"**Prompt:** {prompts[shot_id].split(' \u00b7 ', 1)[1]}\\n\\n\")\n        f.write(\"---\\n\\n\")\n        \nprint(f\"Generated complete codex entries written to {output_file}\")\nprint(f\"Total entries: {len(ru_entries)}\")\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/TIGER/RU/RU_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER/RU/RU_prompts.md",
      "/Users/gaia/resurrecting atlantis/TIGER/RU/RU_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_Simplified.md",
      "/Users/gaia/resurrecting atlantis/TIGER/RU",
      "/Users/gaia/resurrecting atlantis/TIGER/RU_complete_codex_entries.md",
      "^(RU\\d+)\\s+\\[([\\d:]+)\\]\\s+`(RU/.+\\.png)`"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/add_sunflower_to_impala.py",
    "size": 8248,
    "lines": 208,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\n# Source directories and files\nVIDEO_FILE = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ResurrectingAtlantis_OnePerTrack.mp4')\nAUDIO_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/IBEX/SUNFLOWER')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/sunflower_versions')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\ndef get_video_duration(video_path):\n    \"\"\"Get the duration of a video file using FFprobe.\"\"\"\n    try:\n        result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        duration = float(data['format']['duration'])\n        return duration\n    except (subprocess.CalledProcessError, KeyError, json.JSONDecodeError) as e:\n        print(f\"Error getting duration for {video_path}: {e}\")\n        return 0\n\ndef find_audio_files():\n    \"\"\"Find all audio files in the SUNFLOWER directory.\"\"\"\n    audio_extensions = ['.mp3', '.wav', '.m4a', '.aac', '.ogg']\n    audio_files = []\n    \n    for ext in audio_extensions:\n        audio_files.extend(list(AUDIO_DIR.glob(f'*{ext}')))\n    \n    return audio_files\n\ndef create_video_with_audio(video_path, audio_path, output_path, video_duration):\n    \"\"\"Create a new video with the given audio track, maintaining the original video duration.\"\"\"\n    try:\n        # Create a temporary directory for intermediate files\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir_path = Path(temp_dir)\n            \n            # Process audio file (trim or loop to match video duration)\n            audio_processed_path = temp_dir_path / f\"processed_{audio_path.name}\"\n            \n            # Get audio duration\n            audio_info_result = subprocess.run([\n                'ffprobe', \n                '-v', 'error', \n                '-show_entries', 'format=duration', \n                '-of', 'json', \n                str(audio_path)\n            ], capture_output=True, text=True, check=True)\n            \n            audio_info = json.loads(audio_info_result.stdout)\n            audio_duration = float(audio_info['format']['duration'])\n            \n            print(f\"  Audio duration: {audio_duration:.2f}s, Video duration: {video_duration:.2f}s\")\n            \n            # Process audio based on relative durations\n            if abs(audio_duration - video_duration) < 1.0:\n                # If durations are close, just trim exactly to video length\n                subprocess.run([\n                    'ffmpeg',\n                    '-i', str(audio_path),\n                    '-t', str(video_duration),\n                    '-y',\n                    str(audio_processed_path)\n                ], check=True, capture_output=True)\n                \n            elif audio_duration > video_duration:\n                # If audio is longer, trim it\n                print(\"  Trimming audio to match video length\")\n                subprocess.run([\n                    'ffmpeg',\n                    '-i', str(audio_path),\n                    '-t', str(video_duration),\n                    '-y',\n                    str(audio_processed_path)\n                ], check=True, capture_output=True)\n                \n            else:\n                # If audio is shorter, loop it to fill the video duration\n                print(\"  Looping audio to match video length\")\n                \n                # Calculate how many loops we need\n                loops_needed = int(video_duration / audio_duration) + 1\n                \n                # Create a file with the input repeated\n                concat_file = temp_dir_path / \"concat.txt\"\n                with open(concat_file, 'w') as f:\n                    for _ in range(loops_needed):\n                        f.write(f\"file '{audio_path}'\\n\")\n                \n                # Concatenate the audio files\n                looped_audio = temp_dir_path / \"looped_audio.mp3\"\n                subprocess.run([\n                    'ffmpeg',\n                    '-f', 'concat',\n                    '-safe', '0',\n                    '-i', str(concat_file),\n                    '-c', 'copy',\n                    '-y',\n                    str(looped_audio)\n                ], check=True, capture_output=True)\n                \n                # Trim the looped audio to the exact video duration\n                subprocess.run([\n                    'ffmpeg',\n                    '-i', str(looped_audio),\n                    '-t', str(video_duration),\n                    '-y',\n                    str(audio_processed_path)\n                ], check=True, capture_output=True)\n            \n            # Combine video with processed audio\n            subprocess.run([\n                'ffmpeg',\n                '-i', str(video_path),\n                '-i', str(audio_processed_path),\n                '-c:v', 'copy',\n                '-c:a', 'aac',\n                '-map', '0:v:0',\n                '-map', '1:a:0',\n                '-shortest',\n                '-y',\n                str(output_path)\n            ], check=True, capture_output=True)\n            \n            # Verify the output file\n            output_duration = get_video_duration(output_path)\n            print(f\"  Output duration: {output_duration:.2f}s\")\n            \n            if abs(output_duration - video_duration) > 1.0:\n                print(f\"  Warning: Output duration ({output_duration:.2f}s) differs from target ({video_duration:.2f}s)\")\n            \n            return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error creating video with audio: {e}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode() if hasattr(e, 'stderr') else 'No stderr available'}\")\n        return False\n\ndef main():\n    print(\"=== ADDING SUNFLOWER AUDIO TO IMPALA VIDEO ===\\n\")\n    \n    # Get video duration\n    video_duration = get_video_duration(VIDEO_FILE)\n    print(f\"Source video: {VIDEO_FILE}\")\n    print(f\"Video duration: {video_duration:.2f} seconds\")\n    \n    if video_duration == 0:\n        print(\"Error: Could not determine video duration.\")\n        return\n    \n    # Find all audio files\n    audio_files = find_audio_files()\n    print(f\"Found {len(audio_files)} audio files in {AUDIO_DIR}\\n\")\n    \n    if not audio_files:\n        print(\"No audio files found.\")\n        return\n    \n    # Create a new video for each audio file\n    successful_videos = []\n    \n    for i, audio_file in enumerate(audio_files, 1):\n        # Create a clean filename for the output\n        audio_name = audio_file.stem.replace(' ', '_')\n        output_file = OUTPUT_DIR / f\"ResurrectingAtlantis_Sunflower_{audio_name}.mp4\"\n        \n        print(f\"\\n{i}/{len(audio_files)} Processing: {audio_file.name}\")\n        print(f\"Creating: {output_file}\")\n        \n        if create_video_with_audio(VIDEO_FILE, audio_file, output_file, video_duration):\n            print(f\"\u2713 Successfully created {output_file}\")\n            successful_videos.append(output_file)\n        else:\n            print(f\"\u2717 Failed to create {output_file}\")\n    \n    # Create a summary file\n    summary_file = OUTPUT_DIR / \"sunflower_versions_summary.txt\"\n    with open(summary_file, 'w') as f:\n        f.write(\"RESURRECTING ATLANTIS - SUNFLOWER VERSIONS\\n\")\n        f.write(\"=========================================\\n\\n\")\n        f.write(f\"Original video: {VIDEO_FILE}\\n\")\n        f.write(f\"Video duration: {video_duration:.2f} seconds\\n\")\n        f.write(f\"Total versions created: {len(successful_videos)}/{len(audio_files)}\\n\\n\")\n        \n        f.write(\"SUNFLOWER VERSIONS:\\n\")\n        for i, video in enumerate(successful_videos, 1):\n            output_duration = get_video_duration(video)\n            f.write(f\"{i}. {video.name}\\n\")\n            f.write(f\"   Duration: {output_duration:.2f} seconds\\n\\n\")\n    \n    print(f\"\\nCreated {len(successful_videos)}/{len(audio_files)} sunflower versions.\")\n    print(f\"Output directory: {OUTPUT_DIR}\")\n    print(f\"Summary: {summary_file}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ResurrectingAtlantis_OnePerTrack.mp4",
      "looped_audio.mp3",
      "ResurrectingAtlantis_Sunflower_{audio_name}.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ResurrectingAtlantis_OnePerTrack.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA/IBEX/SUNFLOWER",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/sunflower_versions",
      "\n    try:\n        # Create a temporary directory for intermediate files\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir_path = Path(temp_dir)\n            \n            # Process audio file (trim or loop to match video duration)\n            audio_processed_path = temp_dir_path / f",
      ")\n                \n                # Calculate how many loops we need\n                loops_needed = int(video_duration / audio_duration) + 1\n                \n                # Create a file with the input repeated\n                concat_file = temp_dir_path / ",
      ")\n                \n                # Concatenate the audio files\n                looped_audio = temp_dir_path / ",
      ")\n        output_file = OUTPUT_DIR / f",
      "\\n{i}/{len(audio_files)} Processing: {audio_file.name}",
      ")\n    \n    # Create a summary file\n    summary_file = OUTPUT_DIR / ",
      "Total versions created: {len(successful_videos)}/{len(audio_files)}\\n\\n",
      "\\nCreated {len(successful_videos)}/{len(audio_files)} sunflower versions."
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n                'ffprobe', \n                '-v', 'error', \n                '-show_entries', 'format=duration', \n                '-of', 'json', \n                str(audio_path"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-i', str(audio_path"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-i', str(audio_path"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-f', 'concat',\n                    '-safe', '0',\n                    '-i', str(concat_file"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-i', str(looped_audio"
      },
      {
        "type": "run",
        "snippet": "[\n                'ffmpeg',\n                '-i', str(video_path"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "pathlib",
      "tempfile",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/poem_section_video_with_audio.py",
    "size": 6554,
    "lines": 187,
    "source": "#!/usr/bin/env python3\n\"\"\"\nPoem Section Video Generator with Audio\n\nThis script extends the functionality of fl_video_generator_header_prompt.py \nby adding audio from the MANTA/audio directory to the generated videos.\n\nUsage: python3 poem_section_video_with_audio.py --prefix PREFIX\nExample: python3 poem_section_video_with_audio.py --prefix HT\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport subprocess\nimport re\nimport glob\n\n# Base directories\nTIGER_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nJELLYFISH_DIR = \"/Users/gaia/resurrecting atlantis/JELLYFISH\"\nMANTA_AUDIO_DIR = \"/Users/gaia/resurrecting atlantis/MANTA/audio\"\nOUTPUT_DIR = os.path.join(JELLYFISH_DIR, \"video_output\")\n\n# Audio file mappings for each poem section\nAUDIO_MAPPING = {\n    \"SH\": \"Out of Life_Echoes of Departure remix v1_GOOD.mp3\",\n    \"FL\": \"flashing-lights-potent-Digital Testament remix v1.2.1.2.mp3\",\n    \"HT\": \"How To Win My Heart_Echoes of Longing remix v2.2.1.mp3\",\n    \"RU\": \"Reunion_The New Gospel.mp3\",\n    \"NM\": \"Nevermore_Reclaimed Truths.mp3\",\n    \"BE\": \"Bloodline_New Gospel of Creation.mp3\",\n    \"AT\": \"RESURRECTING_ATLANTIS_Resonance of Futures.mp3\",\n    \"DJ\": \"DJ_turn-me-up-Ghosts of Love.mp3\",\n    \"NS\": \"Newly Single_The New Awakening.mp3\",\n    \"YH\": \"Yet, Heard_The New Covenant.mp3\",\n    \"MR\": \"Magic ride_Awakening of the New Gospel remix v2.mp3\",\n    \"HW\": \"How To Win My Heart_Echoes of Longing remix v2.2.1.mp3\",\n    \"HM\": \"Hot Minute_The New Covenant remix v2.mp3\"\n}\n\ndef find_audio_file(prefix):\n    \"\"\"Find the appropriate audio file for the given section prefix.\"\"\"\n    # First try the direct mapping\n    if prefix in AUDIO_MAPPING:\n        audio_path = os.path.join(MANTA_AUDIO_DIR, AUDIO_MAPPING[prefix])\n        if os.path.exists(audio_path):\n            return audio_path\n    \n    # Try a fuzzy search based on prefix\n    pattern = f\"*{prefix}*\"\n    matches = glob.glob(os.path.join(MANTA_AUDIO_DIR, pattern + \".mp3\"))\n    if matches:\n        return matches[0]\n    \n    # Try searching based on poem title patterns\n    title_patterns = {\n        \"SH\": \"*Out of Life*.mp3\",\n        \"FL\": \"*Flashing Lights*.mp3\",\n        \"HT\": \"*How To Win*.mp3\",\n        \"RU\": \"*Reunion*.mp3\",\n        \"NM\": \"*Nevermore*.mp3\",\n        \"BE\": \"*Bloodline*.mp3\",\n        \"AT\": \"*Atlantis*.mp3\",\n        \"DJ\": \"*turn*me*up*.mp3\",\n        \"NS\": \"*Newly Single*.mp3\",\n        \"YH\": \"*Yet*Heard*.mp3\",\n        \"MR\": \"*Magic ride*.mp3\",\n        \"HW\": \"*How To Win*.mp3\",\n        \"HM\": \"*Hot Minute*.mp3\"\n    }\n    \n    if prefix in title_patterns:\n        matches = glob.glob(os.path.join(MANTA_AUDIO_DIR, title_patterns[prefix]))\n        if matches:\n            return matches[0]\n    \n    # If no specific match, return all MP3 files for manual selection\n    print(f\"No specific audio file found for {prefix}\")\n    matches = glob.glob(os.path.join(MANTA_AUDIO_DIR, \"*.mp3\"))\n    for i, audio in enumerate(matches):\n        print(f\"{i+1}. {os.path.basename(audio)}\")\n        \n    if matches:\n        choice = input(\"Enter number of audio file to use (or Enter to continue without audio): \")\n        if choice.isdigit() and 1 <= int(choice) <= len(matches):\n            return matches[int(choice) - 1]\n    \n    return None\n\ndef generate_video_with_audio(prefix):\n    \"\"\"Generate a header_prompt.mp4 video with audio for the specified section prefix.\"\"\"\n    # First generate the video without audio using the existing script\n    generate_video_cmd = [\n        \"python3\", \n        os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\"), \n        f\"--prefix\", prefix\n    ]\n    \n    print(f\"Generating video for {prefix}...\")\n    result = subprocess.run(generate_video_cmd, cwd=JELLYFISH_DIR, text=True, capture_output=True)\n    \n    if result.returncode != 0:\n        print(f\"Error generating video: {result.stderr}\")\n        return False\n    \n    print(result.stdout)\n    \n    # Find the generated video file\n    video_path = os.path.join(OUTPUT_DIR, f\"{prefix}_header_prompt.mp4\")\n    if not os.path.exists(video_path):\n        print(f\"Error: Generated video not found at {video_path}\")\n        return False\n    \n    # Find the appropriate audio file\n    audio_path = find_audio_file(prefix)\n    if not audio_path:\n        print(\"Continuing without audio\")\n        return True\n    \n    # Create output video with audio\n    output_path = os.path.join(OUTPUT_DIR, f\"{prefix}_header_prompt_with_audio.mp4\")\n    \n    # Add audio to video using FFmpeg\n    print(f\"Adding audio from {os.path.basename(audio_path)} to video...\")\n    audio_cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", video_path,\n        \"-i\", audio_path,\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-shortest\",\n        output_path\n    ]\n    \n    audio_result = subprocess.run(audio_cmd, text=True, capture_output=True)\n    \n    if audio_result.returncode != 0:\n        print(f\"Error adding audio to video: {audio_result.stderr}\")\n        return False\n    \n    print(f\"Video with audio generated: {output_path}\")\n    return True\n\ndef generate_codex_entries(prefix):\n    \"\"\"Generate Codex entries for the section using the existing script.\"\"\"\n    codex_generator_path = os.path.join(JELLYFISH_DIR, \"poem_section_codex_generator.py\")\n    \n    if not os.path.exists(codex_generator_path):\n        print(f\"Error: Codex generator script not found at {codex_generator_path}\")\n        return False\n    \n    cmd = [\"python3\", codex_generator_path, prefix]\n    result = subprocess.run(cmd, text=True, capture_output=True)\n    \n    if result.returncode != 0:\n        print(f\"Error generating Codex entries: {result.stderr}\")\n        return False\n    \n    print(result.stdout)\n    return True\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate poem section videos with audio\")\n    parser.add_argument(\"--prefix\", required=True, help=\"Poem section prefix (e.g., SH, FL, HT)\")\n    parser.add_argument(\"--generate-codex\", action=\"store_true\", help=\"Generate Codex entries before video\")\n    \n    args = parser.parse_args()\n    prefix = args.prefix.upper()\n    \n    # Make sure output directory exists\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Generate Codex entries if requested\n    if args.generate_codex:\n        print(f\"Generating Codex entries for {prefix}...\")\n        if not generate_codex_entries(prefix):\n            print(\"Failed to generate Codex entries. Continuing with video generation...\")\n    \n    # Generate video with audio\n    generate_video_with_audio(prefix)\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "Out of Life_Echoes of Departure remix v1_GOOD.mp3",
      "flashing-lights-potent-Digital Testament remix v1.2.1.2.mp3",
      "How To Win My Heart_Echoes of Longing remix v2.2.1.mp3",
      "Reunion_The New Gospel.mp3",
      "Nevermore_Reclaimed Truths.mp3",
      "Bloodline_New Gospel of Creation.mp3",
      "RESURRECTING_ATLANTIS_Resonance of Futures.mp3",
      "DJ_turn-me-up-Ghosts of Love.mp3",
      "Newly Single_The New Awakening.mp3",
      "Yet, Heard_The New Covenant.mp3",
      "Magic ride_Awakening of the New Gospel remix v2.mp3",
      "How To Win My Heart_Echoes of Longing remix v2.2.1.mp3",
      "Hot Minute_The New Covenant remix v2.mp3",
      "*Out of Life*.mp3",
      "*Flashing Lights*.mp3",
      "*How To Win*.mp3",
      "*Reunion*.mp3",
      "*Nevermore*.mp3",
      "*Bloodline*.mp3",
      "*Atlantis*.mp3",
      "*turn*me*up*.mp3",
      "*Newly Single*.mp3",
      "*Yet*Heard*.mp3",
      "*Magic ride*.mp3",
      "*How To Win*.mp3",
      "*Hot Minute*.mp3",
      "*.mp3",
      "{prefix}_header_prompt.mp4",
      "{prefix}_header_prompt_with_audio.mp4",
      "\nPoem Section Video Generator with Audio\n\nThis script extends the functionality of fl_video_generator_header_prompt.py \nby adding audio from the MANTA/audio directory to the generated videos.\n\nUsage: python3 poem_section_video_with_audio.py --prefix PREFIX\nExample: python3 poem_section_video_with_audio.py --prefix HT\n",
      "/Users/gaia/resurrecting atlantis/TIGER",
      "/Users/gaia/resurrecting atlantis/JELLYFISH",
      "/Users/gaia/resurrecting atlantis/MANTA/audio"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "generate_video_cmd, cwd=JELLYFISH_DIR, text=True, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "audio_cmd, text=True, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "cmd, text=True, capture_output=True"
      }
    ],
    "imports": [
      "os",
      "sys",
      "argparse",
      "subprocess",
      "re",
      "glob"
    ],
    "generates": [],
    "reads": [],
    "docstring": "Poem Section Video Generator with Audio\n\nThis script extends the functionality of fl_video_generator_header_prompt.py \nby adding audio from the MANTA/audio directory to the generated videos.\n\nUsage: python3 poem_section_video_with_audio.py --prefix PREFIX\nExample: python3 poem_section_video_with_audio.py --prefix HT"
  },
  {
    "path": "JELLYFISH/create_title_journey_composites.py",
    "size": 18224,
    "lines": 464,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport random\nfrom pathlib import Path\nfrom PIL import Image, ImageFilter, ImageEnhance, ImageOps, ImageDraw, ImageFont\nimport numpy as np\nfrom collections import defaultdict\n\n# Paths to source directories\nMPRE_RUN_TITLES_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run/backup_titles')\nMPOST_JOURNEY_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey')\nMIDJOURNEY_BG_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]')\n\n# Output directory\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/title_journey_composites')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Create a logs directory to keep track of which images were combined\nLOGS_DIR = OUTPUT_DIR / \"logs\"\nLOGS_DIR.mkdir(exist_ok=True)\n\n# Track mapping for consistent naming\nTRACK_MAPPING = {\n    \"Out of Life\": {\"id\": \"01\", \"code\": \"SH\", \"timecode\": \"000000\"},\n    \"Flashing Lights\": {\"id\": \"02\", \"code\": \"FL\", \"timecode\": \"021100\"},\n    \"How to Break Off an Engagement\": {\"id\": \"03\", \"code\": \"HT\", \"timecode\": \"042200\"},\n    \"Nevermore\": {\"id\": \"04\", \"code\": \"NM\", \"timecode\": \"063300\"},\n    \"Bloodline\": {\"id\": \"05\", \"code\": \"BE\", \"timecode\": \"084400\"},\n    \"Resurrecting Atlantis\": {\"id\": \"06\", \"code\": \"AT\", \"timecode\": \"105500\"},\n    \"DJ Turn Me Up\": {\"id\": \"07\", \"code\": \"DJ\", \"timecode\": \"130600\"},\n    \"Newly Single\": {\"id\": \"08\", \"code\": \"NS\", \"timecode\": \"151700\"},\n    \"Yet Heard\": {\"id\": \"09\", \"code\": \"YH\", \"timecode\": \"172800\"},\n    \"Magic Ride\": {\"id\": \"10\", \"code\": \"MR\", \"timecode\": \"193900\"},\n    \"Reunion\": {\"id\": \"12\", \"code\": \"RU\", \"timecode\": \"215000\"},\n    \"How to Win My Heart\": {\"id\": \"13\", \"code\": \"HW\", \"timecode\": \"240100\"},\n    \"Hot Minute\": {\"id\": \"14\", \"code\": \"HM\", \"timecode\": \"261200\"},\n}\n\n# Creative blend modes with Afro-futurist aesthetic focus\nBLEND_MODES = [\n    \"overlay\",\n    \"screen\",\n    \"multiply\",\n    \"difference\",\n    \"luminosity\",\n    \"neon_glow\",\n    \"chromatic_shift\",\n    \"digital_decay\",\n    \"frequency_modulation\",\n    \"data_mosaic\"\n]\n\n# Helper function to find track name in filename\ndef find_track_name(filename):\n    for track_name in TRACK_MAPPING.keys():\n        # Check various forms of the track name in the filename\n        track_variants = [\n            track_name,\n            track_name.lower(),\n            track_name.replace(\" \", \"\"),\n            track_name.lower().replace(\" \", \"\")\n        ]\n        \n        for variant in track_variants:\n            if variant in filename.replace(\" \", \"\"):\n                return track_name\n    \n    return None\n\n# Helper function to categorize title images\ndef categorize_title_image(filename):\n    if \"Title Card\" in filename or \"Prompt Set\" in filename:\n        return \"TitleCard\"\n    elif \"Visual Concept\" in filename:\n        return \"VisualConcept\"\n    elif \"Poetic Ekphrasis\" in filename or \"Ekphrasis\" in filename:\n        return \"Ekphrasis\"\n    elif \"Duration\" in filename or \"Segment\" in filename:\n        return \"Segment\"\n    else:\n        return \"Other\"\n\n# Image processing functions for creative blending\ndef apply_overlay_blend(fg, bg):\n    \"\"\"Blend images using overlay mode\"\"\"\n    return Image.blend(fg, bg, 0.7)\n\ndef apply_screen_blend(fg, bg):\n    \"\"\"Screen blend mode simulation\"\"\"\n    fg_arr = np.array(fg).astype(float)\n    bg_arr = np.array(bg).astype(float)\n    result = 255 - ((255 - fg_arr) * (255 - bg_arr) / 255)\n    return Image.fromarray(np.uint8(result))\n\ndef apply_multiply_blend(fg, bg):\n    \"\"\"Multiply blend mode simulation\"\"\"\n    fg_arr = np.array(fg).astype(float) / 255\n    bg_arr = np.array(bg).astype(float) / 255\n    result = fg_arr * bg_arr * 255\n    return Image.fromarray(np.uint8(result))\n\ndef apply_difference_blend(fg, bg):\n    \"\"\"Difference blend mode simulation\"\"\"\n    fg_arr = np.array(fg).astype(int)\n    bg_arr = np.array(bg).astype(int)\n    result = np.abs(fg_arr - bg_arr)\n    return Image.fromarray(np.uint8(result))\n\ndef apply_luminosity_blend(fg, bg):\n    \"\"\"Custom luminosity blend preserving foreground luminosity, background color\"\"\"\n    fg_arr = np.array(fg).astype(float)\n    bg_arr = np.array(bg).astype(float)\n    \n    # Convert to HSL-like space (simplified)\n    fg_lum = 0.299 * fg_arr[:,:,0] + 0.587 * fg_arr[:,:,1] + 0.114 * fg_arr[:,:,2]\n    \n    # Apply luminosity to background colors\n    result = bg_arr.copy()\n    for i in range(3):  # RGB channels\n        result[:,:,i] = fg_lum * bg_arr[:,:,i] / 128\n    \n    return Image.fromarray(np.uint8(np.clip(result, 0, 255)))\n\ndef apply_neon_glow(fg, bg):\n    \"\"\"Create a neon glow effect on foreground before blending\"\"\"\n    # Add glow to foreground\n    glow_fg = fg.filter(ImageFilter.GaussianBlur(radius=10))\n    glow_fg = ImageEnhance.Brightness(glow_fg).enhance(1.5)\n    glow_fg = ImageEnhance.Color(glow_fg).enhance(2.0)  # Increase color saturation\n    \n    # Blend with background\n    return Image.blend(glow_fg, bg, 0.4)\n\ndef apply_chromatic_shift(fg, bg):\n    \"\"\"Apply RGB channel shifting for a chromatic aberration effect\"\"\"\n    r, g, b = fg.split()\n    \n    # Shift red channel slightly right\n    r_shifted = Image.new('L', (fg.width, fg.height))\n    r_shifted.paste(r, (5, 0))\n    \n    # Shift blue channel slightly left\n    b_shifted = Image.new('L', (fg.width, fg.height))\n    b_shifted.paste(b, (-5, 0))\n    \n    # Recombine with shifted channels\n    shifted_fg = Image.merge('RGB', (r_shifted, g, b_shifted))\n    \n    # Blend with background\n    return Image.blend(shifted_fg, bg, 0.5)\n\ndef apply_digital_decay(fg, bg):\n    \"\"\"Create a digital decay/glitch effect\"\"\"\n    # Split the foreground into horizontal slices\n    slice_height = fg.height // 20\n    result = bg.copy()\n    \n    for i in range(0, fg.height, slice_height):\n        # Get a slice\n        slice_box = (0, i, fg.width, min(i + slice_height, fg.height))\n        fg_slice = fg.crop(slice_box)\n        \n        # Random horizontal offset\n        offset = random.randint(-20, 20)\n        if offset != 0:\n            # Create a new slice with offset\n            shifted_slice = Image.new('RGB', fg_slice.size)\n            shifted_slice.paste(fg_slice, (offset, 0))\n            fg_slice = shifted_slice\n        \n        # Paste the slice onto the result with blend\n        result.paste(Image.blend(fg_slice, bg.crop(slice_box), 0.3), slice_box)\n    \n    return result\n\ndef apply_frequency_modulation(fg, bg):\n    \"\"\"Apply a wave-like distortion pattern\"\"\"\n    fg_arr = np.array(fg)\n    bg_arr = np.array(bg)\n    height, width, _ = fg_arr.shape\n    \n    # Create wave pattern\n    x = np.arange(width)\n    y = np.arange(height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Wave distortion\n    amplitude = 10\n    frequency = 0.1\n    wave = np.sin(X * frequency) * amplitude\n    \n    # Apply distortion\n    result = np.zeros_like(fg_arr)\n    for y in range(height):\n        for x in range(width):\n            # Calculate source x with wave distortion\n            src_x = int(x + wave[y, x])\n            \n            # Boundary check\n            if 0 <= src_x < width:\n                # Mix foreground and background\n                mix_ratio = 0.6\n                result[y, x] = fg_arr[y, src_x] * mix_ratio + bg_arr[y, x] * (1 - mix_ratio)\n            else:\n                result[y, x] = bg_arr[y, x]\n    \n    return Image.fromarray(np.uint8(result))\n\ndef apply_data_mosaic(fg, bg):\n    \"\"\"Create a data mosaic effect with block-based compositing\"\"\"\n    # Define block size\n    block_size = 16\n    \n    # Create a new image with background\n    result = bg.copy()\n    draw = ImageDraw.Draw(result)\n    \n    # Process the image in blocks\n    for y in range(0, fg.height, block_size):\n        for x in range(0, fg.width, block_size):\n            # Define block region\n            block_box = (x, y, min(x + block_size, fg.width), min(y + block_size, fg.height))\n            \n            # Randomly decide whether to use foreground or background for this block\n            if random.random() > 0.5:\n                # Get average color of foreground block\n                fg_block = fg.crop(block_box)\n                avg_color = tuple(map(int, np.array(fg_block).mean(axis=(0, 1))))\n                \n                # Draw rectangle with average color\n                draw.rectangle(block_box, fill=avg_color)\n    \n    return result\n\n# Function to select blend mode and apply it\ndef blend_images(fg, bg, mode=None):\n    \"\"\"Apply a specific blend mode or random one if none specified\"\"\"\n    # Resize images to match\n    bg = bg.resize(fg.size, Image.LANCZOS)\n    \n    # Convert both to RGB mode if needed\n    if fg.mode != 'RGB':\n        fg = fg.convert('RGB')\n    if bg.mode != 'RGB':\n        bg = bg.convert('RGB')\n    \n    # Select blend mode\n    if mode is None or mode not in BLEND_MODES:\n        mode = random.choice(BLEND_MODES)\n    \n    # Apply selected blend mode\n    if mode == \"overlay\":\n        return apply_overlay_blend(fg, bg)\n    elif mode == \"screen\":\n        return apply_screen_blend(fg, bg)\n    elif mode == \"multiply\":\n        return apply_multiply_blend(fg, bg)\n    elif mode == \"difference\":\n        return apply_difference_blend(fg, bg)\n    elif mode == \"luminosity\":\n        return apply_luminosity_blend(fg, bg)\n    elif mode == \"neon_glow\":\n        return apply_neon_glow(fg, bg)\n    elif mode == \"chromatic_shift\":\n        return apply_chromatic_shift(fg, bg)\n    elif mode == \"digital_decay\":\n        return apply_digital_decay(fg, bg)\n    elif mode == \"frequency_modulation\":\n        return apply_frequency_modulation(fg, bg)\n    elif mode == \"data_mosaic\":\n        return apply_data_mosaic(fg, bg)\n    else:\n        # Fallback to simple blend\n        return Image.blend(fg, bg, 0.5)\n\n# Function to add a subtle neon text overlay to highlight track info\ndef add_track_text_overlay(img, track_name, variant):\n    \"\"\"Add track information as neon-style text overlay\"\"\"\n    draw = ImageDraw.Draw(img)\n    \n    # Try to create font object - use default if not available\n    try:\n        font = ImageFont.truetype(\"Arial\", 40)\n        small_font = ImageFont.truetype(\"Arial\", 24)\n    except IOError:\n        font = ImageFont.load_default()\n        small_font = ImageFont.load_default()\n    \n    # Add drop shadow for visibility\n    text = track_name\n    variant_text = variant.upper()\n    \n    # Position at bottom of image\n    text_position = (30, img.height - 100)\n    variant_position = (30, img.height - 50)\n    \n    # Add shadow\n    shadow_offset = 3\n    draw.text((text_position[0] + shadow_offset, text_position[1] + shadow_offset), \n              text, fill=(0, 0, 0), font=font)\n    draw.text((variant_position[0] + shadow_offset, variant_position[1] + shadow_offset), \n              variant_text, fill=(0, 0, 0), font=small_font)\n    \n    # Add neon-colored text\n    neon_colors = {\n        \"TitleCard\": (0, 255, 200),      # Cyan\n        \"VisualConcept\": (255, 0, 255),  # Magenta\n        \"Ekphrasis\": (255, 200, 0),      # Gold\n        \"Segment\": (0, 200, 255),        # Blue\n        \"Other\": (255, 255, 255)         # White\n    }\n    \n    draw.text(text_position, text, fill=neon_colors.get(variant, (255, 255, 255)), font=font)\n    draw.text(variant_position, variant_text, fill=neon_colors.get(variant, (255, 255, 255)), font=small_font)\n    \n    return img\n\n# Organize files by track\ndef organize_files():\n    \"\"\"Organize all source files by track name and type\"\"\"\n    # Dictionary to store files by track and type\n    organized_files = defaultdict(lambda: defaultdict(list))\n    \n    # Process title/visual files from mpre-run/backup_titles\n    for file in MPRE_RUN_TITLES_DIR.glob(\"*.png\"):\n        track_name = find_track_name(file.name)\n        if track_name:\n            file_type = categorize_title_image(file.name)\n            organized_files[track_name][file_type].append(str(file))\n    \n    # Process mpost-journey files\n    for file in MPOST_JOURNEY_DIR.glob(\"*.png\"):\n        if \"backup\" not in file.name.lower():\n            track_id = file.name.split('_')[0]\n            track_code = file.name.split('_')[1]\n            \n            # Find corresponding track name\n            for track_name, info in TRACK_MAPPING.items():\n                if info[\"id\"] == track_id and info[\"code\"] == track_code:\n                    organized_files[track_name][\"Journey\"].append(str(file))\n                    break\n    \n    # Process midjourney background files\n    for file in MIDJOURNEY_BG_DIR.glob(\"*.png\"):\n        if \"backup\" not in file.name.lower() and \"BG\" in file.name:\n            track_id = file.name.split('_')[0]\n            if track_id.isdigit() or (len(track_id) == 2 and track_id[:2].isdigit()):\n                # Find corresponding track name\n                for track_name, info in TRACK_MAPPING.items():\n                    if info[\"id\"] == track_id:\n                        organized_files[track_name][\"Background\"].append(str(file))\n                        break\n    \n    return organized_files\n\n# Main function to create composite images\ndef create_composites():\n    \"\"\"Create creative composites from organized files\"\"\"\n    # Get organized files\n    organized_files = organize_files()\n    \n    # Log file for combinations\n    log_file = LOGS_DIR / \"composite_combinations.txt\"\n    with open(log_file, \"w\") as log:\n        log.write(\"CREATIVE COMPOSITE COMBINATIONS\\n\")\n        log.write(\"==============================\\n\\n\")\n        \n        # Create composites for each track\n        for track_name, file_types in organized_files.items():\n            track_info = TRACK_MAPPING.get(track_name)\n            if not track_info:\n                print(f\"Warning: No track mapping found for {track_name}\")\n                continue\n                \n            track_id = track_info[\"id\"]\n            track_code = track_info[\"code\"]\n            track_timecode = track_info[\"timecode\"]\n            \n            # Base filename format\n            base_filename = f\"{track_id}_{track_code}_{track_name.replace(' ', '')}_{track_timecode}\"\n            \n            # Log track info\n            log.write(f\"\\n{track_name} ({track_id}_{track_code}_{track_timecode})\\n\")\n            log.write(\"-\" * 50 + \"\\n\")\n            \n            # Process each type of title/concept image\n            for title_type in [\"TitleCard\", \"VisualConcept\", \"Ekphrasis\", \"Segment\"]:\n                title_images = file_types.get(title_type, [])\n                journey_images = file_types.get(\"Journey\", [])\n                bg_images = file_types.get(\"Background\", [])\n                \n                if not title_images:\n                    continue\n                    \n                for title_img_path in title_images:\n                    # Choose a background image if available, otherwise use a random one\n                    if bg_images:\n                        bg_img_path = random.choice(bg_images)\n                    else:\n                        # Find any background image from another track\n                        all_bg_images = []\n                        for other_track, other_types in organized_files.items():\n                            if other_track != track_name:\n                                all_bg_images.extend(other_types.get(\"Background\", []))\n                        \n                        bg_img_path = random.choice(all_bg_images) if all_bg_images else None\n                    \n                    # Choose a journey image if available\n                    journey_img_path = random.choice(journey_images) if journey_images else None\n                    \n                    # Skip if we don't have enough images to blend\n                    if not bg_img_path:\n                        print(f\"Warning: No background image found for {track_name} - {title_type}\")\n                        continue\n                    \n                    # Load images\n                    title_img = Image.open(title_img_path)\n                    bg_img = Image.open(bg_img_path)\n                    \n                    # Create unique variant name\n                    blend_mode = random.choice(BLEND_MODES)\n                    variant_name = f\"Composite_{blend_mode.capitalize()}\"\n                    \n                    # Blend title with background\n                    composite = blend_images(title_img, bg_img, blend_mode)\n                    \n                    # If we have a journey image, blend it too with a different mode\n                    if journey_img_path:\n                        journey_img = Image.open(journey_img_path)\n                        journey_blend_mode = random.choice([m for m in BLEND_MODES if m != blend_mode])\n                        composite = blend_images(composite, journey_img, journey_blend_mode)\n                        variant_name += f\"_Journey_{journey_blend_mode.capitalize()}\"\n                    \n                    # Add text overlay\n                    composite = add_track_text_overlay(composite, track_name, title_type)\n                    \n                    # Create output filename\n                    output_filename = f\"{base_filename}_{title_type}_{variant_name}.png\"\n                    output_path = OUTPUT_DIR / output_filename\n                    \n                    # Save the composite\n                    composite.save(output_path)\n                    print(f\"Created: {output_filename}\")\n                    \n                    # Log combination\n                    log.write(f\"Created: {output_filename}\\n\")\n                    log.write(f\"  - Title Image: {Path(title_img_path).name}\\n\")\n                    log.write(f\"  - Background: {Path(bg_img_path).name}\\n\")\n                    if journey_img_path:\n                        log.write(f\"  - Journey Layer: {Path(journey_img_path).name}\\n\")\n                    log.write(f\"  - Blend Modes: {blend_mode}\" + \n                             (f\", {journey_blend_mode}\" if journey_img_path else \"\") + \"\\n\")\n                    log.write(\"\\n\")\n\n# Execute the script\nif __name__ == \"__main__\":\n    print(\"Starting to create creative composite images...\")\n    create_composites()\n    print(\"Done! Check the output directory for the new composite images.\")\n    print(f\"Composites saved to: {OUTPUT_DIR}\")\n    print(f\"Combination log saved to: {LOGS_DIR / 'composite_combinations.txt'}\")\n\n",
    "file_references": [
      "*.png",
      "*.png",
      "*.png",
      "{base_filename}_{title_type}_{variant_name}.png",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run/backup_titles",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/title_journey_composites",
      "\n    fg_arr = np.array(fg).astype(float)\n    bg_arr = np.array(bg).astype(float)\n    result = 255 - ((255 - fg_arr) * (255 - bg_arr) / 255)\n    return Image.fromarray(np.uint8(result))\n\ndef apply_multiply_blend(fg, bg):\n    ",
      "\n    fg_arr = np.array(fg).astype(float) / 255\n    bg_arr = np.array(bg).astype(float) / 255\n    result = fg_arr * bg_arr * 255\n    return Image.fromarray(np.uint8(result))\n\ndef apply_difference_blend(fg, bg):\n    ",
      "\n    fg_arr = np.array(fg).astype(float)\n    bg_arr = np.array(bg).astype(float)\n    \n    # Convert to HSL-like space (simplified)\n    fg_lum = 0.299 * fg_arr[:,:,0] + 0.587 * fg_arr[:,:,1] + 0.114 * fg_arr[:,:,2]\n    \n    # Apply luminosity to background colors\n    result = bg_arr.copy()\n    for i in range(3):  # RGB channels\n        result[:,:,i] = fg_lum * bg_arr[:,:,i] / 128\n    \n    return Image.fromarray(np.uint8(np.clip(result, 0, 255)))\n\ndef apply_neon_glow(fg, bg):\n    ",
      "Create a digital decay/glitch effect",
      "\n    # Split the foreground into horizontal slices\n    slice_height = fg.height // 20\n    result = bg.copy()\n    \n    for i in range(0, fg.height, slice_height):\n        # Get a slice\n        slice_box = (0, i, fg.width, min(i + slice_height, fg.height))\n        fg_slice = fg.crop(slice_box)\n        \n        # Random horizontal offset\n        offset = random.randint(-20, 20)\n        if offset != 0:\n            # Create a new slice with offset\n            shifted_slice = Image.new(",
      "\n    # Dictionary to store files by track and type\n    organized_files = defaultdict(lambda: defaultdict(list))\n    \n    # Process title/visual files from mpre-run/backup_titles\n    for file in MPRE_RUN_TITLES_DIR.glob(",
      "\n    # Get organized files\n    organized_files = organize_files()\n    \n    # Log file for combinations\n    log_file = LOGS_DIR / ",
      ")\n            \n            # Process each type of title/concept image\n            for title_type in [",
      "\n                    output_path = OUTPUT_DIR / output_filename\n                    \n                    # Save the composite\n                    composite.save(output_path)\n                    print(f",
      "Combination log saved to: {LOGS_DIR / "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "random",
      "pathlib",
      "PIL",
      "numpy",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/create_light_overlays.py",
    "size": 15107,
    "lines": 425,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nfrom PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageChops\n\n# Define paths\nmpost_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey\"\nmidjourney_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]\"\noutput_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/light_overlays\"\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Get all image files\nmpost_files = [f for f in os.listdir(mpost_dir) if f.lower().endswith('.png') and not f.startswith('original_')]\nmidjourney_files = [f for f in os.listdir(midjourney_dir) if f.lower().endswith('.png') and not os.path.isdir(os.path.join(midjourney_dir, f))]\n\nprint(f\"Found {len(mpost_files)} mpost files\")\nprint(f\"Found {len(midjourney_files)} midjourney files\")\n\n# Extract ID number from filename\ndef get_id_from_filename(filename):\n    # Pattern to match the ID at the beginning of the filename (e.g., 01_SH, 02_FL)\n    match = re.match(r'^(\\d+)_([A-Z]+)_', filename)\n    if match:\n        return match.group(1)  # Return just the number part\n    return None\n\n# Group files by ID\nmpost_by_id = {}\nmidjourney_by_id = {}\n\nfor file in mpost_files:\n    id_num = get_id_from_filename(file)\n    if id_num:\n        if id_num not in mpost_by_id:\n            mpost_by_id[id_num] = []\n        mpost_by_id[id_num].append(file)\n\nfor file in midjourney_files:\n    id_num = get_id_from_filename(file)\n    if id_num:\n        if id_num not in midjourney_by_id:\n            midjourney_by_id[id_num] = []\n        midjourney_by_id[id_num].append(file)\n\nprint(f\"Grouped into {len(mpost_by_id)} mpost IDs\")\nprint(f\"Grouped into {len(midjourney_by_id)} midjourney IDs\")\n\n# Define 8 light blending techniques\ndef ethereal_glow(fg, bg):\n    # Brighten background and create ethereal glow effect\n    brightened_bg = ImageEnhance.Brightness(bg).enhance(1.5)\n    \n    # Apply slight blur for dreamy effect\n    brightened_bg = brightened_bg.filter(ImageFilter.GaussianBlur(radius=1))\n    \n    # Enhance foreground clarity\n    enhanced_fg = ImageEnhance.Contrast(fg).enhance(1.2)\n    enhanced_fg = ImageEnhance.Sharpness(enhanced_fg).enhance(1.3)\n    \n    # Blend with emphasis on bright background\n    return Image.blend(brightened_bg, enhanced_fg, 0.65)\n\ndef pastel_highlights(fg, bg):\n    # Create soft pastel effect with highlights\n    brightened_bg = ImageEnhance.Brightness(bg).enhance(1.6)\n    brightened_bg = ImageEnhance.Contrast(brightened_bg).enhance(0.85)  # Lower contrast for pastel effect\n    \n    # Apply color shifts for pastel tones\n    r, g, b = brightened_bg.split()\n    r = ImageEnhance.Brightness(r).enhance(1.1)\n    g = ImageEnhance.Brightness(g).enhance(1.15)\n    b = ImageEnhance.Brightness(b).enhance(1.2)\n    brightened_bg = Image.merge(\"RGB\", (r, g, b))\n    \n    # Foreground with enhanced color\n    enhanced_fg = ImageEnhance.Color(fg).enhance(1.2)\n    \n    # Blend with soft light effect\n    return Image.blend(brightened_bg, enhanced_fg, 0.5)\n\ndef high_key_overlay(fg, bg):\n    # Create high-key effect (bright with reduced contrast)\n    high_key_bg = ImageEnhance.Brightness(bg).enhance(1.8)\n    high_key_bg = ImageEnhance.Contrast(high_key_bg).enhance(0.7)\n    \n    # Add slight blur for softness\n    high_key_bg = high_key_bg.filter(ImageFilter.GaussianBlur(radius=0.5))\n    \n    # Enhance foreground details\n    enhanced_fg = ImageEnhance.Sharpness(fg).enhance(1.4)\n    \n    # Blend with more emphasis on background\n    return Image.blend(high_key_bg, enhanced_fg, 0.4)\n\ndef luminous_blend(fg, bg):\n    # Create luminous effect with enhanced lights\n    brightened_bg = ImageEnhance.Brightness(bg).enhance(1.7)\n    \n    # Apply auto levels to enhance contrast in a balanced way\n    brightened_bg = ImageOps.autocontrast(brightened_bg, cutoff=2)\n    \n    # Extract and enhance highlights from foreground\n    enhanced_fg = ImageEnhance.Brightness(fg).enhance(1.3)\n    enhanced_fg = ImageEnhance.Contrast(enhanced_fg).enhance(1.15)\n    \n    # Create screen blend effect\n    result = ImageChops.screen(brightened_bg, enhanced_fg)\n    return result\n\ndef dreamy_haze(fg, bg):\n    # Create dreamy, hazy effect with bright background\n    brightened_bg = ImageEnhance.Brightness(bg).enhance(1.4)\n    \n    # Apply gaussian blur for haze effect\n    hazy_bg = brightened_bg.filter(ImageFilter.GaussianBlur(radius=3))\n    \n    # Mix original with hazy version\n    bg_mix = Image.blend(brightened_bg, hazy_bg, 0.6)\n    \n    # Enhance foreground with clarity\n    enhanced_fg = ImageEnhance.Sharpness(fg).enhance(1.5)\n    enhanced_fg = ImageEnhance.Contrast(enhanced_fg).enhance(1.2)\n    \n    # Final blend\n    return Image.blend(bg_mix, enhanced_fg, 0.55)\n\ndef crystalline_light(fg, bg):\n    # Create crystalline, bright effect\n    brightened_bg = ImageEnhance.Brightness(bg).enhance(1.6)\n    \n    # Enhance edges for crystalline effect\n    edges_bg = brightened_bg.filter(ImageFilter.EDGE_ENHANCE)\n    \n    # Mix original with edge-enhanced version\n    bg_mix = Image.blend(brightened_bg, edges_bg, 0.4)\n    \n    # Enhance foreground with clarity and color\n    enhanced_fg = ImageEnhance.Sharpness(fg).enhance(1.3)\n    enhanced_fg = ImageEnhance.Color(enhanced_fg).enhance(1.25)\n    \n    # Final blend\n    return Image.blend(bg_mix, enhanced_fg, 0.6)\n\ndef golden_hour(fg, bg):\n    # Create warm, golden hour lighting effect\n    brightened_bg = ImageEnhance.Brightness(bg).enhance(1.4)\n    \n    # Apply warm color shift (enhance red and green, reduce blue)\n    r, g, b = brightened_bg.split()\n    r = ImageEnhance.Brightness(r).enhance(1.25)\n    g = ImageEnhance.Brightness(g).enhance(1.1)\n    b = ImageEnhance.Brightness(b).enhance(0.85)\n    warm_bg = Image.merge(\"RGB\", (r, g, b))\n    \n    # Enhance foreground with similar warm tones\n    r, g, b = fg.split()\n    r = ImageEnhance.Brightness(r).enhance(1.2)\n    g = ImageEnhance.Brightness(g).enhance(1.05)\n    b = ImageEnhance.Brightness(b).enhance(0.9)\n    warm_fg = Image.merge(\"RGB\", (r, g, b))\n    \n    # Enhance contrast for both\n    warm_bg = ImageEnhance.Contrast(warm_bg).enhance(1.1)\n    warm_fg = ImageEnhance.Contrast(warm_fg).enhance(1.2)\n    \n    # Final blend\n    return Image.blend(warm_bg, warm_fg, 0.55)\n\ndef airy_light(fg, bg):\n    # Create airy, light and bright effect\n    brightened_bg = ImageEnhance.Brightness(bg).enhance(1.7)\n    brightened_bg = ImageEnhance.Contrast(brightened_bg).enhance(0.75)\n    \n    # Add subtle blur for softness\n    soft_bg = brightened_bg.filter(ImageFilter.GaussianBlur(radius=1.5))\n    \n    # Blend original with soft version\n    bg_mix = Image.blend(brightened_bg, soft_bg, 0.5)\n    \n    # Enhance foreground with clarity but reduced saturation\n    enhanced_fg = ImageEnhance.Sharpness(fg).enhance(1.2)\n    enhanced_fg = ImageEnhance.Color(enhanced_fg).enhance(0.9)  # Slightly desaturate\n    \n    # Final soft blend\n    return Image.blend(bg_mix, enhanced_fg, 0.45)\n\n# Advanced blending techniques with descriptive names\noverlay_modes = [\n    (\"EtherealGlow\", ethereal_glow),\n    (\"PastelHighlights\", pastel_highlights),\n    (\"HighKeyOverlay\", high_key_overlay),\n    (\"LuminousBlend\", luminous_blend),\n    (\"DreamyHaze\", dreamy_haze),\n    (\"CrystallineLight\", crystalline_light),\n    (\"GoldenHour\", golden_hour),\n    (\"AiryLight\", airy_light)\n]\n\n# Function to create overlay and save to output directory\ndef create_overlay(fg_file, bg_file, output_path, mode_name, mode_func):\n    try:\n        # Open images\n        fg_img = Image.open(os.path.join(mpost_dir, fg_file)).convert(\"RGBA\")\n        bg_img = Image.open(os.path.join(midjourney_dir, bg_file)).convert(\"RGBA\")\n        \n        # Resize foreground to match background dimensions\n        fg_img = fg_img.resize(bg_img.size)\n        \n        # Convert to RGB for blend operations\n        fg_rgb = fg_img.convert(\"RGB\")\n        bg_rgb = bg_img.convert(\"RGB\")\n        \n        # Create overlay using the provided function\n        result = mode_func(fg_rgb, bg_rgb)\n        \n        # Save the result - no labels for clean plates\n        result.save(output_path, quality=95)\n        return True\n    except Exception as e:\n        print(f\"Error processing {fg_file} and {bg_file}: {e}\")\n        return False\n\n# Create overlays for each matching ID\noverlay_count = 0\nfor id_num in sorted(set(list(mpost_by_id.keys()) + list(midjourney_by_id.keys()))):\n    # Skip if either directory doesn't have this ID\n    if id_num not in mpost_by_id or id_num not in midjourney_by_id:\n        print(f\"ID {id_num} not found in both directories, skipping\")\n        continue\n    \n    # Get files for this ID\n    fg_files = mpost_by_id[id_num]\n    bg_files = midjourney_by_id[id_num]\n    \n    print(f\"Processing ID {id_num}: {len(fg_files)} foreground files, {len(bg_files)} background files\")\n    \n    # Create overlays with all combinations\n    for fg_file in fg_files:\n        for bg_file in bg_files:\n            # Create a subdirectory for this ID\n            id_dir = os.path.join(output_dir, f\"{id_num}\")\n            os.makedirs(id_dir, exist_ok=True)\n            \n            # Create overlays with different modes\n            for mode_name, mode_func in overlay_modes:\n                # Create descriptive filename\n                fg_base = os.path.splitext(os.path.basename(fg_file))[0]\n                bg_base = os.path.splitext(os.path.basename(bg_file))[0]\n                output_filename = f\"{fg_base}_{mode_name}_{bg_base}.jpg\"\n                output_path = os.path.join(id_dir, output_filename)\n                \n                # Create and save overlay\n                if create_overlay(fg_file, bg_file, output_path, mode_name, mode_func):\n                    overlay_count += 1\n                    print(f\"Created light overlay: {output_filename}\")\n\nprint(f\"\\nCreated {overlay_count} light overlay images in {output_dir}\")\n\n# Create HTML file to view the overlays\nhtml_output = os.path.join(output_dir, \"view_light_overlays.html\")\nhtml_content = f\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <title>Light Experimental Overlays</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1, h2, h3 {{ \n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1200px;\n            margin: 0 auto;\n        }}\n        .track {{\n            margin-bottom: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        .overlays {{\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));\n            gap: 20px;\n            margin-top: 20px;\n        }}\n        .overlay {{\n            background-color: #2a2a2a;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n        img {{ \n            max-width: 100%; \n            height: auto;\n            display: block;\n            border: 1px solid #333;\n        }}\n        .info {{\n            margin-top: 10px;\n            font-size: 14px;\n        }}\n        .technique-desc {{\n            margin-top: 30px;\n            padding: 20px;\n            background-color: #2a2a2a;\n            border-radius: 5px;\n        }}\n        .technique {{\n            margin-bottom: 20px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>Light Experimental Overlay Techniques</h1>\n        <p>This page shows lighter overlay effects with bright backgrounds and ethereal blending techniques.</p>\n        \n        <div class=\"technique-desc\">\n            <h2>Techniques Used</h2>\n            \n            <div class=\"technique\">\n                <h3>Ethereal Glow</h3>\n                <p>Brightens the background and creates a dreamy, ethereal glow effect with enhanced foreground clarity.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Pastel Highlights</h3>\n                <p>Creates soft pastel tones with highlights and reduced contrast for a gentle, airy feeling.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>High Key Overlay</h3>\n                <p>Creates a high-key effect with bright tones and reduced contrast for a clean, modern look.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Luminous Blend</h3>\n                <p>Enhances luminosity with a screen-blend effect that brightens and combines both layers.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Dreamy Haze</h3>\n                <p>Creates a soft, hazy effect with bright background and enhanced foreground clarity.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Crystalline Light</h3>\n                <p>Produces a crystal-clear, bright effect with enhanced edges and colors.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Golden Hour</h3>\n                <p>Emulates warm, golden hour lighting with enhanced red and gold tones.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Airy Light</h3>\n                <p>Creates an airy, light feeling with high brightness, reduced contrast and subtle softness.</p>\n            </div>\n        </div>\n\"\"\"\n\n# Add content for each track ID\nfor id_num in sorted(set(list(mpost_by_id.keys()) + list(midjourney_by_id.keys()))):\n    if id_num not in mpost_by_id or id_num not in midjourney_by_id:\n        continue\n    \n    # Get files for this ID to extract track info\n    fg_files = mpost_by_id[id_num]\n    if not fg_files:\n        continue\n        \n    # Get track name from first file\n    track_parts = fg_files[0].split('_', 3)\n    track_name = track_parts[2] if len(track_parts) > 2 else \"Unknown\"\n    \n    html_content += f\"\"\"\n        <div class=\"track\">\n            <h2>Track {id_num}: {track_name}</h2>\n            <div class=\"overlays\">\n    \"\"\"\n    \n    # Get all overlay images for this ID\n    id_dir = os.path.join(output_dir, f\"{id_num}\")\n    if os.path.exists(id_dir):\n        overlay_files = [f for f in os.listdir(id_dir) if f.lower().endswith(('.jpg', '.png'))]\n        for overlay_file in sorted(overlay_files):\n            # Extract mode from filename\n            mode = \"Unknown\"\n            for technique_name, _ in overlay_modes:\n                if technique_name in overlay_file:\n                    mode = technique_name\n                    break\n                \n            html_content += f\"\"\"\n                <div class=\"overlay\">\n                    <h3>{mode}</h3>\n                    <img src=\"{id_num}/{overlay_file}\" alt=\"{overlay_file}\">\n                    <div class=\"info\">{overlay_file}</div>\n                </div>\n            \"\"\"\n    \n    html_content += \"\"\"\n            </div>\n        </div>\n    \"\"\"\n\nhtml_content += \"\"\"\n    </div>\n</body>\n</html>\n\"\"\"\n\nwith open(html_output, 'w') as f:\n    f.write(html_content)\n\nprint(f\"Created HTML viewer: {html_output}\")\n",
    "file_references": [
      "{fg_base}_{mode_name}_{bg_base}.jpg",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/light_overlays",
      "<!DOCTYPE html>\n<html>\n<head>\n    <title>Light Experimental Overlays</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1, h2, h3 {{ \n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1200px;\n            margin: 0 auto;\n        }}\n        .track {{\n            margin-bottom: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        .overlays {{\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));\n            gap: 20px;\n            margin-top: 20px;\n        }}\n        .overlay {{\n            background-color: #2a2a2a;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n        img {{ \n            max-width: 100%; \n            height: auto;\n            display: block;\n            border: 1px solid #333;\n        }}\n        .info {{\n            margin-top: 10px;\n            font-size: 14px;\n        }}\n        .technique-desc {{\n            margin-top: 30px;\n            padding: 20px;\n            background-color: #2a2a2a;\n            border-radius: 5px;\n        }}\n        .technique {{\n            margin-bottom: 20px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=",
      ">\n        <h1>Light Experimental Overlay Techniques</h1>\n        <p>This page shows lighter overlay effects with bright backgrounds and ethereal blending techniques.</p>\n        \n        <div class=",
      ">\n            <h2>Techniques Used</h2>\n            \n            <div class=",
      ">\n                <h3>Ethereal Glow</h3>\n                <p>Brightens the background and creates a dreamy, ethereal glow effect with enhanced foreground clarity.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Pastel Highlights</h3>\n                <p>Creates soft pastel tones with highlights and reduced contrast for a gentle, airy feeling.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>High Key Overlay</h3>\n                <p>Creates a high-key effect with bright tones and reduced contrast for a clean, modern look.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Luminous Blend</h3>\n                <p>Enhances luminosity with a screen-blend effect that brightens and combines both layers.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Dreamy Haze</h3>\n                <p>Creates a soft, hazy effect with bright background and enhanced foreground clarity.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Crystalline Light</h3>\n                <p>Produces a crystal-clear, bright effect with enhanced edges and colors.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Golden Hour</h3>\n                <p>Emulates warm, golden hour lighting with enhanced red and gold tones.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Airy Light</h3>\n                <p>Creates an airy, light feeling with high brightness, reduced contrast and subtle softness.</p>\n            </div>\n        </div>\n",
      ">\n            <h2>Track {id_num}: {track_name}</h2>\n            <div class=",
      ">\n                    <h3>{mode}</h3>\n                    <img src=",
      ">{overlay_file}</div>\n                </div>\n            ",
      "\n            </div>\n        </div>\n    ",
      "\n    </div>\n</body>\n</html>\n"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "PIL"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/console_overlay_mockup_fixed.py",
    "size": 8886,
    "lines": 226,
    "source": "#!/usr/bin/env python3\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nimport textwrap\nimport random\n\n# Create a test frame with overlay\ndef create_console_overlay(output_path, shot_id=\"FL012\", timestamp=\"02:38:34\"):\n    # Create a black canvas (simulating video frame)\n    width, height = 1280, 720\n    image = Image.new('RGB', (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Create the bottom console overlay\n    overlay_height = 160  # Height of bottom overlay\n    overlay_y = height - overlay_height\n    \n    # Draw semi-transparent black background for console\n    for y in range(overlay_y, height):\n        for x in range(width):\n            r, g, b = image.getpixel((x, y))\n            draw.point((x, y), (r//3, g//3, b//3))\n    \n    # Draw console border/frame\n    border_color = (0, 255, 255)  # Cyan\n    line_thickness = 1\n    \n    # Top horizontal line\n    draw.line([(0, overlay_y), (width, overlay_y)], fill=border_color, width=line_thickness)\n    \n    # Internal lines and markers\n    draw.line([(width//3, overlay_y), (width//3, height)], fill=border_color, width=line_thickness)\n    draw.line([(2*width//3, overlay_y), (2*width//3, height)], fill=border_color, width=line_thickness)\n    \n    # Add retro scanline effect\n    for y in range(overlay_y, height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 50), width=1)\n    \n    # Load custom font if available, otherwise use default\n    try:\n        # Try to use default system fonts\n        font_large = ImageFont.truetype(\"Arial.ttf\", 24)\n        font_mid = ImageFont.truetype(\"Arial.ttf\", 20)\n        font_small = ImageFont.truetype(\"Arial.ttf\", 16)\n    except IOError:\n        # Try another common font\n        try:\n            font_large = ImageFont.truetype(\"DejaVuSans.ttf\", 24)\n            font_mid = ImageFont.truetype(\"DejaVuSans.ttf\", 20)\n            font_small = ImageFont.truetype(\"DejaVuSans.ttf\", 16)\n        except IOError:\n            # Last resort fallback\n            font_large = ImageFont.load_default()\n            font_mid = ImageFont.load_default()\n            font_small = ImageFont.load_default()\n    \n    # Add text content\n    cyan = (0, 255, 255)\n    green = (80, 255, 80)\n    amber = (255, 191, 0)\n    \n    # Left section: Shot metadata\n    x_left = 20\n    y_text = overlay_y + 15\n    \n    draw.text((x_left, y_text), f\"SHOT: {shot_id} [{timestamp}]\", fill=cyan, font=font_large)\n    y_text += 30\n    draw.text((x_left, y_text), \"TYPE: PERCEPTION-IMAGE\", fill=green, font=font_mid)\n    y_text += 25\n    draw.text((x_left, y_text), \"FUNC: SUBJECTIVE FRAME RECALIBRATION\", fill=green, font=font_mid)\n    y_text += 25\n    draw.text((x_left, y_text), \"PATH: FL/FL012__PI__Subjective_Frame_Recalibration__Stars_ex_d33fed8b.png\", fill=(150, 150, 150), font=font_small)\n    \n    # Middle section: Poem fragment\n    x_mid = width//3 + 20\n    y_text = overlay_y + 15\n    \n    draw.text((x_mid, y_text), \"POEM: FLASHING LIGHTS\", fill=cyan, font=font_large)\n    y_text += 30\n    draw.text((x_mid, y_text), \"FRAGMENT:\", fill=amber, font=font_mid)\n    y_text += 25\n    draw.text((x_mid, y_text), '\"a concussion,\"', fill=amber, font=font_mid)\n    y_text += 25\n    draw.rectangle([(x_mid, y_text), (x_mid + 200, y_text+8)], outline=None, fill=amber)\n    \n    # Right section: Prompt\n    x_right = 2*width//3 + 20\n    y_text = overlay_y + 15\n    \n    draw.text((x_right, y_text), \"PROMPT:\", fill=cyan, font=font_large)\n    y_text += 30\n    \n    # Using ASCII hyphen instead of em-dash to avoid Unicode issues\n    prompt_text = \"Stars explode behind eyelids--fireworks seen from inside a skull\"\n    wrapped_prompt = textwrap.wrap(prompt_text, width=35)\n    for line in wrapped_prompt:\n        draw.text((x_right, y_text), line, fill=(255, 255, 255), font=font_mid)\n        y_text += 22\n    \n    # Add some digital noise/glitch effects\n    for _ in range(50):\n        glitch_x = random.randint(0, width-50)\n        glitch_y = random.randint(overlay_y, height-5)\n        glitch_width = random.randint(10, 50)\n        glitch_height = random.randint(1, 3)\n        glitch_color = random.choice([(0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128)])\n        draw.rectangle([(glitch_x, glitch_y), (glitch_x + glitch_width, glitch_y + glitch_height)], \n                      fill=glitch_color)\n    \n    # Add a timeline indicator at the very bottom\n    timeline_y = height - 10\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-4, timeline_y-4, position_x+4, timeline_y+4), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 5 if i % 3 == 0 else 3\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Add a small \"SYSTEM READY\" indicator in top corner\n    draw.text((20, 20), \"[SYSTEM READY]\", fill=cyan, font=font_small)\n    \n    # Add frame counter\n    frame_num = random.randint(1000, 9000)\n    draw.text((width-120, 20), f\"FRAME {frame_num}\", fill=green, font=font_small)\n    \n    # Save the image\n    image.save(output_path)\n    print(f\"Mockup created and saved to: {output_path}\")\n\ndef create_alternative_design(output_path, shot_id=\"FL012\", timestamp=\"02:38:34\"):\n    \"\"\"Create an alternative design with bare minimal overlay and top bar\"\"\"\n    # Create a black canvas (simulating video frame)\n    width, height = 1280, 720\n    image = Image.new('RGB', (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Use default font or try to load system font\n    try:\n        font = ImageFont.truetype(\"Arial.ttf\", 16)\n        font_small = ImageFont.truetype(\"Arial.ttf\", 12)\n    except IOError:\n        font = ImageFont.load_default()\n        font_small = ImageFont.load_default()\n    \n    # Add top bar with minimal info\n    draw.rectangle([(0, 0), (width, 30)], fill=(0, 0, 0, 180))\n    draw.line([(0, 30), (width, 30)], fill=(0, 255, 255), width=1)\n    \n    # Add text to top bar\n    cyan = (0, 255, 255)\n    amber = (255, 191, 0)\n    \n    draw.text((20, 7), f\"{shot_id}\", fill=cyan, font=font)\n    draw.text((100, 7), f\"[{timestamp}]\", fill=(200, 200, 200), font=font)\n    draw.text((220, 7), \"PERCEPTION-IMAGE\", fill=amber, font=font)\n    draw.text((400, 7), '\"a concussion,\"', fill=(255, 255, 255), font=font)\n    \n    # Add frame counter to right side\n    frame_num = random.randint(1000, 9000)\n    draw.text((width-100, 7), f\"F:{frame_num}\", fill=cyan, font=font)\n    \n    # Save the image\n    image.save(output_path)\n    print(f\"Alternative mockup created and saved to: {output_path}\")\n\ndef main():\n    output_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mockups\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create console overlay mockup\n    output_path = os.path.join(output_dir, \"console_overlay_mockup.png\")\n    create_console_overlay(output_path)\n    \n    # Create alternative minimal design\n    alt_output_path = os.path.join(output_dir, \"minimal_overlay_mockup.png\")\n    create_alternative_design(alt_output_path)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "PATH: FL/FL012__PI__Subjective_Frame_Recalibration__Stars_ex_d33fed8b.png",
      "console_overlay_mockup.png",
      "minimal_overlay_mockup.png",
      ", (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Create the bottom console overlay\n    overlay_height = 160  # Height of bottom overlay\n    overlay_y = height - overlay_height\n    \n    # Draw semi-transparent black background for console\n    for y in range(overlay_y, height):\n        for x in range(width):\n            r, g, b = image.getpixel((x, y))\n            draw.point((x, y), (r//3, g//3, b//3))\n    \n    # Draw console border/frame\n    border_color = (0, 255, 255)  # Cyan\n    line_thickness = 1\n    \n    # Top horizontal line\n    draw.line([(0, overlay_y), (width, overlay_y)], fill=border_color, width=line_thickness)\n    \n    # Internal lines and markers\n    draw.line([(width//3, overlay_y), (width//3, height)], fill=border_color, width=line_thickness)\n    draw.line([(2*width//3, overlay_y), (2*width//3, height)], fill=border_color, width=line_thickness)\n    \n    # Add retro scanline effect\n    for y in range(overlay_y, height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 50), width=1)\n    \n    # Load custom font if available, otherwise use default\n    try:\n        # Try to use default system fonts\n        font_large = ImageFont.truetype(",
      "PATH: FL/FL012__PI__Subjective_Frame_Recalibration__Stars_ex_d33fed8b.png",
      ", fill=amber, font=font_mid)\n    y_text += 25\n    draw.rectangle([(x_mid, y_text), (x_mid + 200, y_text+8)], outline=None, fill=amber)\n    \n    # Right section: Prompt\n    x_right = 2*width//3 + 20\n    y_text = overlay_y + 15\n    \n    draw.text((x_right, y_text), ",
      "\n    wrapped_prompt = textwrap.wrap(prompt_text, width=35)\n    for line in wrapped_prompt:\n        draw.text((x_right, y_text), line, fill=(255, 255, 255), font=font_mid)\n        y_text += 22\n    \n    # Add some digital noise/glitch effects\n    for _ in range(50):\n        glitch_x = random.randint(0, width-50)\n        glitch_y = random.randint(overlay_y, height-5)\n        glitch_width = random.randint(10, 50)\n        glitch_height = random.randint(1, 3)\n        glitch_color = random.choice([(0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128)])\n        draw.rectangle([(glitch_x, glitch_y), (glitch_x + glitch_width, glitch_y + glitch_height)], \n                      fill=glitch_color)\n    \n    # Add a timeline indicator at the very bottom\n    timeline_y = height - 10\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-4, timeline_y-4, position_x+4, timeline_y+4), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 5 if i % 3 == 0 else 3\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Add a small ",
      ", (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Use default font or try to load system font\n    try:\n        font = ImageFont.truetype(",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mockups"
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os",
      "textwrap",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/create_overlay_test_sheet.py",
    "size": 7565,
    "lines": 219,
    "source": "#!/usr/bin/env python3\nimport os\nimport random\nfrom PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter\n\n# Define paths\nbase_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH\"\nmidjourney_dir = os.path.join(base_dir, \"midjourney_session_2025-6-2_[0-19]\")\nmpost_dir = os.path.join(base_dir, \"mpost-journey\")\noutput_file = os.path.join(base_dir, \"overlay_test_sheet.jpg\")\n\n# Collect all image files\ndef collect_images(directory):\n    image_files = []\n    for file in os.listdir(directory):\n        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n            image_files.append(os.path.join(directory, file))\n    return image_files\n\n# Get images from both directories\nmidjourney_images = collect_images(midjourney_dir)\nmpost_images = collect_images(mpost_dir)\nall_images = midjourney_images + mpost_images\n\n# Ensure we have enough images\nif len(all_images) < 8:\n    print(f\"Not enough images found. Found {len(all_images)}, need at least 8.\")\n    exit(1)\n\n# Shuffle and select images\nrandom.shuffle(all_images)\nselected_images = all_images[:16]  # Select 16 images for our test sheet\n\n# Define dimensions for the test sheet\nimage_width, image_height = 1200, 800  # Each overlay pair\nmargin = 20\nrows = 4\ncols = 2  # Each column will be a base + overlay pair\n\n# Calculate overall dimensions\ntotal_width = cols * image_width + (cols + 1) * margin\ntotal_height = rows * image_height + (rows + 1) * margin\n\n# Create a blank canvas\nresult = Image.new('RGB', (total_width, total_height), (20, 20, 20))\ndraw = ImageDraw.Draw(result)\n\n# Try to load a font for labels\ntry:\n    font = ImageFont.truetype(\"Arial.ttf\", 30)\nexcept:\n    try:\n        font = ImageFont.truetype(\"/Library/Fonts/Arial.ttf\", 30)\n    except:\n        font = ImageFont.load_default()\n\n# Define overlay modes with descriptions\noverlay_modes = [\n    (\"Screen\", \"Lightens the image - good for creating glowing effects\"),\n    (\"Multiply\", \"Darkens the image - good for shadow effects\"),\n    (\"Overlay\", \"Increases contrast while preserving highlights and shadows\"),\n    (\"Soft Light\", \"Subtle overlay effect - gentler version of 'Overlay'\")\n]\n\n# Process each row with a different overlay mode\nfor row in range(rows):\n    # Get two base images and two overlay images for this row\n    base_img_path1 = selected_images[row * 4]\n    overlay_img_path1 = selected_images[row * 4 + 1]\n    base_img_path2 = selected_images[row * 4 + 2]\n    overlay_img_path2 = selected_images[row * 4 + 3]\n    \n    # Load and resize images\n    try:\n        base_img1 = Image.open(base_img_path1).convert(\"RGB\").resize((image_width, image_height))\n        overlay_img1 = Image.open(overlay_img_path1).convert(\"RGB\").resize((image_width, image_height))\n        base_img2 = Image.open(base_img_path2).convert(\"RGB\").resize((image_width, image_height))\n        overlay_img2 = Image.open(overlay_img_path2).convert(\"RGB\").resize((image_width, image_height))\n    except Exception as e:\n        print(f\"Error processing images for row {row+1}: {e}\")\n        continue\n    \n    # Create the composite images based on the overlay mode\n    mode_name, mode_desc = overlay_modes[row]\n    \n    # First column composite\n    composite1 = base_img1.copy()\n    if mode_name == \"Screen\":\n        # Screen blend mode\n        composite1 = Image.blend(base_img1, ImageEnhance.Brightness(overlay_img1).enhance(1.5), 0.7)\n    elif mode_name == \"Multiply\":\n        # Multiply-like effect\n        composite1 = Image.blend(base_img1, ImageEnhance.Brightness(overlay_img1).enhance(0.5), 0.7)\n    elif mode_name == \"Overlay\":\n        # Overlay-like effect\n        enhanced = ImageEnhance.Contrast(overlay_img1).enhance(1.8)\n        composite1 = Image.blend(base_img1, enhanced, 0.6)\n    elif mode_name == \"Soft Light\":\n        # Soft light-like effect\n        blurred = overlay_img1.filter(ImageFilter.GaussianBlur(radius=2))\n        composite1 = Image.blend(base_img1, blurred, 0.5)\n    \n    # Second column composite\n    composite2 = base_img2.copy()\n    if mode_name == \"Screen\":\n        composite2 = Image.blend(base_img2, ImageEnhance.Brightness(overlay_img2).enhance(1.5), 0.7)\n    elif mode_name == \"Multiply\":\n        composite2 = Image.blend(base_img2, ImageEnhance.Brightness(overlay_img2).enhance(0.5), 0.7)\n    elif mode_name == \"Overlay\":\n        enhanced = ImageEnhance.Contrast(overlay_img2).enhance(1.8)\n        composite2 = Image.blend(base_img2, enhanced, 0.6)\n    elif mode_name == \"Soft Light\":\n        blurred = overlay_img2.filter(ImageFilter.GaussianBlur(radius=2))\n        composite2 = Image.blend(base_img2, blurred, 0.5)\n    \n    # Calculate positions\n    y_pos = margin + row * (image_height + margin)\n    x_pos1 = margin\n    x_pos2 = margin + image_width + margin\n    \n    # Paste composites onto the result canvas\n    result.paste(composite1, (x_pos1, y_pos))\n    result.paste(composite2, (x_pos2, y_pos))\n    \n    # Add labels\n    label_y = y_pos + 10\n    draw.rectangle([(x_pos1, label_y), (x_pos1 + 300, label_y + 40)], fill=(0, 0, 0, 180))\n    draw.rectangle([(x_pos2, label_y), (x_pos2 + 300, label_y + 40)], fill=(0, 0, 0, 180))\n    \n    draw.text((x_pos1 + 10, label_y + 5), f\"Row {row+1}: {mode_name}\", fill=(255, 255, 255), font=font)\n    draw.text((x_pos2 + 10, label_y + 5), f\"Row {row+1}: {mode_name}\", fill=(255, 255, 255), font=font)\n\n# Save the result\nresult.save(output_file, quality=95)\nprint(f\"Overlay test sheet created and saved as: {output_file}\")\n\n# Create an HTML file to view the result\nhtml_output = os.path.join(base_dir, \"overlay_test_sheet.html\")\nhtml_content = f\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <title>Overlay Test Sheet</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1 {{ \n            text-align: center; \n            margin-bottom: 30px;\n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1400px;\n            margin: 0 auto;\n        }}\n        img {{ \n            max-width: 100%; \n            display: block; \n            margin: 0 auto;\n            border: 1px solid #333;\n        }}\n        .description {{\n            margin-top: 30px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        h2 {{\n            border-bottom: 1px solid #444;\n            padding-bottom: 10px;\n        }}\n        .mode {{\n            margin-top: 20px;\n            margin-bottom: 20px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>Overlay Test Sheet - Resurrecting Atlantis</h1>\n        <img src=\"overlay_test_sheet.jpg\" alt=\"Overlay Test Sheet\">\n        \n        <div class=\"description\">\n            <h2>Overlay Effects</h2>\n            \n            <div class=\"mode\">\n                <h3>Row 1: Screen</h3>\n                <p>{overlay_modes[0][1]}</p>\n            </div>\n            \n            <div class=\"mode\">\n                <h3>Row 2: Multiply</h3>\n                <p>{overlay_modes[1][1]}</p>\n            </div>\n            \n            <div class=\"mode\">\n                <h3>Row 3: Overlay</h3>\n                <p>{overlay_modes[2][1]}</p>\n            </div>\n            \n            <div class=\"mode\">\n                <h3>Row 4: Soft Light</h3>\n                <p>{overlay_modes[3][1]}</p>\n            </div>\n        </div>\n    </div>\n</body>\n</html>\n\"\"\"\n\nwith open(html_output, 'w') as f:\n    f.write(html_content)\n\nprint(f\"HTML viewer created and saved as: {html_output}\")\n",
    "file_references": [
      "overlay_test_sheet.jpg",
      "overlay_test_sheet.jpg",
      "/Users/gaia/resurrecting atlantis/JELLYFISH",
      "/Library/Fonts/Arial.ttf",
      "<!DOCTYPE html>\n<html>\n<head>\n    <title>Overlay Test Sheet</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1 {{ \n            text-align: center; \n            margin-bottom: 30px;\n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1400px;\n            margin: 0 auto;\n        }}\n        img {{ \n            max-width: 100%; \n            display: block; \n            margin: 0 auto;\n            border: 1px solid #333;\n        }}\n        .description {{\n            margin-top: 30px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        h2 {{\n            border-bottom: 1px solid #444;\n            padding-bottom: 10px;\n        }}\n        .mode {{\n            margin-top: 20px;\n            margin-bottom: 20px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=",
      ">\n        <h1>Overlay Test Sheet - Resurrecting Atlantis</h1>\n        <img src=",
      ">\n            <h2>Overlay Effects</h2>\n            \n            <div class=",
      ">\n                <h3>Row 1: Screen</h3>\n                <p>{overlay_modes[0][1]}</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Row 2: Multiply</h3>\n                <p>{overlay_modes[1][1]}</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Row 3: Overlay</h3>\n                <p>{overlay_modes[2][1]}</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Row 4: Soft Light</h3>\n                <p>{overlay_modes[3][1]}</p>\n            </div>\n        </div>\n    </div>\n</body>\n</html>\n"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "random",
      "PIL"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/fix_ht_assembly.py",
    "size": 3052,
    "lines": 81,
    "source": "#!/usr/bin/env python3\n\nimport json\nimport os\nfrom pathlib import Path\n\n# File paths\nassembly_file = Path(\"/Users/gaia/resurrecting atlantis/TIGER/HT/HT_assembly.json\")\nbackup_file = Path(\"/Users/gaia/resurrecting atlantis/TIGER/HT/HT_assembly_backup.json\")\nfixed_file = Path(\"/Users/gaia/resurrecting atlantis/TIGER/HT/HT_assembly_fixed.json\")\n\n# The correct poem name\nCORRECT_POEM_NAME = \"How To Win My Heart\"\n\ndef fix_ht_assembly():\n    print(f\"Reading assembly file: {assembly_file}\")\n    \n    # Create a backup first\n    if not backup_file.exists():\n        print(f\"Creating backup: {backup_file}\")\n        with open(assembly_file, 'r') as source, open(backup_file, 'w') as backup:\n            backup.write(source.read())\n    \n    # Read the original file\n    with open(assembly_file, 'r') as f:\n        entries = json.load(f)\n    \n    print(f\"Found {len(entries)} entries\")\n    \n    # Keep track of changes\n    changes_made = 0\n    \n    # Fix each entry\n    for entry in entries:\n        if 'poem' in entry and entry['poem'] != CORRECT_POEM_NAME:\n            # If the poem field doesn't match the correct name, it's likely content\n            if 'content' not in entry or not entry['content']:\n                # If content field is empty or doesn't exist, move the poem text there\n                entry['content'] = entry['poem']\n                changes_made += 1\n                print(f\"Moving text from poem to content field for ID: {entry.get('id', 'unknown')}\")\n                print(f\"  Text: {entry['poem']}\")\n            else:\n                # If content already exists, this is unusual - log it\n                print(f\"WARNING: Entry {entry.get('id', 'unknown')} has both poem={entry['poem']} and content={entry['content']}\")\n                \n            # Set the correct poem name\n            entry['poem'] = CORRECT_POEM_NAME\n    \n    # Write the fixed data to a new file\n    with open(fixed_file, 'w') as f:\n        json.dump(entries, f, indent=2)\n    \n    print(f\"\\nFixed {changes_made} entries\")\n    print(f\"Fixed file written to: {fixed_file}\")\n    \n    # Validate the fixed file\n    try:\n        with open(fixed_file, 'r') as f:\n            fixed_entries = json.load(f)\n        \n        # Check that all entries have the correct poem name\n        invalid_entries = [e for e in fixed_entries if e.get('poem') != CORRECT_POEM_NAME]\n        if invalid_entries:\n            print(f\"WARNING: {len(invalid_entries)} entries still have incorrect poem names\")\n        else:\n            print(\"Validation passed: All entries have the correct poem name\")\n        \n        # Check that all entries have content\n        missing_content = [e for e in fixed_entries if 'content' not in e or not e.get('content')]\n        if missing_content:\n            print(f\"WARNING: {len(missing_content)} entries are missing content\")\n        else:\n            print(\"Validation passed: All entries have content\")\n            \n    except Exception as e:\n        print(f\"Error validating fixed file: {e}\")\n\nif __name__ == \"__main__\":\n    fix_ht_assembly()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/TIGER/HT/HT_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER/HT/HT_assembly_backup.json",
      "/Users/gaia/resurrecting atlantis/TIGER/HT/HT_assembly_fixed.json",
      "/Users/gaia/resurrecting atlantis/TIGER/HT/HT_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER/HT/HT_assembly_backup.json",
      "/Users/gaia/resurrecting atlantis/TIGER/HT/HT_assembly_fixed.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "pathlib"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/assemble_ibex_no_audio.py",
    "size": 8227,
    "lines": 217,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport re\n\n# Directory containing video files\nIBEX_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/IBEX')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Track order specified\nTRACK_ORDER = [\n    \"01_SH_OutOfLife_000000\",\n    \"02_FL_FlashingLights_021100\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\",\n    \"04_NM_Nevermore_063300\",\n    \"05_BE_Bloodline_084400\",\n    \"06_AT_ResurrectingAtlantis_105500\",\n    \"07_DJ_DJTurnMeUp_130600\",\n    \"08_NS_NewlySingle_151700\",\n    \"09_YH_YetHeard_172800\",\n    \"10_MR_MagicRide_193900\",\n    \"12_RU_Reunion_215000\",\n    \"13_HW_HowToWinMyHeart_240100\",\n    \"14_HM_HotMinute_261200\"\n]\n\ndef get_video_duration(video_path):\n    \"\"\"Get the duration of a video file using FFprobe.\"\"\"\n    try:\n        result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        duration = float(data['format']['duration'])\n        return duration\n    except (subprocess.CalledProcessError, KeyError, json.JSONDecodeError) as e:\n        print(f\"Error getting duration for {video_path}: {e}\")\n        return 0\n\ndef find_video_files():\n    \"\"\"Find all video files in the IBEX directory.\"\"\"\n    video_files = []\n    for file in IBEX_DIR.glob('**/*.mp4'):\n        video_files.append(file)\n    return video_files\n\ndef trim_video_if_needed(video_path, output_dir):\n    \"\"\"Trim the video if it's longer than 5.5 seconds.\"\"\"\n    duration = get_video_duration(video_path)\n    \n    if duration > 5.5:\n        print(f\"Video {video_path.name} is {duration:.2f}s long - trimming to 5s\")\n        output_path = output_dir / f\"trimmed_{video_path.name}\"\n        \n        try:\n            subprocess.run([\n                'ffmpeg',\n                '-i', str(video_path),\n                '-t', '5',\n                '-c:v', 'copy',  # Use copy to avoid re-encoding\n                '-an',           # No audio\n                '-y',\n                str(output_path)\n            ], check=True, capture_output=True)\n            \n            return output_path\n        except subprocess.CalledProcessError as e:\n            print(f\"Error trimming video: {e}\")\n            return video_path\n    else:\n        return video_path\n\ndef sort_videos_by_track_order(video_files):\n    \"\"\"Sort video files according to the track order.\"\"\"\n    # Create a dictionary to map track prefix to its position in TRACK_ORDER\n    track_positions = {track: i for i, track in enumerate(TRACK_ORDER)}\n    \n    # Define a function to get the track prefix of a file\n    def get_track_prefix(file):\n        for track in TRACK_ORDER:\n            if file.name.startswith(track):\n                return track\n        return None\n    \n    # Sort the video files based on track order\n    sorted_videos = sorted(\n        video_files,\n        key=lambda file: track_positions.get(get_track_prefix(file), float('inf'))\n    )\n    \n    return sorted_videos\n\ndef assemble_videos():\n    \"\"\"Assemble all videos from IBEX directory with special handling for no audio.\"\"\"\n    print(\"Finding video files in IBEX directory...\")\n    all_videos = find_video_files()\n    print(f\"Found {len(all_videos)} video files.\")\n    \n    if not all_videos:\n        print(\"No videos found in the IBEX directory.\")\n        return\n    \n    # Sort videos according to track order\n    print(\"Sorting videos by track order...\")\n    sorted_videos = sort_videos_by_track_order(all_videos)\n    \n    # Print information about sorted videos\n    for i, video in enumerate(sorted_videos, 1):\n        duration = get_video_duration(video)\n        print(f\"{i}. {video.name} - {duration:.2f}s\")\n    \n    # Create a temp directory for processed videos\n    temp_dir = OUTPUT_DIR / \"temp_ibex\"\n    temp_dir.mkdir(exist_ok=True)\n    \n    # Process videos (trim if needed)\n    processed_videos = []\n    total_duration = 0\n    \n    for video in sorted_videos:\n        processed_video = trim_video_if_needed(video, temp_dir)\n        processed_videos.append(processed_video)\n        total_duration += get_video_duration(processed_video)\n    \n    # Create individual file list for ffmpeg\n    video_files_list = OUTPUT_DIR / \"ibex_files_list.txt\"\n    with open(video_files_list, 'w') as f:\n        for video in processed_videos:\n            f.write(f\"file '{video}'\\n\")\n    \n    # Output filename\n    output_file = OUTPUT_DIR / \"ResurrectingAtlantis_IBEX_Fixed.mp4\"\n    \n    # Use FFmpeg to concatenate videos with copy codec and no audio\n    print(f\"Concatenating videos into {output_file}...\")\n    print(f\"Expected duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n    \n    try:\n        # Method 1: Use concat demuxer with copy\n        subprocess.run([\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(video_files_list),\n            '-c:v', 'copy',   # Just copy video, don't re-encode\n            '-an',            # No audio\n            '-y',\n            str(output_file)\n        ], check=True, capture_output=True)\n        \n        # Verify the output file\n        output_duration = get_video_duration(output_file)\n        print(f\"Output file created successfully: {output_file}\")\n        print(f\"Output duration: {output_duration:.2f} seconds ({output_duration/60:.2f} minutes)\")\n        \n        if abs(output_duration - total_duration) > 1.0:\n            print(f\"Warning: Output duration ({output_duration:.2f}s) doesn't match expected ({total_duration:.2f}s)\")\n            print(\"Trying alternative method with re-encoding...\")\n            \n            # Method 2: Re-encode everything to ensure compatibility\n            output_file_2 = OUTPUT_DIR / \"ResurrectingAtlantis_IBEX_ReEncoded.mp4\"\n            subprocess.run([\n                'ffmpeg',\n                '-f', 'concat',\n                '-safe', '0',\n                '-i', str(video_files_list),\n                '-c:v', 'libx264',  # Re-encode\n                '-preset', 'fast',\n                '-an',               # No audio\n                '-y',\n                str(output_file_2)\n            ], check=True, capture_output=True)\n            \n            output_duration_2 = get_video_duration(output_file_2)\n            print(f\"Alternative output file created: {output_file_2}\")\n            print(f\"Alternative output duration: {output_duration_2:.2f} seconds\")\n        \n        # Create a detailed log\n        video_list_file = OUTPUT_DIR / \"ibex_fixed_sequence_details.txt\"\n        with open(video_list_file, 'w') as f:\n            f.write(\"RESURRECTING ATLANTIS - IBEX FIXED SEQUENCE\\n\")\n            f.write(\"=========================================\\n\\n\")\n            f.write(f\"Total videos used: {len(processed_videos)}\\n\")\n            f.write(f\"Expected duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\\n\")\n            f.write(f\"Actual output duration: {output_duration:.2f} seconds\\n\")\n            f.write(f\"Output file: {output_file}\\n\\n\")\n            f.write(\"TRACK SEQUENCE:\\n\")\n            \n            for i, video in enumerate(processed_videos, 1):\n                # Find the track ID for this video\n                track_id = next((track for track in TRACK_ORDER if video.name.startswith(track) or Path(video.name).name.startswith(track)), \"Unknown\")\n                duration = get_video_duration(video)\n                \n                # Write video details\n                f.write(f\"{i}. {track_id}\\n\")\n                f.write(f\"   File: {video.name}\\n\")\n                f.write(f\"   Duration: {duration:.2f} seconds\\n\\n\")\n                \n        print(f\"IBEX sequence details saved to {video_list_file}\")\n        \n    except subprocess.CalledProcessError as e:\n        print(\"Error during video concatenation:\")\n        print(f\"FFmpeg stdout: {e.stdout.decode()}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode()}\")\n        print(\"Make sure FFmpeg is installed and all videos are valid.\")\n\nif __name__ == \"__main__\":\n    assemble_videos()\n",
    "file_references": [
      "**/*.mp4",
      "ResurrectingAtlantis_IBEX_Fixed.mp4",
      "ResurrectingAtlantis_IBEX_ReEncoded.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA/IBEX",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence",
      "**/*.mp4",
      ")\n        output_path = output_dir / f",
      ")\n    \n    # Create a temp directory for processed videos\n    temp_dir = OUTPUT_DIR / ",
      "\n    temp_dir.mkdir(exist_ok=True)\n    \n    # Process videos (trim if needed)\n    processed_videos = []\n    total_duration = 0\n    \n    for video in sorted_videos:\n        processed_video = trim_video_if_needed(video, temp_dir)\n        processed_videos.append(processed_video)\n        total_duration += get_video_duration(processed_video)\n    \n    # Create individual file list for ffmpeg\n    video_files_list = OUTPUT_DIR / ",
      ")\n    \n    # Output filename\n    output_file = OUTPUT_DIR / ",
      "Expected duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)",
      "Output duration: {output_duration:.2f} seconds ({output_duration/60:.2f} minutes)",
      ")\n            \n            # Method 2: Re-encode everything to ensure compatibility\n            output_file_2 = OUTPUT_DIR / ",
      ")\n        \n        # Create a detailed log\n        video_list_file = OUTPUT_DIR / ",
      "Expected duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\\n"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n                'ffmpeg',\n                '-i', str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(video_files_list"
      },
      {
        "type": "run",
        "snippet": "[\n                'ffmpeg',\n                '-f', 'concat',\n                '-safe', '0',\n                '-i', str(video_files_list"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "pathlib",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/fl_video_generator_letterbox.py",
    "size": 13972,
    "lines": 320,
    "source": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport json\nimport textwrap\nfrom PIL import Image, ImageDraw, ImageFont\nimport subprocess\nimport tempfile\nimport shutil\n\nclass CodexVideoGenerator:\n    def __init__(self, codex_path, output_dir, tiger_dir):\n        self.codex_path = codex_path\n        self.output_dir = output_dir\n        self.tiger_dir = tiger_dir\n        self.temp_dir = os.path.join(output_dir, \"temp_frames\")\n        \n        # Cache for parsed codex data to avoid re-parsing\n        self.poem_entries_cache = {}\n        \n        # Ensure output directories exist\n        os.makedirs(self.output_dir, exist_ok=True)\n        os.makedirs(self.temp_dir, exist_ok=True)\n        \n        # Define dimensions for 3:2 aspect ratio\n        self.width = 1280  # Must be divisible by 2\n        self.height = 852  # 853 (from width*2/3) rounded down to ensure even number\n        \n        # Define heights for header and footer (ensure they're even)\n        self.header_height = 64  # Even number\n        self.footer_height = 160  # Much taller footer to ensure plenty of room for controls\n        \n        # Calculate total canvas height (original image + header + footer)\n        self.canvas_height = self.height + self.header_height + self.footer_height  # Will be 1000 (even number)\n        \n        # Font setup - will fall back to default if specified fonts aren't available\n        self.setup_fonts()\n        \n        # Define colors\n        self.cyan = (0, 255, 255)\n        self.amber = (255, 191, 0)\n        self.white = (255, 255, 255)\n        self.green = (80, 255, 80)\n        self.magenta = (255, 80, 255)\n        self.black = (0, 0, 0)\n        \n    def setup_fonts(self):\n        \"\"\"Setup fonts for the overlay\"\"\"\n        try:\n            self.header_font = ImageFont.truetype(\"Arial.ttf\", 14)\n            self.footer_font = ImageFont.truetype(\"Arial.ttf\", 18)\n            self.title_font = ImageFont.truetype(\"Arial.ttf\", 16)\n        except IOError:\n            try:\n                self.header_font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n                self.footer_font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n                self.title_font = ImageFont.truetype(\"DejaVuSans.ttf\", 16)\n            except IOError:\n                self.header_font = ImageFont.load_default()\n                self.footer_font = ImageFont.load_default()\n                self.title_font = ImageFont.load_default()\n    \n    def parse_codex(self, prefix=\"FL\"):\n        \"\"\"Parse codex data for entries with specified prefix, using cache if available\"\"\"\n        # Return cached entries if available\n        if prefix in self.poem_entries_cache:\n            return self.poem_entries_cache[prefix]\n        \n        print(f\"Parsing codex data for {prefix} entries...\")\n        entries = []\n        \n        with open(self.codex_path, 'r') as f:\n            content = f.read()\n        \n        # Find all entries with the specified prefix\n        pattern = r'### ' + prefix + r'(\\d+) \\[([\\d:]+)\\]\\s*\\*\\*Image:\\*\\* `([^`]+)`\\s*\\*\\*Assembly Source:\\*\\*\\s*```json\\s*({[\\s\\S]*?})\\s*```\\s*\\*\\*Prompt:\\*\\* (.*?)(?=\\n---|\\Z)'\n        matches = re.findall(pattern, content, re.DOTALL)\n        \n        for match in matches:\n            entry_id = prefix + match[0]\n            timestamp = match[1]\n            image_path = match[2]\n            assembly_json = match[3]\n            prompt = match[4].strip()\n            \n            # Parse the assembly JSON\n            try:\n                assembly_data = json.loads(assembly_json)\n            except json.JSONDecodeError:\n                print(f\"Error parsing JSON for {entry_id}\")\n                continue\n            \n            entries.append({\n                'id': entry_id,\n                'timestamp': timestamp,\n                'image_path': image_path,\n                'assembly': assembly_data,\n                'prompt': prompt\n            })\n        \n        # Store in cache\n        self.poem_entries_cache[prefix] = entries\n        print(f\"Found {len(entries)} entries for {prefix}\")\n        return entries\n    \n    def apply_overlay_letterbox(self, image_path, entry, output_path):\n        \"\"\"Apply the codex overlay using letterboxing to preserve the original image\"\"\"\n        # Load the image\n        try:\n            abs_image_path = os.path.join(self.tiger_dir, image_path)\n            if not os.path.exists(abs_image_path):\n                print(f\"Image not found: {abs_image_path}\")\n                return None\n            \n            original_image = Image.open(abs_image_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error opening image {image_path}: {e}\")\n            return None\n        \n        # Get original dimensions\n        orig_width, orig_height = original_image.size\n        \n        # Create a new canvas with 3:2 aspect ratio plus space for header/footer\n        canvas = Image.new('RGB', (self.width, self.canvas_height), self.black)\n        \n        # Resize original to match our target width while maintaining aspect ratio\n        if orig_width != self.width or orig_height != self.height:\n            # Calculate new dimensions, preserving aspect ratio\n            scale = min(self.width / orig_width, self.height / orig_height)\n            new_width = int(orig_width * scale)\n            new_height = int(orig_height * scale)\n            resized_image = original_image.resize((new_width, new_height), Image.LANCZOS)\n            \n            # Calculate position to center the image\n            left = (self.width - new_width) // 2\n            top = self.header_height + (self.height - new_height) // 2\n            \n            # Paste the resized image\n            canvas.paste(resized_image, (left, top))\n        else:\n            # Paste at exact position if already right size\n            canvas.paste(original_image, (0, self.header_height))\n        \n        draw = ImageDraw.Draw(canvas)\n        \n        # Extract data from entry\n        shot_id = entry['id']\n        timestamp = entry['timestamp']\n        assembly = entry['assembly']\n        \n        # Draw header background\n        draw.rectangle([(0, 0), (self.width, self.header_height)], fill=self.black)\n        draw.line([(0, self.header_height), (self.width, self.header_height)], fill=self.cyan, width=1)\n        \n        # Add scanlines to header\n        for y in range(0, self.header_height, 2):\n            draw.line([(0, y), (self.width, y)], fill=(0, 0, 0, 70), width=1)\n        \n        # Define left padding for alignment\n        padding = 20\n        \n        # Add text to header - first line\n        draw.text((padding, 9), f\"{shot_id}\", fill=self.cyan, font=self.title_font)\n        draw.text((padding + 70, 9), f\"[{timestamp}]\", fill=self.white, font=self.header_font)\n        draw.text((padding + 170, 9), f\"{assembly.get('poem', '')}\", fill=self.amber, font=self.title_font)\n        draw.text((padding + 320, 9), f\"\\\"{assembly.get('content', '')}\\\"\", fill=self.white, font=self.header_font)\n        \n        # Get poem prefix (first 2 chars of shot ID)\n        poem_prefix = shot_id[:2]\n        # Parse shot number from ID\n        shot_num = int(shot_id[2:])\n        # Get total frames for this poem\n        total_frames = len(self.parse_codex(prefix=poem_prefix))\n        # Format as current/total\n        frame_text = f\"F:{shot_num}/{total_frames}\"\n        draw.text((self.width-110, 9), frame_text, fill=self.cyan, font=self.header_font)\n        \n        # Second line - TYPE and FUNC left aligned, SYNT right aligned\n        draw.text((padding, 38), f\"TYPE: {assembly.get('imageType', '')}\", fill=self.green, font=self.header_font)\n        draw.text((padding + 240, 38), f\"FUNC: {assembly.get('cineosisFunction', '')}\", fill=self.green, font=self.header_font)\n        \n        # SYNT on the right side with different color\n        synt_text = f\"SYNT: {assembly.get('syntagmaType', '')}\"\n        synt_width = draw.textlength(synt_text, font=self.header_font)\n        draw.text((self.width - padding - synt_width - 90, 38), synt_text, fill=self.magenta, font=self.header_font)\n        \n        # Draw footer background\n        footer_y = self.header_height + self.height\n        draw.rectangle([(0, footer_y), (self.width, self.canvas_height)], fill=self.black)\n        draw.line([(0, footer_y), (self.width, footer_y)], fill=self.cyan, width=1)\n        \n        # Add scanline effect to footer\n        for y in range(footer_y, self.canvas_height, 2):\n            draw.line([(0, y), (self.width, y)], fill=(0, 0, 0, 70), width=1)\n        \n        # Add prompt text to footer - position at the very top of the footer\n        prompt_text = f\"PROMPT: {entry['prompt']}\"\n        \n        # Wrap the prompt text across multiple lines if needed\n        wrapped_prompt = textwrap.wrap(prompt_text, width=110)\n        # Position text right at the top edge of the footer\n        y_text = footer_y + 2\n        \n        # Add a small label for the prompt\n        draw.text((padding, y_text), \"PROMPT:\", fill=self.cyan, font=self.title_font)\n        \n        # Print the wrapped prompt text with adjusted spacing\n        for i, line in enumerate(wrapped_prompt):\n            if i == 0:\n                # For the first line, skip the \"PROMPT:\" label that we manually added\n                if line.startswith(\"PROMPT: \"):\n                    line = line[8:]\n                draw.text((padding + 80, y_text), line, fill=self.white, font=self.footer_font)\n            else:\n                # Reduce line spacing slightly (was 22, now 18) to compress text upward\n                draw.text((padding, y_text + (i * 18)), line, fill=self.white, font=self.footer_font)\n        \n        # Add a timeline indicator higher up, leaving bottom area clear for player controls\n        # Position timeline much higher - 80px from the bottom of the frame\n        timeline_y = self.canvas_height - 80\n        draw.line([(20, timeline_y), (self.width-20, timeline_y)], fill=(100, 100, 100), width=1)\n        \n        # Current position marker (30% through)\n        position_x = 20 + (self.width-40) * 0.3\n        draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=self.cyan)\n        \n        # Draw vertical timestamp markers\n        for i in range(10):\n            marker_x = 20 + (self.width-40) * (i/9)\n            marker_height = 4 if i % 3 == 0 else 2\n            draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n        \n        # Save the image with overlay\n        canvas.save(output_path)\n        return output_path\n\n    def generate_video(self, prefix=\"FL\", duration_per_frame=3, max_frames=None):\n        \"\"\"Generate a video from parsed codex entries\"\"\"\n        entries = self.parse_codex(prefix)\n        \n        if max_frames:\n            entries = entries[:max_frames]\n        \n        if not entries:\n            print(f\"No entries found for {prefix}\")\n            return None\n        \n        # Clear temp directory\n        for f in os.listdir(self.temp_dir):\n            os.remove(os.path.join(self.temp_dir, f))\n        \n        # Process each frame\n        frame_paths = []\n        for i, entry in enumerate(entries):\n            print(f\"Processing frame {i+1}/{len(entries)}: {entry['id']}\")\n            output_path = os.path.join(self.temp_dir, f\"{i:04d}.png\")\n            result = self.apply_overlay_letterbox(entry['image_path'], entry, output_path)\n            if result:\n                frame_paths.append(result)\n        \n        if not frame_paths:\n            print(\"No frames were generated successfully\")\n            return None\n        \n        # Create a video from the frames using ffmpeg\n        output_video = os.path.join(self.output_dir, f\"{prefix}_codex_letterbox.mp4\")\n        try:\n            cmd = [\n                'ffmpeg',\n                '-y',  # Overwrite output file if it exists\n                '-framerate', f'1/{duration_per_frame}',  # Each frame lasts for duration_per_frame seconds\n                '-i', os.path.join(self.temp_dir, '%04d.png'),\n                '-c:v', 'libx264',\n                '-pix_fmt', 'yuv420p',\n                '-crf', '23',  # Higher quality\n                output_video\n            ]\n            print(f\"Running command: {' '.join(cmd)}\")\n            subprocess.run(cmd, check=True)\n            print(f\"Video generated: {output_video}\")\n            return output_video\n        except subprocess.CalledProcessError as e:\n            print(f\"Error generating video: {e}\")\n            return None\n        except Exception as e:\n            print(f\"Unexpected error: {e}\")\n            return None\n\ndef main():\n    import argparse\n    \n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description=\"Generate video with codex overlay\")\n    parser.add_argument(\"--prefix\", type=str, default=\"FL\", help=\"Poem prefix (FL, SH, BE, HT, NM)\")\n    parser.add_argument(\"--duration\", type=int, default=3, help=\"Duration per frame in seconds\")\n    parser.add_argument(\"--max-frames\", type=int, help=\"Maximum number of frames to process\")\n    parser.add_argument(\"--test\", action=\"store_true\", help=\"Generate test video with only 10 frames\")\n    args = parser.parse_args()\n    \n    # Set up paths\n    codex_path = \"/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_Codex.md\"\n    tiger_dir = \"/Users/gaia/resurrecting atlantis/TIGER\"\n    output_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/video_output\"\n    \n    # Create the generator\n    generator = CodexVideoGenerator(codex_path, output_dir, tiger_dir)\n    \n    # Determine max frames\n    max_frames = args.max_frames\n    if args.test:\n        max_frames = 10\n    \n    # Generate video\n    print(f\"Generating video for {args.prefix} with {max_frames if max_frames else 'all'} frames, {args.duration}s per frame\")\n    generator.generate_video(prefix=args.prefix, duration_per_frame=args.duration, max_frames=max_frames)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "{i:04d}.png",
      "{prefix}_codex_letterbox.mp4",
      "%04d.png",
      ")\n        \n        # Cache for parsed codex data to avoid re-parsing\n        self.poem_entries_cache = {}\n        \n        # Ensure output directories exist\n        os.makedirs(self.output_dir, exist_ok=True)\n        os.makedirs(self.temp_dir, exist_ok=True)\n        \n        # Define dimensions for 3:2 aspect ratio\n        self.width = 1280  # Must be divisible by 2\n        self.height = 852  # 853 (from width*2/3) rounded down to ensure even number\n        \n        # Define heights for header and footer (ensure they",
      ")\n            return None\n        \n        # Get original dimensions\n        orig_width, orig_height = original_image.size\n        \n        # Create a new canvas with 3:2 aspect ratio plus space for header/footer\n        canvas = Image.new(",
      ", (self.width, self.canvas_height), self.black)\n        \n        # Resize original to match our target width while maintaining aspect ratio\n        if orig_width != self.width or orig_height != self.height:\n            # Calculate new dimensions, preserving aspect ratio\n            scale = min(self.width / orig_width, self.height / orig_height)\n            new_width = int(orig_width * scale)\n            new_height = int(orig_height * scale)\n            resized_image = original_image.resize((new_width, new_height), Image.LANCZOS)\n            \n            # Calculate position to center the image\n            left = (self.width - new_width) // 2\n            top = self.header_height + (self.height - new_height) // 2\n            \n            # Paste the resized image\n            canvas.paste(resized_image, (left, top))\n        else:\n            # Paste at exact position if already right size\n            canvas.paste(original_image, (0, self.header_height))\n        \n        draw = ImageDraw.Draw(canvas)\n        \n        # Extract data from entry\n        shot_id = entry[",
      ", fill=self.white, font=self.header_font)\n        \n        # Get poem prefix (first 2 chars of shot ID)\n        poem_prefix = shot_id[:2]\n        # Parse shot number from ID\n        shot_num = int(shot_id[2:])\n        # Get total frames for this poem\n        total_frames = len(self.parse_codex(prefix=poem_prefix))\n        # Format as current/total\n        frame_text = f",
      "):\n                    line = line[8:]\n                draw.text((padding + 80, y_text), line, fill=self.white, font=self.footer_font)\n            else:\n                # Reduce line spacing slightly (was 22, now 18) to compress text upward\n                draw.text((padding, y_text + (i * 18)), line, fill=self.white, font=self.footer_font)\n        \n        # Add a timeline indicator higher up, leaving bottom area clear for player controls\n        # Position timeline much higher - 80px from the bottom of the frame\n        timeline_y = self.canvas_height - 80\n        draw.line([(20, timeline_y), (self.width-20, timeline_y)], fill=(100, 100, 100), width=1)\n        \n        # Current position marker (30% through)\n        position_x = 20 + (self.width-40) * 0.3\n        draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=self.cyan)\n        \n        # Draw vertical timestamp markers\n        for i in range(10):\n            marker_x = 20 + (self.width-40) * (i/9)\n            marker_height = 4 if i % 3 == 0 else 2\n            draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n        \n        # Save the image with overlay\n        canvas.save(output_path)\n        return output_path\n\n    def generate_video(self, prefix=",
      "Processing frame {i+1}/{len(entries)}: {entry[",
      "1/{duration_per_frame}",
      "/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_Codex.md",
      "/Users/gaia/resurrecting atlantis/TIGER",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/video_output"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd, check=True"
      }
    ],
    "imports": [
      "os",
      "re",
      "json",
      "textwrap",
      "PIL",
      "subprocess",
      "tempfile",
      "shutil"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/create_final_overlay_sheet.py",
    "size": 9520,
    "lines": 273,
    "source": "#!/usr/bin/env python3\nimport os\nfrom PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter\n\n# Define paths\nbase_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH\"\nmpost_dir = os.path.join(base_dir, \"mpost-journey\")\nmidjourney_dir = os.path.join(base_dir, \"midjourney_session_2025-6-2_[0-19]\")\noutput_file = os.path.join(base_dir, \"final_overlay_test_sheet.jpg\")\n\n# Get all renamed files (they should be sorted by track number already)\ndef get_track_images(directory, prefix=\"\"):\n    image_files = []\n    for file in os.listdir(directory):\n        if file.lower().endswith('.png') and not os.path.isdir(os.path.join(directory, file)):\n            if not file.startswith(\"original_\") and not \"backup\" in file:\n                image_files.append(os.path.join(directory, file))\n    return sorted(image_files)  # Sort by filename (which starts with track number)\n\n# Get all images\nfg_images = get_track_images(mpost_dir)\nbg_images = get_track_images(midjourney_dir)\n\nprint(f\"Found {len(fg_images)} foreground images\")\nprint(f\"Found {len(bg_images)} background images\")\n\n# Define dimensions for the test sheet\nimage_width, image_height = 600, 400  # Each overlay pair\nmargin = 20\nrows = 4\ncols = 2  # Each column is a different overlay effect with the same base images\n\n# Calculate overall dimensions\ntotal_width = cols * image_width + (cols + 1) * margin\ntotal_height = rows * image_height + (rows + 1) * margin\n\n# Create a blank canvas\nresult = Image.new('RGB', (total_width, total_height), (15, 15, 20))\ndraw = ImageDraw.Draw(result)\n\n# Try to load a font for labels\ntry:\n    font = ImageFont.truetype(\"Arial.ttf\", 24)\n    small_font = ImageFont.truetype(\"Arial.ttf\", 16)\nexcept:\n    try:\n        font = ImageFont.truetype(\"/Library/Fonts/Arial.ttf\", 24)\n        small_font = ImageFont.truetype(\"/Library/Fonts/Arial.ttf\", 16)\n    except:\n        font = ImageFont.load_default()\n        small_font = ImageFont.load_default()\n\n# Define overlay modes with descriptions\noverlay_modes = [\n    (\"Screen\", \"Lightens the image - good for creating glowing effects\"),\n    (\"Multiply\", \"Darkens the image - good for shadow effects\"),\n    (\"Overlay\", \"Increases contrast while preserving highlights and shadows\"),\n    (\"Soft Light\", \"Subtle overlay effect - gentler version of 'Overlay'\")\n]\n\n# Select track numbers to showcase (4 different tracks for the 4 rows)\n# We'll try to pick tracks that have both foreground and background images\ntrack_pairs = []\n\n# Function to extract track prefix from filename\ndef get_track_prefix(filepath):\n    filename = os.path.basename(filepath)\n    parts = filename.split('_')\n    if len(parts) >= 2 and parts[0].isdigit():\n        return parts[0] + \"_\" + parts[1]\n    return None\n\n# Group images by track prefix\nfg_by_track = {}\nbg_by_track = {}\n\nfor img in fg_images:\n    prefix = get_track_prefix(img)\n    if prefix:\n        if prefix not in fg_by_track:\n            fg_by_track[prefix] = []\n        fg_by_track[prefix].append(img)\n\nfor img in bg_images:\n    prefix = get_track_prefix(img)\n    if prefix:\n        if prefix not in bg_by_track:\n            bg_by_track[prefix] = []\n        bg_by_track[prefix].append(img)\n\n# Find tracks that have both foreground and background images\ncommon_tracks = []\nfor track in fg_by_track.keys():\n    if track in bg_by_track:\n        common_tracks.append(track)\n\n# If we don't have enough common tracks, add some that have at least one type\nall_tracks = list(set(list(fg_by_track.keys()) + list(bg_by_track.keys())))\nall_tracks.sort()  # Sort by track number\n\ntracks_to_use = common_tracks[:4] if len(common_tracks) >= 4 else all_tracks[:4]\n\nprint(f\"Using tracks: {tracks_to_use}\")\n\n# Process each row with a different overlay mode\nfor row, track_prefix in enumerate(tracks_to_use):\n    if row >= rows:\n        break\n        \n    # Get foreground and background images for this track\n    fg_img_path = fg_by_track.get(track_prefix, [None])[0]\n    bg_img_path = bg_by_track.get(track_prefix, [None])[0]\n    \n    # If we don't have both for this track, pick from available images\n    if not fg_img_path and fg_images:\n        fg_img_path = fg_images[0]\n    if not bg_img_path and bg_images:\n        bg_img_path = bg_images[0]\n    \n    # If we still don't have images, skip this row\n    if not fg_img_path or not bg_img_path:\n        print(f\"Not enough images for row {row+1}, skipping\")\n        continue\n    \n    # Extract track names for labels\n    track_name = os.path.basename(fg_img_path).split('_', 3)[2] if fg_img_path else \"Unknown\"\n    \n    # Load and resize images\n    try:\n        fg_img = Image.open(fg_img_path).convert(\"RGB\").resize((image_width, image_height))\n        bg_img = Image.open(bg_img_path).convert(\"RGB\").resize((image_width, image_height))\n    except Exception as e:\n        print(f\"Error processing images for row {row+1}: {e}\")\n        continue\n    \n    # Create the composite images based on the overlay mode\n    mode_name, mode_desc = overlay_modes[row]\n    \n    # First column - Screen/Multiply\n    composite1 = fg_img.copy()\n    if row % 2 == 0:  # Screen effect\n        composite1 = Image.blend(bg_img, ImageEnhance.Brightness(fg_img).enhance(1.5), 0.7)\n    else:  # Multiply effect\n        composite1 = Image.blend(bg_img, ImageEnhance.Brightness(fg_img).enhance(0.5), 0.7)\n    \n    # Second column - Overlay/Soft Light\n    composite2 = fg_img.copy()\n    if row % 2 == 0:  # Overlay effect\n        enhanced = ImageEnhance.Contrast(fg_img).enhance(1.8)\n        composite2 = Image.blend(bg_img, enhanced, 0.6)\n    else:  # Soft light effect\n        blurred = fg_img.filter(ImageFilter.GaussianBlur(radius=2))\n        composite2 = Image.blend(bg_img, blurred, 0.5)\n    \n    # Calculate positions\n    y_pos = margin + row * (image_height + margin)\n    x_pos1 = margin\n    x_pos2 = margin + image_width + margin\n    \n    # Paste images onto the result canvas\n    result.paste(composite1, (x_pos1, y_pos))\n    result.paste(composite2, (x_pos2, y_pos))\n    \n    # Add labels\n    # First column - effect label\n    draw.rectangle([(x_pos1, y_pos + 10), (x_pos1 + 350, y_pos + 70)], fill=(0, 0, 0, 180))\n    draw.text((x_pos1 + 10, y_pos + 15), f\"{track_name} - {overlay_modes[row][0]}\", fill=(255, 255, 255), font=font)\n    draw.text((x_pos1 + 10, y_pos + 45), f\"{os.path.basename(fg_img_path).split('_')[0]}\", fill=(220, 220, 220), font=small_font)\n    \n    # Second column - alternate effect label\n    draw.rectangle([(x_pos2, y_pos + 10), (x_pos2 + 350, y_pos + 70)], fill=(0, 0, 0, 180))\n    draw.text((x_pos2 + 10, y_pos + 15), f\"{track_name} - {overlay_modes[(row+2)%4][0]}\", fill=(255, 255, 255), font=font)\n    draw.text((x_pos2 + 10, y_pos + 45), f\"{os.path.basename(fg_img_path).split('_')[0]}\", fill=(220, 220, 220), font=small_font)\n\n# Save the result\nresult.save(output_file, quality=95)\nprint(f\"Overlay test sheet created and saved as: {output_file}\")\n\n# Create an HTML file to view the result\nhtml_output = os.path.join(base_dir, \"final_overlay_test_sheet.html\")\nhtml_content = f\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <title>Final Overlay Test Sheet</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1 {{ \n            text-align: center; \n            margin-bottom: 30px;\n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1400px;\n            margin: 0 auto;\n        }}\n        img {{ \n            max-width: 100%; \n            display: block; \n            margin: 0 auto;\n            border: 1px solid #333;\n        }}\n        .description {{\n            margin-top: 30px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        h2 {{\n            border-bottom: 1px solid #444;\n            padding-bottom: 10px;\n        }}\n        .mode {{\n            margin-top: 20px;\n            margin-bottom: 20px;\n        }}\n        .tracks {{\n            margin-top: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>Final Overlay Test Sheet - Resurrecting Atlantis</h1>\n        <img src=\"final_overlay_test_sheet.jpg\" alt=\"Final Overlay Test Sheet\">\n        \n        <div class=\"description\">\n            <h2>Overlay Effects Used</h2>\n            \n            <div class=\"mode\">\n                <h3>Screen</h3>\n                <p>{overlay_modes[0][1]}</p>\n            </div>\n            \n            <div class=\"mode\">\n                <h3>Multiply</h3>\n                <p>{overlay_modes[1][1]}</p>\n            </div>\n            \n            <div class=\"mode\">\n                <h3>Overlay</h3>\n                <p>{overlay_modes[2][1]}</p>\n            </div>\n            \n            <div class=\"mode\">\n                <h3>Soft Light</h3>\n                <p>{overlay_modes[3][1]}</p>\n            </div>\n        </div>\n        \n        <div class=\"tracks\">\n            <h2>Track Organization</h2>\n            <p>The images have been organized according to the track list, with each row showing different overlay effects for the same track.</p>\n            <p>Each cell uses a foreground image overlaid on a background image with the indicated effect.</p>\n        </div>\n    </div>\n</body>\n</html>\n\"\"\"\n\nwith open(html_output, 'w') as f:\n    f.write(html_content)\n\nprint(f\"HTML viewer created and saved as: {html_output}\")\n",
    "file_references": [
      "final_overlay_test_sheet.jpg",
      "final_overlay_test_sheet.jpg",
      "/Users/gaia/resurrecting atlantis/JELLYFISH",
      "/Library/Fonts/Arial.ttf",
      "/Library/Fonts/Arial.ttf",
      ")\n        continue\n    \n    # Create the composite images based on the overlay mode\n    mode_name, mode_desc = overlay_modes[row]\n    \n    # First column - Screen/Multiply\n    composite1 = fg_img.copy()\n    if row % 2 == 0:  # Screen effect\n        composite1 = Image.blend(bg_img, ImageEnhance.Brightness(fg_img).enhance(1.5), 0.7)\n    else:  # Multiply effect\n        composite1 = Image.blend(bg_img, ImageEnhance.Brightness(fg_img).enhance(0.5), 0.7)\n    \n    # Second column - Overlay/Soft Light\n    composite2 = fg_img.copy()\n    if row % 2 == 0:  # Overlay effect\n        enhanced = ImageEnhance.Contrast(fg_img).enhance(1.8)\n        composite2 = Image.blend(bg_img, enhanced, 0.6)\n    else:  # Soft light effect\n        blurred = fg_img.filter(ImageFilter.GaussianBlur(radius=2))\n        composite2 = Image.blend(bg_img, blurred, 0.5)\n    \n    # Calculate positions\n    y_pos = margin + row * (image_height + margin)\n    x_pos1 = margin\n    x_pos2 = margin + image_width + margin\n    \n    # Paste images onto the result canvas\n    result.paste(composite1, (x_pos1, y_pos))\n    result.paste(composite2, (x_pos2, y_pos))\n    \n    # Add labels\n    # First column - effect label\n    draw.rectangle([(x_pos1, y_pos + 10), (x_pos1 + 350, y_pos + 70)], fill=(0, 0, 0, 180))\n    draw.text((x_pos1 + 10, y_pos + 15), f",
      "<!DOCTYPE html>\n<html>\n<head>\n    <title>Final Overlay Test Sheet</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1 {{ \n            text-align: center; \n            margin-bottom: 30px;\n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1400px;\n            margin: 0 auto;\n        }}\n        img {{ \n            max-width: 100%; \n            display: block; \n            margin: 0 auto;\n            border: 1px solid #333;\n        }}\n        .description {{\n            margin-top: 30px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        h2 {{\n            border-bottom: 1px solid #444;\n            padding-bottom: 10px;\n        }}\n        .mode {{\n            margin-top: 20px;\n            margin-bottom: 20px;\n        }}\n        .tracks {{\n            margin-top: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=",
      ">\n        <h1>Final Overlay Test Sheet - Resurrecting Atlantis</h1>\n        <img src=",
      ">\n            <h2>Overlay Effects Used</h2>\n            \n            <div class=",
      ">\n                <h3>Screen</h3>\n                <p>{overlay_modes[0][1]}</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Multiply</h3>\n                <p>{overlay_modes[1][1]}</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Overlay</h3>\n                <p>{overlay_modes[2][1]}</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Soft Light</h3>\n                <p>{overlay_modes[3][1]}</p>\n            </div>\n        </div>\n        \n        <div class=",
      ">\n            <h2>Track Organization</h2>\n            <p>The images have been organized according to the track list, with each row showing different overlay effects for the same track.</p>\n            <p>Each cell uses a foreground image overlaid on a background image with the indicated effect.</p>\n        </div>\n    </div>\n</body>\n</html>\n"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "PIL"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/clean_mpre_run_titles.py",
    "size": 1318,
    "lines": 38,
    "source": "#!/usr/bin/env python3\nimport os\nimport shutil\nfrom pathlib import Path\n\n# Path to the mpre-run directory\nmpre_run_dir = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run')\n\n# Create a backup directory\nbackup_dir = mpre_run_dir / 'backup_titles'\nbackup_dir.mkdir(exist_ok=True)\n\n# Process all files in the directory\nfor file_path in mpre_run_dir.glob('*'):\n    if file_path.is_file() and file_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n        # Skip backup directories\n        if 'backup' in file_path.name:\n            continue\n            \n        # Create backup\n        backup_file = backup_dir / file_path.name\n        shutil.copy2(file_path, backup_file)\n        \n        # Create new name by removing \"Film Title \" and \"Segment Title \"\n        new_name = file_path.name\n        new_name = new_name.replace('Film Title ', '')\n        new_name = new_name.replace('Segment Title ', '')\n        \n        # If the name was changed, rename the file\n        if new_name != file_path.name:\n            new_file_path = file_path.parent / new_name\n            os.rename(file_path, new_file_path)\n            print(f\"Renamed: {file_path.name} -> {new_name}\")\n        else:\n            print(f\"No changes needed for: {file_path.name}\")\n\nprint(f\"\\nAll files processed. Backup copies saved in {backup_dir}\")\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run",
      " in file_path.name:\n            continue\n            \n        # Create backup\n        backup_file = backup_dir / file_path.name\n        shutil.copy2(file_path, backup_file)\n        \n        # Create new name by removing ",
      ")\n        \n        # If the name was changed, rename the file\n        if new_name != file_path.name:\n            new_file_path = file_path.parent / new_name\n            os.rename(file_path, new_file_path)\n            print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "shutil",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/assemble_impala_sequence.py",
    "size": 5891,
    "lines": 158,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nfrom pathlib import Path\nimport re\n\n# Directory containing video files\nIMPALA_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Track order specified\nTRACK_ORDER = [\n    \"01_SH_OutOfLife_000000\",\n    \"02_FL_FlashingLights_021100\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\",\n    \"04_NM_Nevermore_063300\",\n    \"05_BE_Bloodline_084400\",\n    \"06_AT_ResurrectingAtlantis_105500\",\n    \"07_DJ_DJTurnMeUp_130600\",\n    \"08_NS_NewlySingle_151700\",\n    \"09_YH_YetHeard_172800\",\n    \"10_MR_MagicRide_193900\",\n    \"12_RU_Reunion_215000\",\n    \"13_HW_HowToWinMyHeart_240100\",\n    \"14_HM_HotMinute_261200\"  # Added this track if available\n]\n\n# Track title mapping for better output naming\nTRACK_TITLE_MAPPING = {\n    \"01_SH_OutOfLife_000000\": \"OutOfLife\",\n    \"02_FL_FlashingLights_021100\": \"FlashingLights\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\": \"HowToBreakOffAnEngagement\",\n    \"04_NM_Nevermore_063300\": \"Nevermore\",\n    \"05_BE_Bloodline_084400\": \"Bloodline\",\n    \"06_AT_ResurrectingAtlantis_105500\": \"ResurrectingAtlantis\",\n    \"07_DJ_DJTurnMeUp_130600\": \"DJTurnMeUp\",\n    \"08_NS_NewlySingle_151700\": \"NewlySingle\",\n    \"09_YH_YetHeard_172800\": \"YetHeard\",\n    \"10_MR_MagicRide_193900\": \"MagicRide\",\n    \"12_RU_Reunion_215000\": \"Reunion\",\n    \"13_HW_HowToWinMyHeart_240100\": \"HowToWinMyHeart\",\n    \"14_HM_HotMinute_261200\": \"HotMinute\"\n}\n\ndef find_video_files():\n    \"\"\"Find all video files in the IMPALA directory.\"\"\"\n    video_files = []\n    for file in IMPALA_DIR.glob('**/*.mp4'):\n        if 'original_files_backup' not in str(file) and 'final_sequence' not in str(file):\n            video_files.append(file)\n    return video_files\n\ndef group_videos_by_track(video_files):\n    \"\"\"Group video files by track ID.\"\"\"\n    track_videos = {track_id: [] for track_id in TRACK_ORDER}\n    \n    for video_file in video_files:\n        for track_id in TRACK_ORDER:\n            if video_file.name.startswith(track_id):\n                track_videos[track_id].append(video_file)\n                break\n    \n    # Sort videos within each track (optional - for consistency)\n    for track_id in track_videos:\n        track_videos[track_id].sort(key=lambda x: x.name)\n    \n    return track_videos\n\ndef create_concatenation_file(videos, concat_file_path):\n    \"\"\"Create a concatenation file for FFmpeg.\"\"\"\n    with open(concat_file_path, 'w') as f:\n        for video in videos:\n            # Escape single quotes in the path\n            escaped_path = str(video).replace(\"'\", \"'\\\\''\")\n            f.write(f\"file '{escaped_path}'\\n\")\n\ndef assemble_videos():\n    \"\"\"Assemble all videos in sequence according to track order.\"\"\"\n    print(\"Finding video files...\")\n    all_videos = find_video_files()\n    print(f\"Found {len(all_videos)} video files.\")\n    \n    print(\"Grouping videos by track...\")\n    track_videos = group_videos_by_track(all_videos)\n    \n    # Create list of videos in the specified order\n    ordered_videos = []\n    for track_id in TRACK_ORDER:\n        videos = track_videos[track_id]\n        if videos:\n            print(f\"Track {track_id}: {len(videos)} videos\")\n            ordered_videos.extend(videos)\n        else:\n            print(f\"Warning: No videos found for track {track_id}\")\n    \n    if not ordered_videos:\n        print(\"Error: No videos found in the specified tracks.\")\n        return\n    \n    # Create a concat file for FFmpeg\n    concat_file = OUTPUT_DIR / \"concat_list.txt\"\n    print(f\"Creating concatenation file at {concat_file}...\")\n    create_concatenation_file(ordered_videos, concat_file)\n    \n    # Output filename\n    output_file = OUTPUT_DIR / \"ResurrectingAtlantis_FullSequence.mp4\"\n    \n    # Use FFmpeg to concatenate the videos\n    print(f\"Concatenating videos into {output_file}...\")\n    try:\n        result = subprocess.run([\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file),\n            '-c', 'copy',\n            '-y',  # Overwrite output file if it exists\n            str(output_file)\n        ], check=True, capture_output=True)\n        \n        print(\"Video concatenation completed successfully!\")\n        print(f\"Output file: {output_file}\")\n        \n        # Create a text file with the list of videos used\n        video_list_file = OUTPUT_DIR / \"video_sequence_details.txt\"\n        with open(video_list_file, 'w') as f:\n            f.write(\"RESURRECTING ATLANTIS - FULL VIDEO SEQUENCE\\n\")\n            f.write(\"======================================\\n\\n\")\n            f.write(f\"Total videos: {len(ordered_videos)}\\n\")\n            f.write(f\"Output file: {output_file}\\n\\n\")\n            f.write(\"TRACK ORDER:\\n\")\n            \n            current_track = None\n            for i, video in enumerate(ordered_videos, 1):\n                # Identify which track this video belongs to\n                track_id = next((tid for tid in TRACK_ORDER if video.name.startswith(tid)), \"Unknown\")\n                \n                # Print track header if we're starting a new track\n                if track_id != current_track:\n                    current_track = track_id\n                    track_name = TRACK_TITLE_MAPPING.get(track_id, track_id)\n                    f.write(f\"\\n=== {track_id} - {track_name} ===\\n\")\n                \n                # Write video details\n                f.write(f\"{i}. {video.name}\\n\")\n                \n        print(f\"Video sequence details saved to {video_list_file}\")\n        \n    except subprocess.CalledProcessError as e:\n        print(\"Error during video concatenation:\")\n        print(f\"FFmpeg stdout: {e.stdout.decode()}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode()}\")\n        print(\"Make sure FFmpeg is installed and all videos are valid.\")\n\nif __name__ == \"__main__\":\n    assemble_videos()\n",
    "file_references": [
      "**/*.mp4",
      "ResurrectingAtlantis_FullSequence.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence",
      "**/*.mp4",
      ")\n        return\n    \n    # Create a concat file for FFmpeg\n    concat_file = OUTPUT_DIR / ",
      ")\n    create_concatenation_file(ordered_videos, concat_file)\n    \n    # Output filename\n    output_file = OUTPUT_DIR / ",
      ")\n        \n        # Create a text file with the list of videos used\n        video_list_file = OUTPUT_DIR / "
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "pathlib",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/mr_ordered_sequence_generator.py",
    "size": 8549,
    "lines": 212,
    "source": "#!/usr/bin/env python3\n\"\"\"\nMR Ordered Sequence Generator\nThis script generates MR (Magic Ride) codex entries in EXACT shot order from the prompts markdown.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport shutil\nfrom datetime import datetime\n\n# Paths\nJELLYFISH_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(JELLYFISH_DIR)\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nPROMPTS_FILE = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_prompts.md\")\nCODEX_FILE = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Codex.md\")\nMR_PROMPTS_FILE = os.path.join(TIGER_DIR, \"MR\", \"MR_prompts.md\")\nOUTPUT_FILE = os.path.join(TIGER_DIR, \"MR_ordered_codex_entries.md\")\n\ndef extract_mr_shots_from_prompts():\n    \"\"\"Extract MR shots in exact order from MR prompts file.\"\"\"\n    shots = []\n    \n    # Use the dedicated MR prompts file for more reliable extraction\n    if os.path.exists(MR_PROMPTS_FILE):\n        with open(MR_PROMPTS_FILE, 'r') as file:\n            content = file.read()\n            \n            # Extract each shot line by line\n            lines = content.split('\\n')\n            for line in lines:\n                # Match lines like: MR001 \u00b7 DS \u00b7 Mood Environment Stabilizer \u00b7 Title arcs across night highway in neon...\n                match = re.match(r'(MR\\d+)\\s*\u00b7\\s*([A-Z]+)\\s*\u00b7\\s*([^\u00b7]+)\u00b7\\s*(.+)', line.strip())\n                if match:\n                    shot_id, category, function, description = match.groups()\n                    shots.append({\n                        'id': shot_id.strip(),\n                        'category': category.strip(),\n                        'function': function.strip(),\n                        'description': description.strip(),\n                        'timestamp': '00:00:00'  # Placeholder\n                    })\n    \n    # If no shots found or file doesn't exist, try the collection prompts file\n    if not shots:\n        print(\"Using collection prompts file as fallback...\")\n        with open(PROMPTS_FILE, 'r') as file:\n            content = file.read()\n            \n            # Find MR section in the prompts file\n            mr_section_match = re.search(r'## MR Magic Ride.*?(?=^##|\\Z)', content, re.DOTALL | re.MULTILINE)\n            if mr_section_match:\n                mr_section = mr_section_match.group(0)\n                \n                # Extract shots from MR section\n                shot_pattern = r'(MR\\d+)\\s*\u00b7\\s*([A-Z]+)\\s*\u00b7\\s*([^\u00b7]+)\u00b7\\s*(.+?)(?=\\n[A-Z]{2}\\d+|\\Z)'\n                for match in re.finditer(shot_pattern, mr_section, re.DOTALL):\n                    shot_id, category, function, description = match.groups()\n                    shots.append({\n                        'id': shot_id.strip(),\n                        'category': category.strip(),\n                        'function': function.strip(),\n                        'description': description.strip(),\n                        'timestamp': '00:00:00'  # Placeholder\n                    })\n    \n    # Sort by shot ID to ensure correct sequence\n    shots.sort(key=lambda x: int(x['id'][2:]))\n    \n    print(f\"Extracted {len(shots)} MR shots from prompts\")\n    return shots\n\ndef find_image_paths():\n    \"\"\"Find image paths for MR shots from the file system.\"\"\"\n    mr_dir = os.path.join(TIGER_DIR, \"MR\")\n    image_dict = {}\n    \n    # Direct images in MR directory\n    if os.path.exists(mr_dir):\n        for filename in os.listdir(mr_dir):\n            if filename.endswith(('.png', '.jpg', '.jpeg')):\n                # Extract the shot ID (e.g., MR001)\n                shot_match = re.match(r'(MR\\d+)', filename)\n                if shot_match:\n                    shot_id = shot_match.group(1)\n                    if shot_id not in image_dict:\n                        image_dict[shot_id] = []\n                    # Store the path in the format: MR/filename.png\n                    # This matches the successful FL format in the codex\n                    image_dict[shot_id].append(f\"MR/{filename}\")\n    \n    return image_dict\n\ndef generate_ordered_codex_entries(shots, image_paths):\n    \"\"\"Generate ordered Codex entries for MR shots.\"\"\"\n    entries = []\n    shots_with_images = 0\n    shots_without_images = 0\n    \n    for shot in shots:\n        shot_id = shot['id']\n        timestamp = shot['timestamp']\n        \n        # Set default values - missing images need to follow same pattern\n        image_path = f\"MR/{shot_id}__missing_image.png\"\n        assembly_source = {\n            \"FrameGrab\": {\n                \"shot_id\": shot_id,\n                \"poem_section\": \"MR\",\n                \"shot_type\": shot['category'],\n                \"description\": shot['description']\n            }\n        }\n        \n        # Check if we have an image for this shot\n        if shot_id in image_paths and image_paths[shot_id]:\n            image_path = image_paths[shot_id][0]  # Use the first available image\n            shots_with_images += 1\n        else:\n            print(f\"Warning: No image found for {shot_id}\")\n            shots_without_images += 1\n        \n        # Generate the codex entry\n        entry = f\"### {shot_id} [{timestamp}]\\n\\n\"\n        entry += f\"**Image:** `{image_path}`\\n\\n\"\n        entry += \"**Assembly Source:**\\n```json\\n\"\n        entry += json.dumps(assembly_source, indent=2)\n        entry += \"\\n```\\n\\n\"\n        \n        # Add prompt data\n        entry += \"**Prompt:** \" + shot['category'] + \" \u00b7 \" + shot['function'] + \" \u00b7 \" + shot['description']\n        entry += \"\\n\\n---\\n\\n\"\n        \n        entries.append(entry)\n    \n    print(f\"Total entries: {len(entries)}\")\n    print(f\"Shots with images: {shots_with_images}\")\n    print(f\"Shots without images: {shots_without_images}\")\n    \n    return entries\n\ndef write_to_file(entries):\n    \"\"\"Write entries to output file.\"\"\"\n    with open(OUTPUT_FILE, 'w') as file:\n        file.write(\"# MR Magic Ride - Ordered Codex Entries\\n\\n\")\n        file.write(\"*Generated on \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"*\\n\\n\")\n        file.write(\"These entries are in EXACT shot order from the prompts markdown.\\n\\n\")\n        file.write(\"\".join(entries))\n    \n    print(f\"Generated ordered codex entries written to {OUTPUT_FILE}\")\n\ndef update_main_codex(entries):\n    \"\"\"Update the main codex file with the ordered entries.\"\"\"\n    if os.path.exists(CODEX_FILE):\n        with open(CODEX_FILE, 'r') as file:\n            content = file.read()\n        \n        # Check if MR section exists\n        mr_section_match = re.search(r'## MR Magic Ride.*?(?=^##|\\Z)', content, re.DOTALL | re.MULTILINE)\n        \n        if mr_section_match:\n            print(\"MR section already exists in Codex file. Creating a separate 'ORDERED' section.\")\n            # Replace existing MR section\n            ordered_section = \"## MR Magic Ride - ORDERED\\n\\n\" + \"\".join(entries)\n            updated_content = content.replace(mr_section_match.group(0), ordered_section)\n            \n            # Write backup\n            backup_file = CODEX_FILE + \".backup.\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            with open(backup_file, 'w') as file:\n                file.write(content)\n            \n            # Write updated content\n            with open(CODEX_FILE, 'w') as file:\n                file.write(updated_content)\n            \n            print(\"Updated main Codex file with ordered MR entries\")\n        else:\n            # Append MR section\n            with open(CODEX_FILE, 'a') as file:\n                file.write(\"\\n\\n## MR Magic Ride\\n\\n\")\n                file.write(\"\".join(entries))\n            \n            print(\"Added MR section to main Codex file\")\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    print(\"Generating MR ordered sequence...\")\n    \n    # Extract shots from prompts\n    shots = extract_mr_shots_from_prompts()\n    \n    # Find image paths\n    image_paths = find_image_paths()\n    \n    # Generate ordered entries\n    entries = generate_ordered_codex_entries(shots, image_paths)\n    \n    # Write to file\n    write_to_file(entries)\n    \n    # Update main codex\n    update_main_codex(entries)\n    \n    print(\"To generate the video with this EXACT ORDER, you need to:\")\n    print(\"1. Run: python3 fl_video_generator_header_prompt.py --prefix MR --ordered-codex /Users/gaia/resurrecting atlantis/TIGER/MR_ordered_codex_entries.md\")\n    print('2. Add audio: ffmpeg -y -i \"video_output/MR_header_prompt.mp4\" -i \"/Users/gaia/resurrecting atlantis/MANTA/audio/MR_audio.wav\" -map 0:v -map 1:a -c:v copy -c:a aac -shortest \"video_output/MR_header_prompt_with_audio.mp4\"')\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "MR/{shot_id}__missing_image.png",
      "video_output/MR_header_prompt.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/MR_audio.wav",
      "video_output/MR_header_prompt_with_audio.mp4",
      ", filename)\n                if shot_match:\n                    shot_id = shot_match.group(1)\n                    if shot_id not in image_dict:\n                        image_dict[shot_id] = []\n                    # Store the path in the format: MR/filename.png\n                    # This matches the successful FL format in the codex\n                    image_dict[shot_id].append(f",
      "MR/{shot_id}__missing_image.png",
      "1. Run: python3 fl_video_generator_header_prompt.py --prefix MR --ordered-codex /Users/gaia/resurrecting atlantis/TIGER/MR_ordered_codex_entries.md",
      "video_output/MR_header_prompt.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/MR_audio.wav",
      "video_output/MR_header_prompt_with_audio.mp4"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "json",
      "shutil",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "MR Ordered Sequence Generator\nThis script generates MR (Magic Ride) codex entries in EXACT shot order from the prompts markdown."
  },
  {
    "path": "JELLYFISH/update_collection_prompts.py",
    "size": 2862,
    "lines": 85,
    "source": "#!/usr/bin/env python3\n\nimport os\nimport re\n\n# Paths\ncollection_path = \"/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_ImageSequence.md\"\nht_prompts_path = \"/Users/gaia/resurrecting atlantis/TIGER/HT/HT_prompts.md\"\nnm_prompts_path = \"/Users/gaia/resurrecting atlantis/TIGER/NM/NM_prompts.md\"\n\ndef read_prompt_file(prompt_file_path):\n    \"\"\"Read prompts from the prompt file and return a dictionary mapping prompt IDs to their full text\"\"\"\n    with open(prompt_file_path, 'r') as f:\n        prompt_lines = f.readlines()\n    \n    prompt_dict = {}\n    \n    for line in prompt_lines:\n        line = line.strip()\n        if not line:\n            continue\n            \n        # Extract prompt ID and the rest of the text\n        match = re.match(r'^(\\w+\\d+)\\s+\u00b7\\s+(.+)$', line)\n        if match:\n            prompt_id = match.group(1)\n            prompt_text = match.group(2)\n            prompt_dict[prompt_id] = prompt_text\n    \n    return prompt_dict\n\ndef update_collection_file(collection_path, prompt_dict):\n    \"\"\"Update the collection file with prompt texts\"\"\"\n    with open(collection_path, 'r') as f:\n        lines = f.readlines()\n    \n    updated_lines = []\n    changes_made = 0\n    \n    for line in lines:\n        # Check if line contains a missing prompt\n        match = re.search(r'\\*\\*Prompt:\\*\\* (\\w+\\d+) \u00b7 \\*Prompt text not found\\*', line)\n        if match:\n            prompt_id = match.group(1)\n            if prompt_id in prompt_dict:\n                # Replace with the full prompt text\n                new_line = f\"**Prompt:** {prompt_id} \u00b7 {prompt_dict[prompt_id]}\\n\"\n                updated_lines.append(new_line)\n                changes_made += 1\n                print(f\"Updated prompt {prompt_id}\")\n            else:\n                # Keep the original line if no match found\n                updated_lines.append(line)\n                print(f\"No matching prompt found for {prompt_id}\")\n        else:\n            updated_lines.append(line)\n    \n    # Write the updated content back to the file\n    with open(collection_path, 'w') as f:\n        f.writelines(updated_lines)\n    \n    return changes_made\n\ndef main():\n    # Load prompt dictionaries\n    print(\"Loading HT prompts...\")\n    ht_prompts = read_prompt_file(ht_prompts_path)\n    print(f\"Loaded {len(ht_prompts)} HT prompts\")\n    \n    print(\"Loading NM prompts...\")\n    nm_prompts = read_prompt_file(nm_prompts_path)\n    print(f\"Loaded {len(nm_prompts)} NM prompts\")\n    \n    # Combine both prompt dictionaries\n    all_prompts = {**ht_prompts, **nm_prompts}\n    print(f\"Total prompts loaded: {len(all_prompts)}\")\n    \n    # Update the collection file\n    print(\"Updating collection file...\")\n    changes = update_collection_file(collection_path, all_prompts)\n    print(f\"Updated {changes} prompts in the collection file\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_ImageSequence.md",
      "/Users/gaia/resurrecting atlantis/TIGER/HT/HT_prompts.md",
      "/Users/gaia/resurrecting atlantis/TIGER/NM/NM_prompts.md"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/generate_nm_ordered_video.py",
    "size": 4313,
    "lines": 125,
    "source": "#!/usr/bin/env python3\n\"\"\"\nNM (Nevermore) Ordered Video Generator\n\nThis script generates the NM video with correct shot ordering and audio integration:\n1. Renames any existing NM videos to preserve them as MISFIT versions\n2. Generates the NM video using the ordered codex entries\n3. Adds the appropriate audio track to the generated video\n\"\"\"\n\nimport os\nimport subprocess\nimport shutil\nimport datetime\n\n# Base directories\nTIGER_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nJELLYFISH_DIR = \"/Users/gaia/resurrecting atlantis/JELLYFISH\"\nMANTA_DIR = \"/Users/gaia/resurrecting atlantis/MANTA\"\nVIDEO_OUTPUT_DIR = os.path.join(JELLYFISH_DIR, \"video_output\")\nORDERED_CODEX_PATH = os.path.join(TIGER_DIR, \"NM_ordered_codex_entries.md\")\n\n# Audio file for NM\nAUDIO_FILE = os.path.join(MANTA_DIR, \"audio\", \"nevermore-Reclaimed Truths remix v1.mp3\")\n\ndef backup_existing_video():\n    \"\"\"Backup any existing NM videos to MISFIT versions.\"\"\"\n    nm_video_path = os.path.join(VIDEO_OUTPUT_DIR, \"NM_header_prompt.mp4\")\n    nm_audio_video_path = os.path.join(VIDEO_OUTPUT_DIR, \"NM_header_prompt_with_audio.mp4\")\n    \n    # Check if NM videos exist and rename them\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    if os.path.exists(nm_video_path):\n        backup_path = os.path.join(VIDEO_OUTPUT_DIR, f\"NM_header_prompt_MISFIT_{timestamp}.mp4\")\n        print(f\"Backing up existing NM video to {backup_path}\")\n        shutil.move(nm_video_path, backup_path)\n    \n    if os.path.exists(nm_audio_video_path):\n        backup_path = os.path.join(VIDEO_OUTPUT_DIR, f\"NM_header_prompt_with_audio_MISFIT_{timestamp}.mp4\")\n        print(f\"Backing up existing NM video with audio to {backup_path}\")\n        shutil.move(nm_audio_video_path, backup_path)\n\ndef generate_nm_video():\n    \"\"\"Generate the NM video using the ordered codex entries.\"\"\"\n    print(\"Generating NM video with ordered shot sequence...\")\n    \n    # Check if ordered codex exists\n    if not os.path.exists(ORDERED_CODEX_PATH):\n        print(f\"Error: Ordered codex file not found at {ORDERED_CODEX_PATH}\")\n        print(\"Please run nm_ordered_sequence_generator.py first\")\n        return False\n    \n    # Run video generator with ordered codex\n    cmd = [\n        \"python3\", \n        os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\"),\n        \"--prefix\", \"NM\",\n        \"--ordered-codex\", ORDERED_CODEX_PATH\n    ]\n    \n    try:\n        subprocess.run(cmd, check=True)\n        print(\"NM video generated successfully\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error generating NM video: {e}\")\n        return False\n\ndef add_audio_to_video():\n    \"\"\"Add audio track to the generated NM video.\"\"\"\n    nm_video_path = os.path.join(VIDEO_OUTPUT_DIR, \"NM_header_prompt.mp4\")\n    nm_audio_video_path = os.path.join(VIDEO_OUTPUT_DIR, \"NM_header_prompt_with_audio.mp4\")\n    \n    if not os.path.exists(nm_video_path):\n        print(f\"Error: NM video not found at {nm_video_path}\")\n        return False\n    \n    if not os.path.exists(AUDIO_FILE):\n        print(f\"Error: Audio file not found at {AUDIO_FILE}\")\n        return False\n    \n    print(f\"Adding audio to NM video: {os.path.basename(AUDIO_FILE)}\")\n    \n    # Run ffmpeg to add audio\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", nm_video_path,\n        \"-i\", AUDIO_FILE,\n        \"-map\", \"0:v\", \"-map\", \"1:a\",\n        \"-c:v\", \"copy\", \"-c:a\", \"aac\",\n        \"-shortest\",\n        nm_audio_video_path\n    ]\n    \n    try:\n        subprocess.run(cmd, check=True)\n        print(f\"NM video with audio generated successfully: {nm_audio_video_path}\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error adding audio to NM video: {e}\")\n        return False\n\ndef main():\n    print(\"=== NM Ordered Video Generator ===\")\n    \n    # Step 1: Backup existing videos\n    backup_existing_video()\n    \n    # Step 2: Generate NM video\n    if not generate_nm_video():\n        return 1\n    \n    # Step 3: Add audio to video\n    if not add_audio_to_video():\n        return 1\n    \n    print(\"\\nSUCCESS: NM video with correct ordering and audio has been generated!\")\n    print(f\"Video location: {os.path.join(VIDEO_OUTPUT_DIR, 'NM_header_prompt_with_audio.mp4')}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    exit(main())\n",
    "file_references": [
      "nevermore-Reclaimed Truths remix v1.mp3",
      "NM_header_prompt.mp4",
      "NM_header_prompt_with_audio.mp4",
      "NM_header_prompt_MISFIT_{timestamp}.mp4",
      "NM_header_prompt_with_audio_MISFIT_{timestamp}.mp4",
      "NM_header_prompt.mp4",
      "NM_header_prompt_with_audio.mp4",
      "NM_header_prompt_with_audio.mp4",
      "/Users/gaia/resurrecting atlantis/TIGER",
      "/Users/gaia/resurrecting atlantis/JELLYFISH",
      "/Users/gaia/resurrecting atlantis/MANTA"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd, check=True"
      },
      {
        "type": "run",
        "snippet": "cmd, check=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "shutil",
      "datetime"
    ],
    "generates": [],
    "reads": [],
    "docstring": "NM (Nevermore) Ordered Video Generator\n\nThis script generates the NM video with correct shot ordering and audio integration:\n1. Renames any existing NM videos to preserve them as MISFIT versions\n2. Generates the NM video using the ordered codex entries\n3. Adds the appropriate audio track to the generated video"
  },
  {
    "path": "JELLYFISH/nm_ordered_sequence_generator.py",
    "size": 6754,
    "lines": 182,
    "source": "#!/usr/bin/env python3\n\"\"\"\nNM (Nevermore) Ordered Sequence Generator\n\nThis script extracts the exact shot order and prompts for the NM section\nfrom the master prompts markdown file, then generates ordered Codex entries\nwith metadata, prompt overlays, and image references strictly following this order.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport datetime\nimport argparse\n\n# Base directories\nTIGER_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nPROMPTS_FILE = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_prompts.md\")\nSIMPLIFIED_FILE = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\nCODEX_FILE = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Codex.md\")\nOUTPUT_FILE = os.path.join(TIGER_DIR, \"NM_ordered_codex_entries.md\")\n\ndef parse_prompts_file():\n    \"\"\"Parse the prompts file to extract NM shot information in correct order.\"\"\"\n    with open(PROMPTS_FILE, 'r', encoding='utf-8') as f:\n        content = f.read()\n    \n    # Find the NM section\n    nm_section_match = re.search(r'##\\s+\\d+_NM.*?(?=##\\s+\\d+_|\\Z)', content, re.DOTALL)\n    if not nm_section_match:\n        print(\"NM section not found in prompts file\")\n        return []\n    \n    nm_section = nm_section_match.group(0)\n    \n    # Extract each NM shot with its details\n    shot_pattern = r'(NM\\d+)\\s+\\[([^\\]]+)\\]\\s+\u00b7\\s+([^\u00b7]+)\u00b7\\s+([^\u00b7]+)\u00b7\\s+(.+)'\n    nm_shots = re.findall(shot_pattern, nm_section)\n    \n    shots = []\n    for shot_id, timestamp, category, theme, description in nm_shots:\n        shots.append({\n            'id': shot_id,\n            'timestamp': timestamp.strip(),\n            'category': category.strip(),\n            'theme': theme.strip(),\n            'description': description.strip(),\n        })\n    \n    return shots\n\ndef find_image_paths():\n    \"\"\"Find image paths for NM shots from the file system.\"\"\"\n    nm_dir = os.path.join(TIGER_DIR, \"NM\")\n    image_dict = {}\n    \n    # Direct images in NM directory\n    if os.path.exists(nm_dir):\n        for filename in os.listdir(nm_dir):\n            if filename.endswith(('.png', '.jpg', '.jpeg')):\n                # Extract the shot ID (e.g., NM001)\n                shot_match = re.match(r'(NM\\d+)', filename)\n                if shot_match:\n                    shot_id = shot_match.group(1)\n                    if shot_id not in image_dict:\n                        image_dict[shot_id] = []\n                    # Store the path in the format: NM/filename.png\n                    # This matches the successful FL format in the codex\n                    image_dict[shot_id].append(f\"NM/{filename}\")\n    \n    return image_dict\n\ndef generate_ordered_codex_entries(shots, image_paths):\n    \"\"\"Generate ordered Codex entries for NM shots.\"\"\"\n    entries = []\n    shots_with_images = 0\n    shots_without_images = 0\n    \n    for shot in shots:\n        shot_id = shot['id']\n        timestamp = shot['timestamp']\n        \n        # Set default values - missing images need to follow same pattern\n        image_path = f\"NM/{shot_id}__missing_image.png\"\n        assembly_source = {\n            \"FrameGrab\": {\n                \"shot_id\": shot_id,\n                \"poem_section\": \"NM\",\n                \"shot_type\": shot['category'],\n                \"description\": shot['description']\n            }\n        }\n        \n        # Check if we have an image for this shot\n        if shot_id in image_paths and image_paths[shot_id]:\n            image_path = image_paths[shot_id][0]  # Use the first available image\n            shots_with_images += 1\n        else:\n            print(f\"Warning: No image found for {shot_id}\")\n            shots_without_images += 1\n        \n        # Format the Codex entry\n        entry = f\"### {shot_id} [{timestamp}]\\n\\n\"\n        entry += f\"**Image:** `{image_path}`\\n\\n\"\n        entry += \"**Assembly Source:**\\n```json\\n\"\n        entry += json.dumps(assembly_source, indent=2)\n        entry += \"\\n```\\n\\n\"\n        entry += f\"**Prompt:** {shot['description']}\\n\\n---\\n\\n\"\n        \n        entries.append(entry)\n    \n    print(f\"Total entries: {len(shots)}\")\n    print(f\"Shots with images: {shots_with_images}\")\n    print(f\"Shots without images: {shots_without_images}\")\n    \n    return entries\n\ndef update_codex_file(ordered_entries):\n    \"\"\"Update the main Codex file with the ordered NM entries.\"\"\"\n    if not os.path.exists(CODEX_FILE):\n        print(\"Codex file not found\")\n        return False\n    \n    with open(CODEX_FILE, 'r', encoding='utf-8') as f:\n        codex_content = f.read()\n    \n    # Check if NM section already exists\n    if \"## NM (Nevermore)\" in codex_content:\n        print(\"NM section already exists in Codex file. Creating a separate 'ORDERED' section.\")\n        \n        with open(CODEX_FILE, 'a', encoding='utf-8') as f:\n            f.write(\"\\n\\n## NM (Nevermore) - ORDERED SEQUENCE\\n\\n\")\n            f.write(\"\".join(ordered_entries))\n        \n        print(\"Updated main Codex file with ordered NM entries\")\n        return True\n    else:\n        print(\"NM section not found in Codex file. Adding it.\")\n        \n        with open(CODEX_FILE, 'a', encoding='utf-8') as f:\n            f.write(\"\\n\\n## NM (Nevermore)\\n\\n\")\n            f.write(\"\".join(ordered_entries))\n        \n        print(\"Updated main Codex file with ordered NM entries\")\n        return True\n\ndef main():\n    \"\"\"Main function to orchestrate the generation of ordered NM Codex entries.\"\"\"\n    print(\"Generating NM ordered sequence...\")\n    \n    # Parse the prompts file to get the exact shot order\n    nm_shots = parse_prompts_file()\n    if not nm_shots:\n        print(\"Failed to extract NM shots from prompts file\")\n        return 1\n    \n    # Find image paths\n    image_paths = find_image_paths()\n    \n    # Generate ordered Codex entries\n    ordered_entries = generate_ordered_codex_entries(nm_shots, image_paths)\n    \n    # Write the ordered entries to a separate file\n    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n        f.write(\"# NM (Nevermore) Codex Entries - ORDERED SEQUENCE\\n\\n\")\n        f.write(\"\".join(ordered_entries))\n    \n    print(f\"Generated ordered codex entries written to {OUTPUT_FILE}\")\n    \n    # Update the main Codex file\n    update_codex_file(ordered_entries)\n    \n    # Print instructions for using the ordered codex\n    print(\"To generate the video with this EXACT ORDER, you need to:\")\n    print(\"1. Run: python3 fl_video_generator_header_prompt.py --prefix NM --ordered-codex \" + OUTPUT_FILE)\n    print(\"2. Add audio: ffmpeg -y -i \\\"video_output/NM_header_prompt.mp4\\\" -i \\\"/Users/gaia/resurrecting atlantis/MANTA/audio/APPROPRIATE_NM_AUDIO_FILE.mp3\\\" -map 0:v -map 1:a -c:v copy -c:a aac -shortest \\\"video_output/NM_header_prompt_with_audio.mp4\\\"\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    exit(main())\n",
    "file_references": [
      "NM/{shot_id}__missing_image.png",
      "/Users/gaia/resurrecting atlantis/TIGER",
      ", filename)\n                if shot_match:\n                    shot_id = shot_match.group(1)\n                    if shot_id not in image_dict:\n                        image_dict[shot_id] = []\n                    # Store the path in the format: NM/filename.png\n                    # This matches the successful FL format in the codex\n                    image_dict[shot_id].append(f",
      "NM/{shot_id}__missing_image.png",
      "video_output/NM_header_prompt.mp4\\",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/APPROPRIATE_NM_AUDIO_FILE.mp3\\",
      "video_output/NM_header_prompt_with_audio.mp4\\"
    ],
    "subprocess_calls": [],
    "imports": [
      "the",
      "os",
      "re",
      "json",
      "datetime",
      "argparse"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "NM (Nevermore) Ordered Sequence Generator\n\nThis script extracts the exact shot order and prompts for the NM section\nfrom the master prompts markdown file, then generates ordered Codex entries\nwith metadata, prompt overlays, and image references strictly following this order."
  },
  {
    "path": "JELLYFISH/generate_shot_codex.py",
    "size": 7835,
    "lines": 212,
    "source": "#!/usr/bin/env python3\n\nimport os\nimport json\nimport re\nfrom collections import defaultdict\n\n# Paths\nproject_root = \"/Users/gaia/resurrecting atlantis/TIGER\"\nsimplified_path = os.path.join(project_root, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\ncodex_output_path = os.path.join(project_root, \"COLLECTION_WhereYouGoWhenYouLeave_Codex.md\")\n\n# Poem collections and their JSON assembly files\npoem_collections = {\n    \"SH\": {\"assembly_path\": os.path.join(project_root, \"SH\", \"SH_assembly.json\"), \"name\": \"So Here We Are\"},\n    \"FL\": {\"assembly_path\": os.path.join(project_root, \"FL\", \"FL_assembly.json\"), \"name\": \"Flashbulb\"},\n    \"HT\": {\"assembly_path\": os.path.join(project_root, \"HT\", \"HT_assembly.json\"), \"name\": \"How To Win My Heart\"},\n    \"NM\": {\"assembly_path\": os.path.join(project_root, \"NM\", \"NM_assembly.json\"), \"name\": \"Nevermore\"},\n    \"BE\": {\"assembly_path\": os.path.join(project_root, \"BE\", \"BE_assembly.json\"), \"name\": \"Bloodline\"}\n}\n\n# Prompt files\nprompt_files = {\n    \"SH\": os.path.join(project_root, \"SH\", \"SH_prompts.md\"),\n    \"FL\": os.path.join(project_root, \"FL\", \"FL_prompts.md\"),\n    \"HT\": os.path.join(project_root, \"HT\", \"HT_prompts.md\"),\n    \"NM\": os.path.join(project_root, \"NM\", \"NM_prompts.md\"),\n    \"BE\": os.path.join(project_root, \"BE\", \"BE_prompts.md\")\n}\n\ndef load_assembly_data():\n    \"\"\"Load assembly data for all poem collections\"\"\"\n    assembly_data = {}\n    \n    for prefix, info in poem_collections.items():\n        try:\n            with open(info[\"assembly_path\"], 'r') as f:\n                assembly_data[prefix] = json.load(f)\n            print(f\"Loaded {len(assembly_data[prefix])} assembly items for {prefix}\")\n        except FileNotFoundError:\n            print(f\"Warning: Assembly file for {prefix} not found at {info['assembly_path']}\")\n            assembly_data[prefix] = []\n        except json.JSONDecodeError:\n            print(f\"Error: Could not parse assembly JSON for {prefix}\")\n            assembly_data[prefix] = []\n    \n    return assembly_data\n\ndef load_prompt_data():\n    \"\"\"Load prompt data from all prompt markdown files\"\"\"\n    prompt_data = {}\n    \n    for prefix, file_path in prompt_files.items():\n        prompt_data[prefix] = {}\n        try:\n            with open(file_path, 'r') as f:\n                lines = f.readlines()\n                \n            for line in lines:\n                line = line.strip()\n                if not line:\n                    continue\n                \n                # Extract prompt ID and text\n                match = re.match(r'^(\\w+\\d+)\\s+\u00b7\\s+(.+)$', line)\n                if match:\n                    prompt_id = match.group(1)\n                    prompt_text = match.group(2)\n                    prompt_data[prefix][prompt_id] = prompt_text\n            \n            print(f\"Loaded {len(prompt_data[prefix])} prompts for {prefix}\")\n        except FileNotFoundError:\n            print(f\"Warning: Prompt file for {prefix} not found at {file_path}\")\n    \n    return prompt_data\n\ndef extract_shots_from_simplified():\n    \"\"\"Extract shot data from the simplified markdown file\"\"\"\n    shots = []\n    \n    try:\n        with open(simplified_path, 'r') as f:\n            lines = f.readlines()\n        \n        for line in lines:\n            line = line.strip()\n            if not line or line.startswith('#') or line.startswith('*'):\n                continue\n            \n            # Extract shot info: ID [timestamp] filepath\n            match = re.match(r'(\\w+\\d+)\\s+\\[([^\\]]+)\\]\\s+`([^`]+)`', line)\n            if match:\n                shot_id = match.group(1)\n                timestamp = match.group(2)\n                filepath = match.group(3)\n                \n                # Extract poem collection prefix (SH, FL, etc.)\n                prefix = shot_id[:2]\n                \n                shots.append({\n                    \"id\": shot_id,\n                    \"timestamp\": timestamp,\n                    \"filepath\": filepath,\n                    \"prefix\": prefix\n                })\n        \n        print(f\"Extracted {len(shots)} shots from simplified file\")\n    except FileNotFoundError:\n        print(f\"Error: Simplified file not found at {simplified_path}\")\n    \n    return shots\n\ndef match_shot_to_assembly(shot_id, assembly_data):\n    \"\"\"Match a shot ID to its assembly data\"\"\"\n    # Extract numeric part from shot ID (e.g., \"SH001\" -> \"001\")\n    prefix = shot_id[:2]\n    num_part = shot_id[2:]\n    \n    # Check if prefix exists in assembly data\n    if prefix not in assembly_data:\n        return None\n    \n    # Try to find entry with matching ID\n    for entry in assembly_data[prefix]:\n        if 'id' in entry and entry['id'] == num_part:\n            return entry\n    \n    # If no direct ID match, try looking up by index\n    try:\n        shot_num = int(num_part)\n        # Find entry at index shot_num - 1 (1-based to 0-based conversion)\n        if 0 <= shot_num - 1 < len(assembly_data[prefix]):\n            return assembly_data[prefix][shot_num - 1]\n    except ValueError:\n        pass\n    \n    return None\n\ndef create_codex(shots, assembly_data, prompt_data):\n    \"\"\"Create the codex document combining shot, assembly, and prompt data\"\"\"\n    codex_content = [\"# WHERE YOU GO WHEN YOU LEAVE - CODEX\\n\",\n                     \"*Mapping Shots to Original Source Material*\\n\\n\"]\n    \n    # Group shots by poem collection\n    shots_by_collection = defaultdict(list)\n    for shot in shots:\n        shots_by_collection[shot[\"prefix\"]].append(shot)\n    \n    # Process each collection\n    for prefix, collection_shots in shots_by_collection.items():\n        if prefix not in poem_collections:\n            continue\n            \n        # Add collection header\n        poem_name = poem_collections[prefix][\"name\"]\n        codex_content.append(f\"## {prefix} - {poem_name}\\n\\n\")\n        \n        # Process each shot in this collection\n        for shot in collection_shots:\n            shot_id = shot[\"id\"]\n            timestamp = shot[\"timestamp\"]\n            filepath = shot[\"filepath\"]\n            \n            # Get assembly data\n            assembly_entry = match_shot_to_assembly(shot_id, assembly_data)\n            \n            # Get prompt data\n            prompt_text = \"Prompt text not found\"\n            if prefix in prompt_data and shot_id in prompt_data[prefix]:\n                prompt_text = prompt_data[prefix][shot_id]\n            \n            # Format entry\n            codex_content.append(f\"### {shot_id} [{timestamp}]\\n\\n\")\n            codex_content.append(f\"**Image:** `{filepath}`\\n\\n\")\n            \n            if assembly_entry:\n                # Extract poem name and content from assembly\n                poem_name = assembly_entry.get(\"poem\", \"Unknown poem\")\n                poem_content = assembly_entry.get(\"content\", \"No content available\")\n                codex_content.append(f\"**Assembly Source:**\\n```json\\n{{\\n  \\\"poem\\\": \\\"{poem_name}\\\",\\n  \\\"content\\\": \\\"{poem_content}\\\"\\n}}\\n```\\n\\n\")\n            else:\n                codex_content.append(\"**Assembly Source:** Not found\\n\\n\")\n            \n            codex_content.append(f\"**Prompt:** {prompt_text}\\n\\n\")\n            \n            # Add visual separator\n            codex_content.append(\"---\\n\\n\")\n    \n    return \"\".join(codex_content)\n\ndef main():\n    print(\"Loading assembly data...\")\n    assembly_data = load_assembly_data()\n    \n    print(\"Loading prompt data...\")\n    prompt_data = load_prompt_data()\n    \n    print(\"Extracting shots from simplified file...\")\n    shots = extract_shots_from_simplified()\n    \n    print(\"Generating codex...\")\n    codex_content = create_codex(shots, assembly_data, prompt_data)\n    \n    print(f\"Writing codex to {codex_output_path}...\")\n    with open(codex_output_path, 'w') as f:\n        f.write(codex_content)\n    \n    print(\"Codex generation complete!\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SH_assembly.json",
      "FL_assembly.json",
      "HT_assembly.json",
      "NM_assembly.json",
      "BE_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "re",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/normalize_impala_filenames_improved.py",
    "size": 7072,
    "lines": 145,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\n# Directory containing files to normalize\nIMPALA_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA')\n\n# Create backup directory\nBACKUP_DIR = IMPALA_DIR / 'original_files_backup'\nBACKUP_DIR.mkdir(exist_ok=True)\n\n# Track ID mapping with regex patterns\nTRACK_ID_MAPPING = {\n    # Track IDs with variations in spacing and casing\n    r'01[\\s_-]*SH[\\s_-]*(?:OutOfLife|OutofLife|Out[\\s_-]*of[\\s_-]*Life)[\\s_-]*0+': '01_SH_OutOfLife_000000',\n    r'02[\\s_-]*FL[\\s_-]*(?:FlashingLights|Flashing[\\s_-]*Lights)[\\s_-]*021100': '02_FL_FlashingLights_021100',\n    r'03[\\s_-]*HT[\\s_-]*(?:HowToBreakOffAnEngagement|HowtoBreakOffanEngagement|How[\\s_-]*to[\\s_-]*Break[\\s_-]*Off[\\s_-]*an[\\s_-]*Engagement)[\\s_-]*042200': '03_HT_HowToBreakOffAnEngagement_042200',\n    r'04[\\s_-]*NM[\\s_-]*(?:Nevermore)[\\s_-]*063300': '04_NM_Nevermore_063300',\n    r'05[\\s_-]*BE[\\s_-]*(?:Bloodline)[\\s_-]*084400': '05_BE_Bloodline_084400',\n    r'06[\\s_-]*AT[\\s_-]*(?:ResurrectingAtlantis|Resurrecting[\\s_-]*Atlantis)[\\s_-]*105500': '06_AT_ResurrectingAtlantis_105500',\n    r'07[\\s_-]*DJ[\\s_-]*(?:DJTurnMeUp|DJ[\\s_-]*Turn[\\s_-]*Me[\\s_-]*Up)[\\s_-]*130600': '07_DJ_DJTurnMeUp_130600',\n    r'08[\\s_-]*NS[\\s_-]*(?:NewlySingle|Newly[\\s_-]*Single)[\\s_-]*151700': '08_NS_NewlySingle_151700',\n    r'09[\\s_-]*YH[\\s_-]*(?:YetHeard|Yet[\\s_-]*Heard)[\\s_-]*172800': '09_YH_YetHeard_172800',\n    r'10[\\s_-]*MR[\\s_-]*(?:MagicRide|Magic[\\s_-]*Ride)[\\s_-]*193900': '10_MR_MagicRide_193900',\n    r'12[\\s_-]*RU[\\s_-]*(?:Reunion)[\\s_-]*215000': '12_RU_Reunion_215000',\n    r'13[\\s_-]*HW[\\s_-]*(?:HowToWinMyHeart|HowtoWinMyHeart|How[\\s_-]*to[\\s_-]*Win[\\s_-]*My[\\s_-]*Heart)[\\s_-]*240100': '13_HW_HowToWinMyHeart_240100',\n    r'14[\\s_-]*HM[\\s_-]*(?:HotMinute|Hot[\\s_-]*Minute)[\\s_-]*261200': '14_HM_HotMinute_261200',\n    \n    # Partial ID matches (track numbers and codes only)\n    r'01[\\s_-]*SH': '01_SH_OutOfLife_000000',\n    r'02[\\s_-]*FL': '02_FL_FlashingLights_021100',\n    r'03[\\s_-]*HT': '03_HT_HowToBreakOffAnEngagement_042200',\n    r'04[\\s_-]*NM': '04_NM_Nevermore_063300',\n    r'05[\\s_-]*BE': '05_BE_Bloodline_084400',\n    r'06[\\s_-]*AT': '06_AT_ResurrectingAtlantis_105500',\n    r'07[\\s_-]*DJ': '07_DJ_DJTurnMeUp_130600',\n    r'08[\\s_-]*NS': '08_NS_NewlySingle_151700',\n    r'09[\\s_-]*YH': '09_YH_YetHeard_172800',\n    r'10[\\s_-]*MR': '10_MR_MagicRide_193900',\n    r'12[\\s_-]*RU': '12_RU_Reunion_215000',\n    r'13[\\s_-]*HW': '13_HW_HowToWinMyHeart_240100',\n    r'14[\\s_-]*HM': '14_HM_HotMinute_261200',\n    \n    # Track name matches (for files with just the track name)\n    r'(?:^|[\\s,_-])OUT[\\s_-]*OF[\\s_-]*LIFE(?:$|[\\s,_-])': '01_SH_OutOfLife_000000',\n    r'(?:^|[\\s,_-])FLASHING[\\s_-]*LIGHTS(?:$|[\\s,_-])': '02_FL_FlashingLights_021100',\n    r'(?:^|[\\s,_-])HOW[\\s_-]*TO[\\s_-]*BREAK[\\s_-]*OFF[\\s_-]*AN[\\s_-]*ENGAGEMENT(?:$|[\\s,_-])': '03_HT_HowToBreakOffAnEngagement_042200',\n    r'(?:^|[\\s,_-])NEVERMORE(?:$|[\\s,_-])': '04_NM_Nevermore_063300',\n    r'(?:^|[\\s,_-])BLOODLINE(?:$|[\\s,_-])': '05_BE_Bloodline_084400',\n    r'(?:^|[\\s,_-])RESURRECTING[\\s_-]*ATLANTI(?:S|$|[\\s,_-])': '06_AT_ResurrectingAtlantis_105500',\n    r'(?:^|[\\s,_-])DJ[\\s_-]*TURN[\\s_-]*ME[\\s_-]*UP(?:$|[\\s,_-])': '07_DJ_DJTurnMeUp_130600',\n    r'(?:^|[\\s,_-])NEWLY[\\s_-]*SINGLE(?:$|[\\s,_-])': '08_NS_NewlySingle_151700',\n    r'(?:^|[\\s,_-])YET[\\s_-]*HEARD(?:$|[\\s,_-])': '09_YH_YetHeard_172800',\n    r'(?:^|[\\s,_-])MAGIC[\\s_-]*RIDE(?:$|[\\s,_-])': '10_MR_MagicRide_193900',\n    r'(?:^|[\\s,_-])REUNION(?:$|[\\s,_-])': '12_RU_Reunion_215000',\n    r'(?:^|[\\s,_-])HOW[\\s_-]*TO[\\s_-]*WIN[\\s_-]*MY[\\s_-]*HEART(?:$|[\\s,_-])': '13_HW_HowToWinMyHeart_240100',\n    r'(?:^|[\\s,_-])HOT[\\s_-]*MINUTE(?:$|[\\s,_-])': '14_HM_HotMinute_261200'\n}\n\n# Function to extract track ID from filename\ndef extract_track_id(filename):\n    # Remove any leading numbers followed by commas or underscores (common in your files)\n    clean_filename = filename.upper()  # Convert to uppercase for case-insensitive matching\n    \n    # Try all patterns\n    for pattern, track_id in TRACK_ID_MAPPING.items():\n        if re.search(pattern, clean_filename, re.IGNORECASE):\n            return track_id\n    \n    return None\n\n# Function to normalize a filename\ndef normalize_filename(filename, track_id):\n    # Check if the file already has the correct track ID at the beginning\n    if filename.startswith(track_id):\n        return filename\n    \n    # Remove any numbers and commas at the beginning (like \"1235948955, \")\n    clean_name = re.sub(r'^[\\d]+[\\s,_-]*', '', filename)\n    \n    # Remove the track ID or related text from the filename to avoid duplication\n    # This is tricky since the track ID could appear in various formats\n    # For simplicity, we'll keep the original filename after the track ID\n    \n    # Create the new filename\n    return f\"{track_id}_{clean_name}\"\n\n# Main function to process all files\ndef normalize_impala_filenames():\n    print(\"Starting to normalize filenames in IMPALA directory...\")\n    \n    # Track statistics\n    total_files = 0\n    renamed_files = 0\n    skipped_files = 0\n    \n    # Process each file in the directory\n    for file_path in IMPALA_DIR.glob('*'):\n        if file_path.is_file() and file_path.name != '.DS_Store' and str(file_path).find('original_files_backup') == -1:\n            total_files += 1\n            \n            # Extract track ID if present\n            track_id = extract_track_id(file_path.name)\n            \n            if track_id:\n                # Check if the filename already starts with the track ID\n                if file_path.name.startswith(track_id):\n                    print(f\"File already normalized: {file_path.name}\")\n                    skipped_files += 1\n                    continue\n                \n                # Create the new normalized filename\n                new_filename = normalize_filename(file_path.name, track_id)\n                new_file_path = file_path.parent / new_filename\n                \n                # Check if new filename already exists\n                if new_file_path.exists():\n                    print(f\"Skipping (destination exists): {file_path.name}\")\n                    skipped_files += 1\n                    continue\n                \n                # Create backup\n                backup_path = BACKUP_DIR / file_path.name\n                print(f\"Backing up: {file_path.name} -> {backup_path.name}\")\n                shutil.copy2(file_path, backup_path)\n                \n                # Rename the file\n                print(f\"Renaming: {file_path.name} -> {new_filename}\")\n                os.rename(file_path, new_file_path)\n                renamed_files += 1\n            else:\n                print(f\"No track ID found in: {file_path.name}\")\n                skipped_files += 1\n    \n    print(\"\\nNormalization Complete!\")\n    print(f\"Total files processed: {total_files}\")\n    print(f\"Files renamed: {renamed_files}\")\n    print(f\"Files skipped: {skipped_files}\")\n    print(f\"Original files backed up to: {BACKUP_DIR}\")\n\nif __name__ == \"__main__\":\n    normalize_impala_filenames()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/IMPALA",
      ")\n                    skipped_files += 1\n                    continue\n                \n                # Create the new normalized filename\n                new_filename = normalize_filename(file_path.name, track_id)\n                new_file_path = file_path.parent / new_filename\n                \n                # Check if new filename already exists\n                if new_file_path.exists():\n                    print(f",
      ")\n                    skipped_files += 1\n                    continue\n                \n                # Create backup\n                backup_path = BACKUP_DIR / file_path.name\n                print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/sequence_mpre_run_files.py",
    "size": 4213,
    "lines": 104,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\n# Path to the mpre-run directory\nmpre_run_dir = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run')\n\n# Create a backup directory\nbackup_dir = mpre_run_dir / 'backup_sequence'\nbackup_dir.mkdir(exist_ok=True)\n\n# Define the new naming mapping\ntrack_mapping = {\n    \"Out of Life\": {\"new_id\": \"01\", \"code\": \"SH\", \"timecode\": \"000000\"},\n    \"Flashing Lights\": {\"new_id\": \"02\", \"code\": \"FL\", \"timecode\": \"021100\"},\n    \"How to Break Off an Engagement\": {\"new_id\": \"03\", \"code\": \"HT\", \"timecode\": \"042200\"},\n    \"Nevermore\": {\"new_id\": \"04\", \"code\": \"NM\", \"timecode\": \"063300\"},\n    \"Bloodline\": {\"new_id\": \"05\", \"code\": \"BE\", \"timecode\": \"084400\"},\n    \"Resurrecting Atlantis\": {\"new_id\": \"06\", \"code\": \"AT\", \"timecode\": \"105500\"},\n    \"DJ Turn Me Up\": {\"new_id\": \"07\", \"code\": \"DJ\", \"timecode\": \"130600\"},\n    \"Newly Single\": {\"new_id\": \"08\", \"code\": \"NS\", \"timecode\": \"151700\"},\n    \"Yet Heard\": {\"new_id\": \"09\", \"code\": \"YH\", \"timecode\": \"172800\"},\n    \"Magic Ride\": {\"new_id\": \"10\", \"code\": \"MR\", \"timecode\": \"193900\"},\n    \"Reunion\": {\"new_id\": \"12\", \"code\": \"RU\", \"timecode\": \"215000\"},\n    \"How to Win My Heart\": {\"new_id\": \"13\", \"code\": \"HW\", \"timecode\": \"240100\"},\n    \"Hot Minute\": {\"new_id\": \"14\", \"code\": \"HM\", \"timecode\": \"261200\"},\n}\n\n# Helper function to find the track name in a filename\ndef find_track_name(filename):\n    for track_name in track_mapping.keys():\n        # Check if the track name is in the filename (case-insensitive)\n        if re.search(r'\\b' + re.escape(track_name) + r'\\b', filename, re.IGNORECASE):\n            return track_name\n    return None\n\n# Helper function to generate a variant suffix for multiple files of the same track\ndef get_variant_suffix(file_path, base_name, variant_counts):\n    # Check if file contains specific keywords to determine variant type\n    file_name = file_path.name.lower()\n    \n    if \"prompt set\" in file_name or \"title card\" in file_name:\n        variant_type = \"TitleCard\"\n    elif \"visual concept\" in file_name:\n        variant_type = \"VisualConcept\"\n    elif \"poetic ekphrasis\" in file_name or \"duration\" in file_name:\n        variant_type = \"Ekphrasis\"\n    else:\n        # Generic variant if no specific type detected\n        variant_type = \"Variant\"\n    \n    # Increment the count for this variant type\n    if base_name not in variant_counts:\n        variant_counts[base_name] = {}\n    \n    if variant_type not in variant_counts[base_name]:\n        variant_counts[base_name][variant_type] = 0\n    \n    variant_counts[base_name][variant_type] += 1\n    count = variant_counts[base_name][variant_type]\n    \n    return f\"_{variant_type}{count}\"\n\n# Dictionary to track variant counts\nvariant_counts = {}\n\n# Process all files in the directory\nfor file_path in mpre_run_dir.glob('*'):\n    if file_path.is_file() and file_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n        # Skip backup directories\n        if 'backup' in file_path.name:\n            continue\n            \n        # Create backup\n        backup_file = backup_dir / file_path.name\n        shutil.copy2(file_path, backup_file)\n        \n        # Find which track this file belongs to\n        track_name = find_track_name(file_path.name)\n        \n        if track_name:\n            # Get the mapping for this track\n            mapping = track_mapping[track_name]\n            \n            # Create the base part of the new name\n            base_name = f\"{mapping['new_id']}_{mapping['code']}_{track_name.replace(' ', '')}_{mapping['timecode']}\"\n            \n            # Get variant suffix for this file\n            variant_suffix = get_variant_suffix(file_path, base_name, variant_counts)\n            \n            # Full new name with original extension\n            new_name = f\"{base_name}{variant_suffix}{file_path.suffix}\"\n            \n            # Rename the file\n            new_file_path = file_path.parent / new_name\n            os.rename(file_path, new_file_path)\n            print(f\"Renamed: {file_path.name} -> {new_name}\")\n        else:\n            print(f\"Could not determine track for: {file_path.name}\")\n\nprint(f\"\\nAll files processed. Backup copies saved in {backup_dir}\")\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run",
      " in file_path.name:\n            continue\n            \n        # Create backup\n        backup_file = backup_dir / file_path.name\n        shutil.copy2(file_path, backup_file)\n        \n        # Find which track this file belongs to\n        track_name = find_track_name(file_path.name)\n        \n        if track_name:\n            # Get the mapping for this track\n            mapping = track_mapping[track_name]\n            \n            # Create the base part of the new name\n            base_name = f",
      "\n            \n            # Rename the file\n            new_file_path = file_path.parent / new_name\n            os.rename(file_path, new_file_path)\n            print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/generate_full_shot_codex.py",
    "size": 7744,
    "lines": 214,
    "source": "#!/usr/bin/env python3\n\nimport os\nimport json\nimport re\nfrom collections import defaultdict\n\n# Paths\nproject_root = \"/Users/gaia/resurrecting atlantis/TIGER\"\nsimplified_path = os.path.join(project_root, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\ncodex_output_path = os.path.join(project_root, \"COLLECTION_WhereYouGoWhenYouLeave_Codex.md\")\n\n# Poem collections and their JSON assembly files\npoem_collections = {\n    \"SH\": {\"assembly_path\": os.path.join(project_root, \"SH\", \"SH_assembly.json\"), \"name\": \"So Here We Are\"},\n    \"FL\": {\"assembly_path\": os.path.join(project_root, \"FL\", \"FL_assembly.json\"), \"name\": \"Flashbulb\"},\n    \"HT\": {\"assembly_path\": os.path.join(project_root, \"HT\", \"HT_assembly.json\"), \"name\": \"How To Win My Heart\"},\n    \"NM\": {\"assembly_path\": os.path.join(project_root, \"NM\", \"NM_assembly.json\"), \"name\": \"Nevermore\"},\n    \"BE\": {\"assembly_path\": os.path.join(project_root, \"BE\", \"BE_assembly.json\"), \"name\": \"Bloodline\"}\n}\n\n# Prompt files\nprompt_files = {\n    \"SH\": os.path.join(project_root, \"SH\", \"SH_prompts.md\"),\n    \"FL\": os.path.join(project_root, \"FL\", \"FL_prompts.md\"),\n    \"HT\": os.path.join(project_root, \"HT\", \"HT_prompts.md\"),\n    \"NM\": os.path.join(project_root, \"NM\", \"NM_prompts.md\"),\n    \"BE\": os.path.join(project_root, \"BE\", \"BE_prompts.md\")\n}\n\ndef load_assembly_data():\n    \"\"\"Load assembly data for all poem collections\"\"\"\n    assembly_data = {}\n    \n    for prefix, info in poem_collections.items():\n        try:\n            with open(info[\"assembly_path\"], 'r') as f:\n                assembly_data[prefix] = json.load(f)\n            print(f\"Loaded {len(assembly_data[prefix])} assembly items for {prefix}\")\n        except FileNotFoundError:\n            print(f\"Warning: Assembly file for {prefix} not found at {info['assembly_path']}\")\n            assembly_data[prefix] = []\n        except json.JSONDecodeError:\n            print(f\"Error: Could not parse assembly JSON for {prefix}\")\n            assembly_data[prefix] = []\n    \n    return assembly_data\n\ndef load_prompt_data():\n    \"\"\"Load prompt data from all prompt markdown files\"\"\"\n    prompt_data = {}\n    \n    for prefix, file_path in prompt_files.items():\n        prompt_data[prefix] = {}\n        try:\n            with open(file_path, 'r') as f:\n                lines = f.readlines()\n                \n            for line in lines:\n                line = line.strip()\n                if not line:\n                    continue\n                \n                # Extract prompt ID and text\n                match = re.match(r'^(\\w+\\d+)\\s+\u00b7\\s+(.+)$', line)\n                if match:\n                    prompt_id = match.group(1)\n                    prompt_text = match.group(2)\n                    prompt_data[prefix][prompt_id] = prompt_text\n            \n            print(f\"Loaded {len(prompt_data[prefix])} prompts for {prefix}\")\n        except FileNotFoundError:\n            print(f\"Warning: Prompt file for {prefix} not found at {file_path}\")\n    \n    return prompt_data\n\ndef extract_shots_from_simplified():\n    \"\"\"Extract shot data from the simplified markdown file\"\"\"\n    shots = []\n    \n    try:\n        with open(simplified_path, 'r') as f:\n            lines = f.readlines()\n        \n        for line in lines:\n            line = line.strip()\n            if not line or line.startswith('#') or line.startswith('*'):\n                continue\n            \n            # Extract shot info: ID [timestamp] filepath\n            match = re.match(r'(\\w+\\d+)\\s+\\[([^\\]]+)\\]\\s+`([^`]+)`', line)\n            if match:\n                shot_id = match.group(1)\n                timestamp = match.group(2)\n                filepath = match.group(3)\n                \n                # Extract poem collection prefix (SH, FL, etc.)\n                prefix = shot_id[:2]\n                \n                shots.append({\n                    \"id\": shot_id,\n                    \"timestamp\": timestamp,\n                    \"filepath\": filepath,\n                    \"prefix\": prefix\n                })\n        \n        print(f\"Extracted {len(shots)} shots from simplified file\")\n    except FileNotFoundError:\n        print(f\"Error: Simplified file not found at {simplified_path}\")\n    \n    return shots\n\ndef match_shot_to_assembly(shot_id, assembly_data):\n    \"\"\"Match a shot ID to its assembly data\"\"\"\n    # Extract numeric part from shot ID (e.g., \"SH001\" -> \"001\")\n    prefix = shot_id[:2]\n    num_part = shot_id[2:]\n    \n    # Check if prefix exists in assembly data\n    if prefix not in assembly_data:\n        return None\n    \n    # Try to find entry with matching ID\n    for entry in assembly_data[prefix]:\n        if 'id' in entry and entry['id'] == num_part:\n            return entry\n    \n    # If no direct ID match, try looking up by index\n    try:\n        shot_num = int(num_part)\n        # Find entry at index shot_num - 1 (1-based to 0-based conversion)\n        if 0 <= shot_num - 1 < len(assembly_data[prefix]):\n            return assembly_data[prefix][shot_num - 1]\n    except ValueError:\n        pass\n    \n    return None\n\ndef format_json(obj):\n    \"\"\"Format a JSON object with proper indentation\"\"\"\n    return json.dumps(obj, indent=2)\n\ndef create_codex(shots, assembly_data, prompt_data):\n    \"\"\"Create the codex document combining shot, assembly, and prompt data\"\"\"\n    codex_content = [\"# WHERE YOU GO WHEN YOU LEAVE - CODEX\\n\",\n                     \"*Mapping Shots to Original Source Material*\\n\\n\"]\n    \n    # Group shots by poem collection\n    shots_by_collection = defaultdict(list)\n    for shot in shots:\n        shots_by_collection[shot[\"prefix\"]].append(shot)\n    \n    # Process each collection\n    for prefix, collection_shots in shots_by_collection.items():\n        if prefix not in poem_collections:\n            continue\n            \n        # Add collection header\n        poem_name = poem_collections[prefix][\"name\"]\n        codex_content.append(f\"## {prefix} - {poem_name}\\n\\n\")\n        \n        # Process each shot in this collection\n        for shot in collection_shots:\n            shot_id = shot[\"id\"]\n            timestamp = shot[\"timestamp\"]\n            filepath = shot[\"filepath\"]\n            \n            # Get assembly data\n            assembly_entry = match_shot_to_assembly(shot_id, assembly_data)\n            \n            # Get prompt data\n            prompt_text = \"Prompt text not found\"\n            if prefix in prompt_data and shot_id in prompt_data[prefix]:\n                prompt_text = prompt_data[prefix][shot_id]\n            \n            # Format entry\n            codex_content.append(f\"### {shot_id} [{timestamp}]\\n\\n\")\n            codex_content.append(f\"**Image:** `{filepath}`\\n\\n\")\n            \n            if assembly_entry:\n                # Include the complete assembly entry\n                codex_content.append(f\"**Assembly Source:**\\n```json\\n{format_json(assembly_entry)}\\n```\\n\\n\")\n            else:\n                codex_content.append(\"**Assembly Source:** Not found\\n\\n\")\n            \n            codex_content.append(f\"**Prompt:** {prompt_text}\\n\\n\")\n            \n            # Add visual separator\n            codex_content.append(\"---\\n\\n\")\n    \n    return \"\".join(codex_content)\n\ndef main():\n    print(\"Loading assembly data...\")\n    assembly_data = load_assembly_data()\n    \n    print(\"Loading prompt data...\")\n    prompt_data = load_prompt_data()\n    \n    print(\"Extracting shots from simplified file...\")\n    shots = extract_shots_from_simplified()\n    \n    print(\"Generating codex...\")\n    codex_content = create_codex(shots, assembly_data, prompt_data)\n    \n    print(f\"Writing codex to {codex_output_path}...\")\n    with open(codex_output_path, 'w') as f:\n        f.write(codex_content)\n    \n    print(\"Codex generation complete!\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SH_assembly.json",
      "FL_assembly.json",
      "HT_assembly.json",
      "NM_assembly.json",
      "BE_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "re",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/assemble_ibex_sequence.py",
    "size": 5826,
    "lines": 160,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport re\n\n# Directory containing video files\nIBEX_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/IBEX')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Track order specified\nTRACK_ORDER = [\n    \"01_SH_OutOfLife_000000\",\n    \"02_FL_FlashingLights_021100\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\",\n    \"04_NM_Nevermore_063300\",\n    \"05_BE_Bloodline_084400\",\n    \"06_AT_ResurrectingAtlantis_105500\",\n    \"07_DJ_DJTurnMeUp_130600\",\n    \"08_NS_NewlySingle_151700\",\n    \"09_YH_YetHeard_172800\",\n    \"10_MR_MagicRide_193900\",\n    \"12_RU_Reunion_215000\",\n    \"13_HW_HowToWinMyHeart_240100\",\n    \"14_HM_HotMinute_261200\"\n]\n\ndef get_video_duration(video_path):\n    \"\"\"Get the duration of a video file using FFprobe.\"\"\"\n    try:\n        result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        duration = float(data['format']['duration'])\n        return duration\n    except (subprocess.CalledProcessError, KeyError, json.JSONDecodeError) as e:\n        print(f\"Error getting duration for {video_path}: {e}\")\n        return 0\n\ndef find_video_files():\n    \"\"\"Find all video files in the IBEX directory.\"\"\"\n    video_files = []\n    for file in IBEX_DIR.glob('**/*.mp4'):\n        video_files.append(file)\n    return video_files\n\ndef sort_videos_by_track_order(video_files):\n    \"\"\"Sort video files according to the track order.\"\"\"\n    # Create a dictionary to map track prefix to its position in TRACK_ORDER\n    track_positions = {track: i for i, track in enumerate(TRACK_ORDER)}\n    \n    # Define a function to get the track prefix of a file\n    def get_track_prefix(file):\n        for track in TRACK_ORDER:\n            if file.name.startswith(track):\n                return track\n        return None\n    \n    # Sort the video files based on track order\n    sorted_videos = sorted(\n        video_files,\n        key=lambda file: track_positions.get(get_track_prefix(file), float('inf'))\n    )\n    \n    return sorted_videos\n\ndef create_concatenation_file(videos, concat_file_path):\n    \"\"\"Create a concatenation file for FFmpeg.\"\"\"\n    with open(concat_file_path, 'w') as f:\n        for video in videos:\n            # Escape single quotes in the path\n            escaped_path = str(video).replace(\"'\", \"'\\\\''\")\n            f.write(f\"file '{escaped_path}'\\n\")\n\ndef assemble_videos():\n    \"\"\"Assemble all videos in the IBEX directory in track order.\"\"\"\n    print(\"Finding video files in IBEX directory...\")\n    all_videos = find_video_files()\n    print(f\"Found {len(all_videos)} video files.\")\n    \n    if not all_videos:\n        print(\"No videos found in the IBEX directory.\")\n        return\n    \n    # Sort videos according to track order\n    print(\"Sorting videos by track order...\")\n    sorted_videos = sort_videos_by_track_order(all_videos)\n    \n    # Print information about sorted videos\n    for i, video in enumerate(sorted_videos, 1):\n        print(f\"{i}. {video.name}\")\n    \n    # Create a concat file for FFmpeg\n    concat_file = OUTPUT_DIR / \"ibex_concat_list.txt\"\n    print(f\"Creating concatenation file at {concat_file}...\")\n    create_concatenation_file(sorted_videos, concat_file)\n    \n    # Calculate total duration\n    total_duration = sum(get_video_duration(video) for video in sorted_videos)\n    \n    # Output filename\n    output_file = OUTPUT_DIR / \"ResurrectingAtlantis_IBEX_Sequence.mp4\"\n    \n    # Use FFmpeg to concatenate the videos\n    print(f\"Concatenating videos into {output_file}...\")\n    print(f\"Estimated duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n    \n    try:\n        result = subprocess.run([\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file),\n            '-c', 'copy',\n            '-y',  # Overwrite output file if it exists\n            str(output_file)\n        ], check=True, capture_output=True)\n        \n        print(\"IBEX video sequence assembled successfully!\")\n        print(f\"Output file: {output_file}\")\n        \n        # Create a text file with the list of videos used\n        video_list_file = OUTPUT_DIR / \"ibex_sequence_details.txt\"\n        with open(video_list_file, 'w') as f:\n            f.write(\"RESURRECTING ATLANTIS - IBEX SEQUENCE\\n\")\n            f.write(\"=====================================\\n\\n\")\n            f.write(f\"Total videos used: {len(sorted_videos)}\\n\")\n            f.write(f\"Total duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\\n\")\n            f.write(f\"Output file: {output_file}\\n\\n\")\n            f.write(\"TRACK SEQUENCE:\\n\")\n            \n            for i, video in enumerate(sorted_videos, 1):\n                # Find the track ID for this video\n                track_id = next((track for track in TRACK_ORDER if video.name.startswith(track)), \"Unknown\")\n                duration = get_video_duration(video)\n                \n                # Write video details\n                f.write(f\"{i}. {track_id}\\n\")\n                f.write(f\"   File: {video.name}\\n\")\n                f.write(f\"   Duration: {duration:.2f} seconds\\n\\n\")\n                \n        print(f\"IBEX sequence details saved to {video_list_file}\")\n        \n    except subprocess.CalledProcessError as e:\n        print(\"Error during video concatenation:\")\n        print(f\"FFmpeg stdout: {e.stdout.decode()}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode()}\")\n        print(\"Make sure FFmpeg is installed and all videos are valid.\")\n\nif __name__ == \"__main__\":\n    assemble_videos()\n",
    "file_references": [
      "**/*.mp4",
      "ResurrectingAtlantis_IBEX_Sequence.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA/IBEX",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence",
      "**/*.mp4",
      ")\n    \n    # Create a concat file for FFmpeg\n    concat_file = OUTPUT_DIR / ",
      ")\n    create_concatenation_file(sorted_videos, concat_file)\n    \n    # Calculate total duration\n    total_duration = sum(get_video_duration(video) for video in sorted_videos)\n    \n    # Output filename\n    output_file = OUTPUT_DIR / ",
      "Estimated duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)",
      ")\n        \n        # Create a text file with the list of videos used\n        video_list_file = OUTPUT_DIR / ",
      "Total duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\\n"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "pathlib",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/ru_codex_generator.py",
    "size": 2150,
    "lines": 70,
    "source": "#!/usr/bin/env python3\n\nimport json\nimport os\n\n# Source files for RU section\nru_prompts_file = \"/Users/gaia/resurrecting atlantis/TIGER/RU/RU_prompts.md\"\nru_assembly_file = \"/Users/gaia/resurrecting atlantis/TIGER/RU/RU_assembly.json\"\nru_image_dir = \"/Users/gaia/resurrecting atlantis/TIGER/RU\"\n\n# Output file \noutput_file = \"/Users/gaia/resurrecting atlantis/TIGER/RU_codex_entries.md\"\n\n# Load RU assembly data\nwith open(ru_assembly_file, 'r') as f:\n    assembly_data = json.load(f)\n\n# Load RU prompts data\nwith open(ru_prompts_file, 'r') as f:\n    prompts_lines = f.readlines()\n\n# Create a dictionary of prompts by ID\nprompts = {}\nfor line in prompts_lines:\n    parts = line.strip().split(' \u00b7 ', 1)\n    if len(parts) >= 2:\n        shot_id = parts[0].strip()\n        prompt_text = line.strip()\n        prompts[shot_id] = prompt_text\n\n# Find image files for each RU entry\nimage_files = {}\nfor file in os.listdir(ru_image_dir):\n    if file.startswith(\"RU\") and file.endswith(\".png\"):\n        shot_id = file.split(\"__\")[0]\n        if shot_id not in image_files:\n            image_files[shot_id] = file\n\n# Generate codex entries\nwith open(output_file, 'w') as f:\n    f.write(\"# RU (Reunion) Codex Entries\\n\\n\")\n    \n    for entry in assembly_data:\n        shot_id = entry[\"id\"]\n        \n        # Skip if we don't have image file or prompt\n        if shot_id not in image_files or shot_id not in prompts:\n            continue\n            \n        # Format the timestamp (00:00:00 pattern)\n        index = int(shot_id[2:])\n        timestamp = f\"{index//60:02d}:{index%60:02d}:00\"\n        \n        # Write the entry header\n        f.write(f\"### {shot_id} [{timestamp}]\\n\\n\")\n        \n        # Write the image path\n        f.write(f\"**Image:** `RU/{image_files[shot_id]}`\\n\\n\")\n        \n        # Write the assembly source\n        f.write(\"**Assembly Source:**\\n```json\\n\")\n        f.write(json.dumps(entry, indent=2))\n        f.write(\"\\n```\\n\\n\")\n        \n        # Write the prompt\n        f.write(f\"**Prompt:** {prompts[shot_id].split(' \u00b7 ', 1)[1]}\\n\\n\")\n        f.write(\"---\\n\\n\")\n        \nprint(f\"Generated codex entries written to {output_file}\")\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/TIGER/RU/RU_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER/RU/RU_prompts.md",
      "/Users/gaia/resurrecting atlantis/TIGER/RU/RU_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER/RU",
      "/Users/gaia/resurrecting atlantis/TIGER/RU_codex_entries.md",
      "{index//60:02d}:{index%60:02d}:00",
      "**Image:** `RU/{image_files[shot_id]}`\\n\\n"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/codex_overlay_twolines.py",
    "size": 6348,
    "lines": 154,
    "source": "#!/usr/bin/env python3\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nimport textwrap\nimport random\n\ndef create_codex_overlay(output_path, shot_id=\"FL012\", timestamp=\"02:38:34\"):\n    \"\"\"Create a mockup with two-line header and prompt in footer\"\"\"\n    # Create a black canvas (simulating video frame)\n    width, height = 1280, 720\n    image = Image.new('RGB', (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Load font\n    try:\n        header_font = ImageFont.truetype(\"Arial.ttf\", 14)\n        footer_font = ImageFont.truetype(\"Arial.ttf\", 18)\n        title_font = ImageFont.truetype(\"Arial.ttf\", 16)\n    except IOError:\n        try:\n            header_font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n            footer_font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n            title_font = ImageFont.truetype(\"DejaVuSans.ttf\", 16)\n        except IOError:\n            header_font = ImageFont.load_default()\n            footer_font = ImageFont.load_default()\n            title_font = ImageFont.load_default()\n    \n    # Define colors\n    cyan = (0, 255, 255)\n    amber = (255, 191, 0)\n    white = (255, 255, 255)\n    green = (80, 255, 80)\n    \n    # Sample assembly.json data\n    assembly_data = {\n        \"id\": \"FL012\",\n        \"poem\": \"Flashing Lights\",\n        \"content\": \"a concussion,\",\n        \"syntagmaType\": \"Perception-Image\",\n        \"operativeEkphrasis\": \"Stars explode behind eyelids--fireworks seen from inside a skull.\",\n        \"imageType\": \"Perception-Image\",\n        \"cineosisFunction\": \"Subjective Frame Recalibration\"\n    }\n    \n    # Create top header bar with assembly data - now two-line high\n    header_height = 60  # Height increased for two lines\n    draw.rectangle([(0, 0), (width, header_height)], fill=(0, 0, 0, 180))\n    draw.line([(0, header_height), (width, header_height)], fill=cyan, width=1)\n    \n    # Add scanlines to header\n    for y in range(0, header_height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n    \n    # Add text to header - first line\n    padding = 20\n    draw.text((padding, 9), f\"{assembly_data['id']}\", fill=cyan, font=title_font)\n    draw.text((padding + 70, 9), f\"[{timestamp}]\", fill=white, font=header_font)\n    draw.text((padding + 170, 9), f\"{assembly_data['poem']}\", fill=amber, font=title_font)\n    draw.text((padding + 320, 9), f\"\\\"{assembly_data['content']}\\\"\", fill=white, font=header_font)\n    \n    # Add frame counter to far right of first line\n    frame_num = random.randint(1000, 9000)\n    draw.text((width-90, 9), f\"F:{frame_num}\", fill=cyan, font=header_font)\n    \n    # Second line - evenly spaced metadata\n    spacing = width // 4  # Dividing the width into 4 sections\n    \n    draw.text((spacing - 120, 33), f\"TYPE: {assembly_data['imageType']}\", fill=green, font=header_font)\n    draw.text((2 * spacing - 120, 33), f\"FUNC: {assembly_data['cineosisFunction']}\", fill=green, font=header_font)\n    draw.text((3 * spacing - 120, 33), f\"SYNT: {assembly_data['syntagmaType']}\", fill=green, font=header_font)\n    \n    # Create the footer console overlay\n    footer_height = 80  # Height of bottom overlay\n    footer_y = height - footer_height\n    \n    # Draw semi-transparent black background for footer\n    draw.rectangle([(0, footer_y), (width, height)], fill=(0, 0, 0, 180))\n    draw.line([(0, footer_y), (width, footer_y)], fill=cyan, width=1)\n    \n    # Add scanline effect to footer\n    for y in range(footer_y, height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n    \n    # Add prompt text to footer\n    prompt_text = \"PROMPT: Stars explode behind eyelids--fireworks seen from inside a skull \u00b7 POV macro; eyelids closed, inner nebula bursts saffron-teal sparks, sub-bass rumble.\"\n    \n    # Wrap the prompt text across multiple lines if needed\n    wrapped_prompt = textwrap.wrap(prompt_text, width=110)\n    y_text = footer_y + 15\n    \n    # Add a small label for the prompt\n    draw.text((padding, y_text), \"PROMPT:\", fill=cyan, font=title_font)\n    \n    # Print the wrapped prompt text\n    for i, line in enumerate(wrapped_prompt):\n        if i == 0:\n            # For the first line, skip the \"PROMPT:\" label that we manually added\n            if line.startswith(\"PROMPT: \"):\n                line = line[8:]\n            draw.text((padding + 80, y_text), line, fill=white, font=footer_font)\n        else:\n            draw.text((padding, y_text + (i * 22)), line, fill=white, font=footer_font)\n    \n    # Add a timeline indicator at the bottom of the footer\n    timeline_y = height - 15\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 4 if i % 3 == 0 else 2\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f\"Codex overlay mockup created and saved to: {output_path}\")\n\ndef main():\n    output_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mockups\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create codex overlay mockup\n    output_path = os.path.join(output_dir, \"codex_overlay_twolines.png\")\n    create_codex_overlay(output_path)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "codex_overlay_twolines.png",
      ", (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Load font\n    try:\n        header_font = ImageFont.truetype(",
      ", fill=cyan, font=header_font)\n    \n    # Second line - evenly spaced metadata\n    spacing = width // 4  # Dividing the width into 4 sections\n    \n    draw.text((spacing - 120, 33), f",
      "):\n                line = line[8:]\n            draw.text((padding + 80, y_text), line, fill=white, font=footer_font)\n        else:\n            draw.text((padding, y_text + (i * 22)), line, fill=white, font=footer_font)\n    \n    # Add a timeline indicator at the bottom of the footer\n    timeline_y = height - 15\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 4 if i % 3 == 0 else 2\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mockups"
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os",
      "textwrap",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/create_id_matched_overlays.py",
    "size": 9180,
    "lines": 256,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nfrom PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont\n\n# Define paths\nmpost_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey\"\nmidjourney_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]\"\noutput_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/id_matched_overlays\"\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Get all image files\nmpost_files = [f for f in os.listdir(mpost_dir) if f.lower().endswith('.png') and not f.startswith('original_')]\nmidjourney_files = [f for f in os.listdir(midjourney_dir) if f.lower().endswith('.png') and not os.path.isdir(os.path.join(midjourney_dir, f))]\n\nprint(f\"Found {len(mpost_files)} mpost files\")\nprint(f\"Found {len(midjourney_files)} midjourney files\")\n\n# Extract ID number from filename\ndef get_id_from_filename(filename):\n    # Pattern to match the ID at the beginning of the filename (e.g., 01_SH, 02_FL)\n    match = re.match(r'^(\\d+)_([A-Z]+)_', filename)\n    if match:\n        return match.group(1)  # Return just the number part\n    return None\n\n# Group files by ID\nmpost_by_id = {}\nmidjourney_by_id = {}\n\nfor file in mpost_files:\n    id_num = get_id_from_filename(file)\n    if id_num:\n        if id_num not in mpost_by_id:\n            mpost_by_id[id_num] = []\n        mpost_by_id[id_num].append(file)\n\nfor file in midjourney_files:\n    id_num = get_id_from_filename(file)\n    if id_num:\n        if id_num not in midjourney_by_id:\n            midjourney_by_id[id_num] = []\n        midjourney_by_id[id_num].append(file)\n\nprint(f\"Grouped into {len(mpost_by_id)} mpost IDs\")\nprint(f\"Grouped into {len(midjourney_by_id)} midjourney IDs\")\n\n# Define overlay modes to use\noverlay_modes = [\n    (\"Screen\", lambda fg, bg: Image.blend(bg, ImageEnhance.Brightness(fg).enhance(1.5), 0.7)),\n    (\"Multiply\", lambda fg, bg: Image.blend(bg, ImageEnhance.Brightness(fg).enhance(0.5), 0.7)),\n    (\"Overlay\", lambda fg, bg: Image.blend(bg, ImageEnhance.Contrast(fg).enhance(1.8), 0.6)),\n    (\"Soft Light\", lambda fg, bg: Image.blend(bg, fg.filter(ImageFilter.GaussianBlur(radius=2)), 0.5))\n]\n\n# Try to load a font for labels\ntry:\n    font = ImageFont.truetype(\"Arial.ttf\", 24)\n    small_font = ImageFont.truetype(\"Arial.ttf\", 16)\nexcept:\n    try:\n        font = ImageFont.truetype(\"/Library/Fonts/Arial.ttf\", 24)\n        small_font = ImageFont.truetype(\"/Library/Fonts/Arial.ttf\", 16)\n    except:\n        font = ImageFont.load_default()\n        small_font = ImageFont.load_default()\n\n# Function to create overlay and save to output directory\ndef create_overlay(fg_file, bg_file, output_path, mode_name, mode_func):\n    try:\n        # Open images\n        fg_img = Image.open(os.path.join(mpost_dir, fg_file)).convert(\"RGBA\")\n        bg_img = Image.open(os.path.join(midjourney_dir, bg_file)).convert(\"RGBA\")\n        \n        # Resize foreground to match background dimensions\n        fg_img = fg_img.resize(bg_img.size)\n        \n        # Create overlay using the provided function\n        # Convert to RGB for blend operations\n        fg_rgb = fg_img.convert(\"RGB\")\n        bg_rgb = bg_img.convert(\"RGB\")\n        result = mode_func(fg_rgb, bg_rgb)\n        \n        # Add a label to identify the overlay\n        draw = ImageDraw.Draw(result)\n        # Extract track info for label\n        track_name = fg_file.split('_', 3)[2] if len(fg_file.split('_')) > 2 else \"Unknown\"\n        track_id = fg_file.split('_', 2)[0] + \"_\" + fg_file.split('_', 2)[1] if len(fg_file.split('_')) > 1 else \"Unknown\"\n        \n        # Add label with black background for readability\n        draw.rectangle([(10, 10), (400, 80)], fill=(0, 0, 0, 180))\n        draw.text((20, 15), f\"{track_name} - {mode_name}\", fill=(255, 255, 255), font=font)\n        draw.text((20, 45), f\"FG: {os.path.basename(fg_file)}\", fill=(220, 220, 220), font=small_font)\n        draw.text((20, 65), f\"BG: {os.path.basename(bg_file)}\", fill=(220, 220, 220), font=small_font)\n        \n        # Save the result\n        result.save(output_path, quality=95)\n        return True\n    except Exception as e:\n        print(f\"Error processing {fg_file} and {bg_file}: {e}\")\n        return False\n\n# Create overlays for each matching ID\noverlay_count = 0\nfor id_num in sorted(set(list(mpost_by_id.keys()) + list(midjourney_by_id.keys()))):\n    # Skip if either directory doesn't have this ID\n    if id_num not in mpost_by_id or id_num not in midjourney_by_id:\n        print(f\"ID {id_num} not found in both directories, skipping\")\n        continue\n    \n    # Get files for this ID\n    fg_files = mpost_by_id[id_num]\n    bg_files = midjourney_by_id[id_num]\n    \n    print(f\"Processing ID {id_num}: {len(fg_files)} foreground files, {len(bg_files)} background files\")\n    \n    # Create overlays with all combinations\n    for fg_file in fg_files:\n        for bg_file in bg_files:\n            # Create a subdirectory for this ID\n            id_dir = os.path.join(output_dir, f\"{id_num}\")\n            os.makedirs(id_dir, exist_ok=True)\n            \n            # Create overlays with different modes\n            for mode_name, mode_func in overlay_modes:\n                # Create descriptive filename\n                fg_base = os.path.splitext(os.path.basename(fg_file))[0]\n                bg_base = os.path.splitext(os.path.basename(bg_file))[0]\n                output_filename = f\"{fg_base}_over_{bg_base}_{mode_name.replace(' ', '')}.jpg\"\n                output_path = os.path.join(id_dir, output_filename)\n                \n                # Create and save overlay\n                if create_overlay(fg_file, bg_file, output_path, mode_name, mode_func):\n                    overlay_count += 1\n                    print(f\"Created overlay: {output_filename}\")\n\nprint(f\"\\nCreated {overlay_count} overlay images in {output_dir}\")\n\n# Create HTML file to view the overlays\nhtml_output = os.path.join(output_dir, \"view_overlays.html\")\nhtml_content = f\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <title>ID-Matched Overlays</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1, h2, h3 {{ \n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1200px;\n            margin: 0 auto;\n        }}\n        .track {{\n            margin-bottom: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        .overlays {{\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));\n            gap: 20px;\n            margin-top: 20px;\n        }}\n        .overlay {{\n            background-color: #2a2a2a;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n        img {{ \n            max-width: 100%; \n            height: auto;\n            display: block;\n            border: 1px solid #333;\n        }}\n        .info {{\n            margin-top: 10px;\n            font-size: 14px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>ID-Matched Overlays</h1>\n        <p>This page shows the overlay effects created by combining foreground images from mpost-journey with background images from midjourney.</p>\n\"\"\"\n\n# Add content for each track ID\nfor id_num in sorted(set(list(mpost_by_id.keys()) + list(midjourney_by_id.keys()))):\n    if id_num not in mpost_by_id or id_num not in midjourney_by_id:\n        continue\n    \n    # Get files for this ID to extract track info\n    fg_files = mpost_by_id[id_num]\n    if not fg_files:\n        continue\n        \n    # Get track name from first file\n    track_parts = fg_files[0].split('_', 3)\n    track_name = track_parts[2] if len(track_parts) > 2 else \"Unknown\"\n    \n    html_content += f\"\"\"\n        <div class=\"track\">\n            <h2>Track {id_num}: {track_name}</h2>\n            <div class=\"overlays\">\n    \"\"\"\n    \n    # Get all overlay images for this ID\n    id_dir = os.path.join(output_dir, f\"{id_num}\")\n    if os.path.exists(id_dir):\n        overlay_files = [f for f in os.listdir(id_dir) if f.lower().endswith(('.jpg', '.png'))]\n        for overlay_file in sorted(overlay_files):\n            # Extract mode from filename\n            mode = \"Unknown\"\n            if \"Screen\" in overlay_file:\n                mode = \"Screen\"\n            elif \"Multiply\" in overlay_file:\n                mode = \"Multiply\"\n            elif \"Overlay\" in overlay_file:\n                mode = \"Overlay\"\n            elif \"SoftLight\" in overlay_file:\n                mode = \"Soft Light\"\n                \n            html_content += f\"\"\"\n                <div class=\"overlay\">\n                    <h3>{mode}</h3>\n                    <img src=\"{id_num}/{overlay_file}\" alt=\"{overlay_file}\">\n                    <div class=\"info\">{overlay_file}</div>\n                </div>\n            \"\"\"\n    \n    html_content += \"\"\"\n            </div>\n        </div>\n    \"\"\"\n\nhtml_content += \"\"\"\n    </div>\n</body>\n</html>\n\"\"\"\n\nwith open(html_output, 'w') as f:\n    f.write(html_content)\n\nprint(f\"Created HTML viewer: {html_output}\")\n",
    "file_references": [
      ")}.jpg",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/id_matched_overlays",
      "/Library/Fonts/Arial.ttf",
      "/Library/Fonts/Arial.ttf",
      "<!DOCTYPE html>\n<html>\n<head>\n    <title>ID-Matched Overlays</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1, h2, h3 {{ \n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1200px;\n            margin: 0 auto;\n        }}\n        .track {{\n            margin-bottom: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        .overlays {{\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));\n            gap: 20px;\n            margin-top: 20px;\n        }}\n        .overlay {{\n            background-color: #2a2a2a;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n        img {{ \n            max-width: 100%; \n            height: auto;\n            display: block;\n            border: 1px solid #333;\n        }}\n        .info {{\n            margin-top: 10px;\n            font-size: 14px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=",
      ">\n        <h1>ID-Matched Overlays</h1>\n        <p>This page shows the overlay effects created by combining foreground images from mpost-journey with background images from midjourney.</p>\n",
      ">\n            <h2>Track {id_num}: {track_name}</h2>\n            <div class=",
      ">\n                    <h3>{mode}</h3>\n                    <img src=",
      ">{overlay_file}</div>\n                </div>\n            ",
      "\n            </div>\n        </div>\n    ",
      "\n    </div>\n</body>\n</html>\n"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "PIL"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/sequence_mpre_run_files_improved.py",
    "size": 3845,
    "lines": 88,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\n# Path to the mpre-run directory\nmpre_run_dir = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run')\n\n# Create a backup directory\nbackup_dir = mpre_run_dir / 'backup_final'\nbackup_dir.mkdir(exist_ok=True)\n\n# Define the new naming mapping\ntrack_mapping = {\n    \"Out of Life\": {\"new_id\": \"01\", \"code\": \"SH\", \"timecode\": \"000000\"},\n    \"Flashing Lights\": {\"new_id\": \"02\", \"code\": \"FL\", \"timecode\": \"021100\"},\n    \"How to Break Off an Engagement\": {\"new_id\": \"03\", \"code\": \"HT\", \"timecode\": \"042200\"},\n    \"Nevermore\": {\"new_id\": \"04\", \"code\": \"NM\", \"timecode\": \"063300\"},\n    \"Bloodline\": {\"new_id\": \"05\", \"code\": \"BE\", \"timecode\": \"084400\"},\n    \"Resurrecting Atlantis\": {\"new_id\": \"06\", \"code\": \"AT\", \"timecode\": \"105500\"},\n    \"DJ Turn Me Up\": {\"new_id\": \"07\", \"code\": \"DJ\", \"timecode\": \"130600\"},\n    \"Newly Single\": {\"new_id\": \"08\", \"code\": \"NS\", \"timecode\": \"151700\"},\n    \"Yet Heard\": {\"new_id\": \"09\", \"code\": \"YH\", \"timecode\": \"172800\"},\n    \"Magic Ride\": {\"new_id\": \"10\", \"code\": \"MR\", \"timecode\": \"193900\"},\n    \"Reunion\": {\"new_id\": \"12\", \"code\": \"RU\", \"timecode\": \"215000\"},\n    \"How to Win My Heart\": {\"new_id\": \"13\", \"code\": \"HW\", \"timecode\": \"240100\"},\n    \"Hot Minute\": {\"new_id\": \"14\", \"code\": \"HM\", \"timecode\": \"261200\"},\n}\n\n# Process all files in the directory\nfor file_path in mpre_run_dir.glob('*'):\n    if file_path.is_file() and file_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n        # Skip backup directories\n        if 'backup' in str(file_path):\n            continue\n            \n        # Create backup\n        backup_file = backup_dir / file_path.name\n        shutil.copy2(file_path, backup_file)\n        \n        # Extract the original filename\n        filename = file_path.name\n        \n        # Try to match track name more aggressively\n        matched_track = None\n        for track_name, mapping in track_mapping.items():\n            # Replace spaces with empty string to match tracks like \"HotMinute\" vs \"Hot Minute\"\n            no_spaces_track = track_name.replace(\" \", \"\")\n            # Also check for track name without spaces\n            if (track_name in filename or \n                no_spaces_track in filename.replace(\" \", \"\") or\n                track_name.lower() in filename.lower() or\n                no_spaces_track.lower() in filename.lower().replace(\" \", \"\")):\n                matched_track = track_name\n                break\n        \n        # Determine the type of file\n        file_type = \"Misc\"\n        if \"Title Card\" in filename or \"TitleCard\" in filename or \"Prompt Set\" in filename:\n            file_type = \"TitleCard\"\n        elif \"Visual Concept\" in filename or \"VisualConcept\" in filename:\n            file_type = \"VisualConcept\"\n        elif \"Poetic Ekphrasis\" in filename or \"Ekphrasis\" in filename:\n            file_type = \"Ekphrasis\"\n        elif \"Duration\" in filename and not \"Title Card\" in filename:\n            file_type = \"Segment\"\n            \n        # If we found a match, rename the file\n        if matched_track:\n            mapping = track_mapping[matched_track]\n            track_code = mapping[\"code\"]\n            track_id = mapping[\"new_id\"]\n            timecode = mapping[\"timecode\"]\n            \n            # Create new filename\n            cleaned_track_name = matched_track.replace(\" \", \"\")\n            new_name = f\"{track_id}_{track_code}_{cleaned_track_name}_{timecode}_{file_type}{file_path.suffix}\"\n            \n            # Rename the file\n            new_file_path = file_path.parent / new_name\n            os.rename(file_path, new_file_path)\n            print(f\"Renamed: {file_path.name} -> {new_name}\")\n        else:\n            print(f\"Could not determine track for: {file_path.name}\")\n\nprint(f\"\\nAll files processed. Backup copies saved in {backup_dir}\")\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run",
      " in str(file_path):\n            continue\n            \n        # Create backup\n        backup_file = backup_dir / file_path.name\n        shutil.copy2(file_path, backup_file)\n        \n        # Extract the original filename\n        filename = file_path.name\n        \n        # Try to match track name more aggressively\n        matched_track = None\n        for track_name, mapping in track_mapping.items():\n            # Replace spaces with empty string to match tracks like ",
      "\n            \n            # Rename the file\n            new_file_path = file_path.parent / new_name\n            os.rename(file_path, new_file_path)\n            print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/rename_mpost_files.py",
    "size": 2093,
    "lines": 71,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport shutil\n\n# Path to the folder\nfolder_path = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey\"\n\n# Track naming scheme\ntrack_names = [\n    \"01_SH_OutOfLife_000000\",\n    \"02_FL_FlashingLights_021100\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\",\n    \"04_NM_Nevermore_063300\",\n    \"05_BE_Bloodline_084400\",\n    \"06_AT_ResurrectingAtlantis_105500\",\n    \"07_DJ_DJTurnMeUp_130600\",\n    \"08_NS_NewlySingle_151700\",\n    \"09_YH_YetHeard_172800\",\n    \"10_MR_MagicRide_193900\",\n    \"12_RU_Reunion_215000\",\n    \"13_HW_HowToWinMyHeart_240100\",\n    \"14_HM_HotMinute_261200\"\n]\n\n# Get all PNG files in the folder\nfiles = [f for f in os.listdir(folder_path) if f.lower().endswith('.png')]\n\n# Sort files by their timestamp to ensure correct order\nfiles.sort()\n\n# Create a backup folder\nbackup_folder = os.path.join(folder_path, \"original_files_backup\")\nos.makedirs(backup_folder, exist_ok=True)\n\n# Function to extract the descriptive part of the filename\ndef extract_description(filename):\n    # Pattern to match the timestamp and descriptive part\n    pattern = r\"\\d{8}_\\d{4}_(.*?)_simple_compose\"\n    match = re.search(pattern, filename)\n    if match:\n        return match.group(1)\n    return \"Untitled\"  # Fallback if pattern doesn't match\n\n# Rename files\nrenamed_count = 0\nfor i, file in enumerate(files):\n    if i >= len(track_names):\n        print(f\"Warning: More files than track names. Skipping {file}\")\n        continue\n        \n    old_path = os.path.join(folder_path, file)\n    \n    # Extract the descriptive part\n    description = extract_description(file)\n    \n    # Create new filename\n    new_filename = f\"{track_names[i]}_{description}.png\"\n    new_path = os.path.join(folder_path, new_filename)\n    \n    # Backup original file\n    backup_path = os.path.join(backup_folder, file)\n    shutil.copy2(old_path, backup_path)\n    \n    # Rename the file\n    os.rename(old_path, new_path)\n    renamed_count += 1\n    print(f\"Renamed: {file} \u2192 {new_filename}\")\n\nprint(f\"\\nRenamed {renamed_count} files. Originals backed up in: {backup_folder}\")\n",
    "file_references": [
      "{track_names[i]}_{description}.png",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/generate_ht_ordered_video.py",
    "size": 7059,
    "lines": 198,
    "source": "#!/usr/bin/env python3\n\"\"\"\nSpecial HT Video Generator that specifically handles the HT_shots directory structure\nand ensures the correct sequence from the prompts file.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport subprocess\nimport argparse\n\n# Base directories\nTIGER_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nJELLYFISH_DIR = \"/Users/gaia/resurrecting atlantis/JELLYFISH\"\nOUTPUT_DIR = os.path.join(JELLYFISH_DIR, \"video_output\")\nORDERED_CODEX_PATH = os.path.join(TIGER_DIR, \"HT_ordered_codex_entries.md\")\nMANTA_AUDIO_DIR = \"/Users/gaia/resurrecting atlantis/MANTA/audio\"\n\ndef rename_existing_videos():\n    \"\"\"Rename any existing HT videos to preserve them with MISFIT suffix.\"\"\"\n    for filename in [\"HT_header_prompt.mp4\", \"HT_header_prompt_with_audio.mp4\"]:\n        filepath = os.path.join(OUTPUT_DIR, filename)\n        misfit_filepath = os.path.join(OUTPUT_DIR, filename.replace(\".mp4\", \"_MISFIT.mp4\"))\n        \n        if os.path.exists(filepath) and not os.path.exists(misfit_filepath):\n            print(f\"Renaming existing video to {os.path.basename(misfit_filepath)}\")\n            os.rename(filepath, misfit_filepath)\n\ndef extract_available_images():\n    \"\"\"Extract a list of available HT images in the HT_shots directory.\"\"\"\n    shots_dir = os.path.join(TIGER_DIR, \"HT\", \"HT_shots\")\n    available_images = {}\n    \n    if os.path.exists(shots_dir):\n        for filename in os.listdir(shots_dir):\n            if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n                shot_id = filename.split(\"__\")[0]\n                if shot_id not in available_images:\n                    available_images[shot_id] = []\n                available_images[shot_id].append(filename)\n    \n    return available_images\n\ndef create_simplified_codex_for_available_shots():\n    \"\"\"Create a simplified version of the ordered codex that only includes shots with available images.\"\"\"\n    if not os.path.exists(ORDERED_CODEX_PATH):\n        print(f\"Error: Ordered codex file not found at {ORDERED_CODEX_PATH}\")\n        return False\n    \n    # Get available images\n    available_images = extract_available_images()\n    print(f\"Found {len(available_images)} shots with available images\")\n    \n    # Read the ordered codex\n    with open(ORDERED_CODEX_PATH, 'r') as f:\n        codex_content = f.read()\n    \n    # Extract all entries using a more flexible pattern\n    sections = codex_content.split('---')\n    filtered_entries = []\n    \n    for section in sections:\n        section = section.strip()\n        if not section or '### HT' not in section:\n            continue\n        \n        # Extract shot ID (e.g., HT001, HT002)\n        shot_id_match = re.search(r'### (HT\\d+)', section)\n        if not shot_id_match:\n            continue\n            \n        shot_id = shot_id_match.group(1)\n        \n        # Check if we have an image for this shot\n        if shot_id in available_images:\n            # Take the first available image for this shot\n            filename = available_images[shot_id][0]\n            \n            # Replace the image path in the section with our actual image filename\n            # that exists in the HT_shots directory\n            modified_section = re.sub(\n                r'\\*\\*Image:\\*\\* `[^`]*`', \n                f'**Image:** `{os.path.basename(filename)}`', \n                section\n            )\n            \n            filtered_entries.append(modified_section + '\\n\\n---\\n')\n            \n            print(f\"Added entry for {shot_id} with image {os.path.basename(filename)}\")\n    \n    # Create new codex file with only available images\n    simplified_codex_path = os.path.join(TIGER_DIR, \"HT_available_shots_codex.md\")\n    with open(simplified_codex_path, 'w') as f:\n        f.write(\"# HT (How To Win My Heart) Codex Entries - AVAILABLE SHOTS\\n\\n\")\n        f.write(\"\".join(filtered_entries))\n    \n    print(f\"Created simplified codex with {len(filtered_entries)} available shots at {simplified_codex_path}\")\n    return simplified_codex_path\n\ndef generate_video(simplified_codex_path):\n    \"\"\"Generate the HT video using the simplified codex with available shots.\"\"\"\n    # Check if the video generator script exists\n    video_generator = os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\")\n    if not os.path.exists(video_generator):\n        print(f\"Error: Video generator script not found at {video_generator}\")\n        return False\n    \n    # Run the video generator with the simplified codex\n    cmd = [\n        \"python3\",\n        video_generator,\n        \"--prefix\", \"HT\",\n        \"--ordered-codex\", simplified_codex_path\n    ]\n    \n    print(\"Generating video with available shots...\")\n    result = subprocess.run(cmd, cwd=JELLYFISH_DIR, text=True)\n    \n    if result.returncode != 0:\n        print(\"Error generating video\")\n        return False\n    \n    print(\"Video generated successfully\")\n    return True\n\ndef add_audio_to_video():\n    \"\"\"Add audio to the generated video.\"\"\"\n    video_path = os.path.join(OUTPUT_DIR, \"HT_header_prompt.mp4\")\n    if not os.path.exists(video_path):\n        print(f\"Error: Generated video not found at {video_path}\")\n        return False\n    \n    # Find the audio file\n    audio_file = \"How To Win My Heart_Echoes of Longing remix v2.2.1.mp3\"\n    audio_path = os.path.join(MANTA_AUDIO_DIR, audio_file)\n    \n    if not os.path.exists(audio_path):\n        print(f\"Error: Audio file not found at {audio_path}\")\n        return False\n    \n    # Output path for video with audio\n    output_path = os.path.join(OUTPUT_DIR, \"HT_header_prompt_with_audio.mp4\")\n    \n    # Add audio to video using ffmpeg\n    print(\"Adding audio to video...\")\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", video_path,\n        \"-i\", audio_path,\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-shortest\",\n        output_path\n    ]\n    \n    result = subprocess.run(cmd, text=True)\n    \n    if result.returncode != 0:\n        print(\"Error adding audio to video\")\n        return False\n    \n    print(f\"Video with audio generated at {output_path}\")\n    return True\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Generate HT video with correct shot order from available images\")\n    parser.add_argument(\"--skip-rename\", action=\"store_true\", help=\"Skip renaming existing videos to MISFIT\")\n    args = parser.parse_args()\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Rename existing videos if any\n    if not args.skip_rename:\n        rename_existing_videos()\n    \n    # Create a simplified codex with only available shots\n    simplified_codex_path = create_simplified_codex_for_available_shots()\n    if not simplified_codex_path:\n        return 1\n    \n    # Generate the video\n    if not generate_video(simplified_codex_path):\n        return 1\n    \n    # Add audio to the video\n    if not add_audio_to_video():\n        return 1\n    \n    print(\"HT video with correct shot order and audio generated successfully!\")\n    return 0\n\nif __name__ == \"__main__\":\n    exit(main())\n",
    "file_references": [
      "HT_header_prompt.mp4",
      "HT_header_prompt_with_audio.mp4",
      "_MISFIT.mp4",
      "HT_header_prompt.mp4",
      "How To Win My Heart_Echoes of Longing remix v2.2.1.mp3",
      "HT_header_prompt_with_audio.mp4",
      "/Users/gaia/resurrecting atlantis/TIGER",
      "/Users/gaia/resurrecting atlantis/JELLYFISH",
      "/Users/gaia/resurrecting atlantis/MANTA/audio"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd, cwd=JELLYFISH_DIR, text=True"
      },
      {
        "type": "run",
        "snippet": "cmd, text=True"
      }
    ],
    "imports": [
      "os",
      "re",
      "json",
      "subprocess",
      "argparse"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Special HT Video Generator that specifically handles the HT_shots directory structure\nand ensures the correct sequence from the prompts file."
  },
  {
    "path": "JELLYFISH/normalize_impala_filenames.py",
    "size": 4158,
    "lines": 124,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\n# Directory containing files to normalize\nIMPALA_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA')\n\n# Create backup directory\nBACKUP_DIR = IMPALA_DIR / 'original_files_backup'\nBACKUP_DIR.mkdir(exist_ok=True)\n\n# Track ID patterns to search for\nTRACK_PATTERNS = [\n    r'01_SH_OutOfLife_000000',\n    r'02_FL_FlashingLights_021100',\n    r'03_HT_HowToBreakOffAnEngagement_042200',\n    r'04_NM_Nevermore_063300',\n    r'05_BE_Bloodline_084400',\n    r'06_AT_ResurrectingAtlantis_105500',\n    r'07_DJ_DJTurnMeUp_130600',\n    r'08_NS_NewlySingle_151700',\n    r'09_YH_YetHeard_172800',\n    r'10_MR_MagicRide_193900',\n    r'12_RU_Reunion_215000',\n    r'13_HW_HowToWinMyHeart_240100',\n    r'14_HM_HotMinute_261200'\n]\n\n# Case-insensitive variations of the patterns\nTRACK_VARIATIONS = {}\nfor pattern in TRACK_PATTERNS:\n    # Create variations with different casing\n    base_pattern = pattern.lower()\n    TRACK_VARIATIONS[base_pattern] = pattern  # Store the correct casing version\n\n# Function to extract track ID from filename\ndef extract_track_id(filename):\n    filename_lower = filename.lower()\n    \n    # Try to match any of the track patterns\n    for base_pattern, correct_pattern in TRACK_VARIATIONS.items():\n        if base_pattern in filename_lower:\n            return correct_pattern\n    \n    return None\n\n# Function to normalize a filename\ndef normalize_filename(filename, track_id):\n    # Remove the track ID from the filename (to avoid duplication)\n    filename_lower = filename.lower()\n    base_pattern = track_id.lower()\n    \n    # Find the position of the track ID in the filename\n    start_pos = filename_lower.find(base_pattern)\n    \n    if start_pos != -1:\n        # Extract the part after the track ID\n        end_pos = start_pos + len(base_pattern)\n        suffix = filename[end_pos:]\n        \n        # Remove any leading separators or spaces\n        suffix = suffix.lstrip('_ -,')\n        \n        # Create the new filename\n        if suffix:\n            return f\"{track_id}_{suffix}\"\n        else:\n            return track_id\n    else:\n        # Fallback case (shouldn't happen if extract_track_id found a match)\n        return f\"{track_id}_{filename}\"\n\n# Main function to process all files\ndef normalize_impala_filenames():\n    print(\"Starting to normalize filenames in IMPALA directory...\")\n    \n    # Track statistics\n    total_files = 0\n    renamed_files = 0\n    skipped_files = 0\n    \n    # Process each file in the directory\n    for file_path in IMPALA_DIR.glob('*'):\n        if file_path.is_file() and file_path.name != '.DS_Store':\n            total_files += 1\n            \n            # Extract track ID if present\n            track_id = extract_track_id(file_path.name)\n            \n            if track_id:\n                # Check if the filename already starts with the track ID\n                if file_path.name.startswith(track_id):\n                    print(f\"File already normalized: {file_path.name}\")\n                    skipped_files += 1\n                    continue\n                \n                # Create the new normalized filename\n                new_filename = normalize_filename(file_path.name, track_id)\n                new_file_path = file_path.parent / new_filename\n                \n                # Create backup\n                backup_path = BACKUP_DIR / file_path.name\n                print(f\"Backing up: {file_path.name} -> {backup_path.name}\")\n                shutil.copy2(file_path, backup_path)\n                \n                # Rename the file\n                print(f\"Renaming: {file_path.name} -> {new_filename}\")\n                os.rename(file_path, new_file_path)\n                renamed_files += 1\n            else:\n                print(f\"No track ID found in: {file_path.name}\")\n                skipped_files += 1\n    \n    print(\"\\nNormalization Complete!\")\n    print(f\"Total files processed: {total_files}\")\n    print(f\"Files renamed: {renamed_files}\")\n    print(f\"Files skipped: {skipped_files}\")\n    print(f\"Original files backed up to: {BACKUP_DIR}\")\n\nif __name__ == \"__main__\":\n    normalize_impala_filenames()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/IMPALA",
      ")\n                    skipped_files += 1\n                    continue\n                \n                # Create the new normalized filename\n                new_filename = normalize_filename(file_path.name, track_id)\n                new_file_path = file_path.parent / new_filename\n                \n                # Create backup\n                backup_path = BACKUP_DIR / file_path.name\n                print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/analyze_and_fix_ibex_videos.py",
    "size": 15724,
    "lines": 405,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nimport shutil\nfrom pathlib import Path\n\n# Directory containing video files\nIBEX_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/IBEX')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Create a diagnostic directory\nDIAGNOSTIC_DIR = OUTPUT_DIR / \"diagnostic\"\nDIAGNOSTIC_DIR.mkdir(exist_ok=True)\n\n# Track order specified\nTRACK_ORDER = [\n    \"01_SH_OutOfLife_000000\",\n    \"02_FL_FlashingLights_021100\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\",\n    \"04_NM_Nevermore_063300\",\n    \"05_BE_Bloodline_084400\",\n    \"06_AT_ResurrectingAtlantis_105500\",\n    \"07_DJ_DJTurnMeUp_130600\",\n    \"08_NS_NewlySingle_151700\",\n    \"09_YH_YetHeard_172800\",\n    \"10_MR_MagicRide_193900\",\n    \"12_RU_Reunion_215000\",\n    \"13_HW_HowToWinMyHeart_240100\",\n    \"14_HM_HotMinute_261200\"\n]\n\ndef run_ffprobe(video_path, args):\n    \"\"\"Run ffprobe with the specified arguments and return the output.\"\"\"\n    try:\n        result = subprocess.run(\n            ['ffprobe', '-v', 'error'] + args + [str(video_path)],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        return result.stdout.strip()\n    except subprocess.CalledProcessError as e:\n        print(f\"Error running ffprobe on {video_path}: {e}\")\n        print(f\"ffprobe stderr: {e.stderr}\")\n        return None\n\ndef get_detailed_video_info(video_path):\n    \"\"\"Get detailed information about a video file using FFprobe.\"\"\"\n    info = {}\n    \n    # Get format information (duration, bit rate, etc.)\n    format_json = run_ffprobe(\n        video_path, \n        ['-show_entries', 'format=duration,size,bit_rate,format_name', '-of', 'json']\n    )\n    \n    if format_json:\n        try:\n            format_data = json.loads(format_json)\n            if 'format' in format_data:\n                info.update(format_data['format'])\n        except json.JSONDecodeError:\n            print(f\"Error parsing format JSON for {video_path}\")\n    \n    # Get stream information (codecs, resolution, etc.)\n    streams_json = run_ffprobe(\n        video_path,\n        ['-show_entries', 'stream=codec_name,codec_type,width,height,r_frame_rate,duration', '-of', 'json']\n    )\n    \n    if streams_json:\n        try:\n            streams_data = json.loads(streams_json)\n            if 'streams' in streams_data:\n                info['streams'] = streams_data['streams']\n                \n                # Extract video and audio stream info\n                for stream in streams_data['streams']:\n                    if stream.get('codec_type') == 'video':\n                        info['video_codec'] = stream.get('codec_name')\n                        info['width'] = stream.get('width')\n                        info['height'] = stream.get('height')\n                        \n                        # Parse frame rate\n                        fps_str = stream.get('r_frame_rate', '0/1')\n                        if '/' in fps_str:\n                            num, den = map(int, fps_str.split('/'))\n                            info['fps'] = num / den if den else 0\n                        else:\n                            info['fps'] = float(fps_str)\n                            \n                        # If stream has its own duration, use it\n                        if 'duration' in stream:\n                            info['video_duration'] = float(stream['duration'])\n                    \n                    elif stream.get('codec_type') == 'audio':\n                        info['audio_codec'] = stream.get('codec_name')\n                        if 'duration' in stream:\n                            info['audio_duration'] = float(stream['duration'])\n        except json.JSONDecodeError:\n            print(f\"Error parsing streams JSON for {video_path}\")\n    \n    # Convert duration to float\n    if 'duration' in info:\n        info['duration'] = float(info['duration'])\n    \n    return info\n\ndef analyze_video(video_path):\n    \"\"\"Analyze a video file and return information about it.\"\"\"\n    print(f\"\\nAnalyzing: {video_path.name}\")\n    \n    # Get detailed information\n    info = get_detailed_video_info(video_path)\n    \n    # Print key information\n    print(f\"  Format: {info.get('format_name', 'unknown')}\")\n    print(f\"  Duration: {info.get('duration', 0):.2f} seconds\")\n    if 'video_duration' in info:\n        print(f\"  Video stream duration: {info['video_duration']:.2f} seconds\")\n    if 'audio_duration' in info:\n        print(f\"  Audio stream duration: {info['audio_duration']:.2f} seconds\")\n    print(f\"  Video codec: {info.get('video_codec', 'unknown')}\")\n    print(f\"  Audio codec: {info.get('audio_codec', 'unknown')}\")\n    print(f\"  Resolution: {info.get('width', 'unknown')}x{info.get('height', 'unknown')}\")\n    print(f\"  Frame rate: {info.get('fps', 'unknown')}\")\n    print(f\"  Size: {int(info.get('size', 0)) / 1024 / 1024:.2f} MB\")\n    \n    # Identify potential issues\n    issues = []\n    \n    # Check if any duration is missing\n    if 'duration' not in info:\n        issues.append(\"Missing duration information\")\n    \n    # Check for very long or very short duration\n    if info.get('duration', 0) > 30:\n        issues.append(f\"Very long duration: {info.get('duration'):.2f} seconds\")\n    elif 0 < info.get('duration', 0) < 1:\n        issues.append(f\"Very short duration: {info.get('duration'):.2f} seconds\")\n    \n    # Check for duration mismatch between video/audio streams\n    if 'video_duration' in info and 'audio_duration' in info:\n        diff = abs(info['video_duration'] - info['audio_duration'])\n        if diff > 1:  # More than 1 second difference\n            issues.append(f\"Duration mismatch: video={info['video_duration']:.2f}s, audio={info['audio_duration']:.2f}s\")\n    \n    # Check for other potential issues\n    if info.get('video_codec') == 'unknown' or info.get('video_codec') is None:\n        issues.append(\"Unknown or missing video codec\")\n    \n    # Report issues\n    if issues:\n        print(\"  Issues detected:\")\n        for issue in issues:\n            print(f\"  - {issue}\")\n    else:\n        print(\"  No issues detected\")\n    \n    return info, issues\n\ndef fix_video(video_path, info, issues):\n    \"\"\"Fix issues with a video file and return the path to the fixed video.\"\"\"\n    # Skip if no issues\n    if not issues:\n        return video_path, False\n    \n    print(f\"  Fixing issues in {video_path.name}...\")\n    \n    # Create a fixed video filename\n    fixed_path = DIAGNOSTIC_DIR / f\"fixed_{video_path.name}\"\n    \n    # Determine the appropriate action based on issues\n    if any(\"long duration\" in issue for issue in issues):\n        # Trim to 5 seconds\n        print(\"  Trimming long video to 5 seconds...\")\n        subprocess.run([\n            'ffmpeg',\n            '-i', str(video_path),\n            '-t', '5',\n            '-c:v', 'libx264',  # Re-encode video\n            '-preset', 'fast',\n            '-c:a', 'aac',      # Re-encode audio\n            '-y',\n            str(fixed_path)\n        ], check=True, capture_output=True)\n        return fixed_path, True\n    \n    elif any(\"mismatch\" in issue for issue in issues) or any(\"codec\" in issue for issue in issues):\n        # Re-encode to fix format issues\n        print(\"  Re-encoding to fix format issues...\")\n        subprocess.run([\n            'ffmpeg',\n            '-i', str(video_path),\n            '-c:v', 'libx264',\n            '-preset', 'fast',\n            '-c:a', 'aac',\n            '-y',\n            str(fixed_path)\n        ], check=True, capture_output=True)\n        return fixed_path, True\n    \n    # If no specific fix, do a general re-encode\n    print(\"  Performing general re-encode...\")\n    subprocess.run([\n        'ffmpeg',\n        '-i', str(video_path),\n        '-c:v', 'libx264',\n        '-preset', 'fast',\n        '-c:a', 'aac',\n        '-y',\n        str(fixed_path)\n    ], check=True, capture_output=True)\n    \n    return fixed_path, True\n\ndef find_and_fix_videos():\n    \"\"\"Find all videos in the IBEX directory, analyze and fix them if needed.\"\"\"\n    print(\"Finding video files in IBEX directory...\")\n    videos = []\n    for file in IBEX_DIR.glob('**/*.mp4'):\n        videos.append(file)\n    \n    print(f\"Found {len(videos)} video files.\")\n    \n    # Sort videos by track order\n    videos.sort(key=lambda v: next((i for i, t in enumerate(TRACK_ORDER) if v.name.startswith(t)), 999))\n    \n    # Analyze and fix each video\n    fixed_videos = []\n    total_expected_duration = 0\n    \n    for video in videos:\n        # Analyze the video\n        info, issues = analyze_video(video)\n        \n        # Fix the video if needed\n        fixed_path, was_fixed = fix_video(video, info, issues)\n        fixed_videos.append(fixed_path)\n        \n        # Verify the fixed video if it was fixed\n        if was_fixed:\n            print(f\"  Verifying fixed video: {fixed_path.name}\")\n            fixed_info, fixed_issues = analyze_video(fixed_path)\n            if fixed_issues:\n                print(f\"  Warning: Fixed video still has issues: {fixed_issues}\")\n            duration = fixed_info.get('duration', 0)\n        else:\n            duration = info.get('duration', 0)\n        \n        total_expected_duration += duration\n    \n    # Special handling for 14_HM if needed\n    have_14_hm = any(video.name.startswith(\"14_HM\") for video in videos)\n    if not have_14_hm:\n        print(\"\\nWarning: 14_HM_HotMinute track not found!\")\n        # Look for it in parent directory\n        parent_hm = list(IBEX_DIR.parent.glob(\"14_HM*.mp4\"))\n        if parent_hm:\n            print(f\"Found {len(parent_hm)} 14_HM videos in parent directory.\")\n            for hm in parent_hm:\n                print(f\"Adding: {hm.name}\")\n                # Copy to diagnostic dir and analyze\n                diagnostic_copy = DIAGNOSTIC_DIR / hm.name\n                shutil.copy2(hm, diagnostic_copy)\n                info, issues = analyze_video(diagnostic_copy)\n                fixed_path, was_fixed = fix_video(diagnostic_copy, info, issues)\n                fixed_videos.append(fixed_path)\n                if was_fixed:\n                    fixed_info = get_detailed_video_info(fixed_path)\n                    total_expected_duration += fixed_info.get('duration', 0)\n                else:\n                    total_expected_duration += info.get('duration', 0)\n    \n    return fixed_videos, total_expected_duration\n\ndef create_sequential_video(videos, expected_duration):\n    \"\"\"Create a sequential video from the provided video files using re-encoding approach.\"\"\"\n    print(\"\\nCreating sequential video with re-encoding...\")\n    \n    # Create a concatenation file\n    concat_file = DIAGNOSTIC_DIR / \"concat_list.txt\"\n    with open(concat_file, 'w') as f:\n        for video in videos:\n            escaped_path = str(video).replace(\"'\", \"'\\\\''\")\n            f.write(f\"file '{escaped_path}'\\n\")\n    \n    # Output file\n    output_file = OUTPUT_DIR / \"ResurrectingAtlantis_IBEX_ReEncoded.mp4\"\n    \n    # Use FFmpeg to concatenate with re-encoding\n    print(f\"Creating video: {output_file}\")\n    print(f\"Expected duration: {expected_duration:.2f} seconds ({expected_duration/60:.2f} minutes)\")\n    \n    try:\n        # Approach 1: Use concat demuxer but with re-encoding\n        subprocess.run([\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file),\n            '-c:v', 'libx264',  # Re-encode video\n            '-preset', 'fast',\n            '-c:a', 'aac',      # Re-encode audio\n            '-y',\n            str(output_file)\n        ], check=True, capture_output=True)\n        \n        print(\"Sequential video created successfully!\")\n        \n        # Verify the output file\n        print(\"\\nVerifying output file:\")\n        output_info, output_issues = analyze_video(output_file)\n        if output_issues:\n            print(f\"Warning: Output video has issues: {output_issues}\")\n            \n            # If issues, try alternative approach\n            print(\"\\nTrying alternative approach with filter_complex...\")\n            alt_output = OUTPUT_DIR / \"ResurrectingAtlantis_IBEX_Alternative.mp4\"\n            \n            # Build the filter complex string\n            inputs = []\n            filter_complex = []\n            \n            for i, video in enumerate(videos):\n                inputs.extend(['-i', str(video)])\n                filter_complex.append(f\"[{i}:v:0][{i}:a:0]\")\n            \n            filter_str = ''.join(filter_complex) + f\"concat=n={len(videos)}:v=1:a=1[outv][outa]\"\n            \n            # Run FFmpeg with filter_complex\n            cmd = [\n                'ffmpeg',\n                *inputs,\n                '-filter_complex', filter_str,\n                '-map', '[outv]',\n                '-map', '[outa]',\n                '-c:v', 'libx264',\n                '-preset', 'fast',\n                '-c:a', 'aac',\n                '-y',\n                str(alt_output)\n            ]\n            \n            subprocess.run(cmd, check=True, capture_output=True)\n            print(f\"Alternative video created: {alt_output}\")\n            alt_info, alt_issues = analyze_video(alt_output)\n        \n        return output_file, output_info.get('duration', 0)\n        \n    except subprocess.CalledProcessError as e:\n        print(\"Error during video creation:\")\n        print(f\"FFmpeg stdout: {e.stdout.decode()}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode()}\")\n        return None, 0\n\ndef main():\n    \"\"\"Main function to analyze, fix, and create a sequential video.\"\"\"\n    print(\"=== IBEX Video Analysis and Fix ===\")\n    \n    # Find and fix videos\n    fixed_videos, expected_duration = find_and_fix_videos()\n    \n    if not fixed_videos:\n        print(\"No videos found or all videos have errors.\")\n        return\n    \n    # Create sequential video\n    output_file, actual_duration = create_sequential_video(fixed_videos, expected_duration)\n    \n    if output_file:\n        print(\"\\n=== Summary ===\")\n        print(f\"Total videos processed: {len(fixed_videos)}\")\n        print(f\"Expected duration: {expected_duration:.2f} seconds ({expected_duration/60:.2f} minutes)\")\n        print(f\"Actual output duration: {actual_duration:.2f} seconds ({actual_duration/60:.2f} minutes)\")\n        print(f\"Output file: {output_file}\")\n        print(f\"Diagnostic files in: {DIAGNOSTIC_DIR}\")\n        \n        # Create a summary file\n        summary_file = OUTPUT_DIR / \"ibex_diagnostic_summary.txt\"\n        with open(summary_file, 'w') as f:\n            f.write(\"IBEX VIDEO DIAGNOSTIC SUMMARY\\n\")\n            f.write(\"===========================\\n\\n\")\n            f.write(f\"Total videos processed: {len(fixed_videos)}\\n\")\n            f.write(f\"Expected duration: {expected_duration:.2f} seconds ({expected_duration/60:.2f} minutes)\\n\")\n            f.write(f\"Actual output duration: {actual_duration:.2f} seconds ({actual_duration/60:.2f} minutes)\\n\")\n            f.write(f\"Output file: {output_file}\\n\\n\")\n            \n            f.write(\"VIDEO DETAILS:\\n\")\n            for i, video in enumerate(fixed_videos, 1):\n                info = get_detailed_video_info(video)\n                track_id = next((t for t in TRACK_ORDER if video.name.startswith(t) or Path(video.name).name.startswith(t)), \"Unknown\")\n                \n                f.write(f\"{i}. {track_id} - {video.name}\\n\")\n                f.write(f\"   Duration: {info.get('duration', 0):.2f} seconds\\n\")\n                f.write(f\"   Resolution: {info.get('width', 'unknown')}x{info.get('height', 'unknown')}\\n\")\n                f.write(f\"   Codecs: Video={info.get('video_codec', 'unknown')}, Audio={info.get('audio_codec', 'unknown')}\\n\\n\")\n        \n        print(f\"Summary saved to: {summary_file}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "**/*.mp4",
      "14_HM*.mp4",
      "ResurrectingAtlantis_IBEX_ReEncoded.mp4",
      "ResurrectingAtlantis_IBEX_Alternative.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA/IBEX",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence",
      "0/1",
      "] = num / den if den else 0\n                        else:\n                            info[",
      ", 0)) / 1024 / 1024:.2f} MB",
      ")\n    \n    # Check for duration mismatch between video/audio streams\n    if ",
      ")\n    \n    # Create a fixed video filename\n    fixed_path = DIAGNOSTIC_DIR / f",
      "**/*.mp4",
      ")\n                # Copy to diagnostic dir and analyze\n                diagnostic_copy = DIAGNOSTIC_DIR / hm.name\n                shutil.copy2(hm, diagnostic_copy)\n                info, issues = analyze_video(diagnostic_copy)\n                fixed_path, was_fixed = fix_video(diagnostic_copy, info, issues)\n                fixed_videos.append(fixed_path)\n                if was_fixed:\n                    fixed_info = get_detailed_video_info(fixed_path)\n                    total_expected_duration += fixed_info.get(",
      ")\n    \n    # Create a concatenation file\n    concat_file = DIAGNOSTIC_DIR / ",
      ")\n    \n    # Output file\n    output_file = OUTPUT_DIR / ",
      "Expected duration: {expected_duration:.2f} seconds ({expected_duration/60:.2f} minutes)",
      ")\n            alt_output = OUTPUT_DIR / ",
      "Expected duration: {expected_duration:.2f} seconds ({expected_duration/60:.2f} minutes)",
      "Actual output duration: {actual_duration:.2f} seconds ({actual_duration/60:.2f} minutes)",
      ")\n        \n        # Create a summary file\n        summary_file = OUTPUT_DIR / ",
      "Expected duration: {expected_duration:.2f} seconds ({expected_duration/60:.2f} minutes)\\n",
      "Actual output duration: {actual_duration:.2f} seconds ({actual_duration/60:.2f} minutes)\\n"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "\n            ['ffprobe', '-v', 'error'] + args + [str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-i', str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-i', str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n        'ffmpeg',\n        '-i', str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file"
      },
      {
        "type": "run",
        "snippet": "cmd, check=True, capture_output=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "shutil",
      "pathlib"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/create_track_overlay_sheet.py",
    "size": 8553,
    "lines": 238,
    "source": "#!/usr/bin/env python3\nimport os\nfrom PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter\n\n# Define paths\nbase_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH\"\nmpost_dir = os.path.join(base_dir, \"mpost-journey\")\noutput_file = os.path.join(base_dir, \"track_overlay_test_sheet.jpg\")\n\n# Get all renamed files (they should be sorted by track number already)\ndef get_track_images():\n    image_files = []\n    for file in os.listdir(mpost_dir):\n        if file.lower().endswith('.png') and file.startswith(('0', '1')) and not os.path.isdir(os.path.join(mpost_dir, file)):\n            image_files.append(os.path.join(mpost_dir, file))\n    return sorted(image_files)  # Sort by filename (which starts with track number)\n\ntrack_images = get_track_images()\nprint(f\"Found {len(track_images)} track images\")\n\n# Define dimensions for the test sheet\nimage_width, image_height = 600, 400  # Each overlay pair\nmargin = 20\nrows = 4\ncols = 2  # Each column is a base image + one with overlay effect\n\n# Calculate overall dimensions\ntotal_width = cols * image_width + (cols + 1) * margin\ntotal_height = rows * image_height + (rows + 1) * margin\n\n# Create a blank canvas\nresult = Image.new('RGB', (total_width, total_height), (15, 15, 20))\ndraw = ImageDraw.Draw(result)\n\n# Try to load a font for labels\ntry:\n    font = ImageFont.truetype(\"Arial.ttf\", 24)\n    small_font = ImageFont.truetype(\"Arial.ttf\", 16)\nexcept:\n    try:\n        font = ImageFont.truetype(\"/Library/Fonts/Arial.ttf\", 24)\n        small_font = ImageFont.truetype(\"/Library/Fonts/Arial.ttf\", 16)\n    except:\n        font = ImageFont.load_default()\n        small_font = ImageFont.load_default()\n\n# Define overlay modes with descriptions\noverlay_modes = [\n    (\"Screen\", \"Lightens the image - good for creating glowing effects\"),\n    (\"Multiply\", \"Darkens the image - good for shadow effects\"),\n    (\"Overlay\", \"Increases contrast while preserving highlights and shadows\"),\n    (\"Soft Light\", \"Subtle overlay effect - gentler version of 'Overlay'\")\n]\n\n# Process each row with a different overlay mode\nfor row in range(rows):\n    # Make sure we have enough images\n    if len(track_images) < row * 2 + 2:\n        print(f\"Not enough images for row {row+1}, skipping\")\n        continue\n    \n    # Get base image and overlay image for this row\n    base_img_path = track_images[row * 2]\n    overlay_img_path = track_images[row * 2 + 1 if row * 2 + 1 < len(track_images) else 0]\n    \n    # Extract track names for labels\n    base_track_name = os.path.basename(base_img_path).split('_', 3)[0:3]\n    base_track_name = '_'.join(base_track_name)\n    overlay_track_name = os.path.basename(overlay_img_path).split('_', 3)[0:3]\n    overlay_track_name = '_'.join(overlay_track_name)\n    \n    # Extract descriptive part\n    base_desc = os.path.basename(base_img_path).split('_', 3)[3].replace('.png', '')\n    overlay_desc = os.path.basename(overlay_img_path).split('_', 3)[3].replace('.png', '')\n    \n    # Load and resize images\n    try:\n        base_img = Image.open(base_img_path).convert(\"RGB\").resize((image_width, image_height))\n        overlay_img = Image.open(overlay_img_path).convert(\"RGB\").resize((image_width, image_height))\n    except Exception as e:\n        print(f\"Error processing images for row {row+1}: {e}\")\n        continue\n    \n    # Create the composite images based on the overlay mode\n    mode_name, mode_desc = overlay_modes[row]\n    \n    # First column - original image\n    # Second column - with overlay effect\n    composite = base_img.copy()\n    \n    if mode_name == \"Screen\":\n        # Screen blend mode\n        composite = Image.blend(base_img, ImageEnhance.Brightness(overlay_img).enhance(1.5), 0.7)\n    elif mode_name == \"Multiply\":\n        # Multiply-like effect\n        composite = Image.blend(base_img, ImageEnhance.Brightness(overlay_img).enhance(0.5), 0.7)\n    elif mode_name == \"Overlay\":\n        # Overlay-like effect\n        enhanced = ImageEnhance.Contrast(overlay_img).enhance(1.8)\n        composite = Image.blend(base_img, enhanced, 0.6)\n    elif mode_name == \"Soft Light\":\n        # Soft light-like effect\n        blurred = overlay_img.filter(ImageFilter.GaussianBlur(radius=2))\n        composite = Image.blend(base_img, blurred, 0.5)\n    \n    # Calculate positions\n    y_pos = margin + row * (image_height + margin)\n    x_pos1 = margin\n    x_pos2 = margin + image_width + margin\n    \n    # Paste images onto the result canvas\n    result.paste(base_img, (x_pos1, y_pos))\n    result.paste(composite, (x_pos2, y_pos))\n    \n    # Add labels\n    # First column - original image label\n    draw.rectangle([(x_pos1, y_pos + 10), (x_pos1 + 380, y_pos + 70)], fill=(0, 0, 0, 180))\n    draw.text((x_pos1 + 10, y_pos + 15), f\"{base_track_name}\", fill=(255, 255, 255), font=font)\n    draw.text((x_pos1 + 10, y_pos + 45), f\"{base_desc}\", fill=(220, 220, 220), font=small_font)\n    \n    # Second column - overlay effect label\n    draw.rectangle([(x_pos2, y_pos + 10), (x_pos2 + 380, y_pos + 70)], fill=(0, 0, 0, 180))\n    draw.text((x_pos2 + 10, y_pos + 15), f\"{mode_name} Overlay\", fill=(255, 255, 255), font=font)\n    draw.text((x_pos2 + 10, y_pos + 45), f\"Base: {base_track_name} + {overlay_track_name}\", fill=(220, 220, 220), font=small_font)\n\n# Save the result\nresult.save(output_file, quality=95)\nprint(f\"Overlay test sheet created and saved as: {output_file}\")\n\n# Create an HTML file to view the result\nhtml_output = os.path.join(base_dir, \"track_overlay_test_sheet.html\")\nhtml_content = f\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <title>Track Overlay Test Sheet</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1 {{ \n            text-align: center; \n            margin-bottom: 30px;\n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1400px;\n            margin: 0 auto;\n        }}\n        img {{ \n            max-width: 100%; \n            display: block; \n            margin: 0 auto;\n            border: 1px solid #333;\n        }}\n        .description {{\n            margin-top: 30px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        h2 {{\n            border-bottom: 1px solid #444;\n            padding-bottom: 10px;\n        }}\n        .mode {{\n            margin-top: 20px;\n            margin-bottom: 20px;\n        }}\n        .tracks {{\n            margin-top: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>Track Overlay Test Sheet - Resurrecting Atlantis</h1>\n        <img src=\"track_overlay_test_sheet.jpg\" alt=\"Track Overlay Test Sheet\">\n        \n        <div class=\"description\">\n            <h2>Overlay Effects Used</h2>\n            \n            <div class=\"mode\">\n                <h3>Row 1: Screen</h3>\n                <p>{overlay_modes[0][1]}</p>\n            </div>\n            \n            <div class=\"mode\">\n                <h3>Row 2: Multiply</h3>\n                <p>{overlay_modes[1][1]}</p>\n            </div>\n            \n            <div class=\"mode\">\n                <h3>Row 3: Overlay</h3>\n                <p>{overlay_modes[2][1]}</p>\n            </div>\n            \n            <div class=\"mode\">\n                <h3>Row 4: Soft Light</h3>\n                <p>{overlay_modes[3][1]}</p>\n            </div>\n        </div>\n        \n        <div class=\"tracks\">\n            <h2>Track Organization</h2>\n            <p>The images have been organized according to the following track list:</p>\n            <ul>\n                <li>01_SH_OutOfLife_000000</li>\n                <li>02_FL_FlashingLights_021100</li>\n                <li>03_HT_HowToBreakOffAnEngagement_042200</li>\n                <li>04_NM_Nevermore_063300</li>\n                <li>05_BE_Bloodline_084400</li>\n                <li>06_AT_ResurrectingAtlantis_105500</li>\n                <li>07_DJ_DJTurnMeUp_130600</li>\n                <li>08_NS_NewlySingle_151700</li>\n                <li>09_YH_YetHeard_172800</li>\n                <li>10_MR_MagicRide_193900</li>\n                <li>12_RU_Reunion_215000</li>\n                <li>13_HW_HowToWinMyHeart_240100</li>\n                <li>14_HM_HotMinute_261200</li>\n            </ul>\n        </div>\n    </div>\n</body>\n</html>\n\"\"\"\n\nwith open(html_output, 'w') as f:\n    f.write(html_content)\n\nprint(f\"HTML viewer created and saved as: {html_output}\")\n",
    "file_references": [
      "track_overlay_test_sheet.jpg",
      "track_overlay_test_sheet.jpg",
      "/Users/gaia/resurrecting atlantis/JELLYFISH",
      "/Library/Fonts/Arial.ttf",
      "/Library/Fonts/Arial.ttf",
      "<!DOCTYPE html>\n<html>\n<head>\n    <title>Track Overlay Test Sheet</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1 {{ \n            text-align: center; \n            margin-bottom: 30px;\n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1400px;\n            margin: 0 auto;\n        }}\n        img {{ \n            max-width: 100%; \n            display: block; \n            margin: 0 auto;\n            border: 1px solid #333;\n        }}\n        .description {{\n            margin-top: 30px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        h2 {{\n            border-bottom: 1px solid #444;\n            padding-bottom: 10px;\n        }}\n        .mode {{\n            margin-top: 20px;\n            margin-bottom: 20px;\n        }}\n        .tracks {{\n            margin-top: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=",
      ">\n        <h1>Track Overlay Test Sheet - Resurrecting Atlantis</h1>\n        <img src=",
      ">\n            <h2>Overlay Effects Used</h2>\n            \n            <div class=",
      ">\n                <h3>Row 1: Screen</h3>\n                <p>{overlay_modes[0][1]}</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Row 2: Multiply</h3>\n                <p>{overlay_modes[1][1]}</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Row 3: Overlay</h3>\n                <p>{overlay_modes[2][1]}</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Row 4: Soft Light</h3>\n                <p>{overlay_modes[3][1]}</p>\n            </div>\n        </div>\n        \n        <div class=",
      ">\n            <h2>Track Organization</h2>\n            <p>The images have been organized according to the following track list:</p>\n            <ul>\n                <li>01_SH_OutOfLife_000000</li>\n                <li>02_FL_FlashingLights_021100</li>\n                <li>03_HT_HowToBreakOffAnEngagement_042200</li>\n                <li>04_NM_Nevermore_063300</li>\n                <li>05_BE_Bloodline_084400</li>\n                <li>06_AT_ResurrectingAtlantis_105500</li>\n                <li>07_DJ_DJTurnMeUp_130600</li>\n                <li>08_NS_NewlySingle_151700</li>\n                <li>09_YH_YetHeard_172800</li>\n                <li>10_MR_MagicRide_193900</li>\n                <li>12_RU_Reunion_215000</li>\n                <li>13_HW_HowToWinMyHeart_240100</li>\n                <li>14_HM_HotMinute_261200</li>\n            </ul>\n        </div>\n    </div>\n</body>\n</html>\n"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "PIL"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/assemble_ibex_frame_extract.py",
    "size": 11268,
    "lines": 284,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\n# Directory containing video files\nIBEX_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/IBEX')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Track order specified\nTRACK_ORDER = [\n    \"01_SH_OutOfLife_000000\",\n    \"02_FL_FlashingLights_021100\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\",\n    \"04_NM_Nevermore_063300\",\n    \"05_BE_Bloodline_084400\",\n    \"06_AT_ResurrectingAtlantis_105500\",\n    \"07_DJ_DJTurnMeUp_130600\",\n    \"08_NS_NewlySingle_151700\",\n    \"09_YH_YetHeard_172800\",\n    \"10_MR_MagicRide_193900\",\n    \"12_RU_Reunion_215000\",\n    \"13_HW_HowToWinMyHeart_240100\",\n    \"14_HM_HotMinute_261200\"\n]\n\ndef get_video_info(video_path):\n    \"\"\"Get detailed information about a video file using FFprobe.\"\"\"\n    try:\n        # Get format information (duration, etc.)\n        format_result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        format_data = json.loads(format_result.stdout)\n        \n        # Get stream information (frame rate, etc.)\n        stream_result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-select_streams', 'v:0', \n            '-show_entries', 'stream=width,height,r_frame_rate', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        stream_data = json.loads(stream_result.stdout)\n        \n        # Parse the results\n        info = {\n            'duration': float(format_data['format']['duration']),\n            'width': stream_data['streams'][0]['width'],\n            'height': stream_data['streams'][0]['height']\n        }\n        \n        # Parse frame rate (which comes as a fraction like \"30000/1001\")\n        fps_str = stream_data['streams'][0]['r_frame_rate']\n        if '/' in fps_str:\n            num, den = map(int, fps_str.split('/'))\n            info['fps'] = num / den if den else 0\n        else:\n            info['fps'] = float(fps_str)\n        \n        return info\n    except (subprocess.CalledProcessError, KeyError, json.JSONDecodeError) as e:\n        print(f\"Error getting info for {video_path}: {e}\")\n        return {'duration': 0, 'width': 0, 'height': 0, 'fps': 0}\n\ndef find_video_files():\n    \"\"\"Find all video files in the IBEX directory.\"\"\"\n    video_files = []\n    for file in IBEX_DIR.glob('**/*.mp4'):\n        video_files.append(file)\n    return video_files\n\ndef extract_segment(video_path, output_path, duration=5.0):\n    \"\"\"Extract a 5-second segment from the video.\"\"\"\n    try:\n        subprocess.run([\n            'ffmpeg',\n            '-i', str(video_path),\n            '-t', str(duration),\n            '-c:v', 'libx264',  # Use x264 encoding\n            '-preset', 'fast',\n            '-crf', '23',       # Quality setting\n            '-an',              # No audio\n            '-y',               # Overwrite output\n            str(output_path)\n        ], check=True, capture_output=True)\n        \n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error extracting segment from {video_path}: {e}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode()}\")\n        return False\n\ndef sort_videos_by_track_order(video_files):\n    \"\"\"Sort video files according to the track order.\"\"\"\n    # Create a dictionary to map track prefix to its position in TRACK_ORDER\n    track_positions = {track: i for i, track in enumerate(TRACK_ORDER)}\n    \n    # Define a function to get the track prefix of a file\n    def get_track_prefix(file):\n        for track in TRACK_ORDER:\n            if file.name.startswith(track):\n                return track\n        return None\n    \n    # Sort the video files based on track order\n    sorted_videos = sorted(\n        video_files,\n        key=lambda file: track_positions.get(get_track_prefix(file), float('inf'))\n    )\n    \n    return sorted_videos\n\ndef assemble_videos():\n    \"\"\"Assemble all videos from IBEX directory using a segment extraction approach.\"\"\"\n    print(\"Finding video files in IBEX directory...\")\n    all_videos = find_video_files()\n    print(f\"Found {len(all_videos)} video files.\")\n    \n    if not all_videos:\n        print(\"No videos found in the IBEX directory.\")\n        return\n    \n    # Sort videos according to track order\n    print(\"Sorting videos by track order...\")\n    sorted_videos = sort_videos_by_track_order(all_videos)\n    \n    # Create a temporary directory for processed segments\n    temp_dir = Path(tempfile.mkdtemp())\n    try:\n        # Extract 5-second segments from each video\n        segments = []\n        total_duration = 0\n        \n        print(\"\\nExtracting 5-second segments from each video:\")\n        print(\"---------------------------------------------\")\n        \n        for i, video in enumerate(sorted_videos, 1):\n            info = get_video_info(video)\n            \n            # Find the track ID for this video\n            track_id = next((track for track in TRACK_ORDER if video.name.startswith(track)), \"Unknown\")\n            \n            print(f\"\\n{i}. {video.name}\")\n            print(f\"   Track: {track_id}\")\n            print(f\"   Original duration: {info['duration']:.2f} seconds\")\n            print(f\"   Resolution: {info['width']}x{info['height']}\")\n            print(f\"   Frame rate: {info['fps']:.2f} fps\")\n            \n            # Determine segment duration (use original if less than 5s)\n            segment_duration = min(5.0, info['duration']) if info['duration'] > 0 else 5.0\n            \n            # Create segment path\n            segment_path = temp_dir / f\"segment_{i:02d}_{track_id}.mp4\"\n            \n            # Extract the segment\n            print(f\"   Extracting {segment_duration:.2f}s segment...\")\n            if extract_segment(video, segment_path, segment_duration):\n                segments.append(segment_path)\n                \n                # Verify the segment\n                segment_info = get_video_info(segment_path)\n                print(f\"   Segment duration: {segment_info['duration']:.2f} seconds\")\n                total_duration += segment_info['duration']\n            else:\n                print(f\"   Failed to extract segment from {video.name}\")\n        \n        if not segments:\n            print(\"No segments were extracted successfully.\")\n            return\n        \n        # Create segment list file\n        segment_list = temp_dir / \"segments.txt\"\n        with open(segment_list, 'w') as f:\n            for segment in segments:\n                f.write(f\"file '{segment}'\\n\")\n        \n        # Output filename\n        output_file = OUTPUT_DIR / \"ResurrectingAtlantis_IBEX_Segments.mp4\"\n        \n        # Use FFmpeg to concatenate the segments\n        print(f\"\\nConcatenating {len(segments)} segments into {output_file}...\")\n        print(f\"Expected duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n        \n        try:\n            subprocess.run([\n                'ffmpeg',\n                '-f', 'concat',\n                '-safe', '0',\n                '-i', str(segment_list),\n                '-c', 'copy',  # Just copy, don't re-encode\n                '-y',\n                str(output_file)\n            ], check=True, capture_output=True)\n            \n            # Verify the output file\n            output_info = get_video_info(output_file)\n            \n            print(f\"\\nOutput file created: {output_file}\")\n            print(f\"Output duration: {output_info['duration']:.2f} seconds ({output_info['duration']/60:.2f} minutes)\")\n            \n            if abs(output_info['duration'] - total_duration) > 1.0:\n                print(f\"Warning: Output duration ({output_info['duration']:.2f}s) doesn't match expected ({total_duration:.2f}s)\")\n                \n                # Try alternative method with direct re-encoding\n                alt_output = OUTPUT_DIR / \"ResurrectingAtlantis_IBEX_Segments_Alt.mp4\"\n                print(f\"\\nTrying alternative method with direct re-encoding...\")\n                \n                # Create filter complex string for concatenation\n                filter_complex = []\n                for i in range(len(segments)):\n                    filter_complex.append(f\"[{i}:v:0]\")\n                \n                filter_str = ''.join(filter_complex) + f\"concat=n={len(segments)}:v=1:a=0[outv]\"\n                \n                # Build input arguments\n                inputs = []\n                for segment in segments:\n                    inputs.extend(['-i', str(segment)])\n                \n                # Run FFmpeg with filter_complex\n                cmd = [\n                    'ffmpeg',\n                    *inputs,\n                    '-filter_complex', filter_str,\n                    '-map', '[outv]',\n                    '-c:v', 'libx264',\n                    '-preset', 'fast',\n                    '-y',\n                    str(alt_output)\n                ]\n                \n                subprocess.run(cmd, check=True, capture_output=True)\n                \n                alt_info = get_video_info(alt_output)\n                print(f\"Alternative output created: {alt_output}\")\n                print(f\"Alternative duration: {alt_info['duration']:.2f} seconds\")\n            \n            # Create a detailed log\n            details_file = OUTPUT_DIR / \"ibex_segments_details.txt\"\n            with open(details_file, 'w') as f:\n                f.write(\"RESURRECTING ATLANTIS - IBEX SEGMENTS SEQUENCE\\n\")\n                f.write(\"==========================================\\n\\n\")\n                f.write(f\"Total segments used: {len(segments)}\\n\")\n                f.write(f\"Expected total duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\\n\")\n                f.write(f\"Actual output duration: {output_info['duration']:.2f} seconds\\n\")\n                f.write(f\"Output file: {output_file}\\n\\n\")\n                f.write(\"SEGMENT DETAILS:\\n\")\n                \n                for i, segment in enumerate(segments, 1):\n                    segment_info = get_video_info(segment)\n                    track_name = segment.stem.split('_', 2)[2] if len(segment.stem.split('_')) > 2 else segment.stem\n                    \n                    f.write(f\"{i}. {track_name}\\n\")\n                    f.write(f\"   File: {segment.name}\\n\")\n                    f.write(f\"   Duration: {segment_info['duration']:.2f} seconds\\n\")\n                    f.write(f\"   Resolution: {segment_info['width']}x{segment_info['height']}\\n\")\n                    f.write(f\"   Frame rate: {segment_info['fps']:.2f} fps\\n\\n\")\n                \n            print(f\"IBEX segment details saved to {details_file}\")\n            \n        except subprocess.CalledProcessError as e:\n            print(f\"Error concatenating segments: {e}\")\n            print(f\"FFmpeg stderr: {e.stderr.decode()}\")\n    \n    finally:\n        # Clean up temporary directory\n        print(f\"\\nCleaning up temporary files...\")\n        shutil.rmtree(temp_dir)\n\nif __name__ == \"__main__\":\n    assemble_videos()\n",
    "file_references": [
      "**/*.mp4",
      "segment_{i:02d}_{track_id}.mp4",
      "ResurrectingAtlantis_IBEX_Segments.mp4",
      "ResurrectingAtlantis_IBEX_Segments_Alt.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA/IBEX",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence",
      "30000/1001",
      "] = num / den if den else 0\n        else:\n            info[",
      "**/*.mp4",
      "] > 0 else 5.0\n            \n            # Create segment path\n            segment_path = temp_dir / f",
      ")\n            return\n        \n        # Create segment list file\n        segment_list = temp_dir / ",
      ")\n        \n        # Output filename\n        output_file = OUTPUT_DIR / ",
      "Expected duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)",
      "]/60:.2f} minutes)",
      ")\n                \n                # Try alternative method with direct re-encoding\n                alt_output = OUTPUT_DIR / ",
      ")\n            \n            # Create a detailed log\n            details_file = OUTPUT_DIR / ",
      "Expected total duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\\n"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-select_streams', 'v:0', \n            '-show_entries', 'stream=width,height,r_frame_rate', \n            '-of', 'json', \n            s"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-i', str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n                'ffmpeg',\n                '-f', 'concat',\n                '-safe', '0',\n                '-i', str(segment_list"
      },
      {
        "type": "run",
        "snippet": "cmd, check=True, capture_output=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "pathlib",
      "tempfile",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/codex_overlay_final.py",
    "size": 6955,
    "lines": 170,
    "source": "#!/usr/bin/env python3\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nimport textwrap\nimport random\nimport json\n\ndef create_codex_overlay(output_path, shot_id=\"FL012\", timestamp=\"02:38:34\"):\n    \"\"\"Create a mockup with assembly.json data in the header and prompt in footer\"\"\"\n    # Create a black canvas (simulating video frame)\n    width, height = 1280, 720\n    image = Image.new('RGB', (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Load font\n    try:\n        header_font = ImageFont.truetype(\"Arial.ttf\", 14)\n        footer_font = ImageFont.truetype(\"Arial.ttf\", 18)\n        title_font = ImageFont.truetype(\"Arial.ttf\", 16)\n    except IOError:\n        try:\n            header_font = ImageFont.truetype(\"DejaVuSans.ttf\", 14)\n            footer_font = ImageFont.truetype(\"DejaVuSans.ttf\", 18)\n            title_font = ImageFont.truetype(\"DejaVuSans.ttf\", 16)\n        except IOError:\n            header_font = ImageFont.load_default()\n            footer_font = ImageFont.load_default()\n            title_font = ImageFont.load_default()\n    \n    # Define colors\n    cyan = (0, 255, 255)\n    amber = (255, 191, 0)\n    white = (255, 255, 255)\n    green = (80, 255, 80)\n    \n    # Sample assembly.json data\n    assembly_data = {\n        \"id\": \"FL012\",\n        \"poem\": \"Flashing Lights\",\n        \"content\": \"a concussion,\",\n        \"syntagmaType\": \"Perception-Image\",\n        \"operativeEkphrasis\": \"Stars explode behind eyelids--fireworks seen from inside a skull.\",\n        \"imageType\": \"Perception-Image\",\n        \"cineosisFunction\": \"Subjective Frame Recalibration\"\n    }\n    \n    # Create top header bar with assembly data\n    header_height = 35\n    draw.rectangle([(0, 0), (width, header_height)], fill=(0, 0, 0, 180))\n    draw.line([(0, header_height), (width, header_height)], fill=cyan, width=1)\n    \n    # Add scanlines to header\n    for y in range(0, header_height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n    \n    # Add text to header - important assembly data\n    padding = 10\n    draw.text((padding, 9), f\"{assembly_data['id']}\", fill=cyan, font=title_font)\n    draw.text((padding + 60, 9), f\"[{timestamp}]\", fill=white, font=header_font)\n    draw.text((padding + 140, 9), f\"{assembly_data['poem']}\", fill=amber, font=header_font)\n    draw.text((padding + 240, 9), f\"\\\"{assembly_data['content']}\\\"\", fill=white, font=header_font)\n    \n    # Add key metadata fields from assembly.json\n    draw.text((padding + 350, 9), f\"TYPE: {assembly_data['syntagmaType']}\", fill=green, font=header_font)\n    draw.text((padding + 520, 9), f\"FUNC: {assembly_data['cineosisFunction']}\", fill=green, font=header_font)\n    \n    # Add frame counter to right side\n    frame_num = random.randint(1000, 9000)\n    draw.text((width-100, 9), f\"F:{frame_num}\", fill=cyan, font=header_font)\n    \n    # Create the footer console overlay\n    footer_height = 80  # Height of bottom overlay\n    footer_y = height - footer_height\n    \n    # Draw semi-transparent black background for footer\n    draw.rectangle([(0, footer_y), (width, height)], fill=(0, 0, 0, 180))\n    draw.line([(0, footer_y), (width, footer_y)], fill=cyan, width=1)\n    \n    # Add scanline effect to footer\n    for y in range(footer_y, height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 70), width=1)\n    \n    # Add prompt text to footer\n    prompt_text = \"PROMPT: Stars explode behind eyelids--fireworks seen from inside a skull \u00b7 POV macro; eyelids closed, inner nebula bursts saffron-teal sparks, sub-bass rumble.\"\n    \n    # Wrap the prompt text across multiple lines if needed\n    wrapped_prompt = textwrap.wrap(prompt_text, width=110)\n    y_text = footer_y + 15\n    \n    # Add a small label for the prompt\n    draw.text((padding, y_text), \"PROMPT:\", fill=cyan, font=title_font)\n    \n    # Print the wrapped prompt text\n    for i, line in enumerate(wrapped_prompt):\n        if i == 0:\n            # For the first line, skip the \"PROMPT:\" label that we manually added\n            if line.startswith(\"PROMPT: \"):\n                line = line[8:]\n            draw.text((padding + 80, y_text), line, fill=white, font=footer_font)\n        else:\n            draw.text((padding, y_text + (i * 22)), line, fill=white, font=footer_font)\n    \n    # Add digital glitch effects to both header and footer\n    for _ in range(30):\n        # Determine if glitch should be in header or footer\n        if random.random() < 0.3:\n            # Header glitch\n            glitch_y = random.randint(0, header_height-2)\n            glitch_height = random.randint(1, 2)\n        else:\n            # Footer glitch\n            glitch_y = random.randint(footer_y, height-2)\n            glitch_height = random.randint(1, 2)\n        \n        glitch_x = random.randint(0, width-50)\n        glitch_width = random.randint(10, 50)\n        glitch_color = random.choice([(0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128)])\n        draw.rectangle([(glitch_x, glitch_y), (glitch_x + glitch_width, glitch_y + glitch_height)], \n                      fill=glitch_color)\n    \n    # Add a timeline indicator at the bottom of the footer\n    timeline_y = height - 15\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 4 if i % 3 == 0 else 2\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f\"Codex overlay mockup created and saved to: {output_path}\")\n\ndef main():\n    output_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mockups\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create codex overlay mockup\n    output_path = os.path.join(output_dir, \"codex_overlay_final.png\")\n    create_codex_overlay(output_path)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "codex_overlay_final.png",
      ", (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Load font\n    try:\n        header_font = ImageFont.truetype(",
      "):\n                line = line[8:]\n            draw.text((padding + 80, y_text), line, fill=white, font=footer_font)\n        else:\n            draw.text((padding, y_text + (i * 22)), line, fill=white, font=footer_font)\n    \n    # Add digital glitch effects to both header and footer\n    for _ in range(30):\n        # Determine if glitch should be in header or footer\n        if random.random() < 0.3:\n            # Header glitch\n            glitch_y = random.randint(0, header_height-2)\n            glitch_height = random.randint(1, 2)\n        else:\n            # Footer glitch\n            glitch_y = random.randint(footer_y, height-2)\n            glitch_height = random.randint(1, 2)\n        \n        glitch_x = random.randint(0, width-50)\n        glitch_width = random.randint(10, 50)\n        glitch_color = random.choice([(0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128)])\n        draw.rectangle([(glitch_x, glitch_y), (glitch_x + glitch_width, glitch_y + glitch_height)], \n                      fill=glitch_color)\n    \n    # Add a timeline indicator at the bottom of the footer\n    timeline_y = height - 15\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-3, timeline_y-3, position_x+3, timeline_y+3), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 4 if i % 3 == 0 else 2\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mockups"
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os",
      "textwrap",
      "random",
      "json"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/create_advanced_overlays.py",
    "size": 16331,
    "lines": 454,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nfrom PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageChops, ImageDraw\n\n# Define paths\nmpost_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey\"\nmidjourney_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]\"\noutput_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/advanced_overlays\"\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Get all image files\nmpost_files = [f for f in os.listdir(mpost_dir) if f.lower().endswith('.png') and not f.startswith('original_')]\nmidjourney_files = [f for f in os.listdir(midjourney_dir) if f.lower().endswith('.png') and not os.path.isdir(os.path.join(midjourney_dir, f))]\n\nprint(f\"Found {len(mpost_files)} mpost files\")\nprint(f\"Found {len(midjourney_files)} midjourney files\")\n\n# Extract ID number from filename\ndef get_id_from_filename(filename):\n    # Pattern to match the ID at the beginning of the filename (e.g., 01_SH, 02_FL)\n    match = re.match(r'^(\\d+)_([A-Z]+)_', filename)\n    if match:\n        return match.group(1)  # Return just the number part\n    return None\n\n# Group files by ID\nmpost_by_id = {}\nmidjourney_by_id = {}\n\nfor file in mpost_files:\n    id_num = get_id_from_filename(file)\n    if id_num:\n        if id_num not in mpost_by_id:\n            mpost_by_id[id_num] = []\n        mpost_by_id[id_num].append(file)\n\nfor file in midjourney_files:\n    id_num = get_id_from_filename(file)\n    if id_num:\n        if id_num not in midjourney_by_id:\n            midjourney_by_id[id_num] = []\n        midjourney_by_id[id_num].append(file)\n\nprint(f\"Grouped into {len(mpost_by_id)} mpost IDs\")\nprint(f\"Grouped into {len(midjourney_by_id)} midjourney IDs\")\n\n# Define 8 advanced blending techniques\ndef dark_gradient_blend(fg, bg):\n    # Darken background and create gradient blend\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.3)\n    enhanced_fg = ImageEnhance.Contrast(fg).enhance(1.4)\n    return Image.blend(darkened_bg, enhanced_fg, 0.75)\n\ndef vignette_overlay(fg, bg):\n    # Create a dark vignette effect and overlay\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.25)\n    \n    # Create vignette mask\n    width, height = bg.size\n    mask = Image.new(\"L\", (width, height), 0)\n    draw = ImageDraw.Draw(mask)\n    \n    # Draw radial gradient for vignette\n    for i in range(min(width, height)//2, 0, -1):\n        # Increase darkness toward edges\n        value = int(255 * (i / (min(width, height)/2)))\n        x1, y1 = width//2 - i, height//2 - i\n        x2, y2 = width//2 + i, height//2 + i\n        draw.ellipse((x1, y1, x2, y2), fill=value)\n    \n    # Apply vignette to background\n    darkened_bg.putalpha(mask)\n    \n    # Enhance foreground and blend\n    enhanced_fg = ImageEnhance.Contrast(fg).enhance(1.5)\n    enhanced_fg = ImageEnhance.Brightness(enhanced_fg).enhance(1.2)\n    \n    return Image.composite(enhanced_fg, darkened_bg, mask)\n\ndef cinematic_contrast(fg, bg):\n    # Create high-contrast cinematic look\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.2)\n    darkened_bg = ImageEnhance.Contrast(darkened_bg).enhance(1.8)\n    \n    # Cool tone for background\n    r, g, b = darkened_bg.split()\n    r = ImageEnhance.Brightness(r).enhance(0.85)\n    b = ImageEnhance.Brightness(b).enhance(1.2)\n    darkened_bg = Image.merge(\"RGB\", (r, g, b))\n    \n    # Warm tone for foreground\n    r, g, b = fg.split()\n    r = ImageEnhance.Brightness(r).enhance(1.15)\n    b = ImageEnhance.Brightness(b).enhance(0.9)\n    enhanced_fg = Image.merge(\"RGB\", (r, g, b))\n    \n    enhanced_fg = enhanced_fg.filter(ImageFilter.EDGE_ENHANCE)\n    \n    return Image.blend(darkened_bg, enhanced_fg, 0.65)\n\ndef radial_blend(fg, bg):\n    # Radial gradient blend from center\n    width, height = bg.size\n    \n    # Darken background significantly\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.15)\n    \n    # Enhance foreground\n    enhanced_fg = ImageEnhance.Contrast(fg).enhance(1.6)\n    enhanced_fg = ImageEnhance.Brightness(enhanced_fg).enhance(1.25)\n    \n    # Create radial gradient mask\n    mask = Image.new(\"L\", (width, height), 0)\n    draw = ImageDraw.Draw(mask)\n    \n    center_radius = min(width, height) // 3\n    for i in range(center_radius, 0, -1):\n        # Gradually reduce opacity from center outward\n        opacity = int(255 * (i / center_radius))\n        x1, y1 = width//2 - i, height//2 - i\n        x2, y2 = width//2 + i, height//2 + i\n        draw.ellipse((x1, y1, x2, y2), fill=opacity)\n    \n    # Apply the mask for blending\n    return Image.composite(enhanced_fg, darkened_bg, mask)\n\ndef textural_overlay(fg, bg):\n    # Create textured, dark background with overlay foreground\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.15)\n    \n    # Add texture to background\n    noise = bg.filter(ImageFilter.GaussianBlur(radius=1))\n    noise = ImageEnhance.Contrast(noise).enhance(2.0)\n    darkened_bg = ImageChops.add(darkened_bg, noise, scale=2.0, offset=-100)\n    \n    # Enhance foreground details\n    enhanced_fg = fg.filter(ImageFilter.EDGE_ENHANCE_MORE)\n    enhanced_fg = ImageEnhance.Sharpness(enhanced_fg).enhance(1.5)\n    \n    # Create alpha blend\n    return Image.blend(darkened_bg, enhanced_fg, 0.7)\n\ndef shadow_highlight_blend(fg, bg):\n    # Enhance shadows in background, highlights in foreground\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.1)\n    darkened_bg = ImageEnhance.Contrast(darkened_bg).enhance(1.5)\n    \n    # Create shadow mask from foreground\n    fg_gray = fg.convert(\"L\")\n    shadow_mask = ImageOps.invert(fg_gray)\n    shadow_mask = shadow_mask.filter(ImageFilter.GaussianBlur(radius=10))\n    \n    # Apply shadow mask to further darken background\n    darkened_bg_array = darkened_bg.convert(\"RGB\")\n    \n    # Enhance foreground highlights\n    enhanced_fg = ImageOps.autocontrast(fg, cutoff=10)\n    enhanced_fg = ImageEnhance.Brightness(enhanced_fg).enhance(1.3)\n    \n    # Blend with shadow emphasis\n    result = Image.blend(darkened_bg, enhanced_fg, 0.65)\n    result = ImageEnhance.Contrast(result).enhance(1.2)\n    \n    return result\n\ndef spectral_highlights(fg, bg):\n    # Create ethereal glow effect with dark background\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.08)\n    darkened_bg = ImageEnhance.Contrast(darkened_bg).enhance(1.3)\n    \n    # Extract and enhance highlights from foreground\n    r, g, b = fg.split()\n    \n    # Create glow from brightest parts\n    highlight_mask = fg.convert(\"L\")\n    highlight_mask = ImageEnhance.Brightness(highlight_mask).enhance(1.5)\n    highlight_mask = highlight_mask.point(lambda p: p * 1.2)\n    highlight_mask = highlight_mask.filter(ImageFilter.GaussianBlur(radius=8))\n    \n    # Apply color shifts for spectral effect\n    r = ImageEnhance.Brightness(r).enhance(1.4)\n    g = ImageEnhance.Brightness(g).enhance(1.2)\n    b = ImageEnhance.Brightness(b).enhance(1.6)\n    \n    enhanced_fg = Image.merge(\"RGB\", (r, g, b))\n    enhanced_fg = ImageEnhance.Contrast(enhanced_fg).enhance(1.4)\n    \n    # Apply the highlight mask\n    result = Image.blend(darkened_bg, enhanced_fg, 0.75)\n    return result\n\ndef deep_shadow_merge(fg, bg):\n    # Create deep shadow effect with high contrast foreground\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.05)\n    darkened_bg = ImageEnhance.Contrast(darkened_bg).enhance(1.2)\n    \n    # Apply blur to background for depth\n    darkened_bg = darkened_bg.filter(ImageFilter.GaussianBlur(radius=2))\n    \n    # Enhance foreground with high contrast\n    enhanced_fg = ImageEnhance.Contrast(fg).enhance(1.8)\n    enhanced_fg = ImageEnhance.Brightness(enhanced_fg).enhance(1.3)\n    enhanced_fg = ImageEnhance.Sharpness(enhanced_fg).enhance(1.5)\n    \n    # Color manipulation\n    r, g, b = enhanced_fg.split()\n    r = ImageEnhance.Brightness(r).enhance(1.1)\n    g = ImageEnhance.Brightness(g).enhance(0.9)\n    b = ImageEnhance.Brightness(b).enhance(1.2)\n    enhanced_fg = Image.merge(\"RGB\", (r, g, b))\n    \n    # Blend with emphasis on foreground\n    return Image.blend(darkened_bg, enhanced_fg, 0.8)\n\n# Advanced blending techniques with descriptive names\noverlay_modes = [\n    (\"DarkGradientBlend\", dark_gradient_blend),\n    (\"VignetteOverlay\", vignette_overlay),\n    (\"CinematicContrast\", cinematic_contrast),\n    (\"RadialBlend\", radial_blend),\n    (\"TexturalOverlay\", textural_overlay),\n    (\"ShadowHighlightBlend\", shadow_highlight_blend),\n    (\"SpectralHighlights\", spectral_highlights),\n    (\"DeepShadowMerge\", deep_shadow_merge)\n]\n\n# Function to create overlay and save to output directory\ndef create_overlay(fg_file, bg_file, output_path, mode_name, mode_func):\n    try:\n        # Open images\n        fg_img = Image.open(os.path.join(mpost_dir, fg_file)).convert(\"RGBA\")\n        bg_img = Image.open(os.path.join(midjourney_dir, bg_file)).convert(\"RGBA\")\n        \n        # Resize foreground to match background dimensions\n        fg_img = fg_img.resize(bg_img.size)\n        \n        # Convert to RGB for blend operations\n        fg_rgb = fg_img.convert(\"RGB\")\n        bg_rgb = bg_img.convert(\"RGB\")\n        \n        # Create overlay using the provided function\n        result = mode_func(fg_rgb, bg_rgb)\n        \n        # Save the result - no labels for clean plates\n        result.save(output_path, quality=95)\n        return True\n    except Exception as e:\n        print(f\"Error processing {fg_file} and {bg_file}: {e}\")\n        return False\n\n# Create overlays for each matching ID\noverlay_count = 0\nfor id_num in sorted(set(list(mpost_by_id.keys()) + list(midjourney_by_id.keys()))):\n    # Skip if either directory doesn't have this ID\n    if id_num not in mpost_by_id or id_num not in midjourney_by_id:\n        print(f\"ID {id_num} not found in both directories, skipping\")\n        continue\n    \n    # Get files for this ID\n    fg_files = mpost_by_id[id_num]\n    bg_files = midjourney_by_id[id_num]\n    \n    print(f\"Processing ID {id_num}: {len(fg_files)} foreground files, {len(bg_files)} background files\")\n    \n    # Create overlays with all combinations\n    for fg_file in fg_files:\n        for bg_file in bg_files:\n            # Create a subdirectory for this ID\n            id_dir = os.path.join(output_dir, f\"{id_num}\")\n            os.makedirs(id_dir, exist_ok=True)\n            \n            # Create overlays with different modes\n            for mode_name, mode_func in overlay_modes:\n                # Create descriptive filename\n                fg_base = os.path.splitext(os.path.basename(fg_file))[0]\n                bg_base = os.path.splitext(os.path.basename(bg_file))[0]\n                output_filename = f\"{fg_base}_{mode_name}_{bg_base}.jpg\"\n                output_path = os.path.join(id_dir, output_filename)\n                \n                # Create and save overlay\n                if create_overlay(fg_file, bg_file, output_path, mode_name, mode_func):\n                    overlay_count += 1\n                    print(f\"Created advanced overlay: {output_filename}\")\n\nprint(f\"\\nCreated {overlay_count} advanced overlay images in {output_dir}\")\n\n# Create HTML file to view the overlays\nhtml_output = os.path.join(output_dir, \"view_advanced_overlays.html\")\nhtml_content = f\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <title>Advanced ID-Matched Overlays</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1, h2, h3 {{ \n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1200px;\n            margin: 0 auto;\n        }}\n        .track {{\n            margin-bottom: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        .overlays {{\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));\n            gap: 20px;\n            margin-top: 20px;\n        }}\n        .overlay {{\n            background-color: #2a2a2a;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n        img {{ \n            max-width: 100%; \n            height: auto;\n            display: block;\n            border: 1px solid #333;\n        }}\n        .info {{\n            margin-top: 10px;\n            font-size: 14px;\n        }}\n        .technique-desc {{\n            margin-top: 30px;\n            padding: 20px;\n            background-color: #2a2a2a;\n            border-radius: 5px;\n        }}\n        .technique {{\n            margin-bottom: 20px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>Advanced Overlay Techniques</h1>\n        <p>This page shows advanced overlay effects with darker backgrounds and complex blending techniques.</p>\n        \n        <div class=\"technique-desc\">\n            <h2>Techniques Used</h2>\n            \n            <div class=\"technique\">\n                <h3>Dark Gradient Blend</h3>\n                <p>Significantly darkens the background and applies a gradient blend with a high-contrast foreground.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Vignette Overlay</h3>\n                <p>Creates a dark vignette effect around the edges with a radial gradient and enhanced foreground.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Cinematic Contrast</h3>\n                <p>Applies a high-contrast cinematic look with cool tones in the background and warm tones in the foreground.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Radial Blend</h3>\n                <p>Creates a radial gradient blend from the center with a very dark background.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Textural Overlay</h3>\n                <p>Adds texture to a dark background with enhanced edge details in the foreground.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Shadow Highlight Blend</h3>\n                <p>Enhances shadows in the background and highlights in the foreground for dramatic contrast.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Spectral Highlights</h3>\n                <p>Creates an ethereal glow effect with a very dark background and enhanced spectral highlights.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Deep Shadow Merge</h3>\n                <p>Creates extremely deep shadows with high-contrast, sharp foreground details.</p>\n            </div>\n        </div>\n\"\"\"\n\n# Add content for each track ID\nfor id_num in sorted(set(list(mpost_by_id.keys()) + list(midjourney_by_id.keys()))):\n    if id_num not in mpost_by_id or id_num not in midjourney_by_id:\n        continue\n    \n    # Get files for this ID to extract track info\n    fg_files = mpost_by_id[id_num]\n    if not fg_files:\n        continue\n        \n    # Get track name from first file\n    track_parts = fg_files[0].split('_', 3)\n    track_name = track_parts[2] if len(track_parts) > 2 else \"Unknown\"\n    \n    html_content += f\"\"\"\n        <div class=\"track\">\n            <h2>Track {id_num}: {track_name}</h2>\n            <div class=\"overlays\">\n    \"\"\"\n    \n    # Get all overlay images for this ID\n    id_dir = os.path.join(output_dir, f\"{id_num}\")\n    if os.path.exists(id_dir):\n        overlay_files = [f for f in os.listdir(id_dir) if f.lower().endswith(('.jpg', '.png'))]\n        for overlay_file in sorted(overlay_files):\n            # Extract mode from filename\n            mode = \"Unknown\"\n            for technique_name, _ in overlay_modes:\n                if technique_name in overlay_file:\n                    mode = technique_name\n                    break\n                \n            html_content += f\"\"\"\n                <div class=\"overlay\">\n                    <h3>{mode}</h3>\n                    <img src=\"{id_num}/{overlay_file}\" alt=\"{overlay_file}\">\n                    <div class=\"info\">{overlay_file}</div>\n                </div>\n            \"\"\"\n    \n    html_content += \"\"\"\n            </div>\n        </div>\n    \"\"\"\n\nhtml_content += \"\"\"\n    </div>\n</body>\n</html>\n\"\"\"\n\nwith open(html_output, 'w') as f:\n    f.write(html_content)\n\nprint(f\"Created HTML viewer: {html_output}\")\n",
    "file_references": [
      "{fg_base}_{mode_name}_{bg_base}.jpg",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/advanced_overlays",
      ", (width, height), 0)\n    draw = ImageDraw.Draw(mask)\n    \n    # Draw radial gradient for vignette\n    for i in range(min(width, height)//2, 0, -1):\n        # Increase darkness toward edges\n        value = int(255 * (i / (min(width, height)/2)))\n        x1, y1 = width//2 - i, height//2 - i\n        x2, y2 = width//2 + i, height//2 + i\n        draw.ellipse((x1, y1, x2, y2), fill=value)\n    \n    # Apply vignette to background\n    darkened_bg.putalpha(mask)\n    \n    # Enhance foreground and blend\n    enhanced_fg = ImageEnhance.Contrast(fg).enhance(1.5)\n    enhanced_fg = ImageEnhance.Brightness(enhanced_fg).enhance(1.2)\n    \n    return Image.composite(enhanced_fg, darkened_bg, mask)\n\ndef cinematic_contrast(fg, bg):\n    # Create high-contrast cinematic look\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.2)\n    darkened_bg = ImageEnhance.Contrast(darkened_bg).enhance(1.8)\n    \n    # Cool tone for background\n    r, g, b = darkened_bg.split()\n    r = ImageEnhance.Brightness(r).enhance(0.85)\n    b = ImageEnhance.Brightness(b).enhance(1.2)\n    darkened_bg = Image.merge(",
      ", (width, height), 0)\n    draw = ImageDraw.Draw(mask)\n    \n    center_radius = min(width, height) // 3\n    for i in range(center_radius, 0, -1):\n        # Gradually reduce opacity from center outward\n        opacity = int(255 * (i / center_radius))\n        x1, y1 = width//2 - i, height//2 - i\n        x2, y2 = width//2 + i, height//2 + i\n        draw.ellipse((x1, y1, x2, y2), fill=opacity)\n    \n    # Apply the mask for blending\n    return Image.composite(enhanced_fg, darkened_bg, mask)\n\ndef textural_overlay(fg, bg):\n    # Create textured, dark background with overlay foreground\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.15)\n    \n    # Add texture to background\n    noise = bg.filter(ImageFilter.GaussianBlur(radius=1))\n    noise = ImageEnhance.Contrast(noise).enhance(2.0)\n    darkened_bg = ImageChops.add(darkened_bg, noise, scale=2.0, offset=-100)\n    \n    # Enhance foreground details\n    enhanced_fg = fg.filter(ImageFilter.EDGE_ENHANCE_MORE)\n    enhanced_fg = ImageEnhance.Sharpness(enhanced_fg).enhance(1.5)\n    \n    # Create alpha blend\n    return Image.blend(darkened_bg, enhanced_fg, 0.7)\n\ndef shadow_highlight_blend(fg, bg):\n    # Enhance shadows in background, highlights in foreground\n    darkened_bg = ImageEnhance.Brightness(bg).enhance(0.1)\n    darkened_bg = ImageEnhance.Contrast(darkened_bg).enhance(1.5)\n    \n    # Create shadow mask from foreground\n    fg_gray = fg.convert(",
      "<!DOCTYPE html>\n<html>\n<head>\n    <title>Advanced ID-Matched Overlays</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1, h2, h3 {{ \n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1200px;\n            margin: 0 auto;\n        }}\n        .track {{\n            margin-bottom: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        .overlays {{\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));\n            gap: 20px;\n            margin-top: 20px;\n        }}\n        .overlay {{\n            background-color: #2a2a2a;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n        img {{ \n            max-width: 100%; \n            height: auto;\n            display: block;\n            border: 1px solid #333;\n        }}\n        .info {{\n            margin-top: 10px;\n            font-size: 14px;\n        }}\n        .technique-desc {{\n            margin-top: 30px;\n            padding: 20px;\n            background-color: #2a2a2a;\n            border-radius: 5px;\n        }}\n        .technique {{\n            margin-bottom: 20px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=",
      ">\n        <h1>Advanced Overlay Techniques</h1>\n        <p>This page shows advanced overlay effects with darker backgrounds and complex blending techniques.</p>\n        \n        <div class=",
      ">\n            <h2>Techniques Used</h2>\n            \n            <div class=",
      ">\n                <h3>Dark Gradient Blend</h3>\n                <p>Significantly darkens the background and applies a gradient blend with a high-contrast foreground.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Vignette Overlay</h3>\n                <p>Creates a dark vignette effect around the edges with a radial gradient and enhanced foreground.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Cinematic Contrast</h3>\n                <p>Applies a high-contrast cinematic look with cool tones in the background and warm tones in the foreground.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Radial Blend</h3>\n                <p>Creates a radial gradient blend from the center with a very dark background.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Textural Overlay</h3>\n                <p>Adds texture to a dark background with enhanced edge details in the foreground.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Shadow Highlight Blend</h3>\n                <p>Enhances shadows in the background and highlights in the foreground for dramatic contrast.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Spectral Highlights</h3>\n                <p>Creates an ethereal glow effect with a very dark background and enhanced spectral highlights.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Deep Shadow Merge</h3>\n                <p>Creates extremely deep shadows with high-contrast, sharp foreground details.</p>\n            </div>\n        </div>\n",
      ">\n            <h2>Track {id_num}: {track_name}</h2>\n            <div class=",
      ">\n                    <h3>{mode}</h3>\n                    <img src=",
      ">{overlay_file}</div>\n                </div>\n            ",
      "\n            </div>\n        </div>\n    ",
      "\n    </div>\n</body>\n</html>\n"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "PIL"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/update_collection_prompts_md.py",
    "size": 4249,
    "lines": 107,
    "source": "#!/usr/bin/env python3\n\nimport os\nimport re\n\n# Paths\ncollection_prompts_path = \"/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_prompts.md\"\nht_prompts_path = \"/Users/gaia/resurrecting atlantis/TIGER/HT/HT_prompts.md\"\nnm_prompts_path = \"/Users/gaia/resurrecting atlantis/TIGER/NM/NM_prompts.md\"\n\ndef read_prompts_with_timestamps(prompt_file_path, prefix):\n    \"\"\"Read prompts from the prompt file and return a list of formatted prompts with timestamps\"\"\"\n    with open(prompt_file_path, 'r') as f:\n        prompt_lines = f.readlines()\n    \n    formatted_prompts = []\n    base_time = {\"HT\": \"04:22:00\", \"NM\": \"06:33:00\"}\n    increment_minutes = 1\n    increment_seconds = 52\n    \n    for i, line in enumerate(prompt_lines):\n        line = line.strip()\n        if not line:\n            continue\n            \n        # Extract prompt ID and text\n        match = re.match(r'^(\\w+\\d+)\\s+\u00b7\\s+(.+)$', line)\n        if match:\n            prompt_id = match.group(1)\n            prompt_text = match.group(2)\n            \n            # Calculate timestamp\n            prompt_index = int(re.search(r'\\d+', prompt_id).group()) - 1  # Convert to 0-based index\n            total_seconds = prompt_index * (increment_minutes * 60 + increment_seconds)\n            hours = int(base_time[prefix].split(':')[0]) + (total_seconds // 3600)\n            minutes = int(base_time[prefix].split(':')[1]) + ((total_seconds % 3600) // 60)\n            seconds = int(base_time[prefix].split(':')[2]) + (total_seconds % 60)\n            \n            # Handle overflow\n            if seconds >= 60:\n                minutes += seconds // 60\n                seconds = seconds % 60\n            if minutes >= 60:\n                hours += minutes // 60\n                minutes = minutes % 60\n                \n            timestamp = f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n            \n            # Extract sequence parts (CS, PI, etc.) if available in the prompt text\n            parts = prompt_text.split(' \u00b7 ')\n            if len(parts) >= 3:\n                code = parts[0]\n                description = parts[1]\n                content = ' \u00b7 '.join(parts[2:])\n                formatted_prompt = f\"{prompt_id} [{timestamp}] \u00b7 {code} \u00b7 {description} \u00b7 {content}\"\n            else:\n                # If the format is different, use the whole text\n                formatted_prompt = f\"{prompt_id} [{timestamp}] \u00b7 {prompt_text}\"\n                \n            formatted_prompts.append(formatted_prompt)\n    \n    return formatted_prompts\n\ndef update_collection_file(collection_path, ht_prompts, nm_prompts):\n    \"\"\"Update the collection file with formatted HT and NM prompts\"\"\"\n    with open(collection_path, 'r') as f:\n        content = f.read()\n    \n    # Locate sections for HT and NM\n    ht_section_pattern = r'## 03_HT - .*?\\(04:22:00 - 06:33:00\\)\\n+\\n+(---)'\n    nm_section_pattern = r'## 04_NM - .*?\\(06:33:00 - 08:44:00\\)\\n+\\n+(---)'\n    \n    # Replace the HT section\n    ht_replacement = \"## 03_HT - How To Win My Heart (04:22:00 - 06:33:00)\\n\\n\" + \"\\n\".join(ht_prompts) + \"\\n\\n\\n\"\n    content = re.sub(ht_section_pattern, ht_replacement + \"---\", content)\n    \n    # Replace the NM section\n    nm_replacement = \"## 04_NM - Nevermore (06:33:00 - 08:44:00)\\n\\n\" + \"\\n\".join(nm_prompts) + \"\\n\\n\\n\"\n    content = re.sub(nm_section_pattern, nm_replacement + \"---\", content)\n    \n    # Write the updated content back to the file\n    with open(collection_path, 'w') as f:\n        f.write(content)\n    \n    return True\n\ndef main():\n    # Load formatted prompts\n    print(\"Loading HT prompts...\")\n    ht_prompts = read_prompts_with_timestamps(ht_prompts_path, \"HT\")\n    print(f\"Loaded {len(ht_prompts)} HT prompts\")\n    \n    print(\"Loading NM prompts...\")\n    nm_prompts = read_prompts_with_timestamps(nm_prompts_path, \"NM\")\n    print(f\"Loaded {len(nm_prompts)} NM prompts\")\n    \n    # Update the collection file\n    print(\"Updating collection file...\")\n    success = update_collection_file(collection_prompts_path, ht_prompts, nm_prompts)\n    if success:\n        print(f\"Successfully updated the collection prompts file with HT and NM prompts\")\n    else:\n        print(\"Failed to update the collection prompts file\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/TIGER/COLLECTION_WhereYouGoWhenYouLeave_prompts.md",
      "/Users/gaia/resurrecting atlantis/TIGER/HT/HT_prompts.md",
      "/Users/gaia/resurrecting atlantis/TIGER/NM/NM_prompts.md",
      ")[0]) + (total_seconds // 3600)\n            minutes = int(base_time[prefix].split(",
      ")[1]) + ((total_seconds % 3600) // 60)\n            seconds = int(base_time[prefix].split(",
      ")[2]) + (total_seconds % 60)\n            \n            # Handle overflow\n            if seconds >= 60:\n                minutes += seconds // 60\n                seconds = seconds % 60\n            if minutes >= 60:\n                hours += minutes // 60\n                minutes = minutes % 60\n                \n            timestamp = f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/ht_ordered_sequence_generator.py",
    "size": 11449,
    "lines": 305,
    "source": "#!/usr/bin/env python3\n\"\"\"\nHT Ordered Sequence Generator\n\nThis script generates Codex entries that follow the EXACT shot order from the prompts file.\nIt ensures that the video frames are sequenced exactly as specified in the original prompts file.\n\nThe script:\n1. Reads the HT section from the prompts file\n2. Extracts the exact shot order with timestamps\n3. Creates Codex entries preserving this order\n4. Allows for video generation with this order\n\"\"\"\n\nimport os\nimport re\nimport json\nimport sys\nimport subprocess\nfrom collections import OrderedDict\n\n# Base directories\nTIGER_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nJELLYFISH_DIR = \"/Users/gaia/resurrecting atlantis/JELLYFISH\"\nPROMPTS_PATH = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_prompts.md\")\nHT_DIR = os.path.join(TIGER_DIR, \"HT\")\nOUTPUT_PATH = os.path.join(TIGER_DIR, \"HT_ordered_codex_entries.md\")\nMANTA_AUDIO_DIR = \"/Users/gaia/resurrecting atlantis/MANTA/audio\"\n\ndef load_assembly_data():\n    \"\"\"Load the HT assembly JSON data.\"\"\"\n    assembly_path = os.path.join(HT_DIR, \"HT_assembly.json\")\n    with open(assembly_path, 'r') as f:\n        return {item[\"id\"]: item for item in json.load(f)}\n\ndef extract_images_from_simplified():\n    \"\"\"Extract all image paths from the Simplified file for all HT shots.\"\"\"\n    simplified_path = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\n    \n    with open(simplified_path, 'r') as f:\n        simplified_content = f.read()\n    \n    # Extract all HT image paths with timestamps\n    pattern = r'(HT\\d+)\\s+\\[([^\\]]+)\\]\\s+`([^`]+)`'\n    matches = re.findall(pattern, simplified_content)\n    \n    # Build a dictionary of shot_id -> [(timestamp, image_path), ...]\n    image_dict = {}\n    for shot_id, timestamp, image_path in matches:\n        if shot_id not in image_dict:\n            image_dict[shot_id] = []\n        image_dict[shot_id].append((timestamp, image_path))\n    \n    # Sort each shot's images by timestamp\n    for shot_id in image_dict:\n        image_dict[shot_id].sort(key=lambda x: x[0])  # Sort by timestamp\n    \n    return image_dict\n\ndef find_image_path_for_shot(shot_id, image_dict):\n    \"\"\"Find the image path for a given shot ID using the extracted image dictionary.\"\"\"\n    if shot_id in image_dict and image_dict[shot_id]:\n        # Get the first image variant for this shot\n        timestamp, image_path = image_dict[shot_id][0]\n        \n        # For HT entries, adjust path to look directly in HT directory if needed\n        # This is only for generating the codex entries in a way compatible with the video generator\n        if shot_id.startswith(\"HT\") and \"HT_shots\" in image_path:\n            # Extract just the filename portion\n            filename = os.path.basename(image_path)\n            return filename\n        \n        # Make sure the path is absolute\n        if not image_path.startswith('/'):\n            image_path = os.path.join(TIGER_DIR, image_path)\n            \n        return image_path\n    \n    return None\n\ndef extract_ht_shots_from_prompts():\n    \"\"\"Extract HT shots in exact order with timestamps from the prompts file.\"\"\"\n    with open(PROMPTS_PATH, 'r') as f:\n        prompts_text = f.read()\n    \n    # Find the HT section\n    ht_section_match = re.search(r'## 03_HT - How To Win My Heart.*?(?=\\n\\n##|\\Z)', prompts_text, re.DOTALL)\n    \n    if not ht_section_match:\n        print(\"Error: Could not find HT section in prompts file\")\n        return []\n        \n    ht_section = ht_section_match.group(0)\n    \n    # Extract shots with timestamps in order\n    shot_pattern = r'(HT\\d+)\\s+\\[([^\\]]+)\\]\\s+\u00b7\\s+([^\u00b7]+)\u00b7\\s+([^\u00b7]+)\u00b7\\s+(.*)'\n    matches = re.findall(shot_pattern, ht_section)\n    \n    ordered_shots = []\n    for match in matches:\n        shot_id, timestamp, syntagma_type, ekphrasis, prompt_desc = match\n        ordered_shots.append({\n            \"shot_id\": shot_id,\n            \"timestamp\": timestamp,\n            \"syntagma_type\": syntagma_type.strip(),\n            \"ekphrasis\": ekphrasis.strip(),\n            \"prompt\": f\"{syntagma_type.strip()} \u00b7 {ekphrasis.strip()} \u00b7 {prompt_desc.strip()}\"\n        })\n    \n    return ordered_shots\n\ndef generate_ordered_codex_entries():\n    \"\"\"Generate Codex entries in the exact order specified in prompts file.\"\"\"\n    assembly_data = load_assembly_data()\n    ordered_shots = extract_ht_shots_from_prompts()\n    image_dict = extract_images_from_simplified()\n    \n    if not ordered_shots:\n        print(\"Error: No shots extracted from prompts file\")\n        return None\n    \n    # Start output with a header\n    output_text = \"# HT (How To Win My Heart) Codex Entries - ORDERED SEQUENCE\\n\\n\"\n    \n    # Generate formatted entries in specified order\n    entries_count = 0\n    shots_with_images = 0\n    shots_without_images = 0\n    \n    for shot in ordered_shots:\n        shot_id = shot[\"shot_id\"]\n        timestamp = shot[\"timestamp\"]\n        \n        # Find image path\n        image_path = find_image_path_for_shot(shot_id, image_dict)\n        if not image_path:\n            image_path = f\"{shot_id}__missing_image.png\"\n            print(f\"Warning: No image found for {shot_id}\")\n            shots_without_images += 1\n        else:\n            shots_with_images += 1\n        \n        # Get assembly data if available\n        assembly_json = \"{}\"\n        if shot_id in assembly_data:\n            assembly_json = json.dumps(assembly_data[shot_id], indent=2)\n        else:\n            print(f\"Warning: No assembly data found for {shot_id}\")\n        \n        # Format the codex entry\n        entry_text = f\"### {shot_id} [{timestamp}]\\n\\n\"\n        \n        # For HT, we need to adjust the image path for compatibility with the video generator\n        if shot_id.startswith(\"HT\"):\n            # Just use the filename without directory structure\n            image_filename = os.path.basename(image_path) if os.path.basename(image_path) else image_path\n            entry_text += f\"**Image:** `{image_filename}`\\n\\n\"\n        else:\n            entry_text += f\"**Image:** `{os.path.basename(image_path) if os.path.basename(image_path) else image_path}`\\n\\n\"\n            \n        entry_text += \"**Assembly Source:**\\n```json\\n\"\n        entry_text += assembly_json\n        entry_text += \"\\n```\\n\\n\"\n        entry_text += f\"**Prompt:** {shot['prompt']}\\n\\n---\\n\\n\"\n        \n        output_text += entry_text\n        entries_count += 1\n    \n    # Write to output file\n    with open(OUTPUT_PATH, 'w') as f:\n        f.write(output_text)\n    \n    print(f\"Generated ordered codex entries written to {OUTPUT_PATH}\")\n    print(f\"Total entries: {entries_count}\")\n    print(f\"Shots with images: {shots_with_images}\")\n    print(f\"Shots without images: {shots_without_images}\")\n    \n    # Update the main Codex file\n    update_codex_file(output_text)\n    \n    return output_text\n\ndef update_codex_file(codex_entries):\n    \"\"\"Update the main Codex file with the ordered entries.\"\"\"\n    codex_path = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Codex.md\")\n    \n    with open(codex_path, 'r') as f:\n        codex_content = f.read()\n    \n    # Check if HT section already exists\n    section_header = \"# HT (How To Win My Heart) Codex Entries\"\n    \n    if section_header in codex_content:\n        print(\"HT section already exists in Codex file. Creating a separate 'ORDERED' section.\")\n        section_header = \"# HT (How To Win My Heart) Codex Entries - ORDERED SEQUENCE\"\n    \n    # Append the new entries with the appropriate header\n    with open(codex_path, 'a') as f:\n        f.write(\"\\n\\n\" + section_header + codex_entries[codex_entries.find(\"\\n\\n\") + 2:])\n        \n    print(f\"Updated main Codex file with ordered HT entries\")\n    return True\n\ndef generate_ordered_video():\n    \"\"\"Generate a video using the ordered codex entries.\"\"\"\n    # First make sure the original script exists\n    video_script = os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\")\n    if not os.path.exists(video_script):\n        print(f\"Error: Video generator script not found at {video_script}\")\n        return False\n    \n    # Generate the video with the correct ordered entries\n    cmd = [\n        \"python3\",\n        video_script,\n        \"--prefix\", \"HT\",\n        \"--ordered_codex\", OUTPUT_PATH\n    ]\n    \n    # Run the command\n    result = subprocess.run(cmd, cwd=JELLYFISH_DIR, text=True, capture_output=True)\n    \n    if result.returncode != 0:\n        print(f\"Error generating video: {result.stderr}\")\n        return False\n        \n    print(result.stdout)\n    print(\"Video generated successfully in correct order!\")\n\n    # Add audio to the video\n    add_audio_to_video()\n    return True\n\ndef add_audio_to_video():\n    \"\"\"Add audio from MANTA/audio to the generated video.\"\"\"\n    video_path = os.path.join(JELLYFISH_DIR, \"video_output\", \"HT_header_prompt.mp4\")\n    \n    # Find the appropriate audio file\n    audio_file = \"How To Win My Heart_Echoes of Longing remix v2.2.1.mp3\"\n    audio_path = os.path.join(MANTA_AUDIO_DIR, audio_file)\n    \n    if not os.path.exists(audio_path):\n        print(f\"Audio file not found: {audio_path}\")\n        return False\n    \n    # Output path for video with audio\n    output_path = os.path.join(JELLYFISH_DIR, \"video_output\", \"HT_header_prompt_with_audio.mp4\")\n    \n    # Add audio to video using FFmpeg\n    print(f\"Adding audio to video...\")\n    audio_cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", video_path,\n        \"-i\", audio_path,\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-shortest\",\n        output_path\n    ]\n    \n    audio_result = subprocess.run(audio_cmd, text=True, capture_output=True)\n    \n    if audio_result.returncode != 0:\n        print(f\"Error adding audio to video: {audio_result.stderr}\")\n        return False\n    \n    print(f\"Video with audio generated: {output_path}\")\n    return True\n\ndef modify_video_generator():\n    \"\"\"Checks if we need to modify the video generator script to support ordered codex files.\"\"\"\n    video_generator_path = os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\")\n    \n    with open(video_generator_path, 'r') as f:\n        content = f.read()\n    \n    # Check if support for ordered codex is already added\n    if \"--ordered_codex\" in content and \"ordered_codex\" in content:\n        print(\"Video generator already supports ordered codex parameter.\")\n        return True\n    \n    print(\"WARNING: Video generator script needs to be modified to support ordered codex parameter!\")\n    print(\"Please modify the script manually to add support for an '--ordered_codex' parameter\")\n    print(\"that allows specifying a different source for the codex entries.\")\n    \n    return False\n\nif __name__ == \"__main__\":\n    print(\"Generating HT ordered sequence...\")\n    \n    # Generate the ordered codex entries\n    generate_ordered_codex_entries()\n    \n    # Check if video generator needs modification\n    modify_video_generator()\n    \n    # Tell the user how to generate the video\n    print(\"\\nTo generate the video with this EXACT ORDER, you need to:\")\n    print(\"1. Modify fl_video_generator_header_prompt.py to accept an '--ordered_codex' parameter\")\n    print(\"2. Add support for reading from this ordered codex file instead of the main one\")\n    print(\"3. Run: python3 fl_video_generator_header_prompt.py --prefix HT --ordered_codex \" + OUTPUT_PATH)\n    print(\"\\nAlternatively, you can manually ensure the main Codex file has the HT entries in the correct order.\")\n",
    "file_references": [
      "HT_assembly.json",
      "{shot_id}__missing_image.png",
      "HT_header_prompt.mp4",
      "How To Win My Heart_Echoes of Longing remix v2.2.1.mp3",
      "HT_header_prompt_with_audio.mp4",
      "/Users/gaia/resurrecting atlantis/TIGER",
      "/Users/gaia/resurrecting atlantis/JELLYFISH",
      "/Users/gaia/resurrecting atlantis/MANTA/audio",
      "Add audio from MANTA/audio to the generated video."
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd, cwd=JELLYFISH_DIR, text=True, capture_output=True"
      },
      {
        "type": "run",
        "snippet": "audio_cmd, text=True, capture_output=True"
      }
    ],
    "imports": [
      "os",
      "re",
      "json",
      "sys",
      "subprocess",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "HT Ordered Sequence Generator\n\nThis script generates Codex entries that follow the EXACT shot order from the prompts file.\nIt ensures that the video frames are sequenced exactly as specified in the original prompts file.\n\nThe script:\n1. Reads the HT section from the prompts file\n2. Extracts the exact shot order with timestamps\n3. Creates Codex entries preserving this order\n4. Allows for video generation with this order"
  },
  {
    "path": "JELLYFISH/rename_midjourney_files.py",
    "size": 4559,
    "lines": 117,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport shutil\n\n# Path to the folder\nfolder_path = \"/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]\"\n\n# Track naming scheme with identifiers\ntrack_mappings = {\n    \"Out_of_Life\": \"01_SH_OutOfLife_000000_BG\",\n    \"Flashing_Lights\": \"02_FL_FlashingLights_021100_BG\",\n    \"How_to_Break_Off_An_Engagement\": \"03_HT_HowToBreakOffAnEngagement_042200_BG\",\n    \"Nevermore\": \"04_NM_Nevermore_063300_BG\",\n    \"Bloodline\": \"05_BE_Bloodline_084400_BG\",\n    \"Resurrecting_Atlantis\": \"06_AT_ResurrectingAtlantis_105500_BG\",\n    \"DJ_Turn_Me_Up\": \"07_DJ_DJTurnMeUp_130600_BG\",\n    \"Newly_Single\": \"08_NS_NewlySingle_151700_BG\",\n    \"Yet_Heard\": \"09_YH_YetHeard_172800_BG\",\n    \"Magic_Ride\": \"10_MR_MagicRide_193900_BG\",\n    \"Reunion\": \"12_RU_Reunion_215000_BG\",\n    \"How_to_Win_My_Heart\": \"13_HW_HowToWinMyHeart_240100_BG\",\n    \"Hot_Minute\": \"14_HM_HotMinute_261200_BG\"\n}\n\n# Get all PNG files in the folder\nfiles = [f for f in os.listdir(folder_path) if f.lower().endswith('.png')]\n\n# Create a backup folder\nbackup_folder = os.path.join(folder_path, \"original_files_backup\")\nos.makedirs(backup_folder, exist_ok=True)\n\n# Function to determine which track a filename corresponds to\ndef get_track_prefix(filename):\n    # Remove the 'coolradio_' prefix\n    if filename.startswith('coolradio_'):\n        clean_name = filename[len('coolradio_'):]\n    else:\n        clean_name = filename\n    \n    # Look for segment titles that use explicit track names\n    if 'Segment_Title_' in clean_name:\n        # Extract the segment title\n        segment_match = re.search(r'Segment_Title_(\\w+)', clean_name)\n        if segment_match:\n            segment_name = segment_match.group(1)\n            # Check if this segment name is in our mappings\n            for track_key in track_mappings.keys():\n                if segment_name in track_key or track_key in segment_name:\n                    return track_mappings[track_key]\n    \n    # Check for each track name in the filename\n    for track_key, track_prefix in track_mappings.items():\n        # Replace underscores with spaces for matching\n        search_key = track_key.replace('_', ' ')\n        # Also try with underscores\n        if search_key in clean_name or track_key in clean_name:\n            return track_prefix\n    \n    # If no specific match is found, use the first word after 'coolradio_' as a partial match\n    first_word = clean_name.split('_')[0] if '_' in clean_name else ''\n    for track_key, track_prefix in track_mappings.items():\n        if first_word and (first_word in track_key or track_key.startswith(first_word)):\n            return track_prefix\n    \n    # Default for unmatched files\n    return \"00_UNKNOWN_BG\"\n\n# Function to extract the descriptive part of the filename\ndef extract_description(filename):\n    # Remove the 'coolradio_' prefix and any track identifiers\n    if filename.startswith('coolradio_'):\n        clean_name = filename[len('coolradio_'):]\n    else:\n        clean_name = filename\n    \n    # Try to extract the descriptive part after 'Visual_Concept'\n    if 'Visual_Concept_' in clean_name:\n        parts = clean_name.split('Visual_Concept_', 1)\n        if len(parts) > 1:\n            # Extract until the first period or underscore followed by hash\n            desc_part = parts[1].split('.', 1)[0].split('_', 1)[0]\n            return desc_part\n    \n    # If we can't find a good description, use a generic one with the unique part of the filename\n    unique_id = filename.split('_')[-2] if len(filename.split('_')) > 2 else \"img\"\n    return f\"Image_{unique_id[:6]}\"\n\n# Rename files\nrenamed_count = 0\nfor file in files:\n    old_path = os.path.join(folder_path, file)\n    \n    # Get the appropriate track prefix\n    track_prefix = get_track_prefix(file)\n    \n    # Extract a descriptive part if possible\n    description = extract_description(file)\n    \n    # Get a counter suffix from the original filename (last character before .png)\n    counter_suffix = file.split('.')[-2][-1] if file.split('.')[-2][-1].isdigit() else \"\"\n    \n    # Create new filename\n    new_filename = f\"{track_prefix}_{description}{counter_suffix}.png\"\n    new_path = os.path.join(folder_path, new_filename)\n    \n    # Backup original file\n    backup_path = os.path.join(backup_folder, file)\n    shutil.copy2(old_path, backup_path)\n    \n    # Rename the file\n    os.rename(old_path, new_path)\n    renamed_count += 1\n    print(f\"Renamed: {file} \u2192 {new_filename}\")\n\nprint(f\"\\nRenamed {renamed_count} files. Originals backed up in: {backup_folder}\")\n",
    "file_references": [
      "{track_prefix}_{description}{counter_suffix}.png",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/normalize_and_assemble_ibex.py",
    "size": 9276,
    "lines": 252,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\n# Directory containing video files\nIBEX_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/IBEX')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Create a temporary directory for processed videos\nTEMP_DIR = Path(tempfile.mkdtemp())\nprint(f\"Using temporary directory: {TEMP_DIR}\")\n\n# Track order specified\nTRACK_ORDER = [\n    \"01_SH_OutOfLife_000000\",\n    \"02_FL_FlashingLights_021100\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\",\n    \"04_NM_Nevermore_063300\",\n    \"05_BE_Bloodline_084400\",\n    \"06_AT_ResurrectingAtlantis_105500\",\n    \"07_DJ_DJTurnMeUp_130600\",\n    \"08_NS_NewlySingle_151700\",\n    \"09_YH_YetHeard_172800\",\n    \"10_MR_MagicRide_193900\",\n    \"12_RU_Reunion_215000\",\n    \"13_HW_HowToWinMyHeart_240100\",\n    \"14_HM_HotMinute_261200\"\n]\n\ndef get_video_info(video_path):\n    \"\"\"Get information about a video file using FFprobe.\"\"\"\n    try:\n        # Get basic format information\n        result = subprocess.run([\n            'ffprobe',\n            '-v', 'error',\n            '-select_streams', 'v:0',\n            '-show_entries', 'stream=width,height,duration,r_frame_rate',\n            '-show_entries', 'format=duration',\n            '-of', 'json',\n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        \n        # Extract information\n        info = {\n            'width': data.get('streams', [{}])[0].get('width', 0),\n            'height': data.get('streams', [{}])[0].get('height', 0),\n            'duration': float(data.get('format', {}).get('duration', 0))\n        }\n        \n        # Get frame rate\n        fps_str = data.get('streams', [{}])[0].get('r_frame_rate', '0/1')\n        if '/' in fps_str:\n            num, den = map(int, fps_str.split('/'))\n            info['fps'] = num / den if den else 0\n        else:\n            info['fps'] = float(fps_str)\n        \n        return info\n    except (subprocess.CalledProcessError, json.JSONDecodeError) as e:\n        print(f\"Error getting info for {video_path}: {e}\")\n        return {'width': 0, 'height': 0, 'duration': 0, 'fps': 0}\n\ndef find_video_files():\n    \"\"\"Find all video files in the IBEX directory.\"\"\"\n    video_files = []\n    for file in IBEX_DIR.glob('**/*.mp4'):\n        video_files.append(file)\n    \n    # Sort videos according to track order\n    sorted_videos = []\n    for track in TRACK_ORDER:\n        for video in video_files:\n            if video.name.startswith(track):\n                sorted_videos.append(video)\n                break\n    \n    # Add any remaining videos that didn't match track order\n    for video in video_files:\n        if not any(video.name.startswith(track) for track in TRACK_ORDER):\n            sorted_videos.append(video)\n    \n    return sorted_videos\n\ndef normalize_video(video_path, target_width=1280, target_height=720, target_duration=5.0):\n    \"\"\"Normalize a video to the target resolution and duration.\"\"\"\n    # Get video info\n    info = get_video_info(video_path)\n    filename = video_path.name\n    \n    # Create output path\n    output_path = TEMP_DIR / f\"norm_{filename}\"\n    \n    print(f\"\\nNormalizing: {filename}\")\n    print(f\"  Original: {info['width']}x{info['height']}, {info['duration']:.2f}s, {info['fps']:.2f}fps\")\n    print(f\"  Target: {target_width}x{target_height}, {target_duration:.2f}s\")\n    \n    # Set duration (trim if longer than target)\n    duration_option = ['-t', str(target_duration)] if info['duration'] > target_duration else []\n    \n    try:\n        # Run FFmpeg to normalize\n        subprocess.run([\n            'ffmpeg',\n            '-i', str(video_path),\n            *duration_option,\n            '-vf', f'scale={target_width}:{target_height}:force_original_aspect_ratio=decrease,pad={target_width}:{target_height}:(ow-iw)/2:(oh-ih)/2',\n            '-c:v', 'libx264',\n            '-preset', 'fast',\n            '-crf', '22',\n            '-an',  # No audio\n            '-y',\n            str(output_path)\n        ], check=True, capture_output=True)\n        \n        # Verify output\n        output_info = get_video_info(output_path)\n        print(f\"  Result: {output_info['width']}x{output_info['height']}, {output_info['duration']:.2f}s\")\n        \n        if output_info['width'] != target_width or output_info['height'] != target_height:\n            print(f\"  Warning: Output resolution {output_info['width']}x{output_info['height']} doesn't match target {target_width}x{target_height}\")\n        \n        return output_path\n    except subprocess.CalledProcessError as e:\n        print(f\"  Error normalizing {filename}: {e}\")\n        print(f\"  FFmpeg stderr: {e.stderr.decode()}\")\n        return None\n\ndef create_sequential_video(normalized_videos):\n    \"\"\"Create a sequential video from normalized video files.\"\"\"\n    # Skip if no videos\n    if not normalized_videos:\n        print(\"No normalized videos available to create sequence.\")\n        return None\n    \n    # Get total expected duration\n    total_duration = sum(get_video_info(video)['duration'] for video in normalized_videos)\n    \n    # Output filename\n    output_file = OUTPUT_DIR / \"ResurrectingAtlantis_IBEX_Normalized.mp4\"\n    \n    print(f\"\\nCreating sequential video...\")\n    print(f\"Total clips: {len(normalized_videos)}\")\n    print(f\"Expected duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n    \n    # Create a concat file\n    concat_file = TEMP_DIR / \"concat.txt\"\n    with open(concat_file, 'w') as f:\n        for video in normalized_videos:\n            f.write(f\"file '{video}'\\n\")\n    \n    try:\n        # Use FFmpeg to concatenate\n        subprocess.run([\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file),\n            '-c', 'copy',\n            '-y',\n            str(output_file)\n        ], check=True, capture_output=True)\n        \n        # Verify output\n        output_info = get_video_info(output_file)\n        print(f\"\\nOutput file created: {output_file}\")\n        print(f\"Output duration: {output_info['duration']:.2f} seconds ({output_info['duration']/60:.2f} minutes)\")\n        \n        # Create a summary file\n        summary_file = OUTPUT_DIR / \"ibex_normalized_summary.txt\"\n        with open(summary_file, 'w') as f:\n            f.write(\"RESURRECTING ATLANTIS - IBEX NORMALIZED SEQUENCE\\n\")\n            f.write(\"==============================================\\n\\n\")\n            f.write(f\"Total clips: {len(normalized_videos)}\\n\")\n            f.write(f\"Total duration: {output_info['duration']:.2f} seconds ({output_info['duration']/60:.2f} minutes)\\n\")\n            f.write(f\"Output file: {output_file}\\n\\n\")\n            \n            f.write(\"CLIP DETAILS:\\n\")\n            for i, video in enumerate(normalized_videos, 1):\n                info = get_video_info(video)\n                track = next((t for t in TRACK_ORDER if video.name.startswith(t) or Path(video.name).name.startswith(t)), \"Unknown\")\n                \n                f.write(f\"{i}. {track}\\n\")\n                f.write(f\"   File: {video.name}\\n\")\n                f.write(f\"   Duration: {info['duration']:.2f} seconds\\n\")\n                f.write(f\"   Resolution: {info['width']}x{info['height']}\\n\")\n                f.write(f\"   Frame rate: {info['fps']:.2f} fps\\n\\n\")\n        \n        print(f\"Summary saved to: {summary_file}\")\n        \n        return output_file\n    except subprocess.CalledProcessError as e:\n        print(f\"Error creating sequential video: {e}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode()}\")\n        return None\n\ndef main():\n    try:\n        print(\"=== IBEX VIDEO NORMALIZATION AND ASSEMBLY ===\\n\")\n        \n        # Find video files\n        print(\"Finding and sorting video files...\")\n        videos = find_video_files()\n        print(f\"Found {len(videos)} video files.\")\n        \n        if not videos:\n            print(\"No videos found in the IBEX directory.\")\n            return\n        \n        # Print information about found videos\n        for i, video in enumerate(videos, 1):\n            info = get_video_info(video)\n            print(f\"{i}. {video.name} - {info['width']}x{info['height']}, {info['duration']:.2f}s\")\n        \n        # Normalize all videos\n        print(\"\\nNormalizing videos to consistent resolution and duration...\")\n        normalized_videos = []\n        \n        for video in videos:\n            normalized = normalize_video(video)\n            if normalized:\n                normalized_videos.append(normalized)\n        \n        if not normalized_videos:\n            print(\"Failed to normalize any videos.\")\n            return\n        \n        # Create the sequential video\n        output_file = create_sequential_video(normalized_videos)\n        \n        if output_file:\n            print(\"\\n=== SUCCESS ===\")\n            print(f\"IBEX video sequence assembled: {output_file}\")\n        else:\n            print(\"\\n=== FAILED ===\")\n            print(\"Failed to create IBEX video sequence.\")\n    \n    finally:\n        # Clean up temporary directory\n        print(f\"\\nCleaning up temporary files...\")\n        shutil.rmtree(TEMP_DIR)\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "**/*.mp4",
      "ResurrectingAtlantis_IBEX_Normalized.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA/IBEX",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence",
      "0/1",
      "] = num / den if den else 0\n        else:\n            info[",
      "**/*.mp4",
      "\n    # Get video info\n    info = get_video_info(video_path)\n    filename = video_path.name\n    \n    # Create output path\n    output_path = TEMP_DIR / f",
      "scale={target_width}:{target_height}:force_original_aspect_ratio=decrease,pad={target_width}:{target_height}:(ow-iw)/2:(oh-ih)/2",
      "] for video in normalized_videos)\n    \n    # Output filename\n    output_file = OUTPUT_DIR / ",
      "Expected duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)",
      "]/60:.2f} minutes)",
      "]/60:.2f} minutes)\\n"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffprobe',\n            '-v', 'error',\n            '-select_streams', 'v:0',\n            '-show_entries', 'stream=width,height,duration,r_frame_rate',\n            '-show_entries', 'format"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-i', str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "pathlib",
      "tempfile",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/create_multiblend_composites.py",
    "size": 24815,
    "lines": 610,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport random\nfrom pathlib import Path\nfrom PIL import Image, ImageFilter, ImageEnhance, ImageOps, ImageChops, ImageDraw\nimport numpy as np\nfrom collections import defaultdict\n\n# Paths to source directories\nMPRE_RUN_TITLES_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run/backup_titles')\nMPOST_JOURNEY_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey')\nMIDJOURNEY_BG_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]')\n\n# Output directory\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/JELLYFISH/multiblend_composites')\nOUTPUT_DIR.mkdir(exist_ok=True)\nLOGS_DIR = OUTPUT_DIR / \"logs\"\nLOGS_DIR.mkdir(exist_ok=True)\n\n# Track mapping for consistent naming\nTRACK_MAPPING = {\n    \"Out of Life\": {\"id\": \"01\", \"code\": \"SH\", \"timecode\": \"000000\"},\n    \"Flashing Lights\": {\"id\": \"02\", \"code\": \"FL\", \"timecode\": \"021100\"},\n    \"How to Break Off an Engagement\": {\"id\": \"03\", \"code\": \"HT\", \"timecode\": \"042200\"},\n    \"Nevermore\": {\"id\": \"04\", \"code\": \"NM\", \"timecode\": \"063300\"},\n    \"Bloodline\": {\"id\": \"05\", \"code\": \"BE\", \"timecode\": \"084400\"},\n    \"Resurrecting Atlantis\": {\"id\": \"06\", \"code\": \"AT\", \"timecode\": \"105500\"},\n    \"DJ Turn Me Up\": {\"id\": \"07\", \"code\": \"DJ\", \"timecode\": \"130600\"},\n    \"Newly Single\": {\"id\": \"08\", \"code\": \"NS\", \"timecode\": \"151700\"},\n    \"Yet Heard\": {\"id\": \"09\", \"code\": \"YH\", \"timecode\": \"172800\"},\n    \"Magic Ride\": {\"id\": \"10\", \"code\": \"MR\", \"timecode\": \"193900\"},\n    \"Reunion\": {\"id\": \"12\", \"code\": \"RU\", \"timecode\": \"215000\"},\n    \"How to Win My Heart\": {\"id\": \"13\", \"code\": \"HW\", \"timecode\": \"240100\"},\n    \"Hot Minute\": {\"id\": \"14\", \"code\": \"HM\", \"timecode\": \"261200\"},\n}\n\n# Sophisticated Multi-blend Sets (3 sets of paired blends)\nMULTIBLEND_SETS = {\n    \"cosmic_resonance\": [  # Set 1: cosmic resonance\n        {\n            \"name\": \"cosmic_duality\",\n            \"description\": \"Ethereal dual-world effect with spectral traces and quantum fragmentation\",\n            \"techniques\": [\"spectral_weave\", \"quantum_drift\"]\n        },\n        {\n            \"name\": \"ancestral_void\",\n            \"description\": \"Memory and void combining for hauntological effects\",\n            \"techniques\": [\"ancestral_echo\", \"void_emergence\"]\n        }\n    ],\n    \"digital_decay\": [  # Set 2: digital decay\n        {\n            \"name\": \"neon_corruption\",\n            \"description\": \"Glitched neon fracture with data corruption artifacts\",\n            \"techniques\": [\"neon_fracture\", \"data_corruption\"]\n        },\n        {\n            \"name\": \"echo_collapse\",\n            \"description\": \"Temporal echo cascades with collapse patterns\",\n            \"techniques\": [\"digital_echo\", \"time_collapse\"]\n        }\n    ],\n    \"dream_projection\": [  # Set 3: dream projection\n        {\n            \"name\": \"dream_shadow\",\n            \"description\": \"Dreamlike resonance with shadow projections\",\n            \"techniques\": [\"dream_resonance\", \"shadow_projection\"]\n        },\n        {\n            \"name\": \"urban_memory\",\n            \"description\": \"Urban frequencies with memory trace overlays\",\n            \"techniques\": [\"urban_frequency\", \"memory_trace\"]\n        }\n    ]\n}\n\n# Advanced blending technique implementations\ndef apply_spectral_weave(img, intensity=0.7):\n    \"\"\"Creates a spectral weaving pattern\"\"\"\n    width, height = img.size\n    result = img.copy()\n    \n    # Vertical weave stripes\n    stripe_width = width // 40\n    for x in range(0, width, stripe_width * 2):\n        if x + stripe_width <= width:\n            # Create a shifted color version for the stripe\n            stripe_region = (x, 0, x + stripe_width, height)\n            stripe = img.crop(stripe_region)\n            \n            # Apply color shift to the stripe\n            r, g, b = stripe.split()\n            stripe = Image.merge(\"RGB\", (b, r, g))  # Shift color channels\n            \n            # Enhance and paste back\n            stripe = ImageEnhance.Color(stripe).enhance(1.5)\n            result.paste(stripe, (x, 0))\n    \n    # Apply final contrast adjustment\n    result = ImageEnhance.Contrast(result).enhance(1.2)\n    return result\n\ndef apply_quantum_drift(img, intensity=0.7):\n    \"\"\"Creates a quantum-like drift/probability effect\"\"\"\n    height = img.height\n    band_height = height // 25\n    result = img.copy()\n    \n    # Process in bands with quantum-like drift\n    for i in range(0, height, band_height):\n        y_pos = i\n        height_slice = min(band_height, height - i)\n        \n        # Get the band\n        band = img.crop((0, y_pos, img.width, y_pos + height_slice))\n        \n        # Randomly apply band transformations\n        if random.random() < 0.6:  # 60% chance of transformation\n            rand_effect = random.choice([\"shift\", \"invert\", \"enhance\"])\n            \n            if rand_effect == \"shift\":\n                x_offset = random.randint(-10, 10)\n                result.paste(band, (x_offset, y_pos))\n            elif rand_effect == \"invert\":\n                band = ImageOps.invert(band)\n                result.paste(band, (0, y_pos))\n            elif rand_effect == \"enhance\":\n                band = ImageEnhance.Color(band).enhance(1.5)\n                result.paste(band, (0, y_pos))\n    \n    # Apply edge enhancement for definition\n    result = result.filter(ImageFilter.EDGE_ENHANCE_MORE)\n    return result\n\ndef apply_ancestral_echo(img, intensity=0.7):\n    \"\"\"Creates an ancestral echo effect\"\"\"\n    # Create sepia base\n    sepia = ImageOps.colorize(ImageOps.grayscale(img), \"#704214\", \"#C0A080\")\n    \n    # Create ethereal glow layer\n    glow = img.filter(ImageFilter.GaussianBlur(radius=15))\n    glow = ImageEnhance.Brightness(glow).enhance(1.3)\n    \n    # Blend sepia and original with the glow\n    result = Image.blend(sepia, img, 0.4)\n    result = Image.blend(result, glow, 0.3)\n    \n    # Add grain for aged effect\n    grain = np.random.normal(0, 15, (img.height, img.width, 3)).astype(np.uint8)\n    grain_img = Image.fromarray(np.clip(np.array(result) + grain, 0, 255).astype(np.uint8))\n    \n    return grain_img\n\ndef apply_void_emergence(img, intensity=0.7):\n    \"\"\"Creates a void emergence effect\"\"\"\n    # Create inverted base\n    inverted = ImageOps.invert(img)\n    \n    # Create high contrast version for \"emerging\" elements\n    contrast = ImageEnhance.Contrast(img).enhance(2.0)\n    \n    # Create mask based on brightness\n    grayscale = ImageOps.grayscale(img)\n    mask = grayscale.point(lambda x: 0 if x < 100 else 255)\n    \n    # Composite using the mask\n    result = Image.composite(contrast, inverted, mask)\n    \n    # Add final adjustments\n    result = ImageEnhance.Color(result).enhance(1.3)\n    return result\n\ndef apply_neon_fracture(img, intensity=0.7):\n    \"\"\"Creates a neon fracture effect\"\"\"\n    # Split and shift channels\n    r, g, b = img.split()\n    r_shift = Image.new('L', img.size)\n    r_shift.paste(r, (7, 0))\n    b_shift = Image.new('L', img.size)\n    b_shift.paste(b, (-7, 0))\n    \n    # Recombine with shifted channels\n    shifted = Image.merge('RGB', (r_shift, g, b_shift))\n    \n    # Enhance brightness and color\n    result = ImageEnhance.Brightness(shifted).enhance(1.5)\n    result = ImageEnhance.Color(result).enhance(2.0)\n    \n    # Add glow\n    glow = result.filter(ImageFilter.GaussianBlur(radius=5))\n    result = ImageChops.add(result, glow, scale=2.0, offset=-100)\n    \n    return result\n\ndef apply_data_corruption(img, intensity=0.7):\n    \"\"\"Creates digital corruption effects\"\"\"\n    width, height = img.size\n    result = img.copy()\n    \n    # Create glitch blocks\n    num_blocks = random.randint(5, 15)\n    for _ in range(num_blocks):\n        # Define random block\n        block_width = random.randint(10, 50)\n        block_height = random.randint(5, 20)\n        x_pos = random.randint(0, width - block_width)\n        y_pos = random.randint(0, height - block_height)\n        \n        # Get block data\n        block = img.crop((x_pos, y_pos, x_pos + block_width, y_pos + block_height))\n        \n        # Corrupt block with various methods\n        corrupt_method = random.choice([\"shift\", \"color_shift\", \"pixelate\"])\n        \n        if corrupt_method == \"shift\":\n            # Horizontal shift\n            shift_amount = random.randint(5, 15)\n            result.paste(block, (x_pos + shift_amount, y_pos))\n        elif corrupt_method == \"color_shift\":\n            # RGB channel manipulation\n            r, g, b = block.split()\n            block = Image.merge(\"RGB\", (b, r, g))  # Swap channels\n            result.paste(block, (x_pos, y_pos))\n        elif corrupt_method == \"pixelate\":\n            # Pixelation effect\n            small = block.resize((block_width // 4, block_height // 4), Image.NEAREST)\n            pixelated = small.resize((block_width, block_height), Image.NEAREST)\n            result.paste(pixelated, (x_pos, y_pos))\n    \n    # Add scan lines\n    for y in range(0, height, 3):\n        if random.random() < 0.3:  # 30% chance of scan line\n            scan_line = Image.new('RGB', (width, 1), (0, 0, 0))\n            result.paste(scan_line, (0, y))\n    \n    return result\n\ndef apply_digital_echo(img, intensity=0.7):\n    \"\"\"Creates echo/ghost effect\"\"\"\n    result = img.copy()\n    \n    # Create multiple echo layers with different offsets and opacities\n    echo_layers = []\n    for offset in [5, 10, 15, 20]:\n        # Create offset layer\n        echo = Image.new('RGB', img.size)\n        echo.paste(img, (offset, offset))\n        echo_layers.append(echo)\n    \n    # Blend all echo layers\n    opacity = 0.4\n    for echo in echo_layers:\n        result = Image.blend(result, echo, opacity)\n        opacity -= 0.1\n    \n    return result\n\ndef apply_time_collapse(img, intensity=0.7):\n    \"\"\"Creates temporal collapse effect\"\"\"\n    # Create radial gradient mask\n    mask = Image.new('L', img.size, 0)\n    center_x, center_y = img.width // 2, img.height // 2\n    max_radius = min(center_x, center_y)\n    \n    # Fill mask with radial gradient\n    for y in range(img.height):\n        for x in range(img.width):\n            # Calculate distance from center\n            distance = ((x - center_x)**2 + (y - center_y)**2)**0.5\n            # Normalize and invert\n            value = max(0, min(255, int(255 * (1 - distance / max_radius))))\n            if x < mask.width and y < mask.height:\n                mask.putpixel((x, y), value)\n    \n    # Create distorted version\n    distorted = img.filter(ImageFilter.FIND_EDGES)\n    distorted = ImageEnhance.Contrast(distorted).enhance(2.0)\n    \n    # Blend using the mask\n    result = Image.composite(distorted, img, mask)\n    return result\n\ndef apply_dream_resonance(img, intensity=0.7):\n    \"\"\"Creates dreamlike resonance patterns\"\"\"\n    # Apply dreamy blur\n    blurred = img.filter(ImageFilter.GaussianBlur(radius=4))\n    \n    # Enhance colors\n    enhanced = ImageEnhance.Color(blurred).enhance(1.8)\n    \n    # Add soft glow\n    glow = enhanced.filter(ImageFilter.GaussianBlur(radius=10))\n    glow = ImageEnhance.Brightness(glow).enhance(1.2)\n    \n    # Blend original with glow\n    result = Image.blend(enhanced, glow, 0.3)\n    \n    # Add motion effect in one direction\n    motion = Image.new('RGB', img.size)\n    motion.paste(result, (int(intensity * 15), 0))\n    result = Image.blend(result, motion, 0.2)\n    \n    return result\n\ndef apply_shadow_projection(img, intensity=0.7):\n    \"\"\"Creates shadow projection effect\"\"\"\n    # Darken the image\n    darkened = ImageEnhance.Brightness(img).enhance(0.7)\n    \n    # Create edge map for shadows\n    edges = img.filter(ImageFilter.FIND_EDGES)\n    edges = ImageEnhance.Contrast(edges).enhance(2.0)\n    \n    # Create shadow offset\n    shadow = Image.new('RGB', img.size)\n    shadow.paste(edges, (10, 10))\n    shadow = shadow.filter(ImageFilter.GaussianBlur(radius=5))\n    \n    # Composite shadows with darkened image\n    result = Image.blend(darkened, shadow, 0.4)\n    return result\n\ndef apply_urban_frequency(img, intensity=0.7):\n    \"\"\"Creates urban frequency patterns\"\"\"\n    # Enhance contrast for urban feel\n    high_contrast = ImageEnhance.Contrast(img).enhance(1.8)\n    \n    # Create scan line effect for frequency\n    result = high_contrast.copy()\n    width, height = img.size\n    \n    # Add horizontal frequency lines\n    for y in range(0, height, 4):\n        line = high_contrast.crop((0, y, width, y + 1))\n        line = ImageEnhance.Brightness(line).enhance(1.3)\n        result.paste(line, (0, y))\n    \n    # Add vertical frequency elements\n    for x in range(0, width, 40):\n        if random.random() < 0.3:  # 30% chance\n            stripe_width = random.randint(2, 5)\n            if x + stripe_width <= width:\n                stripe = high_contrast.crop((x, 0, x + stripe_width, height))\n                stripe = ImageEnhance.Brightness(stripe).enhance(1.5)\n                result.paste(stripe, (x, 0))\n    \n    return result\n\ndef apply_memory_trace(img, intensity=0.7):\n    \"\"\"Creates memory trace effect\"\"\"\n    # Create blurred base for faded memory\n    blurred = img.filter(ImageFilter.GaussianBlur(radius=2))\n    \n    # Create sharp overlay for memory fragments\n    sharp = ImageEnhance.Sharpness(img).enhance(2.0)\n    \n    # Create mask with random shapes for partial memories\n    mask = Image.new('L', img.size, 0)\n    draw = ImageDraw.Draw(mask)\n    \n    # Draw random memory fragments\n    for _ in range(20):\n        x = random.randint(0, img.width - 100)\n        y = random.randint(0, img.height - 100)\n        size = random.randint(50, 200)\n        \n        shape_type = random.choice([\"circle\", \"rectangle\"])\n        if shape_type == \"circle\":\n            draw.ellipse((x, y, x + size, y + size), fill=255)\n        else:\n            draw.rectangle((x, y, x + size, y + size // 2), fill=255)\n    \n    # Blur the mask for smooth transitions\n    mask = mask.filter(ImageFilter.GaussianBlur(radius=10))\n    \n    # Composite using the mask\n    result = Image.composite(sharp, blurred, mask)\n    \n    # Add slight sepia tone for aged effect\n    sepia_tone = ImageOps.colorize(ImageOps.grayscale(result), \"#222222\", \"#FFE8C0\")\n    result = Image.blend(result, sepia_tone, 0.2)\n    \n    return result\n\n# Master function to apply a blend technique\ndef apply_technique(img, technique_name, intensity=0.7):\n    \"\"\"Apply a specific blending technique to an image\"\"\"\n    if technique_name == \"spectral_weave\":\n        return apply_spectral_weave(img, intensity)\n    elif technique_name == \"quantum_drift\":\n        return apply_quantum_drift(img, intensity)\n    elif technique_name == \"ancestral_echo\":\n        return apply_ancestral_echo(img, intensity)\n    elif technique_name == \"void_emergence\":\n        return apply_void_emergence(img, intensity)\n    elif technique_name == \"neon_fracture\":\n        return apply_neon_fracture(img, intensity)\n    elif technique_name == \"data_corruption\":\n        return apply_data_corruption(img, intensity)\n    elif technique_name == \"digital_echo\":\n        return apply_digital_echo(img, intensity)\n    elif technique_name == \"time_collapse\":\n        return apply_time_collapse(img, intensity)\n    elif technique_name == \"dream_resonance\":\n        return apply_dream_resonance(img, intensity)\n    elif technique_name == \"shadow_projection\":\n        return apply_shadow_projection(img, intensity)\n    elif technique_name == \"urban_frequency\":\n        return apply_urban_frequency(img, intensity)\n    elif technique_name == \"memory_trace\":\n        return apply_memory_trace(img, intensity)\n    else:\n        # Default if technique not found\n        return img\n\n# Apply a multiblend combination (multiple techniques in sequence)\ndef apply_multiblend(img, combo_name, intensity=0.7):\n    \"\"\"Apply a specific multiblend combination to an image\"\"\"\n    # Find the combo in our sets\n    for set_name, combos in MULTIBLEND_SETS.items():\n        for combo in combos:\n            if combo[\"name\"] == combo_name:\n                # Apply each technique in sequence\n                result = img.copy()\n                for technique in combo[\"techniques\"]:\n                    result = apply_technique(result, technique, intensity)\n                return result, combo[\"techniques\"]\n    \n    # Return original if combo not found\n    return img, []\n\n# Helper functions for track name and categorization\ndef find_track_name(filename):\n    for track_name in TRACK_MAPPING.keys():\n        track_variants = [\n            track_name, track_name.lower(), \n            track_name.replace(\" \", \"\"), track_name.lower().replace(\" \", \"\")\n        ]\n        for variant in track_variants:\n            if variant in filename.replace(\" \", \"\"):\n                return track_name\n    return None\n\ndef categorize_title_image(filename):\n    if \"Title Card\" in filename or \"Prompt Set\" in filename:\n        return \"TitleCard\"\n    elif \"Visual Concept\" in filename:\n        return \"VisualConcept\"\n    elif \"Poetic Ekphrasis\" in filename or \"Ekphrasis\" in filename:\n        return \"Ekphrasis\"\n    elif \"Duration\" in filename or \"Segment\" in filename:\n        return \"Segment\"\n    else:\n        return \"Other\"\n\n# Organize files by track\ndef organize_files():\n    \"\"\"Organize all source files by track name and type\"\"\"\n    organized_files = defaultdict(lambda: defaultdict(list))\n    \n    # Process title/visual files from mpre-run/backup_titles\n    for file in MPRE_RUN_TITLES_DIR.glob(\"*.png\"):\n        track_name = find_track_name(file.name)\n        if track_name:\n            file_type = categorize_title_image(file.name)\n            organized_files[track_name][file_type].append(str(file))\n    \n    # Process mpost-journey files\n    for file in MPOST_JOURNEY_DIR.glob(\"*.png\"):\n        if \"backup\" not in file.name.lower():\n            parts = file.name.split('_')\n            if len(parts) >= 2:\n                track_id = parts[0]\n                track_code = parts[1]\n                \n                # Find corresponding track name\n                for track_name, info in TRACK_MAPPING.items():\n                    if info[\"id\"] == track_id and info[\"code\"] == track_code:\n                        organized_files[track_name][\"Journey\"].append(str(file))\n                        break\n    \n    # Process midjourney background files\n    for file in MIDJOURNEY_BG_DIR.glob(\"*.png\"):\n        if \"backup\" not in file.name.lower() and \"BG\" in file.name:\n            parts = file.name.split('_')\n            if parts and (parts[0].isdigit() or (len(parts[0]) == 2 and parts[0][:2].isdigit())):\n                track_id = parts[0]\n                # Find corresponding track name\n                for track_name, info in TRACK_MAPPING.items():\n                    if info[\"id\"] == track_id:\n                        organized_files[track_name][\"Background\"].append(str(file))\n                        break\n    \n    return organized_files\n\n# Main function to create multilayer blended composites\ndef create_multiblend_composites():\n    \"\"\"Create sophisticated composites using multiblend techniques\"\"\"\n    organized_files = organize_files()\n    \n    # Log file for combinations\n    log_file = LOGS_DIR / \"multiblend_combinations.txt\"\n    with open(log_file, \"w\") as log:\n        log.write(\"MULTIBLEND COMPOSITE COMBINATIONS\\n\")\n        log.write(\"=================================\\n\\n\")\n        \n        # Process each track\n        for track_name, file_types in organized_files.items():\n            track_info = TRACK_MAPPING.get(track_name)\n            if not track_info:\n                print(f\"Warning: No track mapping found for {track_name}\")\n                continue\n                \n            track_id = track_info[\"id\"]\n            track_code = track_info[\"code\"]\n            track_timecode = track_info[\"timecode\"]\n            \n            # Base filename format\n            base_filename = f\"{track_id}_{track_code}_{track_name.replace(' ', '')}_{track_timecode}\"\n            \n            # Log track info\n            log.write(f\"\\n{track_name} ({track_id}_{track_code}_{track_timecode})\\n\")\n            log.write(\"-\" * 60 + \"\\n\")\n            \n            # Get available images for this track\n            bg_images = file_types.get(\"Background\", [])\n            journey_images = file_types.get(\"Journey\", [])\n            \n            # Process each type of title/concept image\n            for title_type in [\"TitleCard\", \"VisualConcept\", \"Ekphrasis\", \"Segment\"]:\n                title_images = file_types.get(title_type, [])\n                \n                if not title_images:\n                    continue\n                \n                # For each title image, create multiblend composites\n                for title_img_path in title_images[:1]:  # Limit to first image of each type for variety\n                    # Load title image\n                    title_img = Image.open(title_img_path)\n                    \n                    # For each multiblend set, create rich composites\n                    for set_name, combos in MULTIBLEND_SETS.items():\n                        # Choose background and journey images\n                        bg_img_path = random.choice(bg_images) if bg_images else None\n                        journey_img_path = random.choice(journey_images) if journey_images else None\n                        \n                        # Skip if we don't have necessary images\n                        if not bg_img_path:\n                            print(f\"Warning: No background image found for {track_name} - {title_type}\")\n                            continue\n                        \n                        # Load images\n                        bg_img = Image.open(bg_img_path)\n                        journey_img = Image.open(journey_img_path) if journey_img_path else None\n                        \n                        # Resize images to match\n                        title_img_resized = title_img.resize(bg_img.size, Image.LANCZOS)\n                        if journey_img:\n                            journey_img_resized = journey_img.resize(bg_img.size, Image.LANCZOS)\n                        \n                        # Convert all to RGB\n                        if bg_img.mode != 'RGB':\n                            bg_img = bg_img.convert('RGB')\n                        if title_img_resized.mode != 'RGB':\n                            title_img_resized = title_img_resized.convert('RGB')\n                        if journey_img and journey_img_resized.mode != 'RGB':\n                            journey_img_resized = journey_img_resized.convert('RGB')\n                        \n                        # For each combo in the set, create a multiblend composite\n                        for combo in combos:\n                            combo_name = combo[\"name\"]\n                            \n                            # Create a base blend with background and title\n                            base_blend = Image.blend(bg_img, title_img_resized, 0.6)\n                            \n                            # Add journey layer if available\n                            if journey_img:\n                                # Create a unique blend with the journey image\n                                journey_opacity = random.uniform(0.3, 0.5)\n                                base_blend = Image.blend(base_blend, journey_img_resized, journey_opacity)\n                            \n                            # Apply the multiblend combo\n                            composite, techniques = apply_multiblend(base_blend, combo_name)\n                            \n                            # Create output filename\n                            output_filename = f\"{base_filename}_{title_type}_Multiblend_{set_name}_{combo_name}.png\"\n                            output_path = OUTPUT_DIR / output_filename\n                            \n                            # Save the composite\n                            composite.save(output_path)\n                            print(f\"Created: {output_filename}\")\n                            \n                            # Log combination\n                            log.write(f\"Created: {output_filename}\\n\")\n                            log.write(f\"  - Base: {Path(bg_img_path).name}\\n\")\n                            log.write(f\"  - Layer 1: {Path(title_img_path).name}\\n\")\n                            if journey_img_path:\n                                log.write(f\"  - Layer 2: {Path(journey_img_path).name}\\n\")\n                            log.write(f\"  - Multiblend: {combo_name}\\n\")\n                            log.write(f\"  - Techniques: {', '.join(techniques)}\\n\\n\")\n\n# Execute the script\nif __name__ == \"__main__\":\n    print(\"Starting to create rich multiblend composite images...\")\n    create_multiblend_composites()\n    print(\"Done! Check the output directory for the new multiblend composites.\")\n    print(f\"Composites saved to: {OUTPUT_DIR}\")\n    print(f\"Combination log saved to: {LOGS_DIR / 'multiblend_combinations.txt'}\")\n",
    "file_references": [
      "*.png",
      "*.png",
      "*.png",
      "{base_filename}_{title_type}_Multiblend_{set_name}_{combo_name}.png",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpre-run/backup_titles",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/multiblend_composites",
      "\n    width, height = img.size\n    result = img.copy()\n    \n    # Vertical weave stripes\n    stripe_width = width // 40\n    for x in range(0, width, stripe_width * 2):\n        if x + stripe_width <= width:\n            # Create a shifted color version for the stripe\n            stripe_region = (x, 0, x + stripe_width, height)\n            stripe = img.crop(stripe_region)\n            \n            # Apply color shift to the stripe\n            r, g, b = stripe.split()\n            stripe = Image.merge(",
      "Creates a quantum-like drift/probability effect",
      "\n    height = img.height\n    band_height = height // 25\n    result = img.copy()\n    \n    # Process in bands with quantum-like drift\n    for i in range(0, height, band_height):\n        y_pos = i\n        height_slice = min(band_height, height - i)\n        \n        # Get the band\n        band = img.crop((0, y_pos, img.width, y_pos + height_slice))\n        \n        # Randomly apply band transformations\n        if random.random() < 0.6:  # 60% chance of transformation\n            rand_effect = random.choice([",
      ":\n            # Pixelation effect\n            small = block.resize((block_width // 4, block_height // 4), Image.NEAREST)\n            pixelated = small.resize((block_width, block_height), Image.NEAREST)\n            result.paste(pixelated, (x_pos, y_pos))\n    \n    # Add scan lines\n    for y in range(0, height, 3):\n        if random.random() < 0.3:  # 30% chance of scan line\n            scan_line = Image.new(",
      "Creates echo/ghost effect",
      ", img.size, 0)\n    center_x, center_y = img.width // 2, img.height // 2\n    max_radius = min(center_x, center_y)\n    \n    # Fill mask with radial gradient\n    for y in range(img.height):\n        for x in range(img.width):\n            # Calculate distance from center\n            distance = ((x - center_x)**2 + (y - center_y)**2)**0.5\n            # Normalize and invert\n            value = max(0, min(255, int(255 * (1 - distance / max_radius))))\n            if x < mask.width and y < mask.height:\n                mask.putpixel((x, y), value)\n    \n    # Create distorted version\n    distorted = img.filter(ImageFilter.FIND_EDGES)\n    distorted = ImageEnhance.Contrast(distorted).enhance(2.0)\n    \n    # Blend using the mask\n    result = Image.composite(distorted, img, mask)\n    return result\n\ndef apply_dream_resonance(img, intensity=0.7):\n    ",
      ":\n            draw.ellipse((x, y, x + size, y + size), fill=255)\n        else:\n            draw.rectangle((x, y, x + size, y + size // 2), fill=255)\n    \n    # Blur the mask for smooth transitions\n    mask = mask.filter(ImageFilter.GaussianBlur(radius=10))\n    \n    # Composite using the mask\n    result = Image.composite(sharp, blurred, mask)\n    \n    # Add slight sepia tone for aged effect\n    sepia_tone = ImageOps.colorize(ImageOps.grayscale(result), ",
      "\n    organized_files = defaultdict(lambda: defaultdict(list))\n    \n    # Process title/visual files from mpre-run/backup_titles\n    for file in MPRE_RUN_TITLES_DIR.glob(",
      "\n    organized_files = organize_files()\n    \n    # Log file for combinations\n    log_file = LOGS_DIR / ",
      ", [])\n            \n            # Process each type of title/concept image\n            for title_type in [",
      "\n                            output_path = OUTPUT_DIR / output_filename\n                            \n                            # Save the composite\n                            composite.save(output_path)\n                            print(f",
      "Combination log saved to: {LOGS_DIR / "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "random",
      "pathlib",
      "PIL",
      "numpy",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/create_experimental_overlays.py",
    "size": 17547,
    "lines": 478,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nfrom PIL import Image, ImageEnhance, ImageFilter, ImageOps, ImageChops\nimport numpy as np\nimport random\n\n# Define paths\nmpost_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey\"\nmidjourney_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]\"\noutput_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/experimental_overlays\"\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Get all image files\nmpost_files = [f for f in os.listdir(mpost_dir) if f.lower().endswith('.png') and not f.startswith('original_')]\nmidjourney_files = [f for f in os.listdir(midjourney_dir) if f.lower().endswith('.png') and not os.path.isdir(os.path.join(midjourney_dir, f))]\n\nprint(f\"Found {len(mpost_files)} mpost files\")\nprint(f\"Found {len(midjourney_files)} midjourney files\")\n\n# Extract ID number from filename\ndef get_id_from_filename(filename):\n    # Pattern to match the ID at the beginning of the filename (e.g., 01_SH, 02_FL)\n    match = re.match(r'^(\\d+)_([A-Z]+)_', filename)\n    if match:\n        return match.group(1)  # Return just the number part\n    return None\n\n# Group files by ID\nmpost_by_id = {}\nmidjourney_by_id = {}\n\nfor file in mpost_files:\n    id_num = get_id_from_filename(file)\n    if id_num:\n        if id_num not in mpost_by_id:\n            mpost_by_id[id_num] = []\n        mpost_by_id[id_num].append(file)\n\nfor file in midjourney_files:\n    id_num = get_id_from_filename(file)\n    if id_num:\n        if id_num not in midjourney_by_id:\n            midjourney_by_id[id_num] = []\n        midjourney_by_id[id_num].append(file)\n\nprint(f\"Grouped into {len(mpost_by_id)} mpost IDs\")\nprint(f\"Grouped into {len(midjourney_by_id)} midjourney IDs\")\n\n# Convert PIL Image to numpy array and back for advanced processing\ndef to_array(img):\n    return np.array(img)\n\ndef to_image(arr):\n    return Image.fromarray(np.uint8(np.clip(arr, 0, 255)))\n\n# 8 Novel Experimental Blending Techniques\n\n# 1. Dark Foreground/Bright Background - Inverted silhouette effect\ndef dark_foreground_bright_background(fg, bg):\n    # Darken foreground dramatically\n    darkened_fg = ImageEnhance.Brightness(fg).enhance(0.3)\n    # Brighten background\n    brightened_bg = ImageEnhance.Brightness(bg).enhance(1.5)\n    # Enhance contrast for both\n    darkened_fg = ImageEnhance.Contrast(darkened_fg).enhance(1.5)\n    brightened_bg = ImageEnhance.Contrast(brightened_bg).enhance(0.8)\n    # Blend with more weight on foreground to preserve its darkness\n    return Image.blend(brightened_bg, darkened_fg, 0.7)\n\n# 2. Frequency Separation - Split and recombine texture and color\ndef frequency_separation(fg, bg):\n    # Convert to arrays for detailed manipulation\n    fg_arr = to_array(fg)\n    bg_arr = to_array(bg)\n    \n    # Create blurred versions (low frequency/color information)\n    fg_blur = to_array(fg.filter(ImageFilter.GaussianBlur(radius=10)))\n    bg_blur = to_array(bg.filter(ImageFilter.GaussianBlur(radius=10)))\n    \n    # Extract high frequency (texture) by subtracting blur from original\n    fg_high = fg_arr - fg_blur + 128  # Add 128 to normalize\n    bg_high = bg_arr - bg_blur + 128\n    \n    # Recombine: background color with foreground texture\n    result = bg_blur + (fg_high - 128) * 0.9\n    \n    return to_image(result)\n\n# 3. Gradient Masked Blend - Custom radial gradient for transition\ndef gradient_masked_blend(fg, bg):\n    width, height = fg.size\n    # Create radial gradient mask\n    mask = Image.new('L', (width, height), 0)\n    center_x, center_y = width // 2, height // 2\n    # Randomize center slightly\n    center_x += random.randint(-width//6, width//6)\n    center_y += random.randint(-height//6, height//6)\n    \n    # Draw gradient\n    for y in range(height):\n        for x in range(width):\n            # Distance from center\n            distance = ((x - center_x) ** 2 + (y - center_y) ** 2) ** 0.5\n            # Max distance (to edge)\n            max_distance = ((max(center_x, width - center_x)) ** 2 + \n                          (max(center_y, height - center_y)) ** 2) ** 0.5\n            # Value based on distance (0-255)\n            value = int(255 * (1 - min(1, distance / max_distance)))\n            mask.putpixel((x, y), value)\n    \n    # Apply the mask to create a composite\n    return Image.composite(fg, bg, mask)\n\n# 4. Selective Color Blending - Mix specific RGB channels\ndef selective_color_blending(fg, bg):\n    # Split into RGB channels\n    fg_r, fg_g, fg_b = fg.split()\n    bg_r, bg_g, bg_b = bg.split()\n    \n    # Create new image with mixed channels\n    # Use red from background, green from mix, blue from foreground\n    new_r = bg_r\n    new_g = Image.blend(fg_g, bg_g, 0.5)\n    new_b = fg_b\n    \n    # Merge channels back\n    return Image.merge(\"RGB\", (new_r, new_g, new_b))\n\n# 5. Texture Mapping - Apply texture from one image to another\ndef texture_mapping(fg, bg):\n    # Extract texture from foreground\n    fg_gray = fg.convert(\"L\")\n    fg_texture = ImageEnhance.Contrast(fg_gray).enhance(2.0)\n    \n    # Create emboss effect for dimensional texture\n    fg_emboss = fg_texture.filter(ImageFilter.EMBOSS)\n    \n    # Apply texture to background\n    bg_arr = to_array(bg)\n    texture_arr = to_array(fg_emboss) / 255.0  # Normalize to 0-1\n    \n    # Adjust texture influence\n    texture_influence = 0.4\n    result = bg_arr * (1 + (texture_arr - 0.5) * texture_influence)\n    \n    return to_image(result)\n\n# 6. Channel Swapping - Rearrange RGB channels in creative ways\ndef channel_swapping(fg, bg):\n    # Split into RGB channels\n    fg_r, fg_g, fg_b = fg.split()\n    bg_r, bg_g, bg_b = bg.split()\n    \n    # Invert some channels for more dramatic effect\n    fg_g_inv = ImageOps.invert(fg_g)\n    bg_r_inv = ImageOps.invert(bg_r)\n    \n    # Create unusual channel combinations\n    new_r = bg_b  # Blue channel from background becomes red\n    new_g = fg_g_inv  # Inverted green from foreground\n    new_b = Image.blend(fg_r, bg_r_inv, 0.6)  # Mix of red from foreground and inverted red from background\n    \n    # Merge channels back\n    return Image.merge(\"RGB\", (new_r, new_g, new_b))\n\n# 7. Bloom and Decay - Dual processing for light and dark areas\ndef bloom_and_decay(fg, bg):\n    # Create high contrast version of foreground\n    fg_high_contrast = ImageEnhance.Contrast(fg).enhance(2.0)\n    \n    # Create bloom effect (glow on bright areas)\n    fg_bloom = fg_high_contrast.filter(ImageFilter.GaussianBlur(radius=15))\n    bloom_arr = to_array(fg_bloom)\n    # Only keep bright areas\n    bloom_threshold = 200\n    bloom_mask = bloom_arr > bloom_threshold\n    bloom_only = bloom_arr * bloom_mask\n    \n    # Create decay effect for dark areas of background\n    bg_decay = ImageEnhance.Contrast(bg).enhance(1.5)\n    bg_decay = ImageEnhance.Brightness(bg_decay).enhance(0.7)\n    decay_arr = to_array(bg_decay)\n    # Only keep dark areas\n    decay_threshold = 50\n    decay_mask = decay_arr < decay_threshold\n    decay_only = decay_arr * decay_mask\n    \n    # Combine original images with bloom and decay effects\n    fg_arr = to_array(fg)\n    bg_arr = to_array(bg)\n    \n    # Blend all components\n    result = (fg_arr * 0.4 + bg_arr * 0.4 + bloom_only * 0.6 + decay_only * 0.3)\n    \n    return to_image(result)\n\n# 8. Spectral Inversion - Invert specific frequency bands\ndef spectral_inversion(fg, bg):\n    # Convert to HSV for better control over color ranges\n    fg_hsv = fg.convert(\"HSV\")\n    bg_hsv = bg.convert(\"HSV\")\n    \n    # Split into H, S, V channels\n    fg_h, fg_s, fg_v = fg_hsv.split()\n    bg_h, bg_s, bg_v = bg_hsv.split()\n    \n    # Invert hue in specific ranges (certain colors)\n    fg_h_arr = to_array(fg_h)\n    bg_h_arr = to_array(bg_h)\n    \n    # Define hue ranges to invert (specific color bands)\n    # Hue values in PIL range from 0-255 (not 0-360)\n    lower_range = 40  # Yellows\n    upper_range = 170  # Cyans\n    \n    # Create masks for the ranges\n    fg_mask = (fg_h_arr > lower_range) & (fg_h_arr < upper_range)\n    bg_mask = (bg_h_arr > lower_range) & (bg_h_arr < upper_range)\n    \n    # Invert hues in the specified ranges\n    fg_h_arr[fg_mask] = (fg_h_arr[fg_mask] + 128) % 256\n    bg_h_arr[bg_mask] = (bg_h_arr[bg_mask] + 128) % 256\n    \n    # Convert back to images\n    new_fg_h = Image.fromarray(np.uint8(fg_h_arr))\n    new_bg_h = Image.fromarray(np.uint8(bg_h_arr))\n    \n    # Create new HSV images\n    new_fg_hsv = Image.merge(\"HSV\", (new_fg_h, fg_s, fg_v))\n    new_bg_hsv = Image.merge(\"HSV\", (new_bg_h, bg_s, bg_v))\n    \n    # Convert back to RGB\n    new_fg = new_fg_hsv.convert(\"RGB\")\n    new_bg = new_bg_hsv.convert(\"RGB\")\n    \n    # Blend the spectrally modified images\n    return Image.blend(new_bg, new_fg, 0.6)\n\n# Advanced blending techniques with descriptive names\noverlay_modes = [\n    (\"DarkForegroundBrightBackground\", dark_foreground_bright_background),\n    (\"FrequencySeparation\", frequency_separation),\n    (\"GradientMaskedBlend\", gradient_masked_blend),\n    (\"SelectiveColorBlending\", selective_color_blending),\n    (\"TextureMapping\", texture_mapping),\n    (\"ChannelSwapping\", channel_swapping),\n    (\"BloomAndDecay\", bloom_and_decay),\n    (\"SpectralInversion\", spectral_inversion)\n]\n\n# Function to create overlay and save to output directory\ndef create_overlay(fg_file, bg_file, output_path, mode_name, mode_func):\n    try:\n        # Open images\n        fg_img = Image.open(os.path.join(mpost_dir, fg_file)).convert(\"RGBA\")\n        bg_img = Image.open(os.path.join(midjourney_dir, bg_file)).convert(\"RGBA\")\n        \n        # Resize foreground to match background dimensions\n        fg_img = fg_img.resize(bg_img.size)\n        \n        # Convert to RGB for blend operations\n        fg_rgb = fg_img.convert(\"RGB\")\n        bg_rgb = bg_img.convert(\"RGB\")\n        \n        # Create overlay using the provided function\n        result = mode_func(fg_rgb, bg_rgb)\n        \n        # Save the result - no labels for clean plates\n        result.save(output_path, quality=95)\n        return True\n    except Exception as e:\n        print(f\"Error processing {fg_file} and {bg_file}: {e}\")\n        return False\n\n# Create overlays for each matching ID\noverlay_count = 0\nfor id_num in sorted(set(list(mpost_by_id.keys()) + list(midjourney_by_id.keys()))):\n    # Skip if either directory doesn't have this ID\n    if id_num not in mpost_by_id or id_num not in midjourney_by_id:\n        print(f\"ID {id_num} not found in both directories, skipping\")\n        continue\n    \n    # Get files for this ID\n    fg_files = mpost_by_id[id_num]\n    bg_files = midjourney_by_id[id_num]\n    \n    print(f\"Processing ID {id_num}: {len(fg_files)} foreground files, {len(bg_files)} background files\")\n    \n    # Create overlays with all combinations\n    for fg_file in fg_files:\n        for bg_file in bg_files:\n            # Create a subdirectory for this ID\n            id_dir = os.path.join(output_dir, f\"{id_num}\")\n            os.makedirs(id_dir, exist_ok=True)\n            \n            # Create overlays with different modes\n            for mode_name, mode_func in overlay_modes:\n                # Create descriptive filename\n                fg_base = os.path.splitext(os.path.basename(fg_file))[0]\n                bg_base = os.path.splitext(os.path.basename(bg_file))[0]\n                output_filename = f\"{fg_base}_{mode_name}_{bg_base}.jpg\"\n                output_path = os.path.join(id_dir, output_filename)\n                \n                # Create and save overlay\n                if create_overlay(fg_file, bg_file, output_path, mode_name, mode_func):\n                    overlay_count += 1\n                    print(f\"Created experimental overlay: {output_filename}\")\n\nprint(f\"\\nCreated {overlay_count} experimental overlay images in {output_dir}\")\n\n# Create HTML file to view the overlays\nhtml_output = os.path.join(output_dir, \"view_experimental_overlays.html\")\nhtml_content = f\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <title>Experimental Overlays</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1, h2, h3 {{ \n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1200px;\n            margin: 0 auto;\n        }}\n        .track {{\n            margin-bottom: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        .overlays {{\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));\n            gap: 20px;\n            margin-top: 20px;\n        }}\n        .overlay {{\n            background-color: #2a2a2a;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n        img {{ \n            max-width: 100%; \n            height: auto;\n            display: block;\n            border: 1px solid #333;\n        }}\n        .info {{\n            margin-top: 10px;\n            font-size: 14px;\n        }}\n        .technique-desc {{\n            margin-top: 30px;\n            padding: 20px;\n            background-color: #2a2a2a;\n            border-radius: 5px;\n        }}\n        .technique {{\n            margin-bottom: 20px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>Experimental Overlay Techniques</h1>\n        <p>This page shows novel and unexpected overlay effects with unconventional blending approaches.</p>\n        \n        <div class=\"technique-desc\">\n            <h2>Novel Techniques Used</h2>\n            \n            <div class=\"technique\">\n                <h3>Dark Foreground/Bright Background</h3>\n                <p>An unusual inversion where the foreground is darkened significantly while the background is brightened, creating a reverse silhouette effect.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Frequency Separation</h3>\n                <p>Borrowed from photo retouching, this technique separates the high-frequency details (texture) from the low-frequency colors, then recombines them in novel ways.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Gradient Masked Blend</h3>\n                <p>Uses a custom radial gradient mask for blending, creating a smooth transition that focuses attention on specific parts of the image.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Selective Color Blending</h3>\n                <p>Splits the RGB channels and selectively blends them (e.g., using red from foreground, blue from background, and a mixed green channel).</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Texture Mapping</h3>\n                <p>Extracts texture information from one image and applies it to the other, creating a sculptural effect where one image's details are embossed onto the other.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Channel Swapping</h3>\n                <p>Completely rearranges the RGB channels between images in unexpected ways (e.g., R channel from background, G channel inverted from foreground, B channel as a mix).</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Bloom and Decay</h3>\n                <p>Creates a dual-processing effect where bright areas \"bloom\" outward while dark areas create a decay-like texture, giving the impression of growth and deterioration simultaneously.</p>\n            </div>\n            \n            <div class=\"technique\">\n                <h3>Spectral Inversion</h3>\n                <p>Inverts specific frequency bands of the images rather than the entire image, creating psychedelic color effects where only certain tones are affected.</p>\n            </div>\n        </div>\n\"\"\"\n\n# Add content for each track ID\nfor id_num in sorted(set(list(mpost_by_id.keys()) + list(midjourney_by_id.keys()))):\n    if id_num not in mpost_by_id or id_num not in midjourney_by_id:\n        continue\n    \n    # Get files for this ID to extract track info\n    fg_files = mpost_by_id[id_num]\n    if not fg_files:\n        continue\n        \n    # Get track name from first file\n    track_parts = fg_files[0].split('_', 3)\n    track_name = track_parts[2] if len(track_parts) > 2 else \"Unknown\"\n    \n    html_content += f\"\"\"\n        <div class=\"track\">\n            <h2>Track {id_num}: {track_name}</h2>\n            <div class=\"overlays\">\n    \"\"\"\n    \n    # Get all overlay images for this ID\n    id_dir = os.path.join(output_dir, f\"{id_num}\")\n    if os.path.exists(id_dir):\n        overlay_files = [f for f in os.listdir(id_dir) if f.lower().endswith(('.jpg', '.png'))]\n        for overlay_file in sorted(overlay_files):\n            # Extract mode from filename\n            mode = \"Unknown\"\n            for technique_name, _ in overlay_modes:\n                if technique_name in overlay_file:\n                    mode = technique_name\n                    break\n                \n            html_content += f\"\"\"\n                <div class=\"overlay\">\n                    <h3>{mode}</h3>\n                    <img src=\"{id_num}/{overlay_file}\" alt=\"{overlay_file}\">\n                    <div class=\"info\">{overlay_file}</div>\n                </div>\n            \"\"\"\n    \n    html_content += \"\"\"\n            </div>\n        </div>\n    \"\"\"\n\nhtml_content += \"\"\"\n    </div>\n</body>\n</html>\n\"\"\"\n\nwith open(html_output, 'w') as f:\n    f.write(html_content)\n\nprint(f\"Created HTML viewer: {html_output}\")\n",
    "file_references": [
      "{fg_base}_{mode_name}_{bg_base}.jpg",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mpost-journey",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/midjourney_session_2025-6-2_[0-19]",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/experimental_overlays",
      ")\n\n# Convert PIL Image to numpy array and back for advanced processing\ndef to_array(img):\n    return np.array(img)\n\ndef to_image(arr):\n    return Image.fromarray(np.uint8(np.clip(arr, 0, 255)))\n\n# 8 Novel Experimental Blending Techniques\n\n# 1. Dark Foreground/Bright Background - Inverted silhouette effect\ndef dark_foreground_bright_background(fg, bg):\n    # Darken foreground dramatically\n    darkened_fg = ImageEnhance.Brightness(fg).enhance(0.3)\n    # Brighten background\n    brightened_bg = ImageEnhance.Brightness(bg).enhance(1.5)\n    # Enhance contrast for both\n    darkened_fg = ImageEnhance.Contrast(darkened_fg).enhance(1.5)\n    brightened_bg = ImageEnhance.Contrast(brightened_bg).enhance(0.8)\n    # Blend with more weight on foreground to preserve its darkness\n    return Image.blend(brightened_bg, darkened_fg, 0.7)\n\n# 2. Frequency Separation - Split and recombine texture and color\ndef frequency_separation(fg, bg):\n    # Convert to arrays for detailed manipulation\n    fg_arr = to_array(fg)\n    bg_arr = to_array(bg)\n    \n    # Create blurred versions (low frequency/color information)\n    fg_blur = to_array(fg.filter(ImageFilter.GaussianBlur(radius=10)))\n    bg_blur = to_array(bg.filter(ImageFilter.GaussianBlur(radius=10)))\n    \n    # Extract high frequency (texture) by subtracting blur from original\n    fg_high = fg_arr - fg_blur + 128  # Add 128 to normalize\n    bg_high = bg_arr - bg_blur + 128\n    \n    # Recombine: background color with foreground texture\n    result = bg_blur + (fg_high - 128) * 0.9\n    \n    return to_image(result)\n\n# 3. Gradient Masked Blend - Custom radial gradient for transition\ndef gradient_masked_blend(fg, bg):\n    width, height = fg.size\n    # Create radial gradient mask\n    mask = Image.new(",
      ", (width, height), 0)\n    center_x, center_y = width // 2, height // 2\n    # Randomize center slightly\n    center_x += random.randint(-width//6, width//6)\n    center_y += random.randint(-height//6, height//6)\n    \n    # Draw gradient\n    for y in range(height):\n        for x in range(width):\n            # Distance from center\n            distance = ((x - center_x) ** 2 + (y - center_y) ** 2) ** 0.5\n            # Max distance (to edge)\n            max_distance = ((max(center_x, width - center_x)) ** 2 + \n                          (max(center_y, height - center_y)) ** 2) ** 0.5\n            # Value based on distance (0-255)\n            value = int(255 * (1 - min(1, distance / max_distance)))\n            mask.putpixel((x, y), value)\n    \n    # Apply the mask to create a composite\n    return Image.composite(fg, bg, mask)\n\n# 4. Selective Color Blending - Mix specific RGB channels\ndef selective_color_blending(fg, bg):\n    # Split into RGB channels\n    fg_r, fg_g, fg_b = fg.split()\n    bg_r, bg_g, bg_b = bg.split()\n    \n    # Create new image with mixed channels\n    # Use red from background, green from mix, blue from foreground\n    new_r = bg_r\n    new_g = Image.blend(fg_g, bg_g, 0.5)\n    new_b = fg_b\n    \n    # Merge channels back\n    return Image.merge(",
      ")\n    fg_texture = ImageEnhance.Contrast(fg_gray).enhance(2.0)\n    \n    # Create emboss effect for dimensional texture\n    fg_emboss = fg_texture.filter(ImageFilter.EMBOSS)\n    \n    # Apply texture to background\n    bg_arr = to_array(bg)\n    texture_arr = to_array(fg_emboss) / 255.0  # Normalize to 0-1\n    \n    # Adjust texture influence\n    texture_influence = 0.4\n    result = bg_arr * (1 + (texture_arr - 0.5) * texture_influence)\n    \n    return to_image(result)\n\n# 6. Channel Swapping - Rearrange RGB channels in creative ways\ndef channel_swapping(fg, bg):\n    # Split into RGB channels\n    fg_r, fg_g, fg_b = fg.split()\n    bg_r, bg_g, bg_b = bg.split()\n    \n    # Invert some channels for more dramatic effect\n    fg_g_inv = ImageOps.invert(fg_g)\n    bg_r_inv = ImageOps.invert(bg_r)\n    \n    # Create unusual channel combinations\n    new_r = bg_b  # Blue channel from background becomes red\n    new_g = fg_g_inv  # Inverted green from foreground\n    new_b = Image.blend(fg_r, bg_r_inv, 0.6)  # Mix of red from foreground and inverted red from background\n    \n    # Merge channels back\n    return Image.merge(",
      "<!DOCTYPE html>\n<html>\n<head>\n    <title>Experimental Overlays</title>\n    <style>\n        body {{ \n            background-color: #121212; \n            color: #f0f0f0; \n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n        }}\n        h1, h2, h3 {{ \n            color: #e0e0e0;\n        }}\n        .container {{\n            max-width: 1200px;\n            margin: 0 auto;\n        }}\n        .track {{\n            margin-bottom: 40px;\n            background-color: #1e1e1e;\n            padding: 20px;\n            border-radius: 5px;\n        }}\n        .overlays {{\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));\n            gap: 20px;\n            margin-top: 20px;\n        }}\n        .overlay {{\n            background-color: #2a2a2a;\n            padding: 10px;\n            border-radius: 5px;\n        }}\n        img {{ \n            max-width: 100%; \n            height: auto;\n            display: block;\n            border: 1px solid #333;\n        }}\n        .info {{\n            margin-top: 10px;\n            font-size: 14px;\n        }}\n        .technique-desc {{\n            margin-top: 30px;\n            padding: 20px;\n            background-color: #2a2a2a;\n            border-radius: 5px;\n        }}\n        .technique {{\n            margin-bottom: 20px;\n        }}\n    </style>\n</head>\n<body>\n    <div class=",
      ">\n        <h1>Experimental Overlay Techniques</h1>\n        <p>This page shows novel and unexpected overlay effects with unconventional blending approaches.</p>\n        \n        <div class=",
      ">\n            <h2>Novel Techniques Used</h2>\n            \n            <div class=",
      ">\n                <h3>Dark Foreground/Bright Background</h3>\n                <p>An unusual inversion where the foreground is darkened significantly while the background is brightened, creating a reverse silhouette effect.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Frequency Separation</h3>\n                <p>Borrowed from photo retouching, this technique separates the high-frequency details (texture) from the low-frequency colors, then recombines them in novel ways.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Gradient Masked Blend</h3>\n                <p>Uses a custom radial gradient mask for blending, creating a smooth transition that focuses attention on specific parts of the image.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Selective Color Blending</h3>\n                <p>Splits the RGB channels and selectively blends them (e.g., using red from foreground, blue from background, and a mixed green channel).</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Texture Mapping</h3>\n                <p>Extracts texture information from one image and applies it to the other, creating a sculptural effect where one image",
      ">\n                <h3>Channel Swapping</h3>\n                <p>Completely rearranges the RGB channels between images in unexpected ways (e.g., R channel from background, G channel inverted from foreground, B channel as a mix).</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Bloom and Decay</h3>\n                <p>Creates a dual-processing effect where bright areas ",
      " outward while dark areas create a decay-like texture, giving the impression of growth and deterioration simultaneously.</p>\n            </div>\n            \n            <div class=",
      ">\n                <h3>Spectral Inversion</h3>\n                <p>Inverts specific frequency bands of the images rather than the entire image, creating psychedelic color effects where only certain tones are affected.</p>\n            </div>\n        </div>\n",
      ">\n            <h2>Track {id_num}: {track_name}</h2>\n            <div class=",
      ">\n                    <h3>{mode}</h3>\n                    <img src=",
      ">{overlay_file}</div>\n                </div>\n            ",
      "\n            </div>\n        </div>\n    ",
      "\n    </div>\n</body>\n</html>\n"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "PIL",
      "numpy",
      "random"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/console_overlay_mockup.py",
    "size": 5884,
    "lines": 153,
    "source": "#!/usr/bin/env python3\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nimport json\nimport textwrap\nimport random\n\n# Create a test frame with overlay\ndef create_console_overlay(output_path, shot_id=\"FL012\", timestamp=\"02:38:34\"):\n    # Create a black canvas (simulating video frame)\n    width, height = 1280, 720\n    image = Image.new('RGB', (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Create the bottom console overlay\n    overlay_height = 160  # Height of bottom overlay\n    overlay_y = height - overlay_height\n    \n    # Draw semi-transparent black background for console\n    for y in range(overlay_y, height):\n        for x in range(width):\n            r, g, b = image.getpixel((x, y))\n            draw.point((x, y), (r//3, g//3, b//3))\n    \n    # Draw console border/frame\n    border_color = (0, 255, 255)  # Cyan\n    line_thickness = 1\n    \n    # Top horizontal line\n    draw.line([(0, overlay_y), (width, overlay_y)], fill=border_color, width=line_thickness)\n    \n    # Internal lines and markers\n    draw.line([(width//3, overlay_y), (width//3, height)], fill=border_color, width=line_thickness)\n    draw.line([(2*width//3, overlay_y), (2*width//3, height)], fill=border_color, width=line_thickness)\n    \n    # Add retro scanline effect\n    for y in range(overlay_y, height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 50), width=1)\n    \n    # Load custom font if available, otherwise use default\n    try:\n        # Try to use VT323 font (monospace retro font)\n        font_large = ImageFont.truetype(\"VT323-Regular.ttf\", 24)\n        font_mid = ImageFont.truetype(\"VT323-Regular.ttf\", 20)\n        font_small = ImageFont.truetype(\"VT323-Regular.ttf\", 16)\n    except IOError:\n        # Fallback to default\n        font_large = ImageFont.load_default()\n        font_mid = ImageFont.load_default()\n        font_small = ImageFont.load_default()\n    \n    # Add text content\n    cyan = (0, 255, 255)\n    green = (80, 255, 80)\n    amber = (255, 191, 0)\n    \n    # Left section: Shot metadata\n    x_left = 20\n    y_text = overlay_y + 15\n    \n    draw.text((x_left, y_text), f\"SHOT: {shot_id} [{timestamp}]\", fill=cyan, font=font_large)\n    y_text += 30\n    draw.text((x_left, y_text), \"TYPE: PERCEPTION-IMAGE\", fill=green, font=font_mid)\n    y_text += 25\n    draw.text((x_left, y_text), \"FUNC: SUBJECTIVE FRAME RECALIBRATION\", fill=green, font=font_mid)\n    y_text += 25\n    draw.text((x_left, y_text), \"PATH: FL/FL012__PI__Subjective_Frame_Recalibration__Stars_ex_d33fed8b.png\", fill=(150, 150, 150), font=font_small)\n    \n    # Middle section: Poem fragment\n    x_mid = width//3 + 20\n    y_text = overlay_y + 15\n    \n    draw.text((x_mid, y_text), \"POEM: FLASHING LIGHTS\", fill=cyan, font=font_large)\n    y_text += 30\n    draw.text((x_mid, y_text), \"FRAGMENT:\", fill=amber, font=font_mid)\n    y_text += 25\n    draw.text((x_mid, y_text), '\"a concussion,\"', fill=amber, font=font_mid)\n    y_text += 25\n    draw.rectangle([(x_mid, y_text), (x_mid + 200, y_text+8)], outline=None, fill=amber)\n    \n    # Right section: Prompt\n    x_right = 2*width//3 + 20\n    y_text = overlay_y + 15\n    \n    draw.text((x_right, y_text), \"PROMPT:\", fill=cyan, font=font_large)\n    y_text += 30\n    \n    prompt_text = \"Stars explode behind eyelids\u2014fireworks seen from inside a skull\"\n    wrapped_prompt = textwrap.wrap(prompt_text, width=35)\n    for line in wrapped_prompt:\n        draw.text((x_right, y_text), line, fill=(255, 255, 255), font=font_mid)\n        y_text += 22\n    \n    # Add some digital noise/glitch effects\n    for _ in range(50):\n        glitch_x = random.randint(0, width-50)\n        glitch_y = random.randint(overlay_y, height-5)\n        glitch_width = random.randint(10, 50)\n        glitch_height = random.randint(1, 3)\n        glitch_color = random.choice([(0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128)])\n        draw.rectangle([(glitch_x, glitch_y), (glitch_x + glitch_width, glitch_y + glitch_height)], \n                      fill=glitch_color)\n    \n    # Add a timeline indicator at the very bottom\n    timeline_y = height - 10\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-4, timeline_y-4, position_x+4, timeline_y+4), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 5 if i % 3 == 0 else 3\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f\"Mockup created and saved to: {output_path}\")\n\ndef main():\n    output_dir = \"/Users/gaia/resurrecting atlantis/JELLYFISH/mockups\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Create console overlay mockup\n    output_path = os.path.join(output_dir, \"console_overlay_mockup.png\")\n    create_console_overlay(output_path)\n    \nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "PATH: FL/FL012__PI__Subjective_Frame_Recalibration__Stars_ex_d33fed8b.png",
      "console_overlay_mockup.png",
      ", (width, height), (0, 0, 0))\n    draw = ImageDraw.Draw(image)\n    \n    # Simulating a video frame with gradient background\n    for y in range(height):\n        r = int(40 + (y / height) * 30)\n        g = int(20 + (y / height) * 20) \n        b = int(60 + (y / height) * 40)\n        for x in range(width):\n            noise = random.randint(-10, 10)\n            draw.point((x, y), (r + noise, g + noise, b + noise))\n    \n    # Add some simulated content (circles to represent subjects)\n    for _ in range(5):\n        x = random.randint(100, width-100)\n        y = random.randint(100, height-300)\n        size = random.randint(30, 100)\n        r = random.randint(150, 255)\n        g = random.randint(100, 200)\n        b = random.randint(150, 255)\n        draw.ellipse((x-size//2, y-size//2, x+size//2, y+size//2), \n                     fill=(r, g, b, 128), outline=(r+20, g+20, b+20))\n    \n    # Create the bottom console overlay\n    overlay_height = 160  # Height of bottom overlay\n    overlay_y = height - overlay_height\n    \n    # Draw semi-transparent black background for console\n    for y in range(overlay_y, height):\n        for x in range(width):\n            r, g, b = image.getpixel((x, y))\n            draw.point((x, y), (r//3, g//3, b//3))\n    \n    # Draw console border/frame\n    border_color = (0, 255, 255)  # Cyan\n    line_thickness = 1\n    \n    # Top horizontal line\n    draw.line([(0, overlay_y), (width, overlay_y)], fill=border_color, width=line_thickness)\n    \n    # Internal lines and markers\n    draw.line([(width//3, overlay_y), (width//3, height)], fill=border_color, width=line_thickness)\n    draw.line([(2*width//3, overlay_y), (2*width//3, height)], fill=border_color, width=line_thickness)\n    \n    # Add retro scanline effect\n    for y in range(overlay_y, height, 2):\n        draw.line([(0, y), (width, y)], fill=(0, 0, 0, 50), width=1)\n    \n    # Load custom font if available, otherwise use default\n    try:\n        # Try to use VT323 font (monospace retro font)\n        font_large = ImageFont.truetype(",
      "PATH: FL/FL012__PI__Subjective_Frame_Recalibration__Stars_ex_d33fed8b.png",
      ", fill=amber, font=font_mid)\n    y_text += 25\n    draw.rectangle([(x_mid, y_text), (x_mid + 200, y_text+8)], outline=None, fill=amber)\n    \n    # Right section: Prompt\n    x_right = 2*width//3 + 20\n    y_text = overlay_y + 15\n    \n    draw.text((x_right, y_text), ",
      "\n    wrapped_prompt = textwrap.wrap(prompt_text, width=35)\n    for line in wrapped_prompt:\n        draw.text((x_right, y_text), line, fill=(255, 255, 255), font=font_mid)\n        y_text += 22\n    \n    # Add some digital noise/glitch effects\n    for _ in range(50):\n        glitch_x = random.randint(0, width-50)\n        glitch_y = random.randint(overlay_y, height-5)\n        glitch_width = random.randint(10, 50)\n        glitch_height = random.randint(1, 3)\n        glitch_color = random.choice([(0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128)])\n        draw.rectangle([(glitch_x, glitch_y), (glitch_x + glitch_width, glitch_y + glitch_height)], \n                      fill=glitch_color)\n    \n    # Add a timeline indicator at the very bottom\n    timeline_y = height - 10\n    draw.line([(20, timeline_y), (width-20, timeline_y)], fill=(100, 100, 100), width=1)\n    \n    # Current position marker (30% through)\n    position_x = 20 + (width-40) * 0.3\n    draw.ellipse((position_x-4, timeline_y-4, position_x+4, timeline_y+4), fill=cyan)\n    \n    # Draw vertical timestamp markers\n    for i in range(10):\n        marker_x = 20 + (width-40) * (i/9)\n        marker_height = 5 if i % 3 == 0 else 3\n        draw.line([(marker_x, timeline_y-marker_height), (marker_x, timeline_y)], fill=(180, 180, 180), width=1)\n    \n    # Save the image\n    image.save(output_path)\n    print(f",
      "/Users/gaia/resurrecting atlantis/JELLYFISH/mockups"
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os",
      "json",
      "textwrap",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/normalize_specific_files.py",
    "size": 3450,
    "lines": 87,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\n# Directory containing files to normalize\nIMPALA_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA')\n\n# Create backup directory\nBACKUP_DIR = IMPALA_DIR / 'original_files_backup'\nBACKUP_DIR.mkdir(exist_ok=True)\n\n# Direct mapping for specific files from the screenshot\nDIRECT_MAPPING = {\n    \"701609451, 08_NS_NewlySingle_15, M 5.mp4\": \"08_NS_NewlySingle_151700\",\n    \"748139157, 07_DJ_DJTurnMeUp_130, M 5.mp4\": \"07_DJ_DJTurnMeUp_130600\",\n    \"1456777203, Camera MovementFi, M 5.mp4\": \"01_SH_OutOfLife_000000\",\n    \"1471594611, 10_MR_MagicRide_1939, M 5.mp4\": \"10_MR_MagicRide_193900\",\n    \"1961290024, 02_FL_FlashingLights, M 5.mp4\": \"02_FL_FlashingLights_021100\",\n    \"2007704123, 01_SH_OutofLife_0000, M 5.mp4\": \"01_SH_OutOfLife_000000\",\n    \"2033050426, image-prompt, M 5.mp4\": \"05_BE_Bloodline_084400\",  # Assumption based on pattern\n    \"2101217636, 01_SH_OutofLife_0000, M 5.mp4\": \"01_SH_OutOfLife_000000\",\n    \"2424344039, 02_FL_FlashingLights, M 5.mp4\": \"02_FL_FlashingLights_021100\",\n    \"2966551071, CinePrompt \\\"NEVERM, image-prompt, M 5.mp4\": \"04_NM_Nevermore_063300\",\n    \"3167155023, image-prompt, M 5.mp4\": \"13_HW_HowToWinMyHeart_240100\",  # Assumption based on pattern\n    \"3281121752, 02_FL_FlashingLights, M 5.mp4\": \"02_FL_FlashingLights_021100\",\n    \"3304542369, 02_FL_FlashingLights, M 5.mp4\": \"02_FL_FlashingLights_021100\",\n    \"3751283682, RESURRECTING ATLANT, M 5.mp4\": \"06_AT_ResurrectingAtlantis_105500\",\n    \"3912673341, image-prompt, M 5.mp4\": \"14_HM_HotMinute_261200\"  # Assumption based on pattern\n}\n\ndef normalize_specific_files():\n    print(\"Starting to normalize specific files in IMPALA directory...\")\n    \n    # Track statistics\n    total_files = 0\n    renamed_files = 0\n    skipped_files = 0\n    \n    # Process each specific file\n    for file_name, track_id in DIRECT_MAPPING.items():\n        file_path = IMPALA_DIR / file_name\n        \n        # Skip if file doesn't exist\n        if not file_path.exists():\n            print(f\"File not found: {file_name}\")\n            skipped_files += 1\n            continue\n            \n        total_files += 1\n        \n        # Check if the filename already starts with the track ID\n        if file_path.name.startswith(track_id):\n            print(f\"File already normalized: {file_path.name}\")\n            skipped_files += 1\n            continue\n        \n        # Create the new normalized filename\n        new_filename = f\"{track_id}_{file_path.name}\"\n        new_file_path = file_path.parent / new_filename\n        \n        # Check if new filename already exists\n        if new_file_path.exists():\n            print(f\"Skipping (destination exists): {file_path.name}\")\n            skipped_files += 1\n            continue\n        \n        # Create backup\n        backup_path = BACKUP_DIR / file_path.name\n        print(f\"Backing up: {file_path.name} -> {backup_path.name}\")\n        shutil.copy2(file_path, backup_path)\n        \n        # Rename the file\n        print(f\"Renaming: {file_path.name} -> {new_filename}\")\n        os.rename(file_path, new_file_path)\n        renamed_files += 1\n    \n    print(\"\\nNormalization Complete!\")\n    print(f\"Total files processed: {total_files}\")\n    print(f\"Files renamed: {renamed_files}\")\n    print(f\"Files skipped: {skipped_files}\")\n    print(f\"Original files backed up to: {BACKUP_DIR}\")\n\nif __name__ == \"__main__\":\n    normalize_specific_files()\n",
    "file_references": [
      "701609451, 08_NS_NewlySingle_15, M 5.mp4",
      "748139157, 07_DJ_DJTurnMeUp_130, M 5.mp4",
      "1456777203, Camera MovementFi, M 5.mp4",
      "1471594611, 10_MR_MagicRide_1939, M 5.mp4",
      "1961290024, 02_FL_FlashingLights, M 5.mp4",
      "2007704123, 01_SH_OutofLife_0000, M 5.mp4",
      "2033050426, image-prompt, M 5.mp4",
      "2101217636, 01_SH_OutofLife_0000, M 5.mp4",
      "2424344039, 02_FL_FlashingLights, M 5.mp4",
      "NEVERM, image-prompt, M 5.mp4",
      "3167155023, image-prompt, M 5.mp4",
      "3281121752, 02_FL_FlashingLights, M 5.mp4",
      "3304542369, 02_FL_FlashingLights, M 5.mp4",
      "3751283682, RESURRECTING ATLANT, M 5.mp4",
      "3912673341, image-prompt, M 5.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA",
      ")\n    \n    # Track statistics\n    total_files = 0\n    renamed_files = 0\n    skipped_files = 0\n    \n    # Process each specific file\n    for file_name, track_id in DIRECT_MAPPING.items():\n        file_path = IMPALA_DIR / file_name\n        \n        # Skip if file doesn",
      "\n        new_file_path = file_path.parent / new_filename\n        \n        # Check if new filename already exists\n        if new_file_path.exists():\n            print(f",
      ")\n            skipped_files += 1\n            continue\n        \n        # Create backup\n        backup_path = BACKUP_DIR / file_path.name\n        print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/create_multiple_impala_assemblies.py",
    "size": 9029,
    "lines": 223,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nimport random\nfrom pathlib import Path\nimport re\n\n# Directory containing video files\nIMPALA_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Track order specified\nTRACK_ORDER = [\n    \"01_SH_OutOfLife_000000\",\n    \"02_FL_FlashingLights_021100\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\",\n    \"04_NM_Nevermore_063300\",\n    \"05_BE_Bloodline_084400\",\n    \"06_AT_ResurrectingAtlantis_105500\",\n    \"07_DJ_DJTurnMeUp_130600\",\n    \"08_NS_NewlySingle_151700\",\n    \"09_YH_YetHeard_172800\",\n    \"10_MR_MagicRide_193900\",\n    \"12_RU_Reunion_215000\",\n    \"13_HW_HowToWinMyHeart_240100\",\n    \"14_HM_HotMinute_261200\"\n]\n\n# Track title mapping for better output naming\nTRACK_TITLE_MAPPING = {\n    \"01_SH_OutOfLife_000000\": \"OutOfLife\",\n    \"02_FL_FlashingLights_021100\": \"FlashingLights\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\": \"HowToBreakOffAnEngagement\",\n    \"04_NM_Nevermore_063300\": \"Nevermore\",\n    \"05_BE_Bloodline_084400\": \"Bloodline\",\n    \"06_AT_ResurrectingAtlantis_105500\": \"ResurrectingAtlantis\",\n    \"07_DJ_DJTurnMeUp_130600\": \"DJTurnMeUp\",\n    \"08_NS_NewlySingle_151700\": \"NewlySingle\",\n    \"09_YH_YetHeard_172800\": \"YetHeard\",\n    \"10_MR_MagicRide_193900\": \"MagicRide\",\n    \"12_RU_Reunion_215000\": \"Reunion\",\n    \"13_HW_HowToWinMyHeart_240100\": \"HowToWinMyHeart\",\n    \"14_HM_HotMinute_261200\": \"HotMinute\"\n}\n\ndef get_video_duration(video_path):\n    \"\"\"Get the duration of a video file using FFprobe.\"\"\"\n    try:\n        result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        duration = float(data['format']['duration'])\n        return duration\n    except (subprocess.CalledProcessError, KeyError, json.JSONDecodeError) as e:\n        print(f\"Error getting duration for {video_path}: {e}\")\n        return 0\n\ndef find_video_files():\n    \"\"\"Find all video files in the IMPALA directory.\"\"\"\n    video_files = []\n    for file in IMPALA_DIR.glob('**/*.mp4'):\n        if ('original_files_backup' not in str(file) and \n            'final_sequence' not in str(file) and \n            'images' not in str(file)):\n            video_files.append(file)\n    return video_files\n\ndef group_videos_by_track(video_files):\n    \"\"\"Group video files by track ID.\"\"\"\n    track_videos = {track_id: [] for track_id in TRACK_ORDER}\n    \n    for video_file in video_files:\n        for track_id in TRACK_ORDER:\n            if video_file.name.startswith(track_id):\n                track_videos[track_id].append(video_file)\n                break\n    \n    return track_videos\n\ndef create_concatenation_file(videos, concat_file_path):\n    \"\"\"Create a concatenation file for FFmpeg.\"\"\"\n    with open(concat_file_path, 'w') as f:\n        for video in videos:\n            # Escape single quotes in the path\n            escaped_path = str(video).replace(\"'\", \"'\\\\''\")\n            f.write(f\"file '{escaped_path}'\\n\")\n\ndef create_assembly(assembly_number, selected_videos):\n    \"\"\"Create a single video assembly from the selected videos.\"\"\"\n    # Create a concat file for FFmpeg\n    concat_file = OUTPUT_DIR / f\"concat_list_{assembly_number}.txt\"\n    print(f\"Creating concatenation file at {concat_file}...\")\n    create_concatenation_file(selected_videos, concat_file)\n    \n    # Calculate total duration\n    total_duration = sum(get_video_duration(video) for video in selected_videos)\n    \n    # Output filename\n    output_file = OUTPUT_DIR / f\"ResurrectingAtlantis_Assembly_{assembly_number}.mp4\"\n    \n    # Use FFmpeg to concatenate the videos\n    print(f\"Concatenating videos into {output_file}...\")\n    print(f\"Estimated duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n    \n    try:\n        result = subprocess.run([\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file),\n            '-c', 'copy',\n            '-y',  # Overwrite output file if it exists\n            str(output_file)\n        ], check=True, capture_output=True)\n        \n        print(f\"Assembly {assembly_number} completed successfully!\")\n        print(f\"Output file: {output_file}\")\n        \n        # Create a text file with the list of videos used\n        video_list_file = OUTPUT_DIR / f\"assembly_{assembly_number}_details.txt\"\n        with open(video_list_file, 'w') as f:\n            f.write(f\"RESURRECTING ATLANTIS - ASSEMBLY {assembly_number}\\n\")\n            f.write(\"=\" * (35 + len(str(assembly_number))) + \"\\n\\n\")\n            f.write(f\"Total videos used: {len(selected_videos)}\\n\")\n            f.write(f\"Total duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\\n\")\n            f.write(f\"Output file: {output_file}\\n\\n\")\n            f.write(\"TRACK SEQUENCE:\\n\")\n            \n            for i, video in enumerate(selected_videos, 1):\n                # Identify which track this video belongs to\n                track_id = next((tid for tid in TRACK_ORDER if video.name.startswith(tid)), \"Unknown\")\n                track_name = TRACK_TITLE_MAPPING.get(track_id, track_id)\n                duration = get_video_duration(video)\n                \n                # Write video details\n                f.write(f\"{i}. {track_id} - {track_name}\\n\")\n                f.write(f\"   File: {video.name}\\n\")\n                f.write(f\"   Duration: {duration:.2f} seconds\\n\\n\")\n                \n        print(f\"Assembly {assembly_number} details saved to {video_list_file}\")\n        return True\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"Error during video concatenation for Assembly {assembly_number}:\")\n        print(f\"FFmpeg stdout: {e.stdout.decode()}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode()}\")\n        return False\n\ndef create_multiple_assemblies():\n    \"\"\"Create multiple video assemblies until all videos are used.\"\"\"\n    print(\"Finding all video files...\")\n    all_videos = find_video_files()\n    print(f\"Found {len(all_videos)} video files.\")\n    \n    print(\"Grouping videos by track...\")\n    track_videos = group_videos_by_track(all_videos)\n    \n    # Create a set to track which videos have been used\n    used_videos = set()\n    assembly_number = 1\n    \n    # Create a summary file to track all assemblies\n    summary_file = OUTPUT_DIR / \"all_assemblies_summary.txt\"\n    with open(summary_file, 'w') as f:\n        f.write(\"RESURRECTING ATLANTIS - ALL ASSEMBLIES SUMMARY\\n\")\n        f.write(\"===========================================\\n\\n\")\n        f.write(f\"Total videos available: {len(all_videos)}\\n\\n\")\n    \n    # Keep creating assemblies until all videos are used\n    while len(used_videos) < len(all_videos):\n        print(f\"\\n=== Creating Assembly #{assembly_number} ===\")\n        print(f\"Used videos so far: {len(used_videos)} out of {len(all_videos)}\")\n        \n        # Select videos for this assembly\n        selected_videos = []\n        \n        for track_id in TRACK_ORDER:\n            # Get all videos for this track that haven't been used yet\n            available_videos = [v for v in track_videos[track_id] if str(v) not in used_videos]\n            \n            if available_videos:\n                # Select a random video from the available ones\n                selected_video = random.choice(available_videos)\n                selected_videos.append(selected_video)\n                used_videos.add(str(selected_video))\n                print(f\"Track {track_id}: Selected '{selected_video.name}'\")\n            else:\n                # If all videos for this track have been used, skip this track\n                print(f\"Track {track_id}: No unused videos available - skipping in this assembly\")\n        \n        if not selected_videos:\n            print(\"No unused videos available for any track - ending assembly creation\")\n            break\n        \n        # Create this assembly\n        success = create_assembly(assembly_number, selected_videos)\n        \n        if success:\n            # Update the summary file\n            with open(summary_file, 'a') as f:\n                f.write(f\"Assembly #{assembly_number}:\\n\")\n                f.write(f\"- Videos used: {len(selected_videos)}\\n\")\n                f.write(f\"- Tracks included: {', '.join([track_id for track_id in TRACK_ORDER if any(v.name.startswith(track_id) for v in selected_videos)])}\\n\")\n                f.write(f\"- Output file: ResurrectingAtlantis_Assembly_{assembly_number}.mp4\\n\\n\")\n            \n            assembly_number += 1\n        \n    print(\"\\n=== All Assemblies Complete ===\")\n    print(f\"Total videos used: {len(used_videos)} out of {len(all_videos)}\")\n    print(f\"Total assemblies created: {assembly_number - 1}\")\n    print(f\"Summary saved to: {summary_file}\")\n\nif __name__ == \"__main__\":\n    create_multiple_assemblies()\n",
    "file_references": [
      "**/*.mp4",
      "ResurrectingAtlantis_Assembly_{assembly_number}.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence",
      "**/*.mp4",
      "\n    # Create a concat file for FFmpeg\n    concat_file = OUTPUT_DIR / f",
      ")\n    create_concatenation_file(selected_videos, concat_file)\n    \n    # Calculate total duration\n    total_duration = sum(get_video_duration(video) for video in selected_videos)\n    \n    # Output filename\n    output_file = OUTPUT_DIR / f",
      "Estimated duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)",
      ")\n        \n        # Create a text file with the list of videos used\n        video_list_file = OUTPUT_DIR / f",
      "Total duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\\n",
      ")\n    track_videos = group_videos_by_track(all_videos)\n    \n    # Create a set to track which videos have been used\n    used_videos = set()\n    assembly_number = 1\n    \n    # Create a summary file to track all assemblies\n    summary_file = OUTPUT_DIR / "
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "random",
      "pathlib",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/add_music_to_ibex.py",
    "size": 8219,
    "lines": 208,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\n# Source directories and files\nVIDEO_FILE = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ResurrectingAtlantis_IBEX_Normalized.mp4')\nAUDIO_DIR = Path('/Users/gaia/resurrecting atlantis/MANTA/audio')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ibex_music_versions')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\ndef get_video_duration(video_path):\n    \"\"\"Get the duration of a video file using FFprobe.\"\"\"\n    try:\n        result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        duration = float(data['format']['duration'])\n        return duration\n    except (subprocess.CalledProcessError, KeyError, json.JSONDecodeError) as e:\n        print(f\"Error getting duration for {video_path}: {e}\")\n        return 0\n\ndef find_audio_files():\n    \"\"\"Find all audio files in the MANTA/audio directory.\"\"\"\n    audio_extensions = ['.mp3', '.wav', '.m4a', '.aac', '.ogg']\n    audio_files = []\n    \n    for ext in audio_extensions:\n        audio_files.extend(list(AUDIO_DIR.glob(f'*{ext}')))\n    \n    return audio_files\n\ndef create_video_with_audio(video_path, audio_path, output_path, video_duration):\n    \"\"\"Create a new video with the given audio track, maintaining the original video duration.\"\"\"\n    try:\n        # Create a temporary directory for intermediate files\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir_path = Path(temp_dir)\n            \n            # Process audio file (trim or loop to match video duration)\n            audio_processed_path = temp_dir_path / f\"processed_{audio_path.name}\"\n            \n            # Get audio duration\n            audio_info_result = subprocess.run([\n                'ffprobe', \n                '-v', 'error', \n                '-show_entries', 'format=duration', \n                '-of', 'json', \n                str(audio_path)\n            ], capture_output=True, text=True, check=True)\n            \n            audio_info = json.loads(audio_info_result.stdout)\n            audio_duration = float(audio_info['format']['duration'])\n            \n            print(f\"  Audio duration: {audio_duration:.2f}s, Video duration: {video_duration:.2f}s\")\n            \n            # Process audio based on relative durations\n            if abs(audio_duration - video_duration) < 1.0:\n                # If durations are close, just trim exactly to video length\n                subprocess.run([\n                    'ffmpeg',\n                    '-i', str(audio_path),\n                    '-t', str(video_duration),\n                    '-y',\n                    str(audio_processed_path)\n                ], check=True, capture_output=True)\n                \n            elif audio_duration > video_duration:\n                # If audio is longer, trim it\n                print(\"  Trimming audio to match video length\")\n                subprocess.run([\n                    'ffmpeg',\n                    '-i', str(audio_path),\n                    '-t', str(video_duration),\n                    '-y',\n                    str(audio_processed_path)\n                ], check=True, capture_output=True)\n                \n            else:\n                # If audio is shorter, loop it to fill the video duration\n                print(\"  Looping audio to match video length\")\n                \n                # Calculate how many loops we need\n                loops_needed = int(video_duration / audio_duration) + 1\n                \n                # Create a file with the input repeated\n                concat_file = temp_dir_path / \"concat.txt\"\n                with open(concat_file, 'w') as f:\n                    for _ in range(loops_needed):\n                        f.write(f\"file '{audio_path}'\\n\")\n                \n                # Concatenate the audio files\n                looped_audio = temp_dir_path / \"looped_audio.mp3\"\n                subprocess.run([\n                    'ffmpeg',\n                    '-f', 'concat',\n                    '-safe', '0',\n                    '-i', str(concat_file),\n                    '-c', 'copy',\n                    '-y',\n                    str(looped_audio)\n                ], check=True, capture_output=True)\n                \n                # Trim the looped audio to the exact video duration\n                subprocess.run([\n                    'ffmpeg',\n                    '-i', str(looped_audio),\n                    '-t', str(video_duration),\n                    '-y',\n                    str(audio_processed_path)\n                ], check=True, capture_output=True)\n            \n            # Combine video with processed audio\n            subprocess.run([\n                'ffmpeg',\n                '-i', str(video_path),\n                '-i', str(audio_processed_path),\n                '-c:v', 'copy',\n                '-c:a', 'aac',\n                '-map', '0:v:0',\n                '-map', '1:a:0',\n                '-shortest',\n                '-y',\n                str(output_path)\n            ], check=True, capture_output=True)\n            \n            # Verify the output file\n            output_duration = get_video_duration(output_path)\n            print(f\"  Output duration: {output_duration:.2f}s\")\n            \n            if abs(output_duration - video_duration) > 1.0:\n                print(f\"  Warning: Output duration ({output_duration:.2f}s) differs from target ({video_duration:.2f}s)\")\n            \n            return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error creating video with audio: {e}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode() if hasattr(e, 'stderr') else 'No stderr available'}\")\n        return False\n\ndef main():\n    print(\"=== ADDING MUSIC TO IBEX VIDEOS ===\\n\")\n    \n    # Get video duration\n    video_duration = get_video_duration(VIDEO_FILE)\n    print(f\"Source video: {VIDEO_FILE}\")\n    print(f\"Video duration: {video_duration:.2f} seconds\")\n    \n    if video_duration == 0:\n        print(\"Error: Could not determine video duration.\")\n        return\n    \n    # Find all audio files\n    audio_files = find_audio_files()\n    print(f\"Found {len(audio_files)} audio files in {AUDIO_DIR}\\n\")\n    \n    if not audio_files:\n        print(\"No audio files found.\")\n        return\n    \n    # Create a new video for each audio file\n    successful_videos = []\n    \n    for i, audio_file in enumerate(audio_files, 1):\n        # Create a clean filename for the output\n        audio_name = audio_file.stem.replace(' ', '_')\n        output_file = OUTPUT_DIR / f\"IBEX_Music_{audio_name}.mp4\"\n        \n        print(f\"\\n{i}/{len(audio_files)} Processing: {audio_file.name}\")\n        print(f\"Creating: {output_file}\")\n        \n        if create_video_with_audio(VIDEO_FILE, audio_file, output_file, video_duration):\n            print(f\"\u2713 Successfully created {output_file}\")\n            successful_videos.append(output_file)\n        else:\n            print(f\"\u2717 Failed to create {output_file}\")\n    \n    # Create a summary file\n    summary_file = OUTPUT_DIR / \"ibex_music_versions_summary.txt\"\n    with open(summary_file, 'w') as f:\n        f.write(\"RESURRECTING ATLANTIS - IBEX MUSIC VERSIONS\\n\")\n        f.write(\"=========================================\\n\\n\")\n        f.write(f\"Original video: {VIDEO_FILE}\\n\")\n        f.write(f\"Video duration: {video_duration:.2f} seconds\\n\")\n        f.write(f\"Total music versions created: {len(successful_videos)}/{len(audio_files)}\\n\\n\")\n        \n        f.write(\"MUSIC VERSIONS:\\n\")\n        for i, video in enumerate(successful_videos, 1):\n            output_duration = get_video_duration(video)\n            f.write(f\"{i}. {video.name}\\n\")\n            f.write(f\"   Duration: {output_duration:.2f} seconds\\n\\n\")\n    \n    print(f\"\\nCreated {len(successful_videos)}/{len(audio_files)} IBEX music versions.\")\n    print(f\"Output directory: {OUTPUT_DIR}\")\n    print(f\"Summary: {summary_file}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ResurrectingAtlantis_IBEX_Normalized.mp4",
      "looped_audio.mp3",
      "IBEX_Music_{audio_name}.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ResurrectingAtlantis_IBEX_Normalized.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence/ibex_music_versions",
      "Find all audio files in the MANTA/audio directory.",
      "\n    try:\n        # Create a temporary directory for intermediate files\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir_path = Path(temp_dir)\n            \n            # Process audio file (trim or loop to match video duration)\n            audio_processed_path = temp_dir_path / f",
      ")\n                \n                # Calculate how many loops we need\n                loops_needed = int(video_duration / audio_duration) + 1\n                \n                # Create a file with the input repeated\n                concat_file = temp_dir_path / ",
      ")\n                \n                # Concatenate the audio files\n                looped_audio = temp_dir_path / ",
      ")\n        output_file = OUTPUT_DIR / f",
      "\\n{i}/{len(audio_files)} Processing: {audio_file.name}",
      ")\n    \n    # Create a summary file\n    summary_file = OUTPUT_DIR / ",
      "Total music versions created: {len(successful_videos)}/{len(audio_files)}\\n\\n",
      "\\nCreated {len(successful_videos)}/{len(audio_files)} IBEX music versions."
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n                'ffprobe', \n                '-v', 'error', \n                '-show_entries', 'format=duration', \n                '-of', 'json', \n                str(audio_path"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-i', str(audio_path"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-i', str(audio_path"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-f', 'concat',\n                    '-safe', '0',\n                    '-i', str(concat_file"
      },
      {
        "type": "run",
        "snippet": "[\n                    'ffmpeg',\n                    '-i', str(looped_audio"
      },
      {
        "type": "run",
        "snippet": "[\n                'ffmpeg',\n                '-i', str(video_path"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "pathlib",
      "tempfile",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "JELLYFISH/move_impala_images.py",
    "size": 1407,
    "lines": 43,
    "source": "#!/usr/bin/env python3\nimport os\nimport shutil\nfrom pathlib import Path\n\n# Directory containing files\nIMPALA_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA')\nIMAGES_DIR = IMPALA_DIR / 'images'\nIMAGES_DIR.mkdir(exist_ok=True)\n\ndef move_image_files():\n    \"\"\"Move all image files to a separate folder.\"\"\"\n    print(\"Moving image files to separate folder...\")\n    \n    # Track statistics\n    total_files = 0\n    moved_files = 0\n    \n    # Common image extensions\n    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp']\n    \n    # Process each file in the directory\n    for file_path in IMPALA_DIR.glob('*'):\n        if file_path.is_file() and 'original_files_backup' not in str(file_path) and 'final_sequence' not in str(file_path):\n            total_files += 1\n            \n            # Check if it's an image file\n            if file_path.suffix.lower() in image_extensions:\n                # Create the destination path\n                dest_path = IMAGES_DIR / file_path.name\n                \n                # Move the file\n                print(f\"Moving: {file_path.name} -> {dest_path}\")\n                shutil.move(file_path, dest_path)\n                moved_files += 1\n    \n    print(\"\\nImage Move Complete!\")\n    print(f\"Total files checked: {total_files}\")\n    print(f\"Images moved to {IMAGES_DIR}: {moved_files}\")\n\nif __name__ == \"__main__\":\n    move_image_files()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/IMPALA",
      "s an image file\n            if file_path.suffix.lower() in image_extensions:\n                # Create the destination path\n                dest_path = IMAGES_DIR / file_path.name\n                \n                # Move the file\n                print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "shutil",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/sh_complete_codex_generator.py",
    "size": 3860,
    "lines": 115,
    "source": "#!/usr/bin/env python3\n\"\"\"\nSH Complete Codex Generator\n\nThis script generates properly formatted Codex entries for the SH (Out of Life) poem section\nby combining data from:\n1. The Simplified sequence file (for all image variants with timestamps)\n2. The SH assembly JSON (for metadata)\n3. The SH prompts markdown (for prompt text)\n\nOutput is a markdown file ready to be added to the Codex.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport sys\n\n# Paths\nTIGER_DIR = \"/Users/gaia/resurrecting atlantis/TIGER\"\nSH_DIR = os.path.join(TIGER_DIR, \"SH\")\nSIMPLIFIED_PATH = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\nASSEMBLY_PATH = os.path.join(SH_DIR, \"SH_assembly.json\")\nPROMPTS_PATH = os.path.join(SH_DIR, \"SH_prompts.md\")\nOUTPUT_PATH = os.path.join(TIGER_DIR, \"SH_complete_codex_entries.md\")\n\ndef load_assembly_data():\n    \"\"\"Load the SH assembly JSON data.\"\"\"\n    with open(ASSEMBLY_PATH, 'r') as f:\n        return {item[\"id\"]: item for item in json.load(f)}\n\ndef load_prompts_data():\n    \"\"\"Load the SH prompts markdown data.\"\"\"\n    with open(PROMPTS_PATH, 'r') as f:\n        prompts_text = f.read()\n    \n    # Extract prompts with shot IDs\n    prompts = {}\n    for line in prompts_text.splitlines():\n        line = line.strip()\n        if not line or line.startswith('#'):\n            continue\n            \n        # Match SH shot IDs at the beginning of lines\n        match = re.match(r'(SH\\d+)\\s*\u00b7\\s*(.*)', line)\n        if match:\n            shot_id, prompt_text = match.groups()\n            prompts[shot_id] = prompt_text\n            \n    return prompts\n\ndef extract_sh_entries_from_simplified():\n    \"\"\"Extract all SH entries with timestamps from the Simplified markdown file.\"\"\"\n    with open(SIMPLIFIED_PATH, 'r') as f:\n        simplified_text = f.read()\n    \n    # Extract SH entries with timestamps and image paths\n    pattern = r'(SH\\d+)\\s+\\[([^\\]]+)\\]\\s+`([^`]+)`'\n    matches = re.findall(pattern, simplified_text)\n    \n    entries = []\n    for shot_id, timestamp, image_path in matches:\n        entries.append({\n            \"shot_id\": shot_id,\n            \"timestamp\": timestamp,\n            \"image_path\": image_path\n        })\n    \n    return entries\n\ndef generate_codex_entries():\n    \"\"\"Generate formatted codex entries for all SH images.\"\"\"\n    assembly_data = load_assembly_data()\n    prompts_data = load_prompts_data()\n    simplified_entries = extract_sh_entries_from_simplified()\n    \n    # Start output with a header\n    output_text = \"# SH (Out of Life) Codex Entries - Complete Sequence\\n\\n\"\n    \n    # Generate formatted entries for each image\n    for entry in simplified_entries:\n        shot_id = entry[\"shot_id\"]\n        timestamp = entry[\"timestamp\"]\n        image_path = entry[\"image_path\"]\n        \n        # Skip if we don't have assembly data for this shot ID\n        if shot_id not in assembly_data:\n            print(f\"Warning: No assembly data found for {shot_id}\")\n            continue\n            \n        # Skip if we don't have prompt data for this shot ID\n        if shot_id not in prompts_data:\n            print(f\"Warning: No prompt data found for {shot_id}\")\n            continue\n        \n        # Format the codex entry\n        entry_text = f\"### {shot_id} [{timestamp}]\\n\\n\"\n        entry_text += f\"**Image:** `{image_path}`\\n\\n\"\n        entry_text += \"**Assembly Source:**\\n```json\\n\"\n        entry_text += json.dumps(assembly_data[shot_id], indent=2)\n        entry_text += \"\\n```\\n\\n\"\n        entry_text += f\"**Prompt:** {prompts_data[shot_id]}\\n\\n---\\n\\n\"\n        \n        output_text += entry_text\n    \n    # Write to output file\n    with open(OUTPUT_PATH, 'w') as f:\n        f.write(output_text)\n    \n    print(f\"Generated complete codex entries written to {OUTPUT_PATH}\")\n    print(f\"Total entries: {len(simplified_entries)}\")\n\nif __name__ == \"__main__\":\n    generate_codex_entries()\n",
    "file_references": [
      "SH_assembly.json",
      "/Users/gaia/resurrecting atlantis/TIGER"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "json",
      "sys"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "SH Complete Codex Generator\n\nThis script generates properly formatted Codex entries for the SH (Out of Life) poem section\nby combining data from:\n1. The Simplified sequence file (for all image variants with timestamps)\n2. The SH assembly JSON (for metadata)\n3. The SH prompts markdown (for prompt text)\n\nOutput is a markdown file ready to be added to the Codex."
  },
  {
    "path": "JELLYFISH/normalize_impala_remaining.py",
    "size": 5792,
    "lines": 125,
    "source": "#!/usr/bin/env python3\nimport os\nimport re\nimport shutil\nfrom pathlib import Path\n\n# Directory containing files to normalize\nIMPALA_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA')\n\n# Create backup directory\nBACKUP_DIR = IMPALA_DIR / 'original_files_backup'\nBACKUP_DIR.mkdir(exist_ok=True)\n\n# Track ID mapping with regex patterns\nTRACK_ID_MAPPING = {\n    # Track IDs with variations in spacing and casing\n    r'01[\\s_-]*SH[\\s_-]*(?:OutOfLife|OutofLife|Out[\\s_-]*of[\\s_-]*Life)[\\s_-]*0+': '01_SH_OutOfLife_000000',\n    r'02[\\s_-]*FL[\\s_-]*(?:FlashingLights|Flashing[\\s_-]*Lights)[\\s_-]*021100': '02_FL_FlashingLights_021100',\n    r'03[\\s_-]*HT[\\s_-]*(?:HowToBreakOffAnEngagement|HowtoBreakOffanEngagement|How[\\s_-]*to[\\s_-]*Break[\\s_-]*Off[\\s_-]*an[\\s_-]*Engagement)[\\s_-]*042200': '03_HT_HowToBreakOffAnEngagement_042200',\n    r'04[\\s_-]*NM[\\s_-]*(?:Nevermore)[\\s_-]*063300': '04_NM_Nevermore_063300',\n    r'05[\\s_-]*BE[\\s_-]*(?:Bloodline)[\\s_-]*084400': '05_BE_Bloodline_084400',\n    r'06[\\s_-]*AT[\\s_-]*(?:ResurrectingAtlantis|Resurrecting[\\s_-]*Atlantis)[\\s_-]*105500': '06_AT_ResurrectingAtlantis_105500',\n    r'07[\\s_-]*DJ[\\s_-]*(?:DJTurnMeUp|DJ[\\s_-]*Turn[\\s_-]*Me[\\s_-]*Up)[\\s_-]*130600': '07_DJ_DJTurnMeUp_130600',\n    r'08[\\s_-]*NS[\\s_-]*(?:NewlySingle|Newly[\\s_-]*Single)[\\s_-]*151700': '08_NS_NewlySingle_151700',\n    r'09[\\s_-]*YH[\\s_-]*(?:YetHeard|Yet[\\s_-]*Heard)[\\s_-]*172800': '09_YH_YetHeard_172800',\n    r'10[\\s_-]*MR[\\s_-]*(?:MagicRide|Magic[\\s_-]*Ride)[\\s_-]*193900': '10_MR_MagicRide_193900',\n    r'12[\\s_-]*RU[\\s_-]*(?:Reunion)[\\s_-]*215000': '12_RU_Reunion_215000',\n    r'13[\\s_-]*HW[\\s_-]*(?:HowToWinMyHeart|HowtoWinMyHeart|How[\\s_-]*to[\\s_-]*Win[\\s_-]*My[\\s_-]*Heart)[\\s_-]*240100': '13_HW_HowToWinMyHeart_240100',\n    r'14[\\s_-]*HM[\\s_-]*(?:HotMinute|Hot[\\s_-]*Minute)[\\s_-]*261200': '14_HM_HotMinute_261200',\n    \n    # Track name matches (for files with just the track name)\n    r'(?:^|[\\s,_-])NEVERM': '04_NM_Nevermore_063300',\n    r'(?:^|[\\s,_-])RESURRECTING[\\s_-]*ATLANT': '06_AT_ResurrectingAtlantis_105500',\n    r'(?:^|[\\s,_-])Camera[\\s_-]*Movement': '01_SH_OutOfLife_000000',  # Assuming this is related to Out of Life\n}\n\n# Function to extract track ID from filename\ndef extract_track_id(filename):\n    # Extract the part after the comma in number pattern\n    if re.match(r'^\\d+,\\s*', filename):\n        # Get the content after the number and comma\n        post_comma = re.sub(r'^\\d+,\\s*', '', filename)\n        \n        # Check for track IDs in the post-comma content\n        for pattern, track_id in TRACK_ID_MAPPING.items():\n            if re.search(pattern, post_comma, re.IGNORECASE):\n                return track_id\n                \n    # For filenames without a pattern match but containing known phrases\n    for pattern, track_id in TRACK_ID_MAPPING.items():\n        if re.search(pattern, filename, re.IGNORECASE):\n            return track_id\n    \n    return None\n\n# Function to normalize a filename\ndef normalize_filename(filename, track_id):\n    # Check if the file already has the correct track ID at the beginning\n    if filename.startswith(track_id):\n        return filename\n    \n    # Remove the leading number and comma pattern\n    clean_name = re.sub(r'^\\d+,\\s*', '', filename)\n    \n    # Create the new filename\n    return f\"{track_id}_{clean_name}\"\n\n# Main function to process all files\ndef normalize_impala_filenames():\n    print(\"Starting to normalize remaining files in IMPALA directory...\")\n    \n    # Track statistics\n    total_files = 0\n    renamed_files = 0\n    skipped_files = 0\n    \n    # Process each file in the directory that matches the number pattern\n    for file_path in IMPALA_DIR.glob('*'):\n        if file_path.is_file() and str(file_path).find('original_files_backup') == -1:\n            # Only focus on files with the number and comma pattern\n            if re.match(r'^\\d+,', file_path.name):\n                total_files += 1\n                \n                # Extract track ID if present\n                track_id = extract_track_id(file_path.name)\n                \n                if track_id:\n                    # Check if the filename already starts with the track ID\n                    if file_path.name.startswith(track_id):\n                        print(f\"File already normalized: {file_path.name}\")\n                        skipped_files += 1\n                        continue\n                    \n                    # Create the new normalized filename\n                    new_filename = normalize_filename(file_path.name, track_id)\n                    new_file_path = file_path.parent / new_filename\n                    \n                    # Check if new filename already exists\n                    if new_file_path.exists():\n                        print(f\"Skipping (destination exists): {file_path.name}\")\n                        skipped_files += 1\n                        continue\n                    \n                    # Create backup\n                    backup_path = BACKUP_DIR / file_path.name\n                    print(f\"Backing up: {file_path.name} -> {backup_path.name}\")\n                    shutil.copy2(file_path, backup_path)\n                    \n                    # Rename the file\n                    print(f\"Renaming: {file_path.name} -> {new_filename}\")\n                    os.rename(file_path, new_file_path)\n                    renamed_files += 1\n                else:\n                    print(f\"No track ID found in: {file_path.name}\")\n                    skipped_files += 1\n    \n    print(\"\\nNormalization Complete!\")\n    print(f\"Total files processed: {total_files}\")\n    print(f\"Files renamed: {renamed_files}\")\n    print(f\"Files skipped: {skipped_files}\")\n    print(f\"Original files backed up to: {BACKUP_DIR}\")\n\nif __name__ == \"__main__\":\n    normalize_impala_filenames()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/IMPALA",
      ")\n                        skipped_files += 1\n                        continue\n                    \n                    # Create the new normalized filename\n                    new_filename = normalize_filename(file_path.name, track_id)\n                    new_file_path = file_path.parent / new_filename\n                    \n                    # Check if new filename already exists\n                    if new_file_path.exists():\n                        print(f",
      ")\n                        skipped_files += 1\n                        continue\n                    \n                    # Create backup\n                    backup_path = BACKUP_DIR / file_path.name\n                    print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "shutil",
      "pathlib"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "JELLYFISH/assemble_impala_sequence_improved.py",
    "size": 7522,
    "lines": 199,
    "source": "#!/usr/bin/env python3\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport re\n\n# Directory containing video files\nIMPALA_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA')\nOUTPUT_DIR = Path('/Users/gaia/resurrecting atlantis/IMPALA/final_sequence')\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Track order specified\nTRACK_ORDER = [\n    \"01_SH_OutOfLife_000000\",\n    \"02_FL_FlashingLights_021100\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\",\n    \"04_NM_Nevermore_063300\",\n    \"05_BE_Bloodline_084400\",\n    \"06_AT_ResurrectingAtlantis_105500\",\n    \"07_DJ_DJTurnMeUp_130600\",\n    \"08_NS_NewlySingle_151700\",\n    \"09_YH_YetHeard_172800\",\n    \"10_MR_MagicRide_193900\",\n    \"12_RU_Reunion_215000\",\n    \"13_HW_HowToWinMyHeart_240100\",\n    \"14_HM_HotMinute_261200\"\n]\n\n# Track title mapping for better output naming\nTRACK_TITLE_MAPPING = {\n    \"01_SH_OutOfLife_000000\": \"OutOfLife\",\n    \"02_FL_FlashingLights_021100\": \"FlashingLights\",\n    \"03_HT_HowToBreakOffAnEngagement_042200\": \"HowToBreakOffAnEngagement\",\n    \"04_NM_Nevermore_063300\": \"Nevermore\",\n    \"05_BE_Bloodline_084400\": \"Bloodline\",\n    \"06_AT_ResurrectingAtlantis_105500\": \"ResurrectingAtlantis\",\n    \"07_DJ_DJTurnMeUp_130600\": \"DJTurnMeUp\",\n    \"08_NS_NewlySingle_151700\": \"NewlySingle\",\n    \"09_YH_YetHeard_172800\": \"YetHeard\",\n    \"10_MR_MagicRide_193900\": \"MagicRide\",\n    \"12_RU_Reunion_215000\": \"Reunion\",\n    \"13_HW_HowToWinMyHeart_240100\": \"HowToWinMyHeart\",\n    \"14_HM_HotMinute_261200\": \"HotMinute\"\n}\n\ndef get_video_duration(video_path):\n    \"\"\"Get the duration of a video file using FFprobe.\"\"\"\n    try:\n        result = subprocess.run([\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path)\n        ], capture_output=True, text=True, check=True)\n        \n        data = json.loads(result.stdout)\n        duration = float(data['format']['duration'])\n        return duration\n    except (subprocess.CalledProcessError, KeyError, json.JSONDecodeError) as e:\n        print(f\"Error getting duration for {video_path}: {e}\")\n        return 0\n\ndef select_best_video_for_track(videos):\n    \"\"\"Select the best representative video for each track.\n    Prioritize videos that are around 5 seconds in duration.\"\"\"\n    if not videos:\n        return None\n    \n    # Get durations for all videos\n    video_durations = [(video, get_video_duration(video)) for video in videos]\n    \n    # Sort by how close the duration is to 5 seconds\n    video_durations.sort(key=lambda x: abs(x[1] - 5.0))\n    \n    # Return the video with duration closest to 5 seconds\n    return video_durations[0][0]\n\ndef find_video_files():\n    \"\"\"Find all video files in the IMPALA directory.\"\"\"\n    video_files = []\n    for file in IMPALA_DIR.glob('**/*.mp4'):\n        if ('original_files_backup' not in str(file) and \n            'final_sequence' not in str(file) and \n            'images' not in str(file)):\n            video_files.append(file)\n    return video_files\n\ndef group_videos_by_track(video_files):\n    \"\"\"Group video files by track ID.\"\"\"\n    track_videos = {track_id: [] for track_id in TRACK_ORDER}\n    \n    for video_file in video_files:\n        for track_id in TRACK_ORDER:\n            if video_file.name.startswith(track_id):\n                track_videos[track_id].append(video_file)\n                break\n    \n    return track_videos\n\ndef create_concatenation_file(videos, concat_file_path):\n    \"\"\"Create a concatenation file for FFmpeg.\"\"\"\n    with open(concat_file_path, 'w') as f:\n        for video in videos:\n            # Escape single quotes in the path\n            escaped_path = str(video).replace(\"'\", \"'\\\\''\")\n            f.write(f\"file '{escaped_path}'\\n\")\n\ndef assemble_videos():\n    \"\"\"Assemble one video per track in sequence according to track order.\"\"\"\n    print(\"Finding video files...\")\n    all_videos = find_video_files()\n    print(f\"Found {len(all_videos)} video files.\")\n    \n    print(\"Grouping videos by track...\")\n    track_videos = group_videos_by_track(all_videos)\n    \n    # Select one representative video per track\n    selected_videos = []\n    total_duration = 0\n    \n    for track_id in TRACK_ORDER:\n        videos = track_videos[track_id]\n        if videos:\n            # Select the best video for this track\n            selected_video = select_best_video_for_track(videos)\n            if selected_video:\n                duration = get_video_duration(selected_video)\n                total_duration += duration\n                print(f\"Track {track_id}: Selected '{selected_video.name}' (Duration: {duration:.2f}s)\")\n                selected_videos.append(selected_video)\n            else:\n                print(f\"Warning: Could not select a video for track {track_id}\")\n        else:\n            print(f\"Warning: No videos found for track {track_id}\")\n    \n    if not selected_videos:\n        print(\"Error: No videos selected for any track.\")\n        return\n    \n    # Create a concat file for FFmpeg\n    concat_file = OUTPUT_DIR / \"concat_list.txt\"\n    print(f\"Creating concatenation file at {concat_file}...\")\n    create_concatenation_file(selected_videos, concat_file)\n    \n    # Output filename\n    output_file = OUTPUT_DIR / \"ResurrectingAtlantis_OnePerTrack.mp4\"\n    \n    # Use FFmpeg to concatenate the videos\n    print(f\"Concatenating videos into {output_file}...\")\n    print(f\"Estimated final duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\")\n    \n    try:\n        result = subprocess.run([\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file),\n            '-c', 'copy',\n            '-y',  # Overwrite output file if it exists\n            str(output_file)\n        ], check=True, capture_output=True)\n        \n        print(\"Video concatenation completed successfully!\")\n        print(f\"Output file: {output_file}\")\n        \n        # Create a text file with the list of videos used\n        video_list_file = OUTPUT_DIR / \"video_sequence_details.txt\"\n        with open(video_list_file, 'w') as f:\n            f.write(\"RESURRECTING ATLANTIS - ONE VIDEO PER TRACK SEQUENCE\\n\")\n            f.write(\"=================================================\\n\\n\")\n            f.write(f\"Total videos used: {len(selected_videos)}\\n\")\n            f.write(f\"Total duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\\n\")\n            f.write(f\"Output file: {output_file}\\n\\n\")\n            f.write(\"TRACK SEQUENCE:\\n\")\n            \n            for i, video in enumerate(selected_videos, 1):\n                # Identify which track this video belongs to\n                track_id = next((tid for tid in TRACK_ORDER if video.name.startswith(tid)), \"Unknown\")\n                track_name = TRACK_TITLE_MAPPING.get(track_id, track_id)\n                duration = get_video_duration(video)\n                \n                # Write video details\n                f.write(f\"{i}. {track_id} - {track_name}\\n\")\n                f.write(f\"   File: {video.name}\\n\")\n                f.write(f\"   Duration: {duration:.2f} seconds\\n\\n\")\n                \n        print(f\"Video sequence details saved to {video_list_file}\")\n        \n    except subprocess.CalledProcessError as e:\n        print(\"Error during video concatenation:\")\n        print(f\"FFmpeg stdout: {e.stdout.decode()}\")\n        print(f\"FFmpeg stderr: {e.stderr.decode()}\")\n        print(\"Make sure FFmpeg is installed and all videos are valid.\")\n\nif __name__ == \"__main__\":\n    assemble_videos()\n",
    "file_references": [
      "**/*.mp4",
      "ResurrectingAtlantis_OnePerTrack.mp4",
      "/Users/gaia/resurrecting atlantis/IMPALA",
      "/Users/gaia/resurrecting atlantis/IMPALA/final_sequence",
      "**/*.mp4",
      ")\n        return\n    \n    # Create a concat file for FFmpeg\n    concat_file = OUTPUT_DIR / ",
      ")\n    create_concatenation_file(selected_videos, concat_file)\n    \n    # Output filename\n    output_file = OUTPUT_DIR / ",
      "Estimated final duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)",
      ")\n        \n        # Create a text file with the list of videos used\n        video_list_file = OUTPUT_DIR / ",
      "Total duration: {total_duration:.2f} seconds ({total_duration/60:.2f} minutes)\\n"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n            'ffprobe', \n            '-v', 'error', \n            '-show_entries', 'format=duration', \n            '-of', 'json', \n            str(video_path"
      },
      {
        "type": "run",
        "snippet": "[\n            'ffmpeg',\n            '-f', 'concat',\n            '-safe', '0',\n            '-i', str(concat_file"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "pathlib",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "LIZARD/fix_audio_sync.py",
    "size": 7647,
    "lines": 276,
    "source": "import os\nimport subprocess\nimport time\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_SYNC\")\n\n# Create temp directory\nif not os.path.exists(TEMP_DIR):\n    os.makedirs(TEMP_DIR)\n    print(f\"Created temporary directory: {TEMP_DIR}\")\n\n# Get best voice track\ndef get_best_voice_file():\n    voice_options = [\n        \"ExecutiveVoice\",\n        \"UltraCrispDefinition\",\n        \"BroadcastPerfection\"\n    ]\n    \n    for voice in voice_options:\n        voice_path = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{voice}.mp4\")\n        if os.path.exists(voice_path):\n            print(f\"Using voice track: {voice}\")\n            return voice_path\n    \n    # Fallback to original\n    fallback = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    if os.path.exists(fallback):\n        print(\"Using original assembly as fallback\")\n        return fallback\n    \n    return None\n\n# 1. Extract the audio track\ndef extract_audio(input_video):\n    audio_file = os.path.join(TEMP_DIR, \"extracted_audio.wav\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-i\", input_video,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\",\n        audio_file\n    ]\n    \n    print(\"Extracting audio...\")\n    subprocess.run(cmd)\n    \n    return audio_file if os.path.exists(audio_file) else None\n\n# 2. Create simple title with text\ndef create_title():\n    title_file = os.path.join(TEMP_DIR, \"title.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=10\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n               \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=80:\"\n               \"x=(w-text_w)/2:y=(h-text_h)/2:\"\n               \"enable='between(t,0,10)'\",\n        \"-c:v\", \"libx264\",\n        \"-preset\", \"medium\",\n        \"-y\",\n        title_file\n    ]\n    \n    print(\"Creating title...\")\n    subprocess.run(cmd)\n    \n    return title_file if os.path.exists(title_file) else None\n\n# 3. Create simple credits\ndef create_credits():\n    credits_file = os.path.join(TEMP_DIR, \"credits.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=10\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n               \"text='WHERE YOU GO WHEN YOU LEAVE\\\\n\\\\nA Multimedia Poetry Project\\\\n\\\\n\u00a9 2025 RESURRECTING ATLANTIS':\"\n               \"fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\",\n        \"-preset\", \"medium\",\n        \"-y\",\n        credits_file\n    ]\n    \n    print(\"Creating credits...\")\n    subprocess.run(cmd)\n    \n    return credits_file if os.path.exists(credits_file) else None\n\n# 4. Create audio silence for buffer\ndef create_silence(duration, filename):\n    silence_file = os.path.join(TEMP_DIR, filename)\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anullsrc=r=44100:cl=stereo:d={duration}\",\n        \"-y\",\n        silence_file\n    ]\n    \n    print(f\"Creating {duration}s silence...\")\n    subprocess.run(cmd)\n    \n    return silence_file if os.path.exists(silence_file) else None\n\n# 5. Extract video only\ndef extract_video_only(input_video):\n    video_file = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-i\", input_video,\n        \"-c:v\", \"copy\",\n        \"-an\",\n        \"-y\",\n        video_file\n    ]\n    \n    print(\"Extracting video stream only...\")\n    subprocess.run(cmd)\n    \n    return video_file if os.path.exists(video_file) else None\n\n# 6. Fix audio timing\ndef fix_audio_timing(audio_file):\n    # Shift audio 2 seconds earlier\n    fixed_audio = os.path.join(TEMP_DIR, \"fixed_audio.wav\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-i\", audio_file,\n        \"-filter_complex\", \"adelay=0|0,atrim=start=0\",\n        \"-y\",\n        fixed_audio\n    ]\n    \n    print(\"Fixing audio timing...\")\n    subprocess.run(cmd)\n    \n    return fixed_audio if os.path.exists(fixed_audio) else None\n\n# 7. Create combined title/video/credits concatenation\ndef concatenate_videos(title_file, video_file, credits_file):\n    concat_list = os.path.join(TEMP_DIR, \"concat_list.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    concat_file = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\",\n        concat_file\n    ]\n    \n    print(\"Concatenating videos...\")\n    subprocess.run(cmd)\n    \n    return concat_file if os.path.exists(concat_file) else None\n\n# 8. Add audio starting from 8s mark of title\ndef add_audio_with_offset(video_file, audio_file):\n    # Create 8s silence to start the audio at the 8s mark\n    silence_start = create_silence(8, \"silence_start.wav\")\n    silence_end = create_silence(10, \"silence_end.wav\") # 10s for credits\n    \n    # Concatenate silence + audio + silence\n    audio_with_silence = os.path.join(TEMP_DIR, \"audio_with_silence.wav\")\n    \n    concat_cmd = [\n        \"ffmpeg\",\n        \"-i\", silence_start,\n        \"-i\", audio_file,\n        \"-i\", silence_end,\n        \"-filter_complex\", \"concat=n=3:v=0:a=1\",\n        \"-y\",\n        audio_with_silence\n    ]\n    \n    print(\"Adding silence padding to audio...\")\n    subprocess.run(concat_cmd)\n    \n    # Add the audio to the video\n    final_output = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_Final_Synced.mp4\")\n    \n    add_audio_cmd = [\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_with_silence,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-y\",\n        final_output\n    ]\n    \n    print(\"Adding audio to video...\")\n    subprocess.run(add_audio_cmd)\n    \n    return final_output if os.path.exists(final_output) else None\n\n# Main function\ndef main():\n    print(\"Starting audio sync fix process...\")\n    \n    # Get best voice file\n    input_video = get_best_voice_file()\n    if not input_video:\n        print(\"Error: Could not find input video\")\n        return\n    \n    # Step 1: Extract audio\n    audio_file = extract_audio(input_video)\n    if not audio_file:\n        print(\"Error extracting audio\")\n        return\n    \n    # Step 2: Create title\n    title_file = create_title()\n    if not title_file:\n        print(\"Error creating title\")\n        return\n    \n    # Step 3: Create credits\n    credits_file = create_credits()\n    if not credits_file:\n        print(\"Error creating credits\")\n        return\n    \n    # Step 4: Extract video only\n    video_only = extract_video_only(input_video)\n    if not video_only:\n        print(\"Error extracting video only\")\n        return\n    \n    # Step 5: Concatenate videos\n    concat_file = concatenate_videos(title_file, video_only, credits_file)\n    if not concat_file:\n        print(\"Error concatenating videos\")\n        return\n    \n    # Step 6: Add audio with offset\n    final_output = add_audio_with_offset(concat_file, audio_file)\n    if not final_output:\n        print(\"Error adding audio with offset\")\n        return\n    \n    print(f\"\\nSuccess! Final video created: {final_output}\")\n    print(\"- 10-second title sequence\")\n    print(\"- Voice audio starts 2 seconds earlier (at 8-second mark)\")\n    print(\"- 10-second end credits\")\n    print(\"- Added buffer audio at beginning and end\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_{voice}.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "extracted_audio.wav",
      "title.mp4",
      "credits.mp4",
      "video_only.mp4",
      "fixed_audio.wav",
      "concatenated.mp4",
      "silence_start.wav",
      "silence_end.wav",
      "audio_with_silence.wav",
      "WhereYouGoWhenYouLeave_Final_Synced.mp4",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2",
      ")\n    subprocess.run(cmd)\n    \n    return fixed_audio if os.path.exists(fixed_audio) else None\n\n# 7. Create combined title/video/credits concatenation\ndef concatenate_videos(title_file, video_file, credits_file):\n    concat_list = os.path.join(TEMP_DIR, "
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "concat_cmd"
      },
      {
        "type": "run",
        "snippet": "add_audio_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "time"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/simple_stretched_final.py",
    "size": 14013,
    "lines": 420,
    "source": "import os\nimport subprocess\nimport time\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_SIMPLE_STRETCH\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_STRETCHED_FINAL.mp4\")\n\n# Timing constants (in seconds)\nINTRO_LENGTH = 7\nOUTRO_LENGTH = 8\nVOICE_LENGTH = 110  # 1:50\nTOTAL_LENGTH = 115  # 1:55\nVOICE_DELAY = 3     # Voice starts 3 seconds into the ambient\n\n# Ensure temp directory exists\nos.makedirs(TEMP_DIR, exist_ok=True)\nprint(f\"Using directory: {TEMP_DIR}\")\n\n# Find source files\ndef find_source_files():\n    # Find best voice track\n    voice_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_BroadcastPerfection.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ConfidentPresence.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_CrystalClarity.mp4\")\n    ]\n    \n    for file in voice_files:\n        if os.path.exists(file):\n            print(f\"Using voice track: {os.path.basename(file)}\")\n            voice_file = file\n            break\n    else:\n        print(\"No suitable voice track found\")\n        return None, None\n    \n    # Find original video with native audio\n    original_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    ]\n    \n    for file in original_files:\n        if os.path.exists(file):\n            print(f\"Using original video: {os.path.basename(file)}\")\n            original_file = file\n            break\n    else:\n        print(\"No original video found, using voice track as video source\")\n        original_file = voice_file\n    \n    return voice_file, original_file\n\n# Create intro and outro videos\ndef create_intro_outro():\n    # Create intro - 7 seconds\n    intro_file = os.path.join(TEMP_DIR, \"intro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60,\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHEN YOU LEAVE':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+60\"\n        ),\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", intro_file\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro\")\n    \n    # Create outro - 8 seconds\n    outro_file = os.path.join(TEMP_DIR, \"outro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=60:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60,\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='A Multimedia Poetry Project':fontcolor=white:fontsize=40:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+30,\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=30:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+100\"\n        ),\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", outro_file\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro\")\n    \n    return intro_file, outro_file\n\n# Create audio effects\ndef create_audio_effects():\n    # Intro audio\n    intro_audio = os.path.join(TEMP_DIR, \"intro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.3\",\n        \"-y\", intro_audio\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro audio\")\n    \n    # Outro audio\n    outro_audio = os.path.join(TEMP_DIR, \"outro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.25\",\n        \"-y\", outro_audio\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro audio\")\n    \n    # Ambient audio for whole track\n    ambient_audio = os.path.join(TEMP_DIR, \"ambient.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=pink:duration={TOTAL_LENGTH}:amplitude=0.02\",\n        \"-af\", \"volume=0.15\",\n        \"-y\", ambient_audio\n    ])\n    print(f\"Created {TOTAL_LENGTH}-second ambient audio\")\n    \n    return intro_audio, outro_audio, ambient_audio\n\n# Extract and process audio\ndef process_audio(voice_file, original_file):\n    # Extract voice audio\n    voice_audio = os.path.join(TEMP_DIR, \"voice_original.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ])\n    print(\"Extracted voice audio\")\n    \n    # Get duration\n    cmd_duration = [\n        \"ffprobe\", \"-v\", \"error\",\n        \"-show_entries\", \"format=duration\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        voice_audio\n    ]\n    voice_duration = float(subprocess.run(cmd_duration, capture_output=True, text=True).stdout.strip())\n    print(f\"Original voice duration: {voice_duration:.2f} seconds\")\n    \n    # Stretch voice to 1:50 (110 seconds)\n    stretch_factor = voice_duration / VOICE_LENGTH\n    print(f\"Stretching voice by factor: {stretch_factor:.4f} to reach {VOICE_LENGTH} seconds\")\n    \n    stretched_voice = os.path.join(TEMP_DIR, \"voice_stretched.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_audio,\n        \"-filter:a\", f\"atempo={stretch_factor}\",\n        \"-y\", stretched_voice\n    ])\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ])\n    print(\"Extracted native audio\")\n    \n    return stretched_voice, native_audio\n\n# Extract video without audio\ndef extract_video(video_file):\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", video_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ])\n    print(\"Extracted video without audio\")\n    \n    return video_only\n\n# Concatenate videos\ndef concatenate_videos(intro_file, video_file, outro_file):\n    concat_list = os.path.join(TEMP_DIR, \"concat.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    print(\"Concatenated videos (intro + main + outro)\")\n    \n    return concat_video\n\n# Create a streamlined audio mix\ndef create_final_audio_mix(stretched_voice, native_audio, intro_audio, outro_audio, ambient_audio):\n    # 1. Create silence for the 3-second delay (Using aevalsrc instead of anullsrc)\n    silence_file = os.path.join(TEMP_DIR, \"silence_3s.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=3\",\n        \"-y\", silence_file\n    ])\n    print(\"Created 3-second silence\")\n    \n    # 2. Create voice with delay\n    voice_delayed = os.path.join(TEMP_DIR, \"voice_delayed.wav\")\n    voice_concat = os.path.join(TEMP_DIR, \"voice_concat.txt\")\n    with open(voice_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(silence_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(stretched_voice)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", voice_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", voice_delayed\n    ])\n    print(f\"Added {VOICE_DELAY}-second delay to voice\")\n    \n    # 3. Apply same delay to native audio\n    native_delayed = os.path.join(TEMP_DIR, \"native_delayed.wav\")\n    native_concat = os.path.join(TEMP_DIR, \"native_concat.txt\")\n    with open(native_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(silence_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(native_audio)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", native_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", native_delayed\n    ])\n    print(f\"Added {VOICE_DELAY}-second delay to native audio\")\n    \n    # 4. Create intro audio with padding\n    intro_padded = os.path.join(TEMP_DIR, \"intro_padded.wav\")\n    \n    # Create a silent file to pad the intro audio\n    long_silence = os.path.join(TEMP_DIR, \"long_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=110\",\n        \"-y\", long_silence\n    ])\n    \n    # Concatenate intro with silence\n    intro_concat = os.path.join(TEMP_DIR, \"intro_concat.txt\")\n    with open(intro_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_audio)}'\\n\")\n        f.write(f\"file '{os.path.abspath(long_silence)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", intro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", intro_padded\n    ])\n    print(\"Created padded intro audio\")\n    \n    # 5. Create outro audio with padding\n    outro_padded = os.path.join(TEMP_DIR, \"outro_padded.wav\")\n    \n    # Create silence before outro\n    pre_outro = os.path.join(TEMP_DIR, \"pre_outro.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=107\",\n        \"-y\", pre_outro\n    ])\n    \n    # Concatenate silence with outro\n    outro_concat = os.path.join(TEMP_DIR, \"outro_concat.txt\")\n    with open(outro_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(pre_outro)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_audio)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", outro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", outro_padded\n    ])\n    print(\"Created padded outro audio\")\n    \n    # 6. Mix all audio together\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Mix using simpler filter chain for compatibility\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_delayed,         # Voice with delay\n        \"-i\", native_delayed,        # Native audio with delay\n        \"-i\", ambient_audio,         # Ambient background\n        \"-i\", intro_padded,          # Intro effects\n        \"-i\", outro_padded,          # Outro effects\n        \"-filter_complex\",\n        \"[0:0]volume=0.9[voice];\" +\n        \"[1:0]volume=0.8[native];\" +\n        \"[2:0]volume=0.2[ambient];\" +\n        \"[3:0]volume=0.25[intro];\" +\n        \"[4:0]volume=0.25[outro];\" +\n        \"[voice][native]amix=inputs=2:duration=first[content];\" +\n        \"[ambient][intro]amix=inputs=2:duration=first[amb_intro];\" +\n        \"[amb_intro][outro]amix=inputs=2:duration=first[effects];\" +\n        \"[content][effects]amix=inputs=2:duration=first[out]\",\n        \"-map\", \"[out]\",\n        \"-y\", final_mix\n    ])\n    print(\"Created final audio mix\")\n    \n    return final_mix\n\n# Add audio to video\ndef add_audio_to_video(video_file, audio_file):\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Final version with stretched voice and delayed timing\",\n        \"-y\", OUTPUT_FILE\n    ])\n    print(f\"Added audio to video: {OUTPUT_FILE}\")\n    \n    return os.path.exists(OUTPUT_FILE)\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING FINAL VERSION WITH STRETCHED VOICE AND ADJUSTED TIMING =====\\n\")\n    \n    # Find source files\n    voice_file, original_file = find_source_files()\n    if not voice_file or not original_file:\n        print(\"Error: Missing source files\")\n        return\n    \n    # Create intro and outro videos\n    intro_file, outro_file = create_intro_outro()\n    \n    # Create audio effects\n    intro_audio, outro_audio, ambient_audio = create_audio_effects()\n    \n    # Process audio files\n    stretched_voice, native_audio = process_audio(voice_file, original_file)\n    \n    # Extract video without audio\n    video_only = extract_video(original_file)\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Create final audio mix\n    final_mix = create_final_audio_mix(\n        stretched_voice, native_audio, intro_audio, outro_audio, ambient_audio\n    )\n    \n    # Add audio to video\n    success = add_audio_to_video(concat_video, final_mix)\n    \n    if success:\n        print(\"\\n===== SUCCESS! =====\")\n        print(f\"Final video created: {OUTPUT_FILE}\")\n        print(\"\\nNew timing structure:\")\n        print(f\"- Total length: {TOTAL_LENGTH} seconds (1:55)\")\n        print(f\"- Intro: {INTRO_LENGTH} seconds\")\n        print(f\"- Main content: 100 seconds (1:40)\")\n        print(f\"- Outro: {OUTRO_LENGTH} seconds\")\n        print(f\"- Voice track: Stretched to {VOICE_LENGTH} seconds (1:50)\")\n        print(f\"- Voice and native audio start: After {VOICE_DELAY} seconds of ambiance\")\n        return True\n    else:\n        print(\"Error: Failed to create final video\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_STRETCHED_FINAL.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4",
      "WhereYouGoWhenYouLeave_BroadcastPerfection.mp4",
      "WhereYouGoWhenYouLeave_ConfidentPresence.mp4",
      "WhereYouGoWhenYouLeave_CrystalClarity.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "intro.mp4",
      "outro.mp4",
      "intro_audio.wav",
      "outro_audio.wav",
      "ambient.wav",
      "voice_original.wav",
      "voice_stretched.wav",
      "native_audio.wav",
      "video_only.mp4",
      "concatenated.mp4",
      "silence_3s.wav",
      "voice_delayed.wav",
      "native_delayed.wav",
      "intro_padded.wav",
      "long_silence.wav",
      "outro_padded.wav",
      "pre_outro.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60,",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+60",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60,",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+30,",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+100",
      ")\n    \n    # Stretch voice to 1:50 (110 seconds)\n    stretch_factor = voice_duration / VOICE_LENGTH\n    print(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n    "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n    "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.3\",\n        \"-"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.25\",\n      "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=pink:duration={TOTAL_LENGTH}:amplitude=0.02\",\n        \"-af\", \"volume=0.15\",\n        \"-y\", ambient_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "cmd_duration, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_audio,\n        \"-filter:a\", f\"atempo={stretch_factor}\",\n        \"-y\", stretched_voice\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", video_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=3\",\n        \"-y\", silence_file\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", voice_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", voice_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", native_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", native_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=110\",\n        \"-y\", long_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", intro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", intro_padded\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=107\",\n        \"-y\", pre_outro\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", outro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", outro_padded\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_delayed,         # Voice with delay\n        \"-i\", native_delayed,        # Native audio with delay\n        \"-i\", ambient_audio,         # Ambient background\n   "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-m"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "time"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/create_final_simple.py",
    "size": 12588,
    "lines": 392,
    "source": "import os\nimport subprocess\nimport json\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_SIMPLE\")\nASSETS_DIR = os.path.join(LIZARD_DIR, \"ASSETS\")\n\n# Ensure necessary directories exist\ndef ensure_directories():\n    for dir_path in [TEMP_DIR, ASSETS_DIR]:\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n            print(f\"Created directory: {dir_path}\")\n\n# Get the best voice track from the enhanced versions\ndef get_best_voice_track():\n    # Priority order of voice tracks (from best to acceptable)\n    voice_priorities = [\n        \"ExecutiveVoice\", \n        \"UltraCrispDefinition\", \n        \"BroadcastPerfection\", \n        \"ConfidentPresence\", \n        \"CrystalClarity\"\n    ]\n    \n    for voice in voice_priorities:\n        voice_file = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{voice}.mp4\")\n        if os.path.exists(voice_file):\n            print(f\"Selected voice track: {voice}\")\n            return voice_file\n    \n    # Fallback to original if none of the enhanced versions are found\n    fallback = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    if os.path.exists(fallback):\n        print(\"Using original assembly as fallback\")\n        return fallback\n    \n    raise FileNotFoundError(\"No suitable voice track found\")\n\n# Create simple but effective title animation\ndef create_title_animation():\n    title_file = os.path.join(TEMP_DIR, \"title_animation.mp4\")\n    \n    # Create a simpler but effective title animation\n    title_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'color=c=black:s=1920x1080:r=24:d=10',\n        '-vf', \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='WHERE YOU GO':fontcolor=white:fontsize=120:\"\n        \"x=(w-text_w)/2:y=(h-text_h)/2-100:\"\n        \"enable='between(t,1,10)':alpha='if(lt(t,2),t-1,if(lt(t,8),1,10-t))',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='WHEN YOU LEAVE':fontcolor=white:fontsize=120:\"\n        \"x=(w-text_w)/2:y=(h-text_h)/2+100:\"\n        \"enable='between(t,2,10)':alpha='if(lt(t,3),t-2,if(lt(t,8),1,10-t))',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='A MULTIMEDIA POETRY PROJECT':fontcolor=gray:fontsize=40:\"\n        \"x=(w-text_w)/2:y=(h-text_h)/2+250:\"\n        \"enable='between(t,4,10)':alpha='if(lt(t,5),t-4,if(lt(t,8),1,10-t))',\"\n        \n        # Add visual elements that move\n        \"geq=r='128+127*sin((2*PI*(X/W*t)))':g='128+127*sin((2*PI*(Y/H*(t+0.5))))':b='128+127*sin((2*PI*((X+Y)/(W+H)*(t+1.0))))':\"\n        \"a='0.2*sin(t)':\"\n        \"enable='between(t,0,10)'\",\n        \n        \"-c:v\", \"libx264\",\n        \"-preset\", \"medium\",\n        \"-crf\", \"18\"\n    ]\n    \n    print(\"Creating title animation...\")\n    subprocess.run(title_cmd)\n    \n    return title_file if os.path.exists(title_file) else None\n\n# Create end credits animation\ndef create_end_credits():\n    credits_file = os.path.join(TEMP_DIR, \"end_credits.mp4\")\n    \n    # Create end credits with scrolling text\n    credits_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'color=c=black:s=1920x1080:r=24:d=10',\n        '-vf', \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=80:\"\n        \"x=(w-text_w)/2:y=h-12*t*10:\"\n        \"enable='between(t,0,10)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='A MULTIMEDIA POETRY PROJECT':fontcolor=white:fontsize=40:\"\n        \"x=(w-text_w)/2:y=h+100-12*t*10:\"\n        \"enable='between(t,0,10)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='AUDIO PROCESSING & VISUAL ASSEMBLY':fontcolor=white:fontsize=30:\"\n        \"x=(w-text_w)/2:y=h+200-12*t*10:\"\n        \"enable='between(t,0,10)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='ORIGINAL POETRY & CONCEPT':fontcolor=white:fontsize=30:\"\n        \"x=(w-text_w)/2:y=h+300-12*t*10:\"\n        \"enable='between(t,0,10)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=40:\"\n        \"x=(w-text_w)/2:y=h+450-12*t*10:\"\n        \"enable='between(t,0,10)',\"\n        \n        \"-c:v\", \"libx264\",\n        \"-preset\", \"medium\",\n        \"-crf\", \"18\"\n    ]\n    \n    print(\"Creating end credits...\")\n    subprocess.run(credits_cmd)\n    \n    return credits_file if os.path.exists(credits_file) else None\n\n# Generate audio files\ndef generate_audio_files():\n    # Title sound\n    title_sound = os.path.join(TEMP_DIR, \"title_sound.wav\")\n    title_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'sine=f=220:d=10',  # Base frequency\n        '-filter_complex',\n        'aeval=0.5*sin(2*PI*220*t)+0.3*sin(2*PI*330*t)+0.2*sin(2*PI*440*t):c=stereo,'\n        'tremolo=f=4:d=0.2,'\n        'aphaser=in_gain=0.5:out_gain=0.5:delay=2:decay=0.5,'\n        'aecho=0.8:0.5:800|1600:0.8|0.4,'\n        'afade=t=in:st=0:d=1:curve=par,'\n        'afade=t=out:st=8:d=2:curve=par',\n        '-y',\n        title_sound\n    ]\n    \n    print(\"Creating title sound...\")\n    subprocess.run(title_cmd)\n    \n    # Credits sound\n    credits_sound = os.path.join(TEMP_DIR, \"credits_sound.wav\")\n    credits_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'sine=f=146.83:d=10',  # D3 note\n        '-filter_complex',\n        'aeval=0.4*sin(2*PI*146.83*t)+0.3*sin(2*PI*220*t)+0.2*sin(2*PI*293.66*t)+0.1*sin(2*PI*440*t):c=stereo,'\n        'aecho=0.8:0.6:1000|1800:0.6|0.3,'\n        'afade=t=in:st=0:d=1,'\n        'afade=t=out:st=8:d=2',\n        '-y',\n        credits_sound\n    ]\n    \n    print(\"Creating credits sound...\")\n    subprocess.run(credits_cmd)\n    \n    # Buffer audio for start\n    buffer_start = os.path.join(TEMP_DIR, \"buffer_start.wav\")\n    buffer_start_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.05:c=pink:d=3',\n        '-filter_complex',\n        'afade=t=in:st=0:d=1,'\n        'afade=t=out:st=2:d=1',\n        '-y',\n        buffer_start\n    ]\n    \n    print(\"Creating start buffer audio...\")\n    subprocess.run(buffer_start_cmd)\n    \n    # Buffer audio for end\n    buffer_end = os.path.join(TEMP_DIR, \"buffer_end.wav\")\n    buffer_end_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.03:c=pink:d=3',\n        '-filter_complex',\n        'afade=t=in:st=0:d=1,'\n        'afade=t=out:st=2:d=1',\n        '-y',\n        buffer_end\n    ]\n    \n    print(\"Creating end buffer audio...\")\n    subprocess.run(buffer_end_cmd)\n    \n    return {\n        'title': title_sound if os.path.exists(title_sound) else None,\n        'credits': credits_sound if os.path.exists(credits_sound) else None,\n        'buffer_start': buffer_start if os.path.exists(buffer_start) else None,\n        'buffer_end': buffer_end if os.path.exists(buffer_end) else None\n    }\n\n# Extract audio from the voice track\ndef extract_audio(voice_file):\n    audio_file = os.path.join(TEMP_DIR, \"voice_audio.wav\")\n    \n    extract_cmd = [\n        'ffmpeg',\n        '-i', voice_file,\n        '-vn',\n        '-acodec', 'pcm_s16le',\n        '-y',\n        audio_file\n    ]\n    \n    print(\"Extracting audio from voice track...\")\n    subprocess.run(extract_cmd)\n    \n    return audio_file if os.path.exists(audio_file) else None\n\n# Assemble final video\ndef assemble_final_video(voice_file, audio_files):\n    # Get input video and audio\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    voice_audio = os.path.join(TEMP_DIR, \"voice_audio.wav\")\n    \n    # Extract video without audio\n    extract_video_cmd = [\n        'ffmpeg',\n        '-i', voice_file,\n        '-c:v', 'copy',\n        '-an',\n        '-y',\n        video_only\n    ]\n    \n    print(\"Extracting video without audio...\")\n    subprocess.run(extract_video_cmd)\n    \n    # Extract audio\n    extract_audio_cmd = [\n        'ffmpeg',\n        '-i', voice_file,\n        '-vn',\n        '-acodec', 'pcm_s16le',\n        '-y',\n        voice_audio\n    ]\n    \n    print(\"Extracting audio from voice track...\")\n    subprocess.run(extract_audio_cmd)\n    \n    # Create title animation and end credits\n    title_file = create_title_animation()\n    credits_file = create_end_credits()\n    \n    if not title_file or not credits_file:\n        print(\"Error creating animations\")\n        return False\n    \n    # Create list file for concatenation\n    concat_list = os.path.join(TEMP_DIR, \"concat_list.txt\")\n    with open(concat_list, 'w') as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_only)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    # Create the audio mix with correct timing\n    mixed_audio = os.path.join(TEMP_DIR, \"mixed_audio.wav\")\n    \n    # Calculate overlaps and offsets to align voice 2 seconds earlier\n    # Title is 10s, video is ~100s, credits are 10s\n    # We want the voice to start at 8s (2s before title ends)\n    audio_filter = (\n        # Include buffer start\n        \"[0:a]atrim=0:3,asetpts=PTS-STARTPTS[buffer_start];\"\n        \n        # Include title sound (10s)\n        \"[1:a]atrim=0:10,asetpts=PTS-STARTPTS[title_sound];\"\n        \n        # Voice audio (starts 2s before title ends)\n        \"[2:a]adelay=8000|8000[delayed_voice];\"\n        \n        # Credits sound (10s)\n        \"[3:a]atrim=0:10,asetpts=PTS-STARTPTS[credits_sound];\"\n        \n        # Include buffer end\n        \"[4:a]atrim=0:3,asetpts=PTS-STARTPTS[buffer_end];\"\n        \n        # Combine them all with crossfades\n        \"[buffer_start][title_sound]acrossfade=d=1:c1=tri:c2=tri[intro];\"\n        \"[intro][delayed_voice]acrossfade=d=2:c1=tri:c2=tri[main];\"\n        \"[main][credits_sound]acrossfade=d=2:c1=tri:c2=tri[almost];\"\n        \"[almost][buffer_end]acrossfade=d=1:c1=tri:c2=tri[aout]\"\n    )\n    \n    mix_cmd = [\n        'ffmpeg',\n        '-i', audio_files['buffer_start'],\n        '-i', audio_files['title'],\n        '-i', voice_audio,\n        '-i', audio_files['credits'],\n        '-i', audio_files['buffer_end'],\n        '-filter_complex', audio_filter,\n        '-map', '[aout]',\n        '-y',\n        mixed_audio\n    ]\n    \n    print(\"Creating mixed audio with correct sync...\")\n    subprocess.run(mix_cmd)\n    \n    if not os.path.exists(mixed_audio):\n        print(\"Error creating mixed audio\")\n        return False\n    \n    # Final output\n    final_output = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_Final_WithBuffers.mp4\")\n    \n    # Combine video and audio\n    final_cmd = [\n        'ffmpeg',\n        '-f', 'concat',\n        '-safe', '0',\n        '-i', concat_list,\n        '-i', mixed_audio,\n        '-map', '0:v',\n        '-map', '1:a',\n        '-c:v', 'libx264',\n        '-c:a', 'aac',\n        '-b:a', '192k',\n        '-shortest',\n        '-metadata', 'title=Where You Go When You Leave',\n        '-metadata', 'comment=Final version with proper audio sync and buffer audio',\n        '-y',\n        final_output\n    ]\n    \n    print(\"Creating final video...\")\n    subprocess.run(final_cmd)\n    \n    if os.path.exists(final_output):\n        print(f\"\u2713 Final video created: {final_output}\")\n        return True\n    \n    print(\"Error creating final video\")\n    return False\n\ndef main():\n    print(\"Creating final version with improved titles and audio sync...\")\n    \n    # Ensure directories\n    ensure_directories()\n    \n    # Get the best voice track\n    try:\n        voice_file = get_best_voice_track()\n    except FileNotFoundError as e:\n        print(str(e))\n        return False\n    \n    # Generate all audio files\n    audio_files = generate_audio_files()\n    for key, value in audio_files.items():\n        if value is None:\n            print(f\"Error generating {key} audio\")\n            return False\n    \n    # Assemble the final video\n    success = assemble_final_video(voice_file, audio_files)\n    \n    if success:\n        print(\"\\nFinal video creation complete!\")\n        print(\"- Added dynamic 10-second title animation\")\n        print(\"- Fixed audio sync (voice starts 2s earlier during title)\")\n        print(\"- Added buffer audio at beginning and end\")\n        print(\"- Added animated end credits\")\n        \n        final_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_Final_WithBuffers.mp4\")\n        print(f\"\\nFinal video available at: {final_file}\")\n    else:\n        print(\"Error creating final video\")\n    \n    return success\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_{voice}.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "title_animation.mp4",
      "end_credits.mp4",
      "title_sound.wav",
      "credits_sound.wav",
      "buffer_start.wav",
      "buffer_end.wav",
      "voice_audio.wav",
      "video_only.mp4",
      "voice_audio.wav",
      "mixed_audio.wav",
      "WhereYouGoWhenYouLeave_Final_WithBuffers.mp4",
      "WhereYouGoWhenYouLeave_Final_WithBuffers.mp4",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-100:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+100:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+250:",
      "128+127*sin((2*PI*(X/W*t)))",
      "128+127*sin((2*PI*(Y/H*(t+0.5))))",
      "128+127*sin((2*PI*((X+Y)/(W+H)*(t+1.0))))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=h-12*t*10:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=h+100-12*t*10:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=h+200-12*t*10:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=h+300-12*t*10:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=h+450-12*t*10:"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "title_cmd"
      },
      {
        "type": "run",
        "snippet": "credits_cmd"
      },
      {
        "type": "run",
        "snippet": "title_cmd"
      },
      {
        "type": "run",
        "snippet": "credits_cmd"
      },
      {
        "type": "run",
        "snippet": "buffer_start_cmd"
      },
      {
        "type": "run",
        "snippet": "buffer_end_cmd"
      },
      {
        "type": "run",
        "snippet": "extract_cmd"
      },
      {
        "type": "run",
        "snippet": "extract_video_cmd"
      },
      {
        "type": "run",
        "snippet": "extract_audio_cmd"
      },
      {
        "type": "run",
        "snippet": "mix_cmd"
      },
      {
        "type": "run",
        "snippet": "final_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/fix_final_timing.py",
    "size": 13657,
    "lines": 446,
    "source": "import os\nimport subprocess\nimport shutil\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_LAYERS\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FINAL_LAYERED.mp4\")\n\n# Ensure temp directory exists\nif not os.path.exists(TEMP_DIR):\n    os.makedirs(TEMP_DIR)\n    print(f\"Created directory: {TEMP_DIR}\")\n\n# Find the best voice track\ndef find_voice_track():\n    preferred_voices = [\n        \"ExecutiveVoice\",\n        \"UltraCrispDefinition\",\n        \"BroadcastPerfection\"\n    ]\n    \n    for voice in preferred_voices:\n        voice_file = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{voice}.mp4\")\n        if os.path.exists(voice_file):\n            print(f\"Using voice track: {voice}\")\n            return voice_file\n    \n    fallback = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    if os.path.exists(fallback):\n        print(\"Using original assembly as fallback\")\n        return fallback\n    \n    print(\"No suitable voice track found\")\n    return None\n\n# Create shorter title animation (5 seconds)\ndef create_title():\n    title_file = os.path.join(TEMP_DIR, \"title_short.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2:\"\n            \"enable='between(t,0,5)':alpha='if(lt(t,0.5),t*2,if(lt(t,4.5),1,(5-t)*2))'\"\n        ),\n        \"-c:v\", \"libx264\",\n        \"-pix_fmt\", \"yuv420p\",\n        \"-y\",\n        title_file\n    ]\n    \n    print(\"Creating shorter title card...\")\n    subprocess.run(cmd)\n    \n    return title_file if os.path.exists(title_file) else None\n\n# Create shorter end credits\ndef create_credits():\n    credits_file = os.path.join(TEMP_DIR, \"credits_short.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE\\\\n\\\\nA Multimedia Poetry Project\\\\n\\\\n\u00a9 2025':\"\n            \"fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2\"\n        ),\n        \"-c:v\", \"libx264\",\n        \"-pix_fmt\", \"yuv420p\",\n        \"-y\",\n        credits_file\n    ]\n    \n    print(\"Creating shorter end credits...\")\n    subprocess.run(cmd)\n    \n    return credits_file if os.path.exists(credits_file) else None\n\n# Extract video without audio\ndef extract_video(input_video):\n    video_file = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-i\", input_video,\n        \"-c:v\", \"copy\",\n        \"-an\",\n        \"-y\",\n        video_file\n    ]\n    \n    print(\"Extracting video without audio...\")\n    subprocess.run(cmd)\n    \n    return video_file if os.path.exists(video_file) else None\n\n# Extract audio and create variations\ndef extract_and_process_audio(input_video):\n    # Extract main voice audio\n    main_audio = os.path.join(TEMP_DIR, \"main_voice.wav\")\n    cmd = [\n        \"ffmpeg\",\n        \"-i\", input_video,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\",\n        main_audio\n    ]\n    print(\"Extracting main voice audio...\")\n    subprocess.run(cmd)\n    \n    # Create echo variation\n    echo_audio = os.path.join(TEMP_DIR, \"echo_voice.wav\")\n    cmd_echo = [\n        \"ffmpeg\",\n        \"-i\", main_audio,\n        \"-af\", \"aecho=0.8:0.9:1000|1800:0.3|0.25,volume=0.4\",\n        \"-y\",\n        echo_audio\n    ]\n    print(\"Creating echo variation...\")\n    subprocess.run(cmd_echo)\n    \n    # Create reverb variation\n    reverb_audio = os.path.join(TEMP_DIR, \"reverb_voice.wav\")\n    cmd_reverb = [\n        \"ffmpeg\",\n        \"-i\", main_audio,\n        \"-af\", \"afade=t=in:st=0:d=1,afade=t=out:st=99:d=1,areverse,aecho=0.8:0.8:60|90:0.5|0.4,areverse,volume=0.3\",\n        \"-y\",\n        reverb_audio\n    ]\n    print(\"Creating reverb variation...\")\n    subprocess.run(cmd_reverb)\n    \n    # Create lower pitched variation\n    pitch_audio = os.path.join(TEMP_DIR, \"pitch_voice.wav\")\n    cmd_pitch = [\n        \"ffmpeg\",\n        \"-i\", main_audio,\n        \"-af\", \"asetrate=44100*0.9,aresample=44100,volume=0.25\",\n        \"-y\",\n        pitch_audio\n    ]\n    print(\"Creating pitched variation...\")\n    subprocess.run(cmd_pitch)\n    \n    return {\n        'main': main_audio,\n        'echo': echo_audio,\n        'reverb': reverb_audio,\n        'pitch': pitch_audio\n    }\n\n# Create ambient sounds\ndef create_ambient_sounds():\n    # Create drone sound\n    drone_sound = os.path.join(TEMP_DIR, \"drone.wav\")\n    cmd_drone = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.05*sin(2*PI*55*t)+0.03*sin(2*PI*55*1.5*t):s=44100:d=120\",\n        \"-af\", \"afade=t=in:st=0:d=3,afade=t=out:st=115:d=5\",\n        \"-y\",\n        drone_sound\n    ]\n    print(\"Creating drone sound...\")\n    subprocess.run(cmd_drone)\n    \n    # Create atmospheric wind sound\n    wind_sound = os.path.join(TEMP_DIR, \"wind.wav\")\n    cmd_wind = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anoisesrc=a=0.03:c=pink:d=120\",\n        \"-af\", \"afade=t=in:st=0:d=4,afade=t=out:st=110:d=10,lowpass=f=800\",\n        \"-y\",\n        wind_sound\n    ]\n    print(\"Creating wind sound...\")\n    subprocess.run(cmd_wind)\n    \n    return {\n        'drone': drone_sound,\n        'wind': wind_sound\n    }\n\n# Create title and credits sounds\ndef create_title_credits_sounds():\n    # Create title sound\n    title_sound = os.path.join(TEMP_DIR, \"title_sound.wav\")\n    cmd_title = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.1*sin(2*PI*220*t)+0.07*sin(2*PI*330*t)+0.05*sin(2*PI*440*t):s=44100:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=1,afade=t=out:st=4:d=1\",\n        \"-y\",\n        title_sound\n    ]\n    print(\"Creating title sound...\")\n    subprocess.run(cmd_title)\n    \n    # Create credits sound\n    credits_sound = os.path.join(TEMP_DIR, \"credits_sound.wav\")\n    cmd_credits = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.08*sin(2*PI*146.83*t)+0.06*sin(2*PI*220*t)+0.04*sin(2*PI*293.66*t):s=44100:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=1,afade=t=out:st=4:d=1\",\n        \"-y\",\n        credits_sound\n    ]\n    print(\"Creating credits sound...\")\n    subprocess.run(cmd_credits)\n    \n    return {\n        'title': title_sound,\n        'credits': credits_sound\n    }\n\n# Concatenate videos (title + main + credits)\ndef concatenate_videos(title_file, video_file, credits_file):\n    concat_list = os.path.join(TEMP_DIR, \"concat_list.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    output_file = os.path.join(TEMP_DIR, \"video_concat.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\",\n        output_file\n    ]\n    \n    print(\"Concatenating videos...\")\n    subprocess.run(cmd)\n    \n    return output_file if os.path.exists(output_file) else None\n\n# Create layered audio mix with proper timing\ndef create_layered_audio(audio_variations, ambient_sounds, effect_sounds):\n    # Main voice audio with proper timing (starts 3 seconds in)\n    main_delayed = os.path.join(TEMP_DIR, \"main_delayed.wav\")\n    cmd_main = [\n        \"ffmpeg\",\n        \"-i\", audio_variations['main'],\n        \"-af\", \"adelay=3000|3000\",  # Start 3 seconds in\n        \"-y\",\n        main_delayed\n    ]\n    print(\"Positioning main voice audio...\")\n    subprocess.run(cmd_main)\n    \n    # Echo voice starts 2.7 seconds in\n    echo_delayed = os.path.join(TEMP_DIR, \"echo_delayed.wav\")\n    cmd_echo = [\n        \"ffmpeg\",\n        \"-i\", audio_variations['echo'],\n        \"-af\", \"adelay=2700|2700\",  # Start 2.7 seconds in\n        \"-y\",\n        echo_delayed\n    ]\n    print(\"Positioning echo voice audio...\")\n    subprocess.run(cmd_echo)\n    \n    # Reverb voice starts 2.5 seconds in\n    reverb_delayed = os.path.join(TEMP_DIR, \"reverb_delayed.wav\")\n    cmd_reverb = [\n        \"ffmpeg\",\n        \"-i\", audio_variations['reverb'],\n        \"-af\", \"adelay=2500|2500\",  # Start 2.5 seconds in\n        \"-y\",\n        reverb_delayed\n    ]\n    print(\"Positioning reverb voice audio...\")\n    subprocess.run(cmd_reverb)\n    \n    # Pitched voice starts 2.3 seconds in\n    pitch_delayed = os.path.join(TEMP_DIR, \"pitch_delayed.wav\")\n    cmd_pitch = [\n        \"ffmpeg\",\n        \"-i\", audio_variations['pitch'],\n        \"-af\", \"adelay=2300|2300\",  # Start 2.3 seconds in\n        \"-y\",\n        pitch_delayed\n    ]\n    print(\"Positioning pitched voice audio...\")\n    subprocess.run(cmd_pitch)\n    \n    # Now mix all the audio together\n    layered_audio = os.path.join(TEMP_DIR, \"layered_audio.wav\")\n    \n    # Create a complex filter for mixing\n    filter_complex = [\n        # Title sound\n        \"[0:0]atrim=0:5,asetpts=PTS-STARTPTS,afade=t=in:st=0:d=1,afade=t=out:st=4:d=1[title]\",\n        # Main voice\n        \"[1:0]aformat=sample_rates=44100:channel_layouts=mono[main_fmt]\",\n        # Echo voice\n        \"[2:0]aformat=sample_rates=44100:channel_layouts=mono[echo_fmt]\",\n        # Reverb voice\n        \"[3:0]aformat=sample_rates=44100:channel_layouts=mono[reverb_fmt]\",\n        # Pitched voice\n        \"[4:0]aformat=sample_rates=44100:channel_layouts=mono[pitch_fmt]\",\n        # Drone ambient\n        \"[5:0]volume=0.15,afade=t=in:st=0:d=4,afade=t=out:st=110:d=5[drone_fmt]\",\n        # Wind ambient\n        \"[6:0]volume=0.1,afade=t=in:st=0:d=4,afade=t=out:st=110:d=5[wind_fmt]\",\n        # Credits sound\n        \"[7:0]atrim=0:5,asetpts=PTS-STARTPTS,afade=t=in:st=0:d=1,afade=t=out:st=4:d=1[credits]\",\n        # Mix all audio together\n        \"[title][main_fmt][echo_fmt][reverb_fmt][pitch_fmt][drone_fmt][wind_fmt][credits]amix=inputs=8:normalize=0[aout]\"\n    ]\n    \n    # Build the command\n    mix_cmd = [\n        \"ffmpeg\",\n        \"-i\", effect_sounds['title'],\n        \"-i\", main_delayed,\n        \"-i\", echo_delayed,\n        \"-i\", reverb_delayed,\n        \"-i\", pitch_delayed,\n        \"-i\", ambient_sounds['drone'],\n        \"-i\", ambient_sounds['wind'],\n        \"-i\", effect_sounds['credits'],\n        \"-filter_complex\", \";\".join(filter_complex),\n        \"-map\", \"[aout]\",\n        \"-y\",\n        layered_audio\n    ]\n    \n    print(\"Creating layered audio mix...\")\n    subprocess.run(mix_cmd)\n    \n    return layered_audio if os.path.exists(layered_audio) else None\n\n# Add audio to final video\ndef add_audio_to_video(video_file, audio_file):\n    cmd = [\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"192k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-y\",\n        OUTPUT_FILE\n    ]\n    \n    print(\"Creating final video with layered audio...\")\n    subprocess.run(cmd)\n    \n    return OUTPUT_FILE if os.path.exists(OUTPUT_FILE) else None\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING FINAL VERSION WITH LAYERED AUDIO AND IMPROVED TIMING =====\\n\")\n    \n    # Find best voice track\n    voice_file = find_voice_track()\n    if not voice_file:\n        return\n    \n    # Create short title and credits\n    title_file = create_title()\n    credits_file = create_credits()\n    \n    if not title_file or not credits_file:\n        print(\"Error creating title or credits\")\n        return\n    \n    # Extract video\n    video_only = extract_video(voice_file)\n    if not video_only:\n        print(\"Error extracting video\")\n        return\n    \n    # Extract and process audio into variations\n    audio_variations = extract_and_process_audio(voice_file)\n    for key, value in audio_variations.items():\n        if not value or not os.path.exists(value):\n            print(f\"Error processing {key} audio\")\n            return\n    \n    # Create ambient sounds\n    ambient_sounds = create_ambient_sounds()\n    for key, value in ambient_sounds.items():\n        if not value or not os.path.exists(value):\n            print(f\"Error creating {key} ambient sound\")\n            return\n    \n    # Create title and credits sounds\n    effect_sounds = create_title_credits_sounds()\n    for key, value in effect_sounds.items():\n        if not value or not os.path.exists(value):\n            print(f\"Error creating {key} effect sound\")\n            return\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(title_file, video_only, credits_file)\n    if not concat_video:\n        print(\"Error concatenating videos\")\n        return\n    \n    # Create layered audio mix\n    layered_audio = create_layered_audio(audio_variations, ambient_sounds, effect_sounds)\n    if not layered_audio:\n        print(\"Error creating layered audio\")\n        return\n    \n    # Add audio to video\n    final_output = add_audio_to_video(concat_video, layered_audio)\n    if not final_output:\n        print(\"Error adding audio to video\")\n        return\n    \n    print(f\"\\n===== SUCCESS! =====\")\n    print(f\"Final layered video created: {OUTPUT_FILE}\")\n    print(\"Improvements:\")\n    print(\"- Voice now starts at 2.3-3 seconds (during title) with staggered layered voices\")\n    print(\"- Added layered voice variations (echo, reverb, pitched) for rich texture\")\n    print(\"- Added continuous ambient sounds (drone, wind)\")\n    print(\"- Shorter title (5 seconds) and credits (5 seconds) for better pacing\")\n    print(\"- Voice audio starts before main video for perfect timing\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_FINAL_LAYERED.mp4",
      "WhereYouGoWhenYouLeave_{voice}.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "title_short.mp4",
      "credits_short.mp4",
      "video_only.mp4",
      "main_voice.wav",
      "echo_voice.wav",
      "reverb_voice.wav",
      "pitch_voice.wav",
      "drone.wav",
      "wind.wav",
      "title_sound.wav",
      "credits_sound.wav",
      "video_concat.mp4",
      "main_delayed.wav",
      "echo_delayed.wav",
      "reverb_delayed.wav",
      "pitch_delayed.wav",
      "layered_audio.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd_echo"
      },
      {
        "type": "run",
        "snippet": "cmd_reverb"
      },
      {
        "type": "run",
        "snippet": "cmd_pitch"
      },
      {
        "type": "run",
        "snippet": "cmd_drone"
      },
      {
        "type": "run",
        "snippet": "cmd_wind"
      },
      {
        "type": "run",
        "snippet": "cmd_title"
      },
      {
        "type": "run",
        "snippet": "cmd_credits"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd_main"
      },
      {
        "type": "run",
        "snippet": "cmd_echo"
      },
      {
        "type": "run",
        "snippet": "cmd_reverb"
      },
      {
        "type": "run",
        "snippet": "cmd_pitch"
      },
      {
        "type": "run",
        "snippet": "mix_cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/restore_original_voice.py",
    "size": 11417,
    "lines": 346,
    "source": "import os\nimport subprocess\nimport shutil\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_RESTORE\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_RESTORED_VOICE.mp4\")\n\n# Timing constants\nINTRO_LENGTH = 7\nOUTRO_LENGTH = 8\nVOICE_DELAY = 3\nMAIN_LENGTH = 100.91  # Exact 1:40 length\n\n# Clean and create temp directory\nif os.path.exists(TEMP_DIR):\n    shutil.rmtree(TEMP_DIR)\nos.makedirs(TEMP_DIR)\nprint(f\"Using directory: {TEMP_DIR}\")\n\n# Find original source files\ndef find_source_files():\n    # Find voice track\n    voice_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\")\n    if os.path.exists(voice_file):\n        print(f\"Using voice track: {os.path.basename(voice_file)}\")\n    else:\n        voice_file = None\n        print(\"Voice track not found\")\n    \n    # Find original video with native audio\n    original_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\")\n    if not os.path.exists(original_file):\n        original_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n        \n    if os.path.exists(original_file):\n        print(f\"Using original video: {os.path.basename(original_file)}\")\n    else:\n        original_file = voice_file\n        print(\"Using voice track as video source\")\n    \n    return voice_file, original_file\n\n# Create simple intro and outro\ndef create_title_and_credits():\n    # Create intro\n    intro_file = os.path.join(TEMP_DIR, \"intro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=80:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\", \n        \"-y\", intro_file\n    ])\n    \n    # Create outro\n    outro_file = os.path.join(TEMP_DIR, \"outro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\", \n        \"-y\", outro_file\n    ])\n    \n    return intro_file, outro_file\n\n# Extract audio and video\ndef extract_components(voice_file, original_file):\n    # Extract voice audio exactly as is\n    voice_audio = os.path.join(TEMP_DIR, \"voice_original.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ])\n    print(\"Extracted original voice audio without any processing\")\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ])\n    print(\"Extracted native audio\")\n    \n    # Extract video only\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ])\n    print(\"Extracted video without audio\")\n    \n    return voice_audio, native_audio, video_only\n\n# Create the 3-second delay for voice and native audio\ndef add_delay_to_audio(voice_audio, native_audio):\n    # Create 3 second silent buffer audio\n    silent_buffer = os.path.join(TEMP_DIR, \"silence_3s.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=3\",\n        \"-y\", silent_buffer\n    ])\n    print(\"Created 3-second silent buffer\")\n    \n    # Add silent buffer to voice audio\n    voice_delayed = os.path.join(TEMP_DIR, \"voice_delayed.wav\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-i\", silent_buffer,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_delayed\n    ])\n    print(\"Added 3-second delay to voice audio (preserving original)\")\n    \n    # Add silent buffer to native audio\n    native_delayed = os.path.join(TEMP_DIR, \"native_delayed.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", silent_buffer,\n        \"-i\", native_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", native_delayed\n    ])\n    print(\"Added 3-second delay to native audio\")\n    \n    return voice_delayed, native_delayed\n\n# Create ambient background\ndef create_ambient_background():\n    # Simple ambient background\n    ambient_file = os.path.join(TEMP_DIR, \"ambient.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anoisesrc=color=pink:amplitude=0.01:duration=115.91\",\n        \"-af\", \"volume=0.15\",\n        \"-y\", ambient_file\n    ])\n    print(\"Created ambient background\")\n    \n    # Simple intro audio effect\n    intro_audio = os.path.join(TEMP_DIR, \"intro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.2\",\n        \"-y\", intro_audio\n    ])\n    \n    # Simple outro audio effect\n    outro_audio = os.path.join(TEMP_DIR, \"outro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.2\",\n        \"-y\", outro_audio\n    ])\n    \n    return ambient_file, intro_audio, outro_audio\n\n# Concatenate videos\ndef concatenate_videos(intro_file, main_file, outro_file):\n    concat_file = os.path.join(TEMP_DIR, \"concat.txt\")\n    with open(concat_file, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(main_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concat_video.mp4\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    print(\"Concatenated intro + main video + outro\")\n    \n    return concat_video\n\n# Create final audio mix\ndef create_audio_mix(voice_delayed, native_delayed, ambient, intro_audio, outro_audio):\n    # Position intro audio\n    intro_extended = os.path.join(TEMP_DIR, \"intro_extended.wav\")\n    long_silence = os.path.join(TEMP_DIR, \"long_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=109\",\n        \"-y\", long_silence\n    ])\n    intro_concat = os.path.join(TEMP_DIR, \"intro_concat.txt\")\n    with open(intro_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_audio)}'\\n\")\n        f.write(f\"file '{os.path.abspath(long_silence)}'\\n\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", intro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", intro_extended\n    ])\n    \n    # Position outro audio\n    outro_extended = os.path.join(TEMP_DIR, \"outro_extended.wav\")\n    outro_silence = os.path.join(TEMP_DIR, \"outro_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=108\",\n        \"-y\", outro_silence\n    ])\n    outro_concat = os.path.join(TEMP_DIR, \"outro_concat.txt\")\n    with open(outro_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(outro_silence)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_audio)}'\\n\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", outro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", outro_extended\n    ])\n    \n    # Mix all audio\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Simple mix with appropriate volumes\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_delayed,\n        \"-i\", native_delayed,\n        \"-i\", ambient,\n        \"-i\", intro_extended,\n        \"-i\", outro_extended,\n        \"-filter_complex\",\n        \"[0:0]volume=1.0[voice];\" +\n        \"[1:0]volume=0.8[native];\" +\n        \"[2:0]volume=0.15[ambient];\" +\n        \"[3:0]volume=0.2[intro];\" +\n        \"[4:0]volume=0.2[outro];\" +\n        \"[voice][native]amix=inputs=2:duration=first[content];\" +\n        \"[ambient][intro]amix=inputs=2:duration=first[amb_fx1];\" +\n        \"[amb_fx1][outro]amix=inputs=2:duration=first[amb_fx];\" +\n        \"[content][amb_fx]amix=inputs=2:duration=first[out]\",\n        \"-map\", \"[out]\",\n        \"-y\", final_mix\n    ])\n    print(\"Created final audio mix\")\n    \n    return final_mix\n\n# Add final audio to video\ndef add_audio_to_video(video_file, audio_file):\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Final version with completely unmodified voice\",\n        \"-y\", OUTPUT_FILE\n    ])\n    print(f\"Added audio to video: {OUTPUT_FILE}\")\n    \n    if os.path.exists(OUTPUT_FILE):\n        cmd = [\n            \"ffprobe\", \"-v\", \"error\",\n            \"-show_entries\", \"format=duration\",\n            \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n            OUTPUT_FILE\n        ]\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        duration = float(result.stdout.strip())\n        print(f\"Final video duration: {duration:.2f} seconds\")\n        return True\n    else:\n        return False\n\n# Main function\ndef main():\n    print(\"\\n===== RESTORING ORIGINAL VOICE WITH SIMPLE 3-SECOND DELAY =====\\n\")\n    \n    # Find sources\n    voice_file, original_file = find_source_files()\n    if not voice_file or not original_file:\n        print(\"Error: Source files not found\")\n        return False\n    \n    # Create intro/outro\n    intro_file, outro_file = create_title_and_credits()\n    \n    # Extract components\n    voice_audio, native_audio, video_only = extract_components(voice_file, original_file)\n    \n    # Add delay to audio\n    voice_delayed, native_delayed = add_delay_to_audio(voice_audio, native_audio)\n    \n    # Create ambient background\n    ambient_audio, intro_audio, outro_audio = create_ambient_background()\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Create audio mix\n    final_mix = create_audio_mix(voice_delayed, native_delayed, ambient_audio, intro_audio, outro_audio)\n    \n    # Add audio to video\n    success = add_audio_to_video(concat_video, final_mix)\n    \n    if success:\n        print(\"\\n===== SUCCESS! =====\")\n        print(f\"Final video created: {OUTPUT_FILE}\")\n        print(\"\\nFinal specifications:\")\n        print(\"- Original voice track preserved exactly as is\")\n        print(f\"- Voice starts after {VOICE_DELAY} seconds of intro\")\n        print(f\"- Intro length: {INTRO_LENGTH} seconds\")\n        print(f\"- Main content: {MAIN_LENGTH} seconds (1:40)\")\n        print(f\"- Outro length: {OUTRO_LENGTH} seconds\")\n        print(\"- Minimal processing applied to avoid any distortion\")\n        return True\n    else:\n        print(\"Error: Failed to create final video\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_RESTORED_VOICE.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "intro.mp4",
      "outro.mp4",
      "voice_original.wav",
      "native_audio.wav",
      "video_only.mp4",
      "silence_3s.wav",
      "voice_delayed.wav",
      "native_delayed.wav",
      "ambient.wav",
      "intro_audio.wav",
      "outro_audio.wav",
      "concat_video.mp4",
      "intro_extended.wav",
      "long_silence.wav",
      "outro_extended.wav",
      "outro_silence.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=80:x=(w-text_w)/2:y=(h-text_h)/2",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2",
      ")\n        return False\n    \n    # Create intro/outro\n    intro_file, outro_file = create_title_and_credits()\n    \n    # Extract components\n    voice_audio, native_audio, video_only = extract_components(voice_file, original_file)\n    \n    # Add delay to audio\n    voice_delayed, native_delayed = add_delay_to_audio(voice_audio, native_audio)\n    \n    # Create ambient background\n    ambient_audio, intro_audio, outro_audio = create_ambient_background()\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Create audio mix\n    final_mix = create_audio_mix(voice_delayed, native_delayed, ambient_audio, intro_audio, outro_audio)\n    \n    # Add audio to video\n    success = add_audio_to_video(concat_video, final_mix)\n    \n    if success:\n        print("
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING A"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=3\",\n        \"-y\", silent_buffer\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-i\", silent_buffer,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", silent_buffer,\n        \"-i\", native_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", native_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anoisesrc=color=pink:amplitude=0.01:duration=115.91\",\n        \"-af\", \"volume=0.15\",\n        \"-y\", ambient_file\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.2\",\n        \"-"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.2\",\n       "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=109\",\n        \"-y\", long_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", intro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", intro_extended\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=108\",\n        \"-y\", outro_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", outro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", outro_extended\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_delayed,\n        \"-i\", native_delayed,\n        \"-i\", ambient,\n        \"-i\", intro_extended,\n        \"-i\", outro_extended,\n        \"-filter_complex\",\n        \"[0"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-s"
      },
      {
        "type": "run",
        "snippet": "cmd, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/fix_voiceover_timing.py",
    "size": 14924,
    "lines": 468,
    "source": "import os\nimport subprocess\nimport json\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_FIXED_TIMING\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FIXED_TIMING.mp4\")\nVIDEO_DURATION = 100.91  # Exact 1:40 timing\n\n# Ensure temp directory exists\nif not os.path.exists(TEMP_DIR):\n    os.makedirs(TEMP_DIR)\n    print(f\"Created directory: {TEMP_DIR}\")\n\n# Find the best voice track and original video\ndef find_source_files():\n    # Voice track with professional voiceover\n    voice_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_BroadcastPerfection.mp4\")\n    ]\n    \n    for file in voice_files:\n        if os.path.exists(file):\n            print(f\"Using voice track: {os.path.basename(file)}\")\n            voice_file = file\n            break\n    else:\n        print(\"No suitable voice track found\")\n        return None, None\n    \n    # Original video with native audio\n    original_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    ]\n    \n    for file in original_files:\n        if os.path.exists(file):\n            print(f\"Using original video: {os.path.basename(file)}\")\n            original_file = file\n            break\n    else:\n        # If no original with audio found, use the voice file\n        print(\"No original video found, using voice track as video source\")\n        original_file = voice_file\n    \n    return voice_file, original_file\n\n# Create short title animation\ndef create_title():\n    title_file = os.path.join(TEMP_DIR, \"title.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            \"enable='between(t,0,5)':alpha='if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHEN YOU LEAVE':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+60:\"\n            \"enable='between(t,0.3,5)':alpha='if(lt(t,0.8),(t-0.3)/0.5,if(gt(t,4.5),(5-t)/0.5,1))'\"\n        ),\n        \"-c:v\", \"libx264\",\n        \"-pix_fmt\", \"yuv420p\",\n        \"-y\",\n        title_file\n    ]\n    \n    print(\"Creating title card...\")\n    subprocess.run(cmd)\n    \n    return title_file if os.path.exists(title_file) else None\n\n# Create end credits\ndef create_credits():\n    credits_file = os.path.join(TEMP_DIR, \"credits.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=60:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            \"enable='between(t,0,5)':alpha='if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='A Multimedia Poetry Project':fontcolor=white:fontsize=40:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+30:\"\n            \"enable='between(t,0.5,5)':alpha='if(lt(t,1.0),(t-0.5)/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=30:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+100:\"\n            \"enable='between(t,1,5)':alpha='if(lt(t,1.5),(t-1)/0.5,if(gt(t,4.5),(5-t)/0.5,1))'\"\n        ),\n        \"-c:v\", \"libx264\",\n        \"-pix_fmt\", \"yuv420p\",\n        \"-y\",\n        credits_file\n    ]\n    \n    print(\"Creating end credits...\")\n    subprocess.run(cmd)\n    \n    return credits_file if os.path.exists(credits_file) else None\n\n# Extract different audio components\ndef extract_audio(voice_file, original_file):\n    # 1. Extract voice audio\n    voice_audio = os.path.join(TEMP_DIR, \"voice_audio.wav\")\n    cmd_voice = [\n        \"ffmpeg\",\n        \"-i\", voice_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\",\n        voice_audio\n    ]\n    print(\"Extracting voice audio...\")\n    subprocess.run(cmd_voice)\n    \n    # 2. Extract original/native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    cmd_native = [\n        \"ffmpeg\",\n        \"-i\", original_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\",\n        native_audio\n    ]\n    print(\"Extracting native audio...\")\n    subprocess.run(cmd_native)\n    \n    # 3. Get durations\n    voice_duration = get_duration(voice_audio)\n    native_duration = get_duration(native_audio)\n    \n    print(f\"Voice audio duration: {voice_duration:.2f} seconds\")\n    print(f\"Native audio duration: {native_duration:.2f} seconds\")\n    \n    return {\n        'voice': voice_audio, \n        'native': native_audio,\n        'voice_duration': voice_duration,\n        'native_duration': native_duration\n    }\n\n# Get audio duration\ndef get_duration(audio_file):\n    cmd = [\n        \"ffprobe\",\n        \"-v\", \"error\",\n        \"-show_entries\", \"format=duration\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        audio_file\n    ]\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    return float(result.stdout.strip())\n\n# Create title and credits sound effects\ndef create_special_audio():\n    # Title audio - starts quiet and builds\n    title_audio = os.path.join(TEMP_DIR, \"title_audio.wav\")\n    cmd_title = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.12*sin(2*PI*220*t)+0.08*sin(2*PI*330*t)+0.05*sin(2*PI*440*t):s=44100:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=1.5,afade=t=out:st=3:d=2,volume=0.4\",\n        \"-y\",\n        title_audio\n    ]\n    print(\"Creating title audio...\")\n    subprocess.run(cmd_title)\n    \n    # Credits audio - has fade in/out\n    credits_audio = os.path.join(TEMP_DIR, \"credits_audio.wav\")\n    cmd_credits = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.1*sin(2*PI*146.83*t)+0.07*sin(2*PI*220*t)+0.04*sin(2*PI*293.66*t):s=44100:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=1.5,afade=t=out:st=3:d=2,volume=0.35\",\n        \"-y\",\n        credits_audio\n    ]\n    print(\"Creating credits audio...\")\n    subprocess.run(cmd_credits)\n    \n    return {\n        'title': title_audio,\n        'credits': credits_audio\n    }\n\n# Prepare the voice audio with proper timing\ndef prepare_voice_audio(audio_info):\n    # Calculate speed up factor to match desired duration\n    source_duration = audio_info['voice_duration']\n    target_duration = VIDEO_DURATION  # 1:40 = 100.91 seconds\n    speed_factor = source_duration / target_duration\n    \n    print(f\"Speed factor for voice audio: {speed_factor:.4f}\")\n    \n    # Speed up the voice audio to match the video duration\n    speed_adjusted = os.path.join(TEMP_DIR, \"voice_speed_adjusted.wav\")\n    cmd_speed = [\n        \"ffmpeg\",\n        \"-i\", audio_info['voice'],\n        \"-filter:a\", f\"atempo={speed_factor}\",\n        \"-y\",\n        speed_adjusted\n    ]\n    print(\"Adjusting voice audio speed...\")\n    subprocess.run(cmd_speed)\n    \n    # Create a silent 2-second buffer\n    silent_buffer = os.path.join(TEMP_DIR, \"silent_buffer_2s.wav\")\n    cmd_silence = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anullsrc=r=44100:cl=mono\",\n        \"-t\", \"2\",  # 2 seconds of silence\n        \"-y\",\n        silent_buffer\n    ]\n    print(\"Creating 2-second silent buffer...\")\n    subprocess.run(cmd_silence)\n    \n    # Concatenate the silent buffer with the voice track to create a delay\n    delayed_voice = os.path.join(TEMP_DIR, \"voice_delayed.wav\")\n    cmd_concat = [\n        \"ffmpeg\",\n        \"-i\", silent_buffer,\n        \"-i\", speed_adjusted,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\",\n        delayed_voice\n    ]\n    print(\"Adding 2-second delay to voice audio (via concatenation)...\")\n    subprocess.run(cmd_concat)\n    \n    # Verify timing\n    delay_duration = get_duration(delayed_voice)\n    print(f\"Delayed voice duration: {delay_duration:.2f} seconds\")\n    \n    return delayed_voice\n\n# Extract video without audio\ndef extract_video(input_video):\n    video_file = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-i\", input_video,\n        \"-c:v\", \"copy\",\n        \"-an\",\n        \"-y\",\n        video_file\n    ]\n    \n    print(\"Extracting video without audio...\")\n    subprocess.run(cmd)\n    \n    return video_file if os.path.exists(video_file) else None\n\n# Concatenate videos\ndef concatenate_videos(title_file, video_file, credits_file):\n    concat_list = os.path.join(TEMP_DIR, \"concat.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\",\n        concat_video\n    ]\n    \n    print(\"Concatenating videos...\")\n    subprocess.run(cmd)\n    \n    return concat_video if os.path.exists(concat_video) else None\n\n# Create the final audio mix with all three layers\ndef create_final_audio_mix(voice_audio, native_audio, special_audio):\n    # Create final audio mix with three layers\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Create a title audio with padding\n    title_padded = os.path.join(TEMP_DIR, \"title_padded.wav\")\n    title_cmd = [\n        \"ffmpeg\",\n        \"-i\", special_audio['title'],\n        \"-af\", \"apad=pad_dur=106\",  # Pad to full length (5 title + 101 video)\n        \"-y\",\n        title_padded\n    ]\n    print(\"Creating padded title audio...\")\n    subprocess.run(title_cmd)\n    \n    # Create credits audio with padding\n    credits_padded = os.path.join(TEMP_DIR, \"credits_padded.wav\")\n    credits_cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anullsrc=r=44100:cl=mono:d=106\",\n        \"-i\", special_audio['credits'],\n        \"-filter_complex\", \"[0:0][1:0]acrossfade=d=0:c1=tri:c2=tri\",\n        \"-y\",\n        credits_padded\n    ]\n    print(\"Creating padded credits audio...\")\n    subprocess.run(credits_cmd)\n    \n    # Mix the three layers with careful timing\n    filter_complex = [\n        # Layer 1: Native audio\n        \"[0:0]volume=1.0[native]\",\n        \n        # Layer 2: Voice audio (already delayed by 2 seconds via concatenation)\n        \"[1:0]volume=1.0[voice]\",\n        \n        # Layer 3: Title audio effects\n        \"[2:0]volume=0.35[title_effect]\",\n        \n        # Layer 4: Credits audio effects  \n        \"[3:0]volume=0.3[credits_effect]\",\n        \n        # Mix title and credits effects\n        \"[title_effect][credits_effect]amix=inputs=2:duration=longest[effects]\",\n        \n        # Mix native audio with voice\n        \"[native][voice]amix=inputs=2:duration=longest[main]\",\n        \n        # Mix main with effects\n        \"[main][effects]amix=inputs=2:duration=longest[final]\"\n    ]\n    \n    cmd_mix = [\n        \"ffmpeg\",\n        \"-i\", native_audio,\n        \"-i\", voice_audio,\n        \"-i\", title_padded,\n        \"-i\", credits_padded,\n        \"-filter_complex\", \";\".join(filter_complex),\n        \"-map\", \"[final]\",\n        \"-y\",\n        final_mix\n    ]\n    \n    print(\"Creating final three-layer audio mix...\")\n    subprocess.run(cmd_mix)\n    \n    # Verify the final mix duration\n    if os.path.exists(final_mix):\n        final_duration = get_duration(final_mix)\n        print(f\"Final audio mix duration: {final_duration:.2f} seconds\")\n    \n    return final_mix\n\n# Add audio to video\ndef add_audio_to_video(video_file, audio_file):\n    cmd = [\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"192k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Three-layer audio mix with exact 2-second voice delay\",\n        \"-y\",\n        OUTPUT_FILE\n    ]\n    \n    print(\"Adding final audio mix to video...\")\n    subprocess.run(cmd)\n    \n    return OUTPUT_FILE if os.path.exists(OUTPUT_FILE) else None\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING THREE-LAYER AUDIO MIX WITH EXACT 2-SECOND VOICE DELAY =====\\n\")\n    \n    # Find source files\n    voice_file, original_file = find_source_files()\n    if not voice_file:\n        return\n    \n    # Create title and credits\n    title_file = create_title()\n    credits_file = create_credits()\n    \n    if not title_file or not credits_file:\n        print(\"Error creating title or credits\")\n        return\n    \n    # Extract audio components\n    audio_info = extract_audio(voice_file, original_file)\n    \n    # Create special audio effects\n    special_audio = create_special_audio()\n    \n    # Extract video only\n    video_only = extract_video(original_file)\n    if not video_only:\n        print(\"Error extracting video\")\n        return\n    \n    # Prepare voice audio with exact 2-second delay using concatenation\n    processed_voice = prepare_voice_audio(audio_info)\n    if not os.path.exists(processed_voice):\n        print(\"Error processing voice audio\")\n        return\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(title_file, video_only, credits_file)\n    if not concat_video:\n        print(\"Error concatenating videos\")\n        return\n    \n    # Create final audio mix with all three layers\n    final_mix = create_final_audio_mix(processed_voice, audio_info['native'], special_audio)\n    if not final_mix:\n        print(\"Error creating final audio mix\")\n        return\n    \n    # Add audio to video\n    final_output = add_audio_to_video(concat_video, final_mix)\n    if not final_output:\n        print(\"Error adding audio to video\")\n        return\n    \n    print(f\"\\n===== SUCCESS! =====\")\n    print(f\"Final video created: {OUTPUT_FILE}\")\n    print(\"Improvements:\")\n    print(\"- THREE LAYER AUDIO:\")\n    print(\"  1. Native audio from original video (starts at time 0)\")\n    print(\"  2. Voice audio starts EXACTLY 2 seconds after video begins\")\n    print(\"  3. Title/credits audio effects blend underneath the other layers\")\n    print(\"- Voice audio speed-adjusted to match the 1:40 (100.91 seconds) timing\")\n    print(\"- Dynamic title (5s) and credits (5s) with fade effects\")\n    print(\"- Perfect synchronization of all three audio layers\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_FIXED_TIMING.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4",
      "WhereYouGoWhenYouLeave_BroadcastPerfection.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "title.mp4",
      "credits.mp4",
      "voice_audio.wav",
      "native_audio.wav",
      "title_audio.wav",
      "credits_audio.wav",
      "voice_speed_adjusted.wav",
      "silent_buffer_2s.wav",
      "voice_delayed.wav",
      "video_only.mp4",
      "concatenated.mp4",
      "final_mix.wav",
      "title_padded.wav",
      "credits_padded.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+60:",
      "if(lt(t,0.8),(t-0.3)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+30:",
      "if(lt(t,1.0),(t-0.5)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+100:",
      "if(lt(t,1.5),(t-1)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      ")\n    subprocess.run(cmd_voice)\n    \n    # 2. Extract original/native audio\n    native_audio = os.path.join(TEMP_DIR, ",
      ")\n    subprocess.run(cmd_title)\n    \n    # Credits audio - has fade in/out\n    credits_audio = os.path.join(TEMP_DIR, ",
      "]\n    target_duration = VIDEO_DURATION  # 1:40 = 100.91 seconds\n    speed_factor = source_duration / target_duration\n    \n    print(f",
      "  3. Title/credits audio effects blend underneath the other layers"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd_voice"
      },
      {
        "type": "run",
        "snippet": "cmd_native"
      },
      {
        "type": "run",
        "snippet": "cmd, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "cmd_title"
      },
      {
        "type": "run",
        "snippet": "cmd_credits"
      },
      {
        "type": "run",
        "snippet": "cmd_speed"
      },
      {
        "type": "run",
        "snippet": "cmd_silence"
      },
      {
        "type": "run",
        "snippet": "cmd_concat"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "title_cmd"
      },
      {
        "type": "run",
        "snippet": "credits_cmd"
      },
      {
        "type": "run",
        "snippet": "cmd_mix"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/create_final_version.py",
    "size": 10220,
    "lines": 344,
    "source": "import os\nimport subprocess\nimport glob\nimport time\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_FINAL_FIX\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FINAL_FIXED.mp4\")\n\n# Ensure temp directory exists\nif not os.path.exists(TEMP_DIR):\n    os.makedirs(TEMP_DIR)\n    print(f\"Created directory: {TEMP_DIR}\")\n\n# Find the best voice track\ndef find_voice_track():\n    # Check for ExecutiveVoice first, then others\n    preferred_voices = [\n        \"ExecutiveVoice\",\n        \"UltraCrispDefinition\",\n        \"BroadcastPerfection\",\n        \"ConfidentPresence\"\n    ]\n    \n    for voice in preferred_voices:\n        voice_file = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{voice}.mp4\")\n        if os.path.exists(voice_file):\n            print(f\"Using voice track: {voice}\")\n            return voice_file\n    \n    # Fallback to original\n    fallback = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    if os.path.exists(fallback):\n        print(\"Using original assembly as fallback\")\n        return fallback\n    \n    print(\"No suitable voice track found\")\n    return None\n\n# Create title card\ndef create_title():\n    title_file = os.path.join(TEMP_DIR, \"title_card.mp4\")\n    \n    # Create a title with pulsing text and RGB color effect\n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=10\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO':fontcolor=white:fontsize=100:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-100:\"\n            \"enable='between(t,1,10)':alpha='sin(2*PI*t/5)'\"\n            \",\"\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHEN YOU LEAVE':fontcolor=white:fontsize=100:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+100:\"\n            \"enable='between(t,2,10)':alpha='sin(2*PI*(t-1)/5)'\"\n        ),\n        \"-c:v\", \"libx264\",\n        \"-pix_fmt\", \"yuv420p\",\n        \"-y\",\n        title_file\n    ]\n    \n    print(\"Creating title card...\")\n    subprocess.run(cmd)\n    \n    return title_file if os.path.exists(title_file) else None\n\n# Create end credits\ndef create_credits():\n    credits_file = os.path.join(TEMP_DIR, \"end_credits.mp4\")\n    \n    # Create scrolling credits\n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=10\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE\\\\n\\\\nA Multimedia Poetry Project\\\\n\\\\n\"\n            \"Audio by Cascade AI\\\\n\\\\nOriginal Poetry\\\\n\\\\n\u00a9 2025 RESURRECTING ATLANTIS':\"\n            \"fontcolor=white:fontsize=60:x=(w-text_w)/2:y=h-t*70:\"\n            \"line_spacing=20\"\n        ),\n        \"-c:v\", \"libx264\",\n        \"-pix_fmt\", \"yuv420p\",\n        \"-y\",\n        credits_file\n    ]\n    \n    print(\"Creating end credits...\")\n    subprocess.run(cmd)\n    \n    return credits_file if os.path.exists(credits_file) else None\n\n# Extract video without audio\ndef extract_video(input_video):\n    video_file = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-i\", input_video,\n        \"-c:v\", \"copy\",\n        \"-an\",\n        \"-y\",\n        video_file\n    ]\n    \n    print(\"Extracting video without audio...\")\n    subprocess.run(cmd)\n    \n    return video_file if os.path.exists(video_file) else None\n\n# Extract audio\ndef extract_audio(input_video):\n    audio_file = os.path.join(TEMP_DIR, \"audio.wav\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-i\", input_video,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\",\n        audio_file\n    ]\n    \n    print(\"Extracting audio...\")\n    subprocess.run(cmd)\n    \n    return audio_file if os.path.exists(audio_file) else None\n\n# Create concatenation file list\ndef create_concat_list(title_file, video_file, credits_file):\n    concat_file = os.path.join(TEMP_DIR, \"concat_list.txt\")\n    \n    with open(concat_file, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    return concat_file\n\n# Concatenate videos\ndef concatenate_videos(concat_list):\n    output_file = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\",\n        output_file\n    ]\n    \n    print(\"Concatenating videos...\")\n    subprocess.run(cmd)\n    \n    return output_file if os.path.exists(output_file) else None\n\n# Create buffer audio\ndef create_buffer_audio():\n    # Create start buffer\n    start_buffer = os.path.join(TEMP_DIR, \"start_buffer.wav\")\n    cmd1 = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.05*sin(1000*2*PI*t)*exp(-4*t):s=44100:d=3\",\n        \"-y\",\n        start_buffer\n    ]\n    print(\"Creating start buffer audio...\")\n    subprocess.run(cmd1)\n    \n    # Create end buffer\n    end_buffer = os.path.join(TEMP_DIR, \"end_buffer.wav\")\n    cmd2 = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.03*sin(800*2*PI*t)*exp(-3*t):s=44100:d=3\",\n        \"-y\",\n        end_buffer\n    ]\n    print(\"Creating end buffer audio...\")\n    subprocess.run(cmd2)\n    \n    return (start_buffer, end_buffer) if os.path.exists(start_buffer) and os.path.exists(end_buffer) else (None, None)\n\n# Create title and credits audio\ndef create_title_credits_audio():\n    # Create title audio\n    title_audio = os.path.join(TEMP_DIR, \"title_audio.wav\")\n    cmd1 = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.3*sin(2*PI*200*t)+0.2*sin(2*PI*300*t):s=44100:d=10\",\n        \"-af\", \"afade=t=in:st=0:d=2,afade=t=out:st=8:d=2\",\n        \"-y\",\n        title_audio\n    ]\n    print(\"Creating title audio...\")\n    subprocess.run(cmd1)\n    \n    # Create credits audio\n    credits_audio = os.path.join(TEMP_DIR, \"credits_audio.wav\")\n    cmd2 = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.2*sin(2*PI*150*t)+0.1*sin(2*PI*225*t):s=44100:d=10\",\n        \"-af\", \"afade=t=in:st=0:d=2,afade=t=out:st=8:d=2\",\n        \"-y\",\n        credits_audio\n    ]\n    print(\"Creating credits audio...\")\n    subprocess.run(cmd2)\n    \n    return (title_audio, credits_audio) if os.path.exists(title_audio) and os.path.exists(credits_audio) else (None, None)\n\n# Create properly timed audio file\ndef create_timed_audio(main_audio, title_audio, credits_audio, start_buffer, end_buffer):\n    # Create audio that starts 2 seconds earlier\n    shifted_audio = os.path.join(TEMP_DIR, \"shifted_audio.wav\")\n    cmd = [\n        \"ffmpeg\",\n        \"-i\", main_audio,\n        \"-af\", \"adelay=8000|8000\",  # Start 8 seconds in, during title sequence\n        \"-y\",\n        shifted_audio\n    ]\n    print(\"Creating shifted audio...\")\n    subprocess.run(cmd)\n    \n    # Combine all audio segments\n    combined_audio = os.path.join(TEMP_DIR, \"combined_audio.wav\")\n    cmd2 = [\n        \"ffmpeg\",\n        \"-i\", start_buffer,\n        \"-i\", title_audio,\n        \"-i\", shifted_audio,\n        \"-i\", credits_audio,\n        \"-i\", end_buffer,\n        \"-filter_complex\", \n        \"[0:0][1:0][2:0][3:0][4:0]concat=n=5:v=0:a=1[out]\",\n        \"-map\", \"[out]\",\n        \"-y\",\n        combined_audio\n    ]\n    print(\"Combining audio segments...\")\n    subprocess.run(cmd2)\n    \n    return combined_audio if os.path.exists(combined_audio) else None\n\n# Add audio to video\ndef add_audio_to_video(video_file, audio_file):\n    cmd = [\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-y\",\n        OUTPUT_FILE\n    ]\n    \n    print(\"Adding audio to video...\")\n    subprocess.run(cmd)\n    \n    return OUTPUT_FILE if os.path.exists(OUTPUT_FILE) else None\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING FINAL VERSION WITH FIXED AUDIO SYNC =====\\n\")\n    \n    # Step 1: Find the best voice track\n    voice_file = find_voice_track()\n    if not voice_file:\n        return\n    \n    # Step 2: Create title and end credits\n    title_file = create_title()\n    credits_file = create_credits()\n    \n    if not title_file or not credits_file:\n        print(\"Error creating title or credits\")\n        return\n    \n    # Step 3: Extract video without audio\n    video_only = extract_video(voice_file)\n    if not video_only:\n        print(\"Error extracting video\")\n        return\n    \n    # Step 4: Extract audio\n    audio_file = extract_audio(voice_file)\n    if not audio_file:\n        print(\"Error extracting audio\")\n        return\n    \n    # Step 5: Create concat list and concatenate videos\n    concat_list = create_concat_list(title_file, video_only, credits_file)\n    concat_video = concatenate_videos(concat_list)\n    if not concat_video:\n        print(\"Error concatenating videos\")\n        return\n    \n    # Step 6: Create buffer, title, and credits audio\n    start_buffer, end_buffer = create_buffer_audio()\n    title_audio, credits_audio = create_title_credits_audio()\n    \n    if not start_buffer or not end_buffer or not title_audio or not credits_audio:\n        print(\"Error creating buffer or title/credits audio\")\n        return\n    \n    # Step 7: Create properly timed audio file\n    combined_audio = create_timed_audio(audio_file, title_audio, credits_audio, start_buffer, end_buffer)\n    if not combined_audio:\n        print(\"Error creating combined audio\")\n        return\n    \n    # Step 8: Add audio to video\n    final_file = add_audio_to_video(concat_video, combined_audio)\n    if not final_file:\n        print(\"Error adding audio to video\")\n        return\n    \n    print(f\"\\n===== SUCCESS! =====\")\n    print(f\"Final video created: {OUTPUT_FILE}\")\n    print(\"Improvements:\")\n    print(\"- Added 10-second dynamic title animation with pulsing text\")\n    print(\"- Fixed audio sync (voice starts during title sequence)\")\n    print(\"- Added buffer audio at beginning and end\")\n    print(\"- Added 10-second scrolling end credits\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_FINAL_FIXED.mp4",
      "WhereYouGoWhenYouLeave_{voice}.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "title_card.mp4",
      "end_credits.mp4",
      "video_only.mp4",
      "audio.wav",
      "concatenated.mp4",
      "start_buffer.wav",
      "end_buffer.wav",
      "title_audio.wav",
      "credits_audio.wav",
      "shifted_audio.wav",
      "combined_audio.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-100:",
      "sin(2*PI*t/5)",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+100:",
      "sin(2*PI*(t-1)/5)",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "fontcolor=white:fontsize=60:x=(w-text_w)/2:y=h-t*70:",
      "Error creating buffer or title/credits audio"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd1"
      },
      {
        "type": "run",
        "snippet": "cmd2"
      },
      {
        "type": "run",
        "snippet": "cmd1"
      },
      {
        "type": "run",
        "snippet": "cmd2"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd2"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "glob",
      "time"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/delayed_voice_after_native.py",
    "size": 14126,
    "lines": 398,
    "source": "import os\nimport subprocess\nimport shutil\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_NATIVE_FIRST\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_NATIVE_FIRST.mp4\")\n\n# Timing constants\nINTRO_LENGTH = 7\nOUTRO_LENGTH = 8\nVOICE_AFTER_NATIVE_DELAY = 2  # Voice starts 2 seconds after native audio\nMAIN_LENGTH = 100.91  # Exact 1:40 length as per memory requirements\n\n# Clean and create temp directory\nif os.path.exists(TEMP_DIR):\n    shutil.rmtree(TEMP_DIR)\nos.makedirs(TEMP_DIR)\nprint(f\"Using directory: {TEMP_DIR}\")\n\n# Find original source files\ndef find_source_files():\n    # Find voice track\n    voice_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\")\n    if os.path.exists(voice_file):\n        print(f\"Using voice track: {os.path.basename(voice_file)}\")\n    else:\n        voice_file = None\n        print(\"Voice track not found\")\n    \n    # Find original video with native audio\n    original_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\")\n    if not os.path.exists(original_file):\n        original_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n        \n    if os.path.exists(original_file):\n        print(f\"Using original video: {os.path.basename(original_file)}\")\n    else:\n        original_file = voice_file\n        print(\"Using voice track as video source\")\n    \n    return voice_file, original_file\n\n# Create simple intro and outro\ndef create_title_and_credits():\n    # Create intro\n    intro_file = os.path.join(TEMP_DIR, \"intro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=80:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\", \n        \"-y\", intro_file\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro\")\n    \n    # Create outro\n    outro_file = os.path.join(TEMP_DIR, \"outro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\", \n        \"-y\", outro_file\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro\")\n    \n    return intro_file, outro_file\n\n# Extract audio and video\ndef extract_components(voice_file, original_file):\n    # Extract voice audio exactly as is\n    voice_audio = os.path.join(TEMP_DIR, \"voice_original.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ])\n    print(\"Extracted original voice audio without any processing\")\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ])\n    print(\"Extracted native audio\")\n    \n    # Extract video only\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ])\n    print(\"Extracted video without audio\")\n    \n    return voice_audio, native_audio, video_only\n\n# Add delay to voice so it starts AFTER native audio\ndef position_audio(voice_audio, native_audio):\n    # Native audio has no delay after intro\n    # Voice audio has VOICE_AFTER_NATIVE_DELAY seconds delay after native audio starts\n    \n    # Create silence for voice delay\n    voice_silence = os.path.join(TEMP_DIR, \"voice_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={VOICE_AFTER_NATIVE_DELAY}\",\n        \"-y\", voice_silence\n    ])\n    print(f\"Created {VOICE_AFTER_NATIVE_DELAY}-second silence for voice delay\")\n    \n    # Add delay to voice (after native starts)\n    voice_delayed = os.path.join(TEMP_DIR, \"voice_delayed.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_silence,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_delayed\n    ])\n    print(f\"Added {VOICE_AFTER_NATIVE_DELAY}-second delay to voice after native audio\")\n    \n    return voice_delayed, native_audio\n\n# Create ambient background and audio effects\ndef create_audio_effects():\n    # Simple ambient background\n    ambient_file = os.path.join(TEMP_DIR, \"ambient.wav\")\n    total_length = INTRO_LENGTH + MAIN_LENGTH + OUTRO_LENGTH\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=pink:amplitude=0.01:duration={total_length}\",\n        \"-af\", \"volume=0.1\",\n        \"-y\", ambient_file\n    ])\n    print(f\"Created {total_length}-second ambient background\")\n    \n    # Simple intro audio effect\n    intro_audio = os.path.join(TEMP_DIR, \"intro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.15\",\n        \"-y\", intro_audio\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro audio effect\")\n    \n    # Simple outro audio effect\n    outro_audio = os.path.join(TEMP_DIR, \"outro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.15\",\n        \"-y\", outro_audio\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro audio effect\")\n    \n    return ambient_file, intro_audio, outro_audio\n\n# Concatenate videos\ndef concatenate_videos(intro_file, main_file, outro_file):\n    concat_file = os.path.join(TEMP_DIR, \"concat.txt\")\n    with open(concat_file, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(main_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concat_video.mp4\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    print(\"Concatenated intro + main video + outro\")\n    \n    return concat_video\n\n# Position all audio layers correctly\ndef position_all_audio(voice_delayed, native_audio, ambient, intro_audio, outro_audio):\n    # Timing for layering all audio elements:\n    # 0-7s: Intro section (title screen)\n    # 7s: Native audio starts\n    # 9s: Voice starts (2s after native)\n    # 7s+100.91s=107.91s: Outro begins\n    # 115.91s: Video ends\n    \n    # Create silence for intro length (to position native audio)\n    intro_silence = os.path.join(TEMP_DIR, \"intro_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={INTRO_LENGTH}\",\n        \"-y\", intro_silence\n    ])\n    \n    # Position native audio after intro\n    native_positioned = os.path.join(TEMP_DIR, \"native_positioned.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", intro_silence,\n        \"-i\", native_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", native_positioned\n    ])\n    print(f\"Positioned native audio to start after {INTRO_LENGTH}s intro\")\n    \n    # Position voice after intro + voice delay\n    voice_positioned = os.path.join(TEMP_DIR, \"voice_positioned.wav\")\n    intro_plus_delay = INTRO_LENGTH + VOICE_AFTER_NATIVE_DELAY\n    voice_start_silence = os.path.join(TEMP_DIR, \"voice_start_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={intro_plus_delay}\",\n        \"-y\", voice_start_silence\n    ])\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_start_silence,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_positioned\n    ])\n    print(f\"Positioned voice to start {intro_plus_delay}s from beginning (intro+{VOICE_AFTER_NATIVE_DELAY}s)\")\n    \n    # Position intro audio at start\n    intro_positioned = os.path.join(TEMP_DIR, \"intro_positioned.wav\")\n    post_intro_silence = os.path.join(TEMP_DIR, \"post_intro_silence.wav\")\n    post_intro_length = MAIN_LENGTH + OUTRO_LENGTH\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={post_intro_length}\",\n        \"-y\", post_intro_silence\n    ])\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", intro_audio,\n        \"-i\", post_intro_silence,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", intro_positioned\n    ])\n    print(\"Positioned intro audio at beginning\")\n    \n    # Position outro audio at end\n    outro_positioned = os.path.join(TEMP_DIR, \"outro_positioned.wav\")\n    pre_outro_silence = os.path.join(TEMP_DIR, \"pre_outro_silence.wav\")\n    pre_outro_length = INTRO_LENGTH + MAIN_LENGTH\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={pre_outro_length}\",\n        \"-y\", pre_outro_silence\n    ])\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", pre_outro_silence,\n        \"-i\", outro_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", outro_positioned\n    ])\n    print(\"Positioned outro audio at end\")\n    \n    return voice_positioned, native_positioned, ambient, intro_positioned, outro_positioned\n\n# Create final audio mix\ndef create_audio_mix(voice_positioned, native_positioned, ambient, intro_audio, outro_audio):\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Mix all audio layers with proper volumes\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_positioned,\n        \"-i\", native_positioned,\n        \"-i\", ambient,\n        \"-i\", intro_audio,\n        \"-i\", outro_audio,\n        \"-filter_complex\",\n        \"[0:0]volume=1.0[voice];\" +\n        \"[1:0]volume=0.7[native];\" +\n        \"[2:0]volume=0.1[ambient];\" +\n        \"[3:0]volume=0.15[intro];\" +\n        \"[4:0]volume=0.15[outro];\" +\n        \"[voice][native]amix=inputs=2:duration=longest[content];\" +\n        \"[ambient][intro]amix=inputs=2:duration=longest[amb_fx1];\" +\n        \"[amb_fx1][outro]amix=inputs=2:duration=longest[amb_fx];\" +\n        \"[content][amb_fx]amix=inputs=2:duration=longest[out]\",\n        \"-map\", \"[out]\",\n        \"-y\", final_mix\n    ])\n    print(\"Created final audio mix\")\n    \n    # Check final mix length\n    cmd_duration = [\n        \"ffprobe\", \"-v\", \"error\",\n        \"-show_entries\", \"format=duration\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        final_mix\n    ]\n    mix_duration = float(subprocess.run(cmd_duration, capture_output=True, text=True).stdout.strip())\n    print(f\"Final audio mix duration: {mix_duration:.2f} seconds\")\n    \n    return final_mix\n\n# Add final audio to video\ndef add_audio_to_video(video_file, audio_file):\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Final version with voice starting 2 seconds after native audio\",\n        \"-y\", OUTPUT_FILE\n    ])\n    print(f\"Added audio to video: {OUTPUT_FILE}\")\n    \n    if os.path.exists(OUTPUT_FILE):\n        cmd = [\n            \"ffprobe\", \"-v\", \"error\",\n            \"-show_entries\", \"format=duration\",\n            \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n            OUTPUT_FILE\n        ]\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        duration = float(result.stdout.strip())\n        print(f\"Final video duration: {duration:.2f} seconds\")\n        return True\n    else:\n        return False\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING VERSION WITH VOICE STARTING 2s AFTER NATIVE AUDIO =====\\n\")\n    \n    # Find sources\n    voice_file, original_file = find_source_files()\n    if not voice_file or not original_file:\n        print(\"Error: Source files not found\")\n        return False\n    \n    # Create intro/outro\n    intro_file, outro_file = create_title_and_credits()\n    \n    # Extract components\n    voice_audio, native_audio, video_only = extract_components(voice_file, original_file)\n    \n    # Position voice to start after native audio\n    voice_delayed, native_audio = position_audio(voice_audio, native_audio)\n    \n    # Create audio effects\n    ambient_audio, intro_audio, outro_audio = create_audio_effects()\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Position all audio layers\n    voice_positioned, native_positioned, ambient, intro_positioned, outro_positioned = position_all_audio(\n        voice_delayed, native_audio, ambient_audio, intro_audio, outro_audio\n    )\n    \n    # Create final audio mix\n    final_mix = create_audio_mix(voice_positioned, native_positioned, ambient, intro_positioned, outro_positioned)\n    \n    # Add audio to video\n    success = add_audio_to_video(concat_video, final_mix)\n    \n    if success:\n        print(\"\\n===== SUCCESS! =====\")\n        print(f\"Final video created: {OUTPUT_FILE}\")\n        print(\"\\nFinal specifications:\")\n        print(\"- Original voice track preserved exactly as is\")\n        print(f\"- Native audio starts immediately after the {INTRO_LENGTH}s intro\")\n        print(f\"- Voice starts {VOICE_AFTER_NATIVE_DELAY}s after native audio begins\")\n        print(f\"- Intro length: {INTRO_LENGTH} seconds\")\n        print(f\"- Main content: {MAIN_LENGTH} seconds (1:40)\")\n        print(f\"- Outro length: {OUTRO_LENGTH} seconds\")\n        print(\"- No pitch or other audio processing applied to voice\")\n        return True\n    else:\n        print(\"Error: Failed to create final video\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_NATIVE_FIRST.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "intro.mp4",
      "outro.mp4",
      "voice_original.wav",
      "native_audio.wav",
      "video_only.mp4",
      "voice_silence.wav",
      "voice_delayed.wav",
      "ambient.wav",
      "intro_audio.wav",
      "outro_audio.wav",
      "concat_video.mp4",
      "intro_silence.wav",
      "native_positioned.wav",
      "voice_positioned.wav",
      "voice_start_silence.wav",
      "intro_positioned.wav",
      "post_intro_silence.wav",
      "outro_positioned.wav",
      "pre_outro_silence.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=80:x=(w-text_w)/2:y=(h-text_h)/2",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2",
      ")\n        return False\n    \n    # Create intro/outro\n    intro_file, outro_file = create_title_and_credits()\n    \n    # Extract components\n    voice_audio, native_audio, video_only = extract_components(voice_file, original_file)\n    \n    # Position voice to start after native audio\n    voice_delayed, native_audio = position_audio(voice_audio, native_audio)\n    \n    # Create audio effects\n    ambient_audio, intro_audio, outro_audio = create_audio_effects()\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Position all audio layers\n    voice_positioned, native_positioned, ambient, intro_positioned, outro_positioned = position_all_audio(\n        voice_delayed, native_audio, ambient_audio, intro_audio, outro_audio\n    )\n    \n    # Create final audio mix\n    final_mix = create_audio_mix(voice_positioned, native_positioned, ambient, intro_positioned, outro_positioned)\n    \n    # Add audio to video\n    success = add_audio_to_video(concat_video, final_mix)\n    \n    if success:\n        print("
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING A"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={VOICE_AFTER_NATIVE_DELAY}\",\n        \"-y\", voice_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_silence,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=pink:amplitude=0.01:duration={total_length}\",\n        \"-af\", \"volume=0.1\",\n        \"-y\", ambient_file\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.15\",\n        \""
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.15\",\n      "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={INTRO_LENGTH}\",\n        \"-y\", intro_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", intro_silence,\n        \"-i\", native_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", native_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={intro_plus_delay}\",\n        \"-y\", voice_start_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_start_silence,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={post_intro_length}\",\n        \"-y\", post_intro_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", intro_audio,\n        \"-i\", post_intro_silence,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", intro_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={pre_outro_length}\",\n        \"-y\", pre_outro_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", pre_outro_silence,\n        \"-i\", outro_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", outro_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_positioned,\n        \"-i\", native_positioned,\n        \"-i\", ambient,\n        \"-i\", intro_audio,\n        \"-i\", outro_audio,\n        \"-filter_complex\",\n        \"[0"
      },
      {
        "type": "run",
        "snippet": "cmd_duration, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-s"
      },
      {
        "type": "run",
        "snippet": "cmd, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/create_final_video_with_titles.py",
    "size": 9867,
    "lines": 319,
    "source": "import os\nimport subprocess\nimport json\nimport glob\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_FINAL\")\nASSETS_DIR = os.path.join(LIZARD_DIR, \"ASSETS\")\n\n# Ensure necessary directories exist\ndef ensure_directories():\n    for dir_path in [TEMP_DIR, ASSETS_DIR]:\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n            print(f\"Created directory: {dir_path}\")\n\n# Get the best voice track from the enhanced versions\ndef get_best_voice_track():\n    # Priority order of voice tracks (from best to acceptable)\n    voice_priorities = [\n        \"ExecutiveVoice\", \n        \"UltraCrispDefinition\", \n        \"BroadcastPerfection\", \n        \"ConfidentPresence\", \n        \"CrystalClarity\"\n    ]\n    \n    for voice in voice_priorities:\n        voice_file = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{voice}.mp4\")\n        if os.path.exists(voice_file):\n            print(f\"Selected voice track: {voice}\")\n            return voice_file\n    \n    # Fallback to original if none of the enhanced versions are found\n    fallback = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    if os.path.exists(fallback):\n        print(\"Using original assembly as fallback\")\n        return fallback\n    \n    raise FileNotFoundError(\"No suitable voice track found\")\n\n# Create title animation with Saul Bass / John Whitney style\ndef create_title_animation():\n    title_file = os.path.join(TEMP_DIR, \"title_animation.mp4\")\n    \n    # Generate a geometric animation with text\n    title_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'color=c=black:s=1920x1080:r=24:d=5',\n        '-vf', \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=72:\"\n        \"x=(w-text_w)/2:y=(h-text_h)/2:enable='between(t,1,5)'\"\n        \",geq=r='X/W*255':g='(Y/H)*255':b='128'\"\n        \",format=yuv420p\",\n        '-c:v', 'libx264',\n        '-preset', 'medium',\n        '-crf', '18',\n        '-y',\n        title_file\n    ]\n    \n    print(\"Creating title animation...\")\n    subprocess.run(title_cmd)\n    \n    return title_file if os.path.exists(title_file) else None\n\n# Create end credits animation\ndef create_end_credits():\n    credits_file = os.path.join(TEMP_DIR, \"end_credits.mp4\")\n    \n    # Generate end credits with scrolling text\n    credits_text = (\n        \"WHERE YOU GO WHEN YOU LEAVE\\\\n\\\\n\"\n        \"A Multimedia Poetry Project\\\\n\\\\n\"\n        \"Audio Processing and Visual Assembly\\\\n\"\n        \"by Cascade AI\\\\n\\\\n\"\n        \"Original Poetry and Concept\\\\n\"\n        \"\u00a9 2025\\\\n\\\\n\"\n        \"Resurrecting Atlantis\"\n    )\n    \n    credits_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'color=c=black:s=1920x1080:r=24:d=5',\n        '-vf', \n        f\"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        f\"text='{credits_text}':fontcolor=white:fontsize=48:\"\n        f\"x=(w-text_w)/2:y=h-10*t*12:line_spacing=12\",\n        '-c:v', 'libx264',\n        '-preset', 'medium',\n        '-crf', '18',\n        '-y',\n        credits_file\n    ]\n    \n    print(\"Creating end credits...\")\n    subprocess.run(credits_cmd)\n    \n    return credits_file if os.path.exists(credits_file) else None\n\n# Extract audio from the voice track\ndef extract_audio(voice_file):\n    audio_file = os.path.join(TEMP_DIR, \"voice_audio.wav\")\n    \n    extract_cmd = [\n        'ffmpeg',\n        '-i', voice_file,\n        '-vn',\n        '-acodec', 'pcm_s16le',\n        '-y',\n        audio_file\n    ]\n    \n    print(\"Extracting audio from voice track...\")\n    subprocess.run(extract_cmd)\n    \n    return audio_file if os.path.exists(audio_file) else None\n\n# Create title sound using FFmpeg's synthesizers\ndef create_title_sound():\n    title_sound = os.path.join(TEMP_DIR, \"title_sound.wav\")\n    \n    # Create a synthetic tone sequence\n    sound_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.1:c=pink:d=5',\n        '-filter_complex',\n        'aeval=0.8*sin(2*PI*t*110)+0.2*sin(2*PI*t*220):c=same,'\n        'apad=pad_dur=5,'\n        'afade=t=in:st=0:d=0.5,'\n        'afade=t=out:st=4.5:d=0.5,'\n        'aformat=sample_fmts=s16:sample_rates=44100:channel_layouts=mono',\n        '-y',\n        title_sound\n    ]\n    \n    print(\"Creating title sound...\")\n    subprocess.run(sound_cmd)\n    \n    return title_sound if os.path.exists(title_sound) else None\n\n# Create end credits sound\ndef create_credits_sound():\n    credits_sound = os.path.join(TEMP_DIR, \"credits_sound.wav\")\n    \n    # Create a synthetic tone sequence for credits\n    sound_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'sine=f=146.83:d=5',\n        '-filter_complex',\n        'aeval=0.5*sin(2*PI*t*146.83)+0.3*sin(2*PI*t*220)+0.2*sin(2*PI*t*293.66):c=same,'\n        'apad=pad_dur=5,'\n        'afade=t=in:st=0:d=0.5,'\n        'afade=t=out:st=4.5:d=0.5,'\n        'aformat=sample_fmts=s16:sample_rates=44100:channel_layouts=mono',\n        '-y',\n        credits_sound\n    ]\n    \n    print(\"Creating credits sound...\")\n    subprocess.run(sound_cmd)\n    \n    return credits_sound if os.path.exists(credits_sound) else None\n\n# Get video information\ndef get_video_info(video_file):\n    cmd = [\n        'ffprobe',\n        '-v', 'error',\n        '-select_streams', 'v:0',\n        '-show_entries', 'stream=width,height,r_frame_rate,duration',\n        '-of', 'json',\n        video_file\n    ]\n    \n    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n    info = json.loads(result.stdout)\n    \n    # Extract frame rate as a float\n    frame_rate_parts = info['streams'][0]['r_frame_rate'].split('/')\n    frame_rate = float(frame_rate_parts[0]) / float(frame_rate_parts[1])\n    \n    return {\n        'width': int(info['streams'][0]['width']),\n        'height': int(info['streams'][0]['height']),\n        'frame_rate': frame_rate,\n        'duration': float(info['streams'][0]['duration'])\n    }\n\n# Assemble final video with fixed audio sync\ndef assemble_final_video(voice_file, title_file, title_sound, credits_file, credits_sound):\n    final_output = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_Final_WithTitles.mp4\")\n    \n    # Extract video without audio\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    video_cmd = [\n        'ffmpeg',\n        '-i', voice_file,\n        '-c:v', 'copy',\n        '-an',\n        '-y',\n        video_only\n    ]\n    \n    print(\"Extracting video without audio...\")\n    subprocess.run(video_cmd)\n    \n    # Extract audio\n    audio_file = extract_audio(voice_file)\n    if not audio_file:\n        print(\"Error extracting audio\")\n        return False\n    \n    # Create list file for concatenation\n    file_list = os.path.join(TEMP_DIR, \"concat_list.txt\")\n    with open(file_list, 'w') as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_only)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    # Create a complex filter to fix audio sync and merge audio files\n    filter_complex = [\n        # Delay audio by 2 seconds to fix sync issue\n        \"[1:a]adelay=2000|2000[delayed_audio]\",\n        # Concatenate title sound, delayed audio, and credits sound\n        \"[0:a][delayed_audio][2:a]concat=n=3:v=0:a=1[aout]\"\n    ]\n    \n    # Final assembly command\n    final_cmd = [\n        'ffmpeg',\n        '-i', title_sound,\n        '-i', audio_file,\n        '-i', credits_sound,\n        '-f', 'concat',\n        '-safe', '0',\n        '-i', file_list,\n        '-filter_complex', ';'.join(filter_complex),\n        '-map', '3:v',\n        '-map', '[aout]',\n        '-c:v', 'libx264',\n        '-c:a', 'aac',\n        '-b:a', '192k',\n        '-shortest',\n        '-metadata', 'title=Where You Go When You Leave',\n        '-metadata', 'comment=Final version with titles and synchronized audio',\n        '-y',\n        final_output\n    ]\n    \n    print(\"Assembling final video with titles and synchronized audio...\")\n    subprocess.run(final_cmd)\n    \n    if os.path.exists(final_output):\n        print(f\"\u2713 Final video created: {final_output}\")\n        return True\n    \n    print(\"Error creating final video\")\n    return False\n\ndef main():\n    print(\"Creating final video with titles, credits, and fixed audio sync...\")\n    \n    # Setup directories\n    ensure_directories()\n    \n    # Get the best voice track\n    try:\n        voice_file = get_best_voice_track()\n    except FileNotFoundError as e:\n        print(str(e))\n        return False\n    \n    # Create title animation and sound\n    title_file = create_title_animation()\n    title_sound = create_title_sound()\n    \n    if not title_file or not title_sound:\n        print(\"Error creating title animation or sound\")\n        return False\n    \n    # Create end credits and sound\n    credits_file = create_end_credits()\n    credits_sound = create_credits_sound()\n    \n    if not credits_file or not credits_sound:\n        print(\"Error creating end credits or sound\")\n        return False\n    \n    # Assemble the final video\n    success = assemble_final_video(voice_file, title_file, title_sound, credits_file, credits_sound)\n    \n    if success:\n        print(\"Final video creation complete!\")\n        \n        # Get video info to display for user\n        final_output = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_Final_WithTitles.mp4\")\n        info = get_video_info(final_output)\n        \n        print(\"\\nFinal Video Information:\")\n        print(f\"Resolution: {info['width']}x{info['height']}\")\n        print(f\"Frame Rate: {info['frame_rate']} fps\")\n        print(f\"Duration: {info['duration']} seconds\")\n        print(f\"Audio: Synchronized with 2-second delay correction\")\n    else:\n        print(\"Error creating final video\")\n    \n    return success\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_{voice}.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "title_animation.mp4",
      "end_credits.mp4",
      "voice_audio.wav",
      "title_sound.wav",
      "credits_sound.wav",
      "WhereYouGoWhenYouLeave_Final_WithTitles.mp4",
      "video_only.mp4",
      "WhereYouGoWhenYouLeave_Final_WithTitles.mp4",
      "/Users/gaia/resurrecting atlantis",
      ")\n\n# Create title animation with Saul Bass / John Whitney style\ndef create_title_animation():\n    title_file = os.path.join(TEMP_DIR, ",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2:enable=",
      "X/W*255",
      "(Y/H)*255",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=h-10*t*12:line_spacing=12",
      ")\n    frame_rate = float(frame_rate_parts[0]) / float(frame_rate_parts[1])\n    \n    return {\n        "
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "title_cmd"
      },
      {
        "type": "run",
        "snippet": "credits_cmd"
      },
      {
        "type": "run",
        "snippet": "extract_cmd"
      },
      {
        "type": "run",
        "snippet": "sound_cmd"
      },
      {
        "type": "run",
        "snippet": "sound_cmd"
      },
      {
        "type": "run",
        "snippet": "cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "video_cmd"
      },
      {
        "type": "run",
        "snippet": "final_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "glob"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "LIZARD/create_final_improved.py",
    "size": 18959,
    "lines": 503,
    "source": "import os\nimport subprocess\nimport json\nimport random\nimport math\nfrom datetime import datetime\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_IMPROVED\")\nASSETS_DIR = os.path.join(LIZARD_DIR, \"ASSETS\")\n\n# Ensure necessary directories exist\ndef ensure_directories():\n    for dir_path in [TEMP_DIR, ASSETS_DIR]:\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n            print(f\"Created directory: {dir_path}\")\n\n# Get the best voice track from the enhanced versions\ndef get_best_voice_track():\n    # Priority order of voice tracks (from best to acceptable)\n    voice_priorities = [\n        \"ExecutiveVoice\", \n        \"UltraCrispDefinition\", \n        \"BroadcastPerfection\", \n        \"ConfidentPresence\", \n        \"CrystalClarity\"\n    ]\n    \n    for voice in voice_priorities:\n        voice_file = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{voice}.mp4\")\n        if os.path.exists(voice_file):\n            print(f\"Selected voice track: {voice}\")\n            return voice_file\n    \n    # Fallback to original if none of the enhanced versions are found\n    fallback = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    if os.path.exists(fallback):\n        print(\"Using original assembly as fallback\")\n        return fallback\n    \n    raise FileNotFoundError(\"No suitable voice track found\")\n\n# Create dynamic title animation with Saul Bass / John Whitney style\ndef create_title_animation():\n    title_file = os.path.join(TEMP_DIR, \"title_animation.mp4\")\n    \n    # Generate a more dynamic geometric animation with moving text and elements\n    title_duration = 10  # Extended to 10 seconds\n    \n    # Create a complex filter for dynamic title animation\n    filter_complex = [\n        # Start with black background\n        \"[0:v]scale=1920:1080,setsar=1[bg]\",\n        \n        # Generate animated lines (John Whitney style)\n        \"[bg][1:v]overlay=(W-w)/2:(H-h)/2[withgrid]\",\n        \n        # Generate subtitle text with animation\n        \"[withgrid]drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='WHERE':fontcolor=white:fontsize=120:\"\n        \"x='if(lt(t,3),-(t)*300,if(lt(t,6),600-100*cos(t),600-30*sin(t)))':y='if(lt(t,2),1080/2-60,if(lt(t,4),1080/2-60+10*sin(t*4),1080/2-60+5*sin(t*2)))':'enable=between(t,1,10)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='YOU':fontcolor=white:fontsize=120:\"\n        \"x='if(lt(t,3.5),1920+600-(t)*300,if(lt(t,6.5),850+100*cos(t),850+30*sin(t)))':y='if(lt(t,2.5),1080/2-60,if(lt(t,4.5),1080/2-60+10*sin(t*4),1080/2-60+5*sin(t*2)))':'enable=between(t,1.5,10)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='GO':fontcolor=white:fontsize=120:\"\n        \"x='if(lt(t,4),-(t)*300,if(lt(t,7),1100-100*cos(t),1100-30*sin(t)))':y='if(lt(t,3),1080/2-60,if(lt(t,5),1080/2-60+10*sin(t*4),1080/2-60+5*sin(t*2)))':'enable=between(t,2,10)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='WHEN':fontcolor=white:fontsize=120:\"\n        \"x='if(lt(t,4.5),1920+600-(t)*300,if(lt(t,7.5),450+100*cos(t),450+30*sin(t)))':y='if(lt(t,3.5),1080/2+60,if(lt(t,5.5),1080/2+60+10*sin(t*4),1080/2+60+5*sin(t*2)))':'enable=between(t,2.5,10)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='YOU':fontcolor=white:fontsize=120:\"\n        \"x='if(lt(t,5),-(t)*300,if(lt(t,8),650-100*cos(t),650-30*sin(t)))':y='if(lt(t,4),1080/2+60,if(lt(t,6),1080/2+60+10*sin(t*4),1080/2+60+5*sin(t*2)))':'enable=between(t,3,10)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='LEAVE':fontcolor=white:fontsize=120:\"\n        \"x='if(lt(t,5.5),1920+600-(t)*300,if(lt(t,8.5),950+100*cos(t),950+30*sin(t)))':y='if(lt(t,4.5),1080/2+60,if(lt(t,6.5),1080/2+60+10*sin(t*4),1080/2+60+5*sin(t*2)))':'enable=between(t,3.5,10)',\"\n        \n        # Add subtitle text\n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='A MULTIMEDIA POETRY PROJECT':fontcolor=gray:fontsize=30:\"\n        \"x=(w-text_w)/2:y=h-120:alpha='if(lt(t,7),0,if(lt(t,8),(t-7)/1,1))',\"\n        \n        # Add year and credit\n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=gray:fontsize=20:\"\n        \"x=(w-text_w)/2:y=h-80:alpha='if(lt(t,8),0,if(lt(t,9),(t-8)/1,1))',\"\n        \n        # Add geometric elements (Saul Bass style)\n        \"geq=r='128+127*sin(2*PI*(X/W*8+Y/H*8+t/2))':g='128+127*sin(2*PI*(X/W*8+Y/H*8+t/2+1/3))':b='128+127*sin(2*PI*(X/W*8+Y/H*8+t/2+2/3))':a='if(lt(mod(hypot(X-W/2,Y-H/2)/80,2),1,if(lt(random(1),0.5),0.1,0))*if(lt(t,6),sin(t*PI/6),sin((10-t)*PI/4))'[with_elements]\",\n        \n        # Add final fade-in/out\n        \"[with_elements]fade=in:0:30,fade=out:550:30[final]\"\n    ]\n    \n    # Create a grid animation (John Whitney style)\n    grid_file = os.path.join(TEMP_DIR, \"grid.mp4\")\n    grid_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', f'nullsrc=size=1920x1080:duration={title_duration}:rate=24',\n        '-vf', \"mandelbrot=rate=24:size=1920x1080:start_scale=3:end_scale=3.5:start_x=-0.1:end_x=0.1:start_y=0:end_y=0.2:inner=mincost,format=yuva420p,colorlevels=romin=0:gomin=0:bomin=0:aomin=0:romax=0.5:gomax=0.5:bomax=0.5:aomax=0.3\",\n        '-t', str(title_duration),\n        '-y',\n        grid_file\n    ]\n    \n    print(\"Creating grid animation...\")\n    subprocess.run(grid_cmd)\n    \n    # Now create the title animation with complex filter\n    title_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', f'color=black:s=1920x1080:r=24:d={title_duration}',\n        '-i', grid_file,\n        '-filter_complex', ''.join(filter_complex),\n        '-map', '[final]',\n        '-c:v', 'libx264',\n        '-preset', 'medium',\n        '-crf', '18',\n        '-t', str(title_duration),\n        '-y',\n        title_file\n    ]\n    \n    print(\"Creating dynamic title animation...\")\n    subprocess.run(title_cmd)\n    \n    return title_file if os.path.exists(title_file) else None\n\n# Create end credits animation\ndef create_end_credits():\n    credits_file = os.path.join(TEMP_DIR, \"end_credits.mp4\")\n    \n    # Generate end credits with scrolling text and animated elements\n    credits_duration = 10  # Extended to 10 seconds\n    \n    # Create a more complex filter for end credits\n    filter_complex = [\n        # Start with black background\n        \"color=black:1920x1080:d=10[bg]\",\n        \n        # Add scrolling text with multiple styles\n        \"[bg]drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=72:\"\n        \"x=(w-text_w)/2:y='h-80*t':enable='between(t,0,4)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='A MULTIMEDIA POETRY PROJECT':fontcolor=white:fontsize=36:\"\n        \"x=(w-text_w)/2:y='h+60-80*t':enable='between(t,1,5)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='AUDIO PROCESSING AND VISUAL ASSEMBLY':fontcolor=white:fontsize=28:\"\n        \"x=(w-text_w)/2:y='h+140-80*t':enable='between(t,2,6)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='BY CASCADE AI':fontcolor=white:fontsize=28:\"\n        \"x=(w-text_w)/2:y='h+180-80*t':enable='between(t,2.5,6.5)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='ORIGINAL POETRY AND CONCEPT':fontcolor=white:fontsize=28:\"\n        \"x=(w-text_w)/2:y='h+260-80*t':enable='between(t,3.5,7.5)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='\u00a9 2025':fontcolor=white:fontsize=28:\"\n        \"x=(w-text_w)/2:y='h+300-80*t':enable='between(t,4,8)',\"\n        \n        \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n        \"text='RESURRECTING ATLANTIS':fontcolor=white:fontsize=40:\"\n        \"x=(w-text_w)/2:y='h+360-80*t':enable='between(t,5,9)',\"\n        \n        # Add animated elements (lines radiating from center)\n        \"geq=r='128+127*sin(2*PI*t/10)':g='128+127*sin(2*PI*t/10+2*PI/3)':b='128+127*sin(2*PI*t/10+4*PI/3)':\"\n        \"a='if(lt(mod(atan2(Y-H/2,X-W/2)+t/1.5,PI/16),0.02),if(lt(t,9.5),0.15*sin(t*PI/5),0.15*sin((10-t)*PI/0.5)),0)',\"\n        \n        # Add final fade-out\n        \"fade=in:0:24,fade=out:216:24[out]\"\n    ]\n    \n    credits_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'nullsrc=s=1920x1080:d=10:r=24',\n        '-filter_complex', ''.join(filter_complex),\n        '-map', '[out]',\n        '-c:v', 'libx264',\n        '-preset', 'medium',\n        '-crf', '18',\n        '-t', str(credits_duration),\n        '-y',\n        credits_file\n    ]\n    \n    print(\"Creating dynamic end credits...\")\n    subprocess.run(credits_cmd)\n    \n    return credits_file if os.path.exists(credits_file) else None\n\n# Create title sound using FFmpeg's synthesizers with more complexity\ndef create_title_sound():\n    title_sound = os.path.join(TEMP_DIR, \"title_sound.wav\")\n    \n    # Create a more complex synthetic sound sequence for title\n    filter_complex = [\n        # Create oscillator layers with modulation\n        \"aevalsrc=0.5*sin(2*PI*110*t)+0.3*sin(2*PI*110*1.5*t)+0.2*sin(2*PI*110*2*t)\",\n        # Add swoosh effects\n        \"aevalsrc=0.4*sin(2*PI*1000*(1-t/10))*exp(-2*t)\",\n        # Add noise layer\n        \"anoisesrc=a=0.15:c=pink:d=10\",\n        # Mix them together\n        \"amix=inputs=3\",\n        # Apply effects\n        \"aphaser=in_gain=0.5:out_gain=0.5:delay=2:decay=0.5:speed=0.5\",\n        \"aecho=0.8:0.7:40|60|80:0.6|0.4|0.2\",\n        \"afade=t=in:st=0:d=0.5,afade=t=out:st=9.5:d=0.5\",\n        # Format\n        \"aformat=sample_fmts=s16:sample_rates=44100:channel_layouts=stereo\"\n    ]\n    \n    # Join filters\n    filter_str = ','.join(filter_complex)\n    \n    sound_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anullsrc=channel_layout=stereo:sample_rate=44100:d=10',\n        '-filter_complex', filter_str,\n        '-y',\n        title_sound\n    ]\n    \n    print(\"Creating dynamic title sound...\")\n    subprocess.run(sound_cmd)\n    \n    return title_sound if os.path.exists(title_sound) else None\n\n# Create buffer audio for beginning and end\ndef create_buffer_audio():\n    buffer_start = os.path.join(TEMP_DIR, \"buffer_start.wav\")\n    buffer_end = os.path.join(TEMP_DIR, \"buffer_end.wav\")\n    \n    # Create ambient buffer sound for beginning\n    start_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.03:c=pink:d=3',\n        '-af', 'aformat=sample_fmts=s16:sample_rates=44100:channel_layouts=stereo,afade=t=in:st=0:d=1,afade=t=out:st=2:d=1',\n        '-y',\n        buffer_start\n    ]\n    \n    print(\"Creating start buffer audio...\")\n    subprocess.run(start_cmd)\n    \n    # Create ambient buffer sound for end\n    end_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anoisesrc=a=0.02:c=pink:d=3',\n        '-af', 'aformat=sample_fmts=s16:sample_rates=44100:channel_layouts=stereo,afade=t=in:st=0:d=1,afade=t=out:st=2:d=1',\n        '-y',\n        buffer_end\n    ]\n    \n    print(\"Creating end buffer audio...\")\n    subprocess.run(end_cmd)\n    \n    return (buffer_start, buffer_end) if os.path.exists(buffer_start) and os.path.exists(buffer_end) else (None, None)\n\n# Create end credits sound\ndef create_credits_sound():\n    credits_sound = os.path.join(TEMP_DIR, \"credits_sound.wav\")\n    \n    # Create a more complex synthetic sound for credits\n    filter_complex = [\n        # Create layered oscillators\n        \"aevalsrc=0.3*sin(2*PI*146.83*t)+0.2*sin(2*PI*220*t)+0.15*sin(2*PI*293.66*t)+0.1*sin(2*PI*440*t)\",\n        # Add atmospheric elements\n        \"aevalsrc=0.2*sin(2*PI*20*t)*sin(2*PI*0.5*t)\",\n        # Mix them\n        \"amix=inputs=2\",\n        # Apply effects\n        \"aecho=0.6:0.3:1000|1800:0.5|0.3\",\n        \"areverse,aphaser=in_gain=0.6:out_gain=0.6:delay=2:decay=0.6:speed=0.1,areverse\",\n        \"afade=t=in:st=0:d=1,afade=t=out:st=9:d=1\",\n        # Format\n        \"aformat=sample_fmts=s16:sample_rates=44100:channel_layouts=stereo\"\n    ]\n    \n    # Join filters\n    filter_str = ','.join(filter_complex)\n    \n    sound_cmd = [\n        'ffmpeg',\n        '-f', 'lavfi',\n        '-i', 'anullsrc=channel_layout=stereo:sample_rate=44100:d=10',\n        '-filter_complex', filter_str,\n        '-y',\n        credits_sound\n    ]\n    \n    print(\"Creating dynamic credits sound...\")\n    subprocess.run(sound_cmd)\n    \n    return credits_sound if os.path.exists(credits_sound) else None\n\n# Extract audio from the voice track\ndef extract_audio(voice_file):\n    audio_file = os.path.join(TEMP_DIR, \"voice_audio.wav\")\n    \n    extract_cmd = [\n        'ffmpeg',\n        '-i', voice_file,\n        '-vn',\n        '-acodec', 'pcm_s16le',\n        '-y',\n        audio_file\n    ]\n    \n    print(\"Extracting audio from voice track...\")\n    subprocess.run(extract_cmd)\n    \n    return audio_file if os.path.exists(audio_file) else None\n\n# Get video information\ndef get_video_info(video_file):\n    cmd = [\n        'ffprobe',\n        '-v', 'error',\n        '-select_streams', 'v:0',\n        '-show_entries', 'stream=width,height,r_frame_rate,duration',\n        '-of', 'json',\n        video_file\n    ]\n    \n    result = subprocess.run(cmd, stdout=subprocess.PIPE, text=True)\n    info = json.loads(result.stdout)\n    \n    # Extract frame rate as a float\n    frame_rate_parts = info['streams'][0]['r_frame_rate'].split('/')\n    frame_rate = float(frame_rate_parts[0]) / float(frame_rate_parts[1])\n    \n    return {\n        'width': int(info['streams'][0]['width']),\n        'height': int(info['streams'][0]['height']),\n        'frame_rate': frame_rate,\n        'duration': float(info['streams'][0]['duration'])\n    }\n\n# Assemble final video with fixed audio sync\ndef assemble_final_video(voice_file, title_file, title_sound, credits_file, credits_sound, buffer_start, buffer_end):\n    final_output = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_Final_Enhanced.mp4\")\n    \n    # Extract video without audio\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    video_cmd = [\n        'ffmpeg',\n        '-i', voice_file,\n        '-c:v', 'copy',\n        '-an',\n        '-y',\n        video_only\n    ]\n    \n    print(\"Extracting video without audio...\")\n    subprocess.run(video_cmd)\n    \n    # Extract audio\n    audio_file = extract_audio(voice_file)\n    if not audio_file:\n        print(\"Error extracting audio\")\n        return False\n    \n    # Create list file for concatenation\n    file_list = os.path.join(TEMP_DIR, \"concat_list.txt\")\n    with open(file_list, 'w') as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_only)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    # Create a complex filter to fix audio sync and merge audio files\n    # The key change is that we now align the audio to start 2 seconds earlier within the title\n    filter_complex = [\n        # Start with buffer audio\n        \"[0:a]atrim=0:3,asetpts=PTS-STARTPTS[buffer_start]\",\n        # Title sound\n        \"[1:a]atrim=0:10,asetpts=PTS-STARTPTS[title_sound]\",\n        # Voice audio starts during the title (-2s offset), no delay\n        \"[2:a]atrim=0:100.91,asetpts=PTS-STARTPTS[voice_audio]\",\n        # Credits sound\n        \"[3:a]atrim=0:10,asetpts=PTS-STARTPTS[credits_sound]\",\n        # End buffer audio\n        \"[4:a]atrim=0:3,asetpts=PTS-STARTPTS[buffer_end]\",\n        # Concatenate all audio elements with a 2-second head start for the voice\n        \"[buffer_start][title_sound]concat=n=2:v=0:a=1[title_full]\",\n        \"[title_full][voice_audio]acrossfade=d=8:c1=tri:c2=tri[with_voice]\",\n        \"[with_voice][credits_sound]acrossfade=d=2:c1=tri:c2=tri[with_credits]\",\n        \"[with_credits][buffer_end]concat=n=2:v=0:a=1[aout]\"\n    ]\n    \n    # Final assembly command\n    final_cmd = [\n        'ffmpeg',\n        '-i', buffer_start,\n        '-i', title_sound,\n        '-i', audio_file,\n        '-i', credits_sound,\n        '-i', buffer_end,\n        '-f', 'concat',\n        '-safe', '0',\n        '-i', file_list,\n        '-filter_complex', ';'.join(filter_complex),\n        '-map', '5:v',\n        '-map', '[aout]',\n        '-c:v', 'libx264',\n        '-c:a', 'aac',\n        '-b:a', '192k',\n        '-shortest',\n        '-metadata', 'title=Where You Go When You Leave',\n        '-metadata', 'comment=Enhanced version with dynamic titles and properly synchronized audio',\n        '-y',\n        final_output\n    ]\n    \n    print(\"Assembling final enhanced video...\")\n    subprocess.run(final_cmd)\n    \n    if os.path.exists(final_output):\n        print(f\"\u2713 Enhanced final video created: {final_output}\")\n        return True\n    \n    print(\"Error creating enhanced final video\")\n    return False\n\ndef main():\n    print(\"Creating enhanced final video with dynamic titles, properly aligned audio, and buffer audio...\")\n    \n    # Setup directories\n    ensure_directories()\n    \n    # Get the best voice track\n    try:\n        voice_file = get_best_voice_track()\n    except FileNotFoundError as e:\n        print(str(e))\n        return False\n    \n    # Create title animation and sound\n    title_file = create_title_animation()\n    title_sound = create_title_sound()\n    \n    if not title_file or not title_sound:\n        print(\"Error creating title animation or sound\")\n        return False\n    \n    # Create end credits and sound\n    credits_file = create_end_credits()\n    credits_sound = create_credits_sound()\n    \n    if not credits_file or not credits_sound:\n        print(\"Error creating end credits or sound\")\n        return False\n    \n    # Create buffer audio\n    buffer_start, buffer_end = create_buffer_audio()\n    if not buffer_start or not buffer_end:\n        print(\"Error creating buffer audio\")\n        return False\n    \n    # Assemble the final video\n    success = assemble_final_video(voice_file, title_file, title_sound, credits_file, credits_sound, buffer_start, buffer_end)\n    \n    if success:\n        print(\"Enhanced final video creation complete!\")\n        \n        # Get video info to display for user\n        final_output = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_Final_Enhanced.mp4\")\n        info = get_video_info(final_output)\n        \n        print(\"\\nEnhanced Final Video Information:\")\n        print(f\"Resolution: {info['width']}x{info['height']}\")\n        print(f\"Frame Rate: {info['frame_rate']} fps\")\n        print(f\"Duration: {info['duration']} seconds\")\n        print(f\"Audio: Voice starts during title sequence with buffer audio at beginning and end\")\n        print(f\"Title Sequence: 10-second dynamic animation with moving elements\")\n        print(f\"End Credits: 10-second animated sequence\")\n    else:\n        print(\"Error creating enhanced final video\")\n    \n    return success\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_{voice}.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "title_animation.mp4",
      "grid.mp4",
      "end_credits.mp4",
      "title_sound.wav",
      "buffer_start.wav",
      "buffer_end.wav",
      "credits_sound.wav",
      "voice_audio.wav",
      "WhereYouGoWhenYouLeave_Final_Enhanced.mp4",
      "video_only.mp4",
      "WhereYouGoWhenYouLeave_Final_Enhanced.mp4",
      "/Users/gaia/resurrecting atlantis",
      ")\n\n# Create dynamic title animation with Saul Bass / John Whitney style\ndef create_title_animation():\n    title_file = os.path.join(TEMP_DIR, ",
      "[bg][1:v]overlay=(W-w)/2:(H-h)/2[withgrid]",
      "[withgrid]drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "if(lt(t,2),1080/2-60,if(lt(t,4),1080/2-60+10*sin(t*4),1080/2-60+5*sin(t*2)))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "if(lt(t,2.5),1080/2-60,if(lt(t,4.5),1080/2-60+10*sin(t*4),1080/2-60+5*sin(t*2)))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "if(lt(t,3),1080/2-60,if(lt(t,5),1080/2-60+10*sin(t*4),1080/2-60+5*sin(t*2)))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "if(lt(t,3.5),1080/2+60,if(lt(t,5.5),1080/2+60+10*sin(t*4),1080/2+60+5*sin(t*2)))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "if(lt(t,4),1080/2+60,if(lt(t,6),1080/2+60+10*sin(t*4),1080/2+60+5*sin(t*2)))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "if(lt(t,4.5),1080/2+60,if(lt(t,6.5),1080/2+60+10*sin(t*4),1080/2+60+5*sin(t*2)))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=h-120:alpha=",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=h-80:alpha=",
      "128+127*sin(2*PI*(X/W*8+Y/H*8+t/2))",
      "128+127*sin(2*PI*(X/W*8+Y/H*8+t/2+1/3))",
      "128+127*sin(2*PI*(X/W*8+Y/H*8+t/2+2/3))",
      "if(lt(mod(hypot(X-W/2,Y-H/2)/80,2),1,if(lt(random(1),0.5),0.1,0))*if(lt(t,6),sin(t*PI/6),sin((10-t)*PI/4))",
      ",\n        \n        # Add final fade-in/out\n        ",
      "[bg]drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=",
      "128+127*sin(2*PI*t/10)",
      "128+127*sin(2*PI*t/10+2*PI/3)",
      "128+127*sin(2*PI*t/10+4*PI/3)",
      "if(lt(mod(atan2(Y-H/2,X-W/2)+t/1.5,PI/16),0.02),if(lt(t,9.5),0.15*sin(t*PI/5),0.15*sin((10-t)*PI/0.5)),0)",
      "aevalsrc=0.4*sin(2*PI*1000*(1-t/10))*exp(-2*t)",
      ")\n    frame_rate = float(frame_rate_parts[0]) / float(frame_rate_parts[1])\n    \n    return {\n        "
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "grid_cmd"
      },
      {
        "type": "run",
        "snippet": "title_cmd"
      },
      {
        "type": "run",
        "snippet": "credits_cmd"
      },
      {
        "type": "run",
        "snippet": "sound_cmd"
      },
      {
        "type": "run",
        "snippet": "start_cmd"
      },
      {
        "type": "run",
        "snippet": "end_cmd"
      },
      {
        "type": "run",
        "snippet": "sound_cmd"
      },
      {
        "type": "run",
        "snippet": "extract_cmd"
      },
      {
        "type": "run",
        "snippet": "cmd, stdout=subprocess.PIPE, text=True"
      },
      {
        "type": "run",
        "snippet": "video_cmd"
      },
      {
        "type": "run",
        "snippet": "final_cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json",
      "random",
      "math",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "LIZARD/simple_voice_delay.py",
    "size": 10691,
    "lines": 328,
    "source": "import os\nimport subprocess\nimport json\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_SIMPLE_DELAY\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_2SEC_VOICE_DELAY.mp4\")\n\n# Ensure temp directory exists\nos.makedirs(TEMP_DIR, exist_ok=True)\nprint(f\"Using temp directory: {TEMP_DIR}\")\n\n# Step 1: Find the source files\ndef find_best_files():\n    # Find voice track\n    voice_sources = [\n        \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\",\n        \"WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4\",\n        \"WhereYouGoWhenYouLeave_BroadcastPerfection.mp4\"\n    ]\n    \n    for source in voice_sources:\n        path = os.path.join(LIZARD_DIR, source)\n        if os.path.exists(path):\n            voice_file = path\n            print(f\"Using voice file: {os.path.basename(voice_file)}\")\n            break\n    else:\n        print(\"Error: No voice file found\")\n        return None, None\n    \n    # Find original video with native audio\n    video_sources = [\n        \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\",\n        \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\"\n    ]\n    \n    for source in video_sources:\n        path = os.path.join(LIZARD_DIR, source)\n        if os.path.exists(path):\n            video_file = path\n            print(f\"Using original video: {os.path.basename(video_file)}\")\n            break\n    else:\n        print(\"Using voice file as video source\")\n        video_file = voice_file\n    \n    return voice_file, video_file\n\n# Step 2: Extract audio from both files\ndef extract_audio(voice_file, video_file):\n    # Extract voice audio\n    voice_audio = os.path.join(TEMP_DIR, \"voice_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file, \n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ])\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", video_file, \n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ])\n    \n    return voice_audio, native_audio\n\n# Step 3: Get durations and process the voice\ndef process_voice_audio(voice_audio):\n    # Get duration\n    cmd = [\"ffprobe\", \"-v\", \"error\",\n           \"-show_entries\", \"format=duration\",\n           \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n           voice_audio]\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    duration = float(result.stdout.strip())\n    \n    print(f\"Voice audio duration: {duration:.2f} seconds\")\n    target_duration = 100.91  # Exact 1:40 timing\n    \n    # Speed adjust if needed\n    speed_factor = duration / target_duration\n    print(f\"Speed adjustment factor: {speed_factor:.4f}\")\n    \n    speed_adjusted = os.path.join(TEMP_DIR, \"voice_adjusted.wav\")\n    if abs(speed_factor - 1.0) < 0.02:\n        # Close enough to 1, no need to adjust\n        print(\"Voice length close to target, skipping speed adjustment\")\n        subprocess.run([\"cp\", voice_audio, speed_adjusted])\n    else:\n        # Apply speed adjustment\n        subprocess.run([\n            \"ffmpeg\", \"-i\", voice_audio,\n            \"-filter:a\", f\"atempo={speed_factor}\",\n            \"-y\", speed_adjusted\n        ])\n    \n    # Add 2-second silent buffer to beginning\n    # Method 1: Create a silence file\n    silence = os.path.join(TEMP_DIR, \"silence_2s.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", \"anoisesrc=color=white:amplitude=0.0:duration=2\",  # Use anoisesrc instead of anullsrc\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", silence\n    ])\n    \n    # Concatenate silence with voice\n    voice_delayed = os.path.join(TEMP_DIR, \"voice_delayed.wav\")\n    \n    # Create a concat file\n    concat_list = os.path.join(TEMP_DIR, \"concat_audio.txt\")\n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(silence)}'\\n\")\n        f.write(f\"file '{os.path.abspath(speed_adjusted)}'\\n\")\n    \n    # Concat files\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", voice_delayed\n    ])\n    \n    print(f\"Created voice with exact 2-second delay\")\n    \n    return voice_delayed\n\n# Step 4: Create title and credits\ndef create_title_credits():\n    # Create title\n    title_file = os.path.join(TEMP_DIR, \"title.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=70:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", title_file\n    ])\n    \n    # Create credits\n    credits_file = os.path.join(TEMP_DIR, \"credits.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=50:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", credits_file\n    ])\n    \n    return title_file, credits_file\n\n# Step 5: Create audio effects\ndef create_audio_effects():\n    # Title audio\n    title_audio = os.path.join(TEMP_DIR, \"title_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"sine=frequency=440:duration=5\",\n        \"-af\", \"afade=t=in:st=0:d=1,afade=t=out:st=4:d=1,volume=0.2\",\n        \"-y\", title_audio\n    ])\n    \n    # Credits audio\n    credits_audio = os.path.join(TEMP_DIR, \"credits_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"sine=frequency=220:duration=5\",\n        \"-af\", \"afade=t=in:st=0:d=1,afade=t=out:st=4:d=1,volume=0.2\",\n        \"-y\", credits_audio\n    ])\n    \n    return title_audio, credits_audio\n\n# Step 6: Extract video only\ndef extract_video(video_file):\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", video_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ])\n    \n    return video_only\n\n# Step 7: Concatenate videos\ndef concatenate_videos(title_file, video_file, credits_file):\n    concat_list = os.path.join(TEMP_DIR, \"concat.txt\")\n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    \n    return concat_video\n\n# Step 8: Create final audio mix\ndef create_final_mix(native_audio, voice_delayed, title_audio, credits_audio):\n    # Create a simple audio mix with the three layers\n    \n    # Create title+silence+credits sequence\n    title_concat = os.path.join(TEMP_DIR, \"title_concat.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", title_audio,\n        \"-filter_complex\", \"apad=pad_dur=110\",\n        \"-y\", title_concat\n    ])\n    \n    # Create silence+credits sequence\n    silence_105s = os.path.join(TEMP_DIR, \"silence_105s.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"anoisesrc=color=white:amplitude=0.0:duration=105\",\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", silence_105s\n    ])\n    \n    credits_concat = os.path.join(TEMP_DIR, \"credits_concat.wav\")\n    concat_credits = os.path.join(TEMP_DIR, \"concat_credits.txt\")\n    with open(concat_credits, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(silence_105s)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_audio)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\",\n        \"-i\", concat_credits,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", credits_concat\n    ])\n    \n    # Mix all audio\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-i\", native_audio,\n        \"-i\", voice_delayed,\n        \"-i\", title_concat,\n        \"-i\", credits_concat,\n        \"-filter_complex\",\n        \"[0:0]volume=1.0[native];\" +\n        \"[1:0]volume=0.9[voice];\" +\n        \"[2:0]volume=0.2[title_fx];\" +\n        \"[3:0]volume=0.2[credits_fx];\" +\n        \"[native][voice]amix=inputs=2:duration=longest[main];\" +\n        \"[title_fx][credits_fx]amix=inputs=2:duration=longest[fx];\" +\n        \"[main][fx]amix=inputs=2:duration=longest[out]\",\n        \"-map\", \"[out]\",\n        \"-y\", final_mix\n    ])\n    \n    return final_mix\n\n# Step 9: Add audio to video\ndef add_audio_to_video(video_file, audio_file):\n    subprocess.run([\n        \"ffmpeg\", \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"192k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-y\", OUTPUT_FILE\n    ])\n    \n    return OUTPUT_FILE if os.path.exists(OUTPUT_FILE) else None\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING FINAL VERSION WITH EXACT 2-SECOND VOICE DELAY =====\\n\")\n    \n    # Find the best source files\n    voice_file, video_file = find_best_files()\n    if not voice_file or not video_file:\n        return\n    \n    # Extract audio from source files\n    voice_audio, native_audio = extract_audio(voice_file, video_file)\n    \n    # Process voice audio with 2-second delay\n    voice_delayed = process_voice_audio(voice_audio)\n    \n    # Create title and credits\n    title_file, credits_file = create_title_credits()\n    \n    # Create title and credits audio\n    title_audio, credits_audio = create_audio_effects()\n    \n    # Extract video without audio\n    video_only = extract_video(video_file)\n    \n    # Concatenate the videos\n    concat_video = concatenate_videos(title_file, video_only, credits_file)\n    \n    # Create final audio mix\n    final_mix = create_final_mix(native_audio, voice_delayed, title_audio, credits_audio)\n    \n    # Add audio to video\n    final_output = add_audio_to_video(concat_video, final_mix)\n    \n    if final_output:\n        print(\"\\n===== SUCCESS! =====\")\n        print(f\"Final video created: {final_output}\")\n        print(\"\\nImportant improvements:\")\n        print(\"1. Voice track now starts EXACTLY 2 seconds after video begins\")\n        print(\"2. Native audio plays from the start of the video\")\n        print(\"3. Title and credits audio provide subtle ambiance\")\n        print(\"4. All audio synchronized to match the exact 1:40 (100.91s) timing\")\n        return True\n    else:\n        print(\"Failed to create final video\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_2SEC_VOICE_DELAY.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4",
      "WhereYouGoWhenYouLeave_BroadcastPerfection.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "voice_audio.wav",
      "native_audio.wav",
      "voice_adjusted.wav",
      "silence_2s.wav",
      "voice_delayed.wav",
      "title.mp4",
      "credits.mp4",
      "title_audio.wav",
      "credits_audio.wav",
      "video_only.mp4",
      "concatenated.mp4",
      "title_concat.wav",
      "silence_105s.wav",
      "credits_concat.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      ")\n    target_duration = 100.91  # Exact 1:40 timing\n    \n    # Speed adjust if needed\n    speed_factor = duration / target_duration\n    print(f",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=70:x=(w-text_w)/2:y=(h-text_h)/2",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=50:x=(w-text_w)/2:y=(h-text_h)/2"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file, \n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", video_file, \n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "cmd, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "[\"cp\", voice_audio, speed_adjusted]"
      },
      {
        "type": "run",
        "snippet": "[\n            \"ffmpeg\", \"-i\", voice_audio,\n            \"-filter:a\", f\"atempo={speed_factor}\",\n            \"-y\", speed_adjusted\n        ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", \"anoisesrc=color=white:amplitude=0.0:duration=2\",  # Use anoisesrc instead of anullsrc\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", voice_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU LEAVE':fontcolo"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING ATLANTIS':fontcol"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"sine=frequency=440:duration=5\",\n        \"-af\", \"afade=t=in:st=0:d=1,afade=t=out:st=4:d=1,volume=0.2\",\n        \"-y\", title_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"sine=frequency=220:duration=5\",\n        \"-af\", \"afade=t=in:st=0:d=1,afade=t=out:st=4:d=1,volume=0.2\",\n        \"-y\", credits_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", video_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", title_audio,\n        \"-filter_complex\", \"apad=pad_dur=110\",\n        \"-y\", title_concat\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", \"anoisesrc=color=white:amplitude=0.0:duration=105\",\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", silence_105s\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"concat\", \"-safe\", \"0\",\n        \"-i\", concat_credits,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", credits_concat\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-i\", native_audio,\n        \"-i\", voice_delayed,\n        \"-i\", title_concat,\n        \"-i\", credits_concat,\n        \"-filter_complex\",\n        \"[0:0]volume=1.0[native];\" +\n"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"192k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\""
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/create_timing_variations.py",
    "size": 13132,
    "lines": 373,
    "source": "import os\nimport subprocess\nimport shutil\nimport time\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_VARIATIONS\")\n\n# Timing constants\nINTRO_LENGTH = 7\nOUTRO_LENGTH = 8\nNATIVE_START_DELAY = 2  # Native audio starts 2 seconds after intro\nMAIN_LENGTH = 100.91  # Must be exactly 1:40 (100.91s) per requirements\n\n# These are the variations we'll create\nVARIATIONS = [\n    {\"name\": \"Version_1\", \"voice_starts_after_native\": 0.5, \"description\": \"Voice starts 0.5 seconds after native audio\"},\n    {\"name\": \"Version_2\", \"voice_starts_after_native\": 0.75, \"description\": \"Voice starts 0.75 seconds after native audio\"},\n    {\"name\": \"Version_3\", \"voice_starts_after_native\": 1.0, \"description\": \"Voice starts 1 second after native audio\"},\n    {\"name\": \"Version_4\", \"voice_starts_after_native\": 1.25, \"description\": \"Voice starts 1.25 seconds after native audio\"},\n    {\"name\": \"Version_5\", \"voice_starts_after_native\": 1.5, \"description\": \"Voice starts 1.5 seconds after native audio\"}\n]\n\n# Clean and create temp directory\nif os.path.exists(TEMP_DIR):\n    shutil.rmtree(TEMP_DIR)\nos.makedirs(TEMP_DIR)\nprint(f\"Using directory: {TEMP_DIR}\")\n\n# Find source files\ndef find_source_files():\n    # Find voice track\n    voice_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\")\n    if os.path.exists(voice_file):\n        print(f\"Using voice track: {os.path.basename(voice_file)}\")\n    else:\n        print(\"Voice track not found\")\n        return None, None\n    \n    # Find original video\n    original_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\")\n    if not os.path.exists(original_file):\n        original_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    \n    if os.path.exists(original_file):\n        print(f\"Using original video: {os.path.basename(original_file)}\")\n    else:\n        print(\"Original video not found, using voice file\")\n        original_file = voice_file\n    \n    return voice_file, original_file\n\n# Create intro and outro videos\ndef create_intro_outro():\n    # Create intro\n    intro_file = os.path.join(TEMP_DIR, \"intro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=80:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", intro_file\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro\")\n    \n    # Create outro\n    outro_file = os.path.join(TEMP_DIR, \"outro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", outro_file\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro\")\n    \n    return intro_file, outro_file\n\n# Extract audio components\ndef extract_audio_components(voice_file, original_file):\n    # Extract original voice audio without any processing\n    voice_audio = os.path.join(TEMP_DIR, \"voice_original.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ])\n    print(\"Extracted original voice audio without any processing\")\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ])\n    print(\"Extracted native audio\")\n    \n    # Extract video only\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ])\n    print(\"Extracted video without audio\")\n    \n    return voice_audio, native_audio, video_only\n\n# Create necessary audio effects\ndef create_audio_effects():\n    # Ambient background noise\n    total_length = INTRO_LENGTH + MAIN_LENGTH + OUTRO_LENGTH\n    ambient_file = os.path.join(TEMP_DIR, \"ambient.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=pink:amplitude=0.01:duration={total_length}\",\n        \"-af\", \"volume=0.1\",\n        \"-y\", ambient_file\n    ])\n    print(f\"Created {total_length}-second ambient background\")\n    \n    # Intro audio effect\n    intro_audio = os.path.join(TEMP_DIR, \"intro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.15\",\n        \"-y\", intro_audio\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro audio effect\")\n    \n    # Outro audio effect\n    outro_audio = os.path.join(TEMP_DIR, \"outro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.15\",\n        \"-y\", outro_audio\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro audio effect\")\n    \n    return ambient_file, intro_audio, outro_audio\n\n# Concatenate videos\ndef concatenate_videos(intro_file, main_file, outro_file):\n    concat_file = os.path.join(TEMP_DIR, \"concat.txt\")\n    with open(concat_file, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(main_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concat_video.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"concat\", \n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    print(\"Concatenated intro + main video + outro\")\n    \n    return concat_video\n\n# Position intro and outro effects for each version\ndef position_intro_outro(intro_audio, outro_audio):\n    total_length = INTRO_LENGTH + MAIN_LENGTH + OUTRO_LENGTH\n    \n    # Position intro at beginning\n    intro_positioned = os.path.join(TEMP_DIR, \"intro_positioned.wav\")\n    intro_padding = os.path.join(TEMP_DIR, \"intro_padding.wav\")\n    intro_pad_length = total_length - INTRO_LENGTH\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={intro_pad_length}\",\n        \"-y\", intro_padding\n    ])\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", intro_audio,\n        \"-i\", intro_padding,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", intro_positioned\n    ])\n    print(\"Positioned intro audio at beginning\")\n    \n    # Position outro at end\n    outro_positioned = os.path.join(TEMP_DIR, \"outro_positioned.wav\")\n    outro_padding = os.path.join(TEMP_DIR, \"outro_padding.wav\")\n    outro_pad_length = total_length - OUTRO_LENGTH\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={outro_pad_length}\",\n        \"-y\", outro_padding\n    ])\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", outro_padding,\n        \"-i\", outro_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", outro_positioned\n    ])\n    print(\"Positioned outro audio at end\")\n    \n    return intro_positioned, outro_positioned\n\n# Create a variation with specific voice delay\ndef create_variation(variation_info, concat_video, voice_audio, native_audio, ambient_audio, intro_positioned, outro_positioned):\n    # Extract variation details\n    name = variation_info[\"name\"]\n    voice_delay = variation_info[\"voice_starts_after_native\"]\n    description = variation_info[\"description\"]\n    \n    variation_dir = os.path.join(TEMP_DIR, name)\n    os.makedirs(variation_dir, exist_ok=True)\n    \n    output_file = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{name}.mp4\")\n    \n    # Calculate exact timing delays\n    native_delay = INTRO_LENGTH + NATIVE_START_DELAY  # 7s intro + 2s delay = 9s\n    voice_delay_total = native_delay + voice_delay\n    \n    print(f\"\\n===== Creating {name}: {description} =====\")\n    print(f\"Native audio starts at: {native_delay}s\")\n    print(f\"Voice starts at: {voice_delay_total}s ({voice_delay}s after native)\")\n    \n    # Create silence for native audio delay\n    native_silence = os.path.join(variation_dir, \"native_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={native_delay}\",\n        \"-y\", native_silence\n    ])\n    \n    # Create silence for voice delay\n    voice_silence = os.path.join(variation_dir, \"voice_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={voice_delay_total}\",\n        \"-y\", voice_silence\n    ])\n    \n    # Position native audio with delay\n    native_positioned = os.path.join(variation_dir, \"native_positioned.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", native_silence,\n        \"-i\", native_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", native_positioned\n    ])\n    \n    # Position voice with delay\n    voice_positioned = os.path.join(variation_dir, \"voice_positioned.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_silence,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_positioned\n    ])\n    \n    # Create final audio mix\n    final_mix = os.path.join(variation_dir, \"final_mix.wav\")\n    \n    # Mix all audio layers with proper volumes\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_positioned,\n        \"-i\", native_positioned,\n        \"-i\", ambient_audio,\n        \"-i\", intro_positioned,\n        \"-i\", outro_positioned,\n        \"-filter_complex\",\n        \"[0:0]volume=1.0[voice];\" +\n        \"[1:0]volume=0.7[native];\" +\n        \"[2:0]volume=0.1[ambient];\" +\n        \"[3:0]volume=0.15[intro];\" +\n        \"[4:0]volume=0.15[outro];\" +\n        \"[voice][native]amix=inputs=2:duration=longest[content];\" +\n        \"[ambient][intro]amix=inputs=2:duration=longest[amb_fx1];\" +\n        \"[amb_fx1][outro]amix=inputs=2:duration=longest[amb_fx];\" +\n        \"[content][amb_fx]amix=inputs=2:duration=longest[out]\",\n        \"-map\", \"[out]\",\n        \"-y\", final_mix\n    ])\n    \n    # Add audio to video\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", concat_video,\n        \"-i\", final_mix,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", f\"title=Where You Go When You Leave - {name}\",\n        \"-metadata\", f\"comment={description}\",\n        \"-y\", output_file\n    ])\n    \n    if os.path.exists(output_file):\n        cmd = [\n            \"ffprobe\", \"-v\", \"error\",\n            \"-show_entries\", \"format=duration\",\n            \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n            output_file\n        ]\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        duration = float(result.stdout.strip())\n        print(f\"Created {name}: {output_file}\")\n        print(f\"Duration: {duration:.2f} seconds\")\n        return True\n    else:\n        print(f\"Failed to create {name}\")\n        return False\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING MULTIPLE VERSIONS WITH VARYING VOICE DELAYS =====\\n\")\n    \n    # Find source files\n    voice_file, original_file = find_source_files()\n    if not voice_file or not original_file:\n        print(\"Error: Source files not found\")\n        return False\n    \n    # Create intro and outro\n    intro_file, outro_file = create_intro_outro()\n    \n    # Extract audio and video components\n    voice_audio, native_audio, video_only = extract_audio_components(voice_file, original_file)\n    \n    # Create ambient and effect audio\n    ambient_audio, intro_audio, outro_audio = create_audio_effects()\n    \n    # Concatenate videos (only need to do this once)\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Position intro and outro effects (only need to do this once)\n    intro_positioned, outro_positioned = position_intro_outro(intro_audio, outro_audio)\n    \n    # Create each variation\n    success_count = 0\n    for variation in VARIATIONS:\n        success = create_variation(\n            variation,\n            concat_video,\n            voice_audio,\n            native_audio,\n            ambient_audio,\n            intro_positioned,\n            outro_positioned\n        )\n        if success:\n            success_count += 1\n    \n    print(\"\\n===== SUMMARY =====\")\n    print(f\"Successfully created {success_count} out of {len(VARIATIONS)} variations\")\n    print(\"\\nVersions available:\")\n    for i, variation in enumerate(VARIATIONS, 1):\n        print(f\"{i}. {variation['name']}: {variation['description']}\")\n        print(f\"   File: WhereYouGoWhenYouLeave_{variation['name']}.mp4\")\n    \n    return success_count > 0\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "intro.mp4",
      "outro.mp4",
      "voice_original.wav",
      "native_audio.wav",
      "video_only.mp4",
      "ambient.wav",
      "intro_audio.wav",
      "outro_audio.wav",
      "concat_video.mp4",
      "intro_positioned.wav",
      "intro_padding.wav",
      "outro_positioned.wav",
      "outro_padding.wav",
      "WhereYouGoWhenYouLeave_{name}.mp4",
      "native_silence.wav",
      "voice_silence.wav",
      "native_positioned.wav",
      "voice_positioned.wav",
      "final_mix.wav",
      "]}.mp4",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=80:x=(w-text_w)/2:y=(h-text_h)/2",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU L"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING ATL"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=pink:amplitude=0.01:duration={total_length}\",\n        \"-af\", \"volume=0.1\",\n        \"-y\", ambient_file\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.15\",\n        \"-y\", int"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.15\",\n        \"-y\", "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"concat\", \n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={intro_pad_length}\",\n        \"-y\", intro_padding\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", intro_audio,\n        \"-i\", intro_padding,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", intro_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={outro_pad_length}\",\n        \"-y\", outro_padding\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", outro_padding,\n        \"-i\", outro_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", outro_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={native_delay}\",\n        \"-y\", native_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={voice_delay_total}\",\n        \"-y\", voice_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", native_silence,\n        \"-i\", native_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", native_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_silence,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_positioned,\n        \"-i\", native_positioned,\n        \"-i\", ambient_audio,\n        \"-i\", intro_positioned,\n        \"-i\", outro_positioned,\n        \"-filter_compl"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", concat_video,\n        \"-i\", final_mix,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-"
      },
      {
        "type": "run",
        "snippet": "cmd, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "shutil",
      "time"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/add_more_voice_delay.py",
    "size": 12086,
    "lines": 357,
    "source": "import os\nimport subprocess\nimport shutil\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_MORE_DELAY\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_MORE_DELAY.mp4\")\n\n# Timing constants\nINTRO_LENGTH = 7\nOUTRO_LENGTH = 8\nVOICE_DELAY = 5  # Increased to 5 seconds\nNATIVE_DELAY = 3  # Keep native audio at 3 seconds\nMAIN_LENGTH = 100.91  # Exact 1:40 length\n\n# Clean and create temp directory\nif os.path.exists(TEMP_DIR):\n    shutil.rmtree(TEMP_DIR)\nos.makedirs(TEMP_DIR)\nprint(f\"Using directory: {TEMP_DIR}\")\n\n# Find original source files\ndef find_source_files():\n    # Find voice track\n    voice_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\")\n    if os.path.exists(voice_file):\n        print(f\"Using voice track: {os.path.basename(voice_file)}\")\n    else:\n        voice_file = None\n        print(\"Voice track not found\")\n    \n    # Find original video with native audio\n    original_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\")\n    if not os.path.exists(original_file):\n        original_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n        \n    if os.path.exists(original_file):\n        print(f\"Using original video: {os.path.basename(original_file)}\")\n    else:\n        original_file = voice_file\n        print(\"Using voice track as video source\")\n    \n    return voice_file, original_file\n\n# Create simple intro and outro\ndef create_title_and_credits():\n    # Create intro\n    intro_file = os.path.join(TEMP_DIR, \"intro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=80:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\", \n        \"-y\", intro_file\n    ])\n    \n    # Create outro\n    outro_file = os.path.join(TEMP_DIR, \"outro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\", \n        \"-y\", outro_file\n    ])\n    \n    return intro_file, outro_file\n\n# Extract audio and video\ndef extract_components(voice_file, original_file):\n    # Extract voice audio exactly as is\n    voice_audio = os.path.join(TEMP_DIR, \"voice_original.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ])\n    print(\"Extracted original voice audio without any processing\")\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ])\n    print(\"Extracted native audio\")\n    \n    # Extract video only\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ])\n    print(\"Extracted video without audio\")\n    \n    return voice_audio, native_audio, video_only\n\n# Create the 5-second delay for voice and 3-second delay for native audio\ndef add_delay_to_audio(voice_audio, native_audio):\n    # Create 5 second silent buffer audio for voice\n    voice_silence = os.path.join(TEMP_DIR, \"silence_5s.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={VOICE_DELAY}\",\n        \"-y\", voice_silence\n    ])\n    print(f\"Created {VOICE_DELAY}-second silent buffer for voice\")\n    \n    # Create 3 second silent buffer for native audio\n    native_silence = os.path.join(TEMP_DIR, \"silence_3s.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={NATIVE_DELAY}\",\n        \"-y\", native_silence\n    ])\n    print(f\"Created {NATIVE_DELAY}-second silent buffer for native audio\")\n    \n    # Add silent buffer to voice audio (5 seconds)\n    voice_delayed = os.path.join(TEMP_DIR, \"voice_delayed.wav\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-i\", voice_silence,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_delayed\n    ])\n    print(f\"Added {VOICE_DELAY}-second delay to voice audio (preserving original)\")\n    \n    # Add silent buffer to native audio (3 seconds)\n    native_delayed = os.path.join(TEMP_DIR, \"native_delayed.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", native_silence,\n        \"-i\", native_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", native_delayed\n    ])\n    print(f\"Added {NATIVE_DELAY}-second delay to native audio\")\n    \n    return voice_delayed, native_delayed\n\n# Create ambient background\ndef create_ambient_background():\n    # Simple ambient background\n    ambient_file = os.path.join(TEMP_DIR, \"ambient.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anoisesrc=color=pink:amplitude=0.01:duration=115.91\",\n        \"-af\", \"volume=0.15\",\n        \"-y\", ambient_file\n    ])\n    print(\"Created ambient background\")\n    \n    # Simple intro audio effect\n    intro_audio = os.path.join(TEMP_DIR, \"intro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.2\",\n        \"-y\", intro_audio\n    ])\n    \n    # Simple outro audio effect\n    outro_audio = os.path.join(TEMP_DIR, \"outro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.2\",\n        \"-y\", outro_audio\n    ])\n    \n    return ambient_file, intro_audio, outro_audio\n\n# Concatenate videos\ndef concatenate_videos(intro_file, main_file, outro_file):\n    concat_file = os.path.join(TEMP_DIR, \"concat.txt\")\n    with open(concat_file, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(main_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concat_video.mp4\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    print(\"Concatenated intro + main video + outro\")\n    \n    return concat_video\n\n# Create final audio mix\ndef create_audio_mix(voice_delayed, native_delayed, ambient, intro_audio, outro_audio):\n    # Position intro audio\n    intro_extended = os.path.join(TEMP_DIR, \"intro_extended.wav\")\n    long_silence = os.path.join(TEMP_DIR, \"long_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=109\",\n        \"-y\", long_silence\n    ])\n    intro_concat = os.path.join(TEMP_DIR, \"intro_concat.txt\")\n    with open(intro_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_audio)}'\\n\")\n        f.write(f\"file '{os.path.abspath(long_silence)}'\\n\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", intro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", intro_extended\n    ])\n    \n    # Position outro audio\n    outro_extended = os.path.join(TEMP_DIR, \"outro_extended.wav\")\n    outro_silence = os.path.join(TEMP_DIR, \"outro_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=108\",\n        \"-y\", outro_silence\n    ])\n    outro_concat = os.path.join(TEMP_DIR, \"outro_concat.txt\")\n    with open(outro_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(outro_silence)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_audio)}'\\n\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", outro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", outro_extended\n    ])\n    \n    # Mix all audio\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Simple mix with appropriate volumes\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_delayed,\n        \"-i\", native_delayed,\n        \"-i\", ambient,\n        \"-i\", intro_extended,\n        \"-i\", outro_extended,\n        \"-filter_complex\",\n        \"[0:0]volume=1.0[voice];\" +\n        \"[1:0]volume=0.8[native];\" +\n        \"[2:0]volume=0.15[ambient];\" +\n        \"[3:0]volume=0.2[intro];\" +\n        \"[4:0]volume=0.2[outro];\" +\n        \"[voice][native]amix=inputs=2:duration=first[content];\" +\n        \"[ambient][intro]amix=inputs=2:duration=first[amb_fx1];\" +\n        \"[amb_fx1][outro]amix=inputs=2:duration=first[amb_fx];\" +\n        \"[content][amb_fx]amix=inputs=2:duration=first[out]\",\n        \"-map\", \"[out]\",\n        \"-y\", final_mix\n    ])\n    print(\"Created final audio mix\")\n    \n    return final_mix\n\n# Add final audio to video\ndef add_audio_to_video(video_file, audio_file):\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Final version with completely unmodified voice and 5-second delay\",\n        \"-y\", OUTPUT_FILE\n    ])\n    print(f\"Added audio to video: {OUTPUT_FILE}\")\n    \n    if os.path.exists(OUTPUT_FILE):\n        cmd = [\n            \"ffprobe\", \"-v\", \"error\",\n            \"-show_entries\", \"format=duration\",\n            \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n            OUTPUT_FILE\n        ]\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        duration = float(result.stdout.strip())\n        print(f\"Final video duration: {duration:.2f} seconds\")\n        return True\n    else:\n        return False\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING VERSION WITH INCREASED VOICE DELAY (5 SECONDS) =====\\n\")\n    \n    # Find sources\n    voice_file, original_file = find_source_files()\n    if not voice_file or not original_file:\n        print(\"Error: Source files not found\")\n        return False\n    \n    # Create intro/outro\n    intro_file, outro_file = create_title_and_credits()\n    \n    # Extract components\n    voice_audio, native_audio, video_only = extract_components(voice_file, original_file)\n    \n    # Add delay to audio (5 seconds for voice, 3 seconds for native)\n    voice_delayed, native_delayed = add_delay_to_audio(voice_audio, native_audio)\n    \n    # Create ambient background\n    ambient_audio, intro_audio, outro_audio = create_ambient_background()\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Create audio mix\n    final_mix = create_audio_mix(voice_delayed, native_delayed, ambient_audio, intro_audio, outro_audio)\n    \n    # Add audio to video\n    success = add_audio_to_video(concat_video, final_mix)\n    \n    if success:\n        print(\"\\n===== SUCCESS! =====\")\n        print(f\"Final video created: {OUTPUT_FILE}\")\n        print(\"\\nFinal specifications:\")\n        print(\"- Original voice track preserved exactly as is\")\n        print(f\"- Voice starts after {VOICE_DELAY} seconds (increased delay)\")\n        print(f\"- Native audio starts after {NATIVE_DELAY} seconds\")\n        print(f\"- Intro length: {INTRO_LENGTH} seconds\")\n        print(f\"- Main content: {MAIN_LENGTH} seconds (1:40)\")\n        print(f\"- Outro length: {OUTRO_LENGTH} seconds\")\n        print(\"- Minimal processing applied to avoid any distortion\")\n        return True\n    else:\n        print(\"Error: Failed to create final video\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_MORE_DELAY.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "intro.mp4",
      "outro.mp4",
      "voice_original.wav",
      "native_audio.wav",
      "video_only.mp4",
      "silence_5s.wav",
      "silence_3s.wav",
      "voice_delayed.wav",
      "native_delayed.wav",
      "ambient.wav",
      "intro_audio.wav",
      "outro_audio.wav",
      "concat_video.mp4",
      "intro_extended.wav",
      "long_silence.wav",
      "outro_extended.wav",
      "outro_silence.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=80:x=(w-text_w)/2:y=(h-text_h)/2",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2",
      ")\n        return False\n    \n    # Create intro/outro\n    intro_file, outro_file = create_title_and_credits()\n    \n    # Extract components\n    voice_audio, native_audio, video_only = extract_components(voice_file, original_file)\n    \n    # Add delay to audio (5 seconds for voice, 3 seconds for native)\n    voice_delayed, native_delayed = add_delay_to_audio(voice_audio, native_audio)\n    \n    # Create ambient background\n    ambient_audio, intro_audio, outro_audio = create_ambient_background()\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Create audio mix\n    final_mix = create_audio_mix(voice_delayed, native_delayed, ambient_audio, intro_audio, outro_audio)\n    \n    # Add audio to video\n    success = add_audio_to_video(concat_video, final_mix)\n    \n    if success:\n        print("
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\", \n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\", \n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING A"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={VOICE_DELAY}\",\n        \"-y\", voice_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={NATIVE_DELAY}\",\n        \"-y\", native_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-i\", voice_silence,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", native_silence,\n        \"-i\", native_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", native_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anoisesrc=color=pink:amplitude=0.01:duration=115.91\",\n        \"-af\", \"volume=0.15\",\n        \"-y\", ambient_file\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.2\",\n        \"-"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.2\",\n       "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=109\",\n        \"-y\", long_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", intro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", intro_extended\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=108\",\n        \"-y\", outro_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", outro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", outro_extended\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_delayed,\n        \"-i\", native_delayed,\n        \"-i\", ambient,\n        \"-i\", intro_extended,\n        \"-i\", outro_extended,\n        \"-filter_complex\",\n        \"[0"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-s"
      },
      {
        "type": "run",
        "snippet": "cmd, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/fix_voice_with_delay.py",
    "size": 13338,
    "lines": 384,
    "source": "import os\nimport subprocess\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_FIXED_VOICE\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FIXED_VOICE_DELAY.mp4\")\n\n# Ensure temp directory exists\nos.makedirs(TEMP_DIR, exist_ok=True)\nprint(f\"Using temp directory: {TEMP_DIR}\")\n\n# Find source files\ndef find_source_files():\n    # Find best voice track\n    voice_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_BroadcastPerfection.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ConfidentPresence.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_CrystalClarity.mp4\")\n    ]\n    \n    for file in voice_files:\n        if os.path.exists(file):\n            print(f\"Using voice track: {os.path.basename(file)}\")\n            voice_file = file\n            break\n    else:\n        print(\"No suitable voice track found\")\n        return None, None\n    \n    # Find original video with native audio\n    original_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    ]\n    \n    for file in original_files:\n        if os.path.exists(file):\n            print(f\"Using original video: {os.path.basename(file)}\")\n            original_file = file\n            break\n    else:\n        print(\"No original video found, using voice track as video source\")\n        original_file = voice_file\n    \n    return voice_file, original_file\n\n# Extract audio from sources\ndef extract_audio(voice_file, video_file):\n    # Extract voice audio (preserving exact quality)\n    voice_audio = os.path.join(TEMP_DIR, \"voice_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-ar\", \"48000\",  # Use standard 48kHz sample rate\n        \"-y\", voice_audio\n    ])\n    print(\"Extracted voice audio (preserved original quality)\")\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", video_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-ar\", \"48000\",  # Use standard 48kHz sample rate\n        \"-y\", native_audio\n    ])\n    print(\"Extracted native audio\")\n    \n    return voice_audio, native_audio\n\n# Add 2-second delay to voice audio\ndef add_delay_to_voice(voice_audio):\n    # Create a 2-second silent buffer (using same settings as voice track)\n    silent_buffer = os.path.join(TEMP_DIR, \"silent_2s.wav\")\n    cmd_probe = [\n        \"ffprobe\", \"-v\", \"error\",\n        \"-select_streams\", \"a:0\",\n        \"-show_entries\", \"stream=channels\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        voice_audio\n    ]\n    channels = subprocess.run(cmd_probe, capture_output=True, text=True).stdout.strip()\n    channels = \"1\" if not channels else channels  # Default to mono if not detected\n    \n    # Create silent file with matching properties\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anullsrc=r=48000:cl={'mono' if channels == '1' else 'stereo'}\",\n        \"-t\", \"2\",  # 2 second silence\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", silent_buffer\n    ])\n    print(f\"Created 2-second silence buffer ({channels} channel(s))\")\n    \n    # Concatenate silence with voice audio\n    delayed_voice = os.path.join(TEMP_DIR, \"voice_delayed.wav\")\n    concat_list = os.path.join(TEMP_DIR, \"voice_concat.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(silent_buffer)}'\\n\")\n        f.write(f\"file '{os.path.abspath(voice_audio)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", delayed_voice\n    ])\n    print(\"Created voice track with exact 2-second delay (preserved quality)\")\n    \n    return delayed_voice\n\n# Create title and credits videos\ndef create_title_and_credits():\n    # Create title\n    title_file = os.path.join(TEMP_DIR, \"title.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            \"enable='between(t,0,5)':alpha='if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHEN YOU LEAVE':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+60:\"\n            \"enable='between(t,0.3,5)':alpha='if(lt(t,0.8),(t-0.3)/0.5,if(gt(t,4.5),(5-t)/0.5,1))'\"\n        ),\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", title_file\n    ])\n    print(\"Created title animation\")\n    \n    # Create credits\n    credits_file = os.path.join(TEMP_DIR, \"credits.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=60:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            \"enable='between(t,0,5)':alpha='if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='A Multimedia Poetry Project':fontcolor=white:fontsize=40:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+30:\"\n            \"enable='between(t,0.5,5)':alpha='if(lt(t,1.0),(t-0.5)/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=30:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+100:\"\n            \"enable='between(t,1,5)':alpha='if(lt(t,1.5),(t-1)/0.5,if(gt(t,4.5),(5-t)/0.5,1))'\"\n        ),\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", credits_file\n    ])\n    print(\"Created credits animation\")\n    \n    return title_file, credits_file\n\n# Create title and credits audio effects\ndef create_audio_effects():\n    # Title audio with smoother fade\n    title_audio = os.path.join(TEMP_DIR, \"title_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.08*sin(2*PI*330*t)+0.06*sin(2*PI*440*t):s=48000:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=2,afade=t=out:st=3:d=2,volume=0.35\",\n        \"-y\", title_audio\n    ])\n    print(\"Created title audio effect\")\n    \n    # Credits audio\n    credits_audio = os.path.join(TEMP_DIR, \"credits_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.07*sin(2*PI*220*t)+0.05*sin(2*PI*294*t):s=48000:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=2,afade=t=out:st=3:d=2,volume=0.3\",\n        \"-y\", credits_audio\n    ])\n    print(\"Created credits audio effect\")\n    \n    return title_audio, credits_audio\n\n# Extract video without audio\ndef extract_video(video_file):\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", video_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ])\n    print(\"Extracted video without audio\")\n    \n    return video_only\n\n# Concatenate videos\ndef concatenate_videos(title_file, video_file, credits_file):\n    concat_list = os.path.join(TEMP_DIR, \"concat.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    print(\"Concatenated videos (title + main + credits)\")\n    \n    return concat_video\n\n# Create final audio mix with smooth transitions\ndef create_final_mix(native_audio, voice_delayed, title_audio, credits_audio):\n    # First, get native audio duration\n    cmd_duration = [\n        \"ffprobe\", \"-v\", \"error\",\n        \"-show_entries\", \"format=duration\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        native_audio\n    ]\n    native_duration = float(subprocess.run(cmd_duration, capture_output=True, text=True).stdout.strip())\n    print(f\"Native audio duration: {native_duration:.2f} seconds\")\n    \n    # Extend title audio with smooth crossfade\n    title_extended = os.path.join(TEMP_DIR, \"title_extended.wav\")\n    cmd_title = [\n        \"ffmpeg\",\n        \"-i\", title_audio,\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st=3:d=2,apad=pad_dur={native_duration+10}\",\n        \"-y\", title_extended\n    ]\n    subprocess.run(cmd_title)\n    print(\"Created extended title audio with smooth fade\")\n    \n    # Create credits audio with proper timing\n    credits_extended = os.path.join(TEMP_DIR, \"credits_extended.wav\")\n    cmd_credits = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={native_duration+5}:s=48000\",\n        \"-i\", credits_audio,\n        \"-filter_complex\", \"[0][1]concat=n=2:v=0:a=1\",\n        \"-y\", credits_extended\n    ]\n    subprocess.run(cmd_credits)\n    print(\"Created extended credits audio with proper timing\")\n    \n    # Mix everything with better balance\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Create filter complex with smoother transitions\n    filter_complex = [\n        # Layer 1: Native audio (original volume)\n        \"[0:0]volume=1.0[native]\",\n        \n        # Layer 2: Voice audio (slightly enhanced but not pitched)\n        \"[1:0]volume=0.95,highpass=f=80,lowpass=f=12000[voice]\",\n        \n        # Layer 3: Title audio (subtle)\n        \"[2:0]volume=0.25[title_fx]\",\n        \n        # Layer 4: Credits audio (subtle)\n        \"[3:0]volume=0.22[credits_fx]\",\n        \n        # Mix title and credits\n        \"[title_fx][credits_fx]amix=inputs=2:duration=longest[fx]\",\n        \n        # Mix native and voice with crossfade\n        \"[native][voice]amix=inputs=2:duration=longest[main]\",\n        \n        # Mix main with effects\n        \"[main][fx]amix=inputs=2:duration=longest[out]\"\n    ]\n    \n    cmd_mix = [\n        \"ffmpeg\",\n        \"-i\", native_audio,\n        \"-i\", voice_delayed,\n        \"-i\", title_extended,\n        \"-i\", credits_extended,\n        \"-filter_complex\", \";\".join(filter_complex),\n        \"-map\", \"[out]\",\n        \"-y\", final_mix\n    ]\n    \n    subprocess.run(cmd_mix)\n    print(\"Created final audio mix with preserved voice quality and smooth transitions\")\n    \n    return final_mix\n\n# Add audio to video\ndef add_audio_to_video(video_file, audio_file):\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",  # Higher quality audio\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Final version with preserved voice and 2-second delay\",\n        \"-y\", OUTPUT_FILE\n    ])\n    print(f\"Created final video: {OUTPUT_FILE}\")\n    \n    return OUTPUT_FILE if os.path.exists(OUTPUT_FILE) else None\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING FINAL VERSION WITH PRESERVED VOICE QUALITY AND 2-SECOND DELAY =====\\n\")\n    \n    # Find source files\n    voice_file, video_file = find_source_files()\n    if not voice_file or not video_file:\n        print(\"Error: Missing source files\")\n        return\n    \n    # Extract audio\n    voice_audio, native_audio = extract_audio(voice_file, video_file)\n    \n    # Add 2-second delay to voice\n    voice_delayed = add_delay_to_voice(voice_audio)\n    \n    # Create title and credits\n    title_file, credits_file = create_title_and_credits()\n    \n    # Create audio effects\n    title_audio, credits_audio = create_audio_effects()\n    \n    # Extract video\n    video_only = extract_video(video_file)\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(title_file, video_only, credits_file)\n    \n    # Create final audio mix\n    final_mix = create_final_mix(native_audio, voice_delayed, title_audio, credits_audio)\n    \n    # Add audio to video\n    final_output = add_audio_to_video(concat_video, final_mix)\n    \n    if final_output:\n        print(\"\\n===== SUCCESS! =====\")\n        print(f\"Final video created: {final_output}\")\n        print(\"\\nImprovements in this version:\")\n        print(\"1. Original voice quality preserved (no pitch changes)\")\n        print(\"2. Voice starts exactly 2 seconds after video begins\")\n        print(\"3. Smoother blending between intro audio and main content\")\n        print(\"4. Better audio balance with subtle background effects\")\n        print(\"5. Higher quality audio encoding (256k AAC)\")\n        return True\n    else:\n        print(\"Error: Failed to create final video\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_FIXED_VOICE_DELAY.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4",
      "WhereYouGoWhenYouLeave_BroadcastPerfection.mp4",
      "WhereYouGoWhenYouLeave_ConfidentPresence.mp4",
      "WhereYouGoWhenYouLeave_CrystalClarity.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "voice_audio.wav",
      "native_audio.wav",
      "silent_2s.wav",
      "voice_delayed.wav",
      "title.mp4",
      "credits.mp4",
      "title_audio.wav",
      "credits_audio.wav",
      "video_only.mp4",
      "concatenated.mp4",
      "title_extended.wav",
      "credits_extended.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+60:",
      "if(lt(t,0.8),(t-0.3)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+30:",
      "if(lt(t,1.0),(t-0.5)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+100:",
      "if(lt(t,1.5),(t-1)/0.5,if(gt(t,4.5),(5-t)/0.5,1))"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-ar\", \"48000\",  # Use standard 48kHz sample rate\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", video_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-ar\", \"48000\",  # Use standard 48kHz sample rate\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "cmd_probe, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anullsrc=r=48000:cl={'mono' if channels == '1' else 'stereo'}\",\n        \"-t\", \"2\",  # 2 second silence\n        \"-c:a\", \"pcm_s16le\",\n        \""
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", delayed_voice\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text="
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text="
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.08*sin(2*PI*330*t"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.07*sin(2*PI*220*t"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", video_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "cmd_duration, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "cmd_title"
      },
      {
        "type": "run",
        "snippet": "cmd_credits"
      },
      {
        "type": "run",
        "snippet": "cmd_mix"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",  # Higher quality audio\n        \"-map\", \"0:v\",\n        \"-"
      }
    ],
    "imports": [
      "os",
      "subprocess"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/create_layered_final.py",
    "size": 13941,
    "lines": 466,
    "source": "import os\nimport subprocess\nimport time\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_LAYERED\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FINAL_LAYERED.mp4\")\nVIDEO_DURATION = 100.91  # Exact 1:40 timing from memory\n\n# Ensure temp directory exists\nif not os.path.exists(TEMP_DIR):\n    os.makedirs(TEMP_DIR)\n    print(f\"Created directory: {TEMP_DIR}\")\n\n# Find the best voice track\ndef find_voice_track():\n    preferred_voices = [\n        \"ExecutiveVoice\",\n        \"UltraCrispDefinition\",\n        \"BroadcastPerfection\"\n    ]\n    \n    for voice in preferred_voices:\n        voice_file = os.path.join(LIZARD_DIR, f\"WhereYouGoWhenYouLeave_{voice}.mp4\")\n        if os.path.exists(voice_file):\n            print(f\"Using voice track: {voice}\")\n            return voice_file\n    \n    fallback = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    if os.path.exists(fallback):\n        print(\"Using original assembly as fallback\")\n        return fallback\n    \n    print(\"No suitable voice track found\")\n    return None\n\n# Create short title animation\ndef create_title():\n    title_file = os.path.join(TEMP_DIR, \"title.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            \"enable='between(t,0,5)',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHEN YOU LEAVE':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+60:\"\n            \"enable='between(t,0.3,5)'\"\n        ),\n        \"-c:v\", \"libx264\",\n        \"-pix_fmt\", \"yuv420p\",\n        \"-y\",\n        title_file\n    ]\n    \n    print(\"Creating title card...\")\n    subprocess.run(cmd)\n    \n    return title_file if os.path.exists(title_file) else None\n\n# Create end credits\ndef create_credits():\n    credits_file = os.path.join(TEMP_DIR, \"credits.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=60:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            \"enable='between(t,0,5)',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='A Multimedia Poetry Project':fontcolor=white:fontsize=40:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+30:\"\n            \"enable='between(t,0.5,5)',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=30:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+100:\"\n            \"enable='between(t,1,5)'\"\n        ),\n        \"-c:v\", \"libx264\",\n        \"-pix_fmt\", \"yuv420p\",\n        \"-y\",\n        credits_file\n    ]\n    \n    print(\"Creating end credits...\")\n    subprocess.run(cmd)\n    \n    return credits_file if os.path.exists(credits_file) else None\n\n# Extract video without audio\ndef extract_video(input_video):\n    video_file = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-i\", input_video,\n        \"-c:v\", \"copy\",\n        \"-an\",\n        \"-y\",\n        video_file\n    ]\n    \n    print(\"Extracting video without audio...\")\n    subprocess.run(cmd)\n    \n    return video_file if os.path.exists(video_file) else None\n\n# Create layered voice tracks\ndef create_layered_voice(input_video):\n    # 1. Extract main voice\n    main_voice = os.path.join(TEMP_DIR, \"main_voice.wav\")\n    cmd_main = [\n        \"ffmpeg\",\n        \"-i\", input_video,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\",\n        main_voice\n    ]\n    print(\"Extracting main voice...\")\n    subprocess.run(cmd_main)\n    \n    # 2. Create echo layer\n    echo_voice = os.path.join(TEMP_DIR, \"echo_voice.wav\")\n    cmd_echo = [\n        \"ffmpeg\",\n        \"-i\", main_voice,\n        \"-af\", \"aecho=0.6:0.3:60|90:0.4|0.3,volume=0.4\",\n        \"-y\",\n        echo_voice\n    ]\n    print(\"Creating echo layer...\")\n    subprocess.run(cmd_echo)\n    \n    # 3. Create reverb layer\n    reverb_voice = os.path.join(TEMP_DIR, \"reverb_voice.wav\")\n    cmd_reverb = [\n        \"ffmpeg\",\n        \"-i\", main_voice,\n        \"-af\", \"aecho=0.8:0.5:500|700:0.2|0.1,volume=0.3\",\n        \"-y\",\n        reverb_voice\n    ]\n    print(\"Creating reverb layer...\")\n    subprocess.run(cmd_reverb)\n    \n    # 4. Create lower octave layer\n    low_voice = os.path.join(TEMP_DIR, \"low_voice.wav\")\n    cmd_low = [\n        \"ffmpeg\",\n        \"-i\", main_voice,\n        \"-af\", \"asetrate=44100*0.85,aresample=44100,volume=0.25\",\n        \"-y\",\n        low_voice\n    ]\n    print(\"Creating lower octave layer...\")\n    subprocess.run(cmd_low)\n    \n    return {\n        'main': main_voice,\n        'echo': echo_voice,\n        'reverb': reverb_voice,\n        'low': low_voice\n    }\n\n# Mix voice layers\ndef mix_voice_layers(voice_layers):\n    # Mix the voices first\n    mixed_voice = os.path.join(TEMP_DIR, \"mixed_voice.wav\")\n    \n    # Use amix with compatible parameters\n    mix_cmd = [\n        \"ffmpeg\",\n        \"-i\", voice_layers['main'],\n        \"-i\", voice_layers['echo'],\n        \"-i\", voice_layers['reverb'],\n        \"-i\", voice_layers['low'],\n        \"-filter_complex\", \"amix=inputs=4:duration=longest\",\n        \"-y\",\n        mixed_voice\n    ]\n    \n    print(\"Mixing voice layers...\")\n    subprocess.run(mix_cmd)\n    \n    return mixed_voice if os.path.exists(mixed_voice) else None\n\n# Create ambient tracks\ndef create_ambient_tracks():\n    # Create drone sound\n    drone = os.path.join(TEMP_DIR, \"drone.wav\")\n    cmd_drone = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.07*sin(2*PI*55*t)+0.04*sin(2*PI*55*1.5*t):s=44100:d=110\",\n        \"-af\", \"afade=t=in:st=0:d=3,afade=t=out:st=105:d=5,volume=0.2\",\n        \"-y\",\n        drone\n    ]\n    print(\"Creating drone ambient...\")\n    subprocess.run(cmd_drone)\n    \n    # Create ambient wind\n    wind = os.path.join(TEMP_DIR, \"wind.wav\")\n    cmd_wind = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anoisesrc=a=0.05:c=pink:d=110\",\n        \"-af\", \"lowpass=f=600,afade=t=in:st=0:d=2,afade=t=out:st=105:d=5,volume=0.15\",\n        \"-y\",\n        wind\n    ]\n    print(\"Creating wind ambient...\")\n    subprocess.run(cmd_wind)\n    \n    return {\n        'drone': drone,\n        'wind': wind\n    }\n\n# Create title and credits audio\ndef create_title_credits_audio():\n    # Title audio\n    title_audio = os.path.join(TEMP_DIR, \"title_audio.wav\")\n    cmd_title = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.2*sin(2*PI*220*t)+0.15*sin(2*PI*330*t)+0.1*sin(2*PI*440*t):s=44100:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=1,afade=t=out:st=3.5:d=1.5\",\n        \"-y\",\n        title_audio\n    ]\n    print(\"Creating title audio...\")\n    subprocess.run(cmd_title)\n    \n    # Credits audio\n    credits_audio = os.path.join(TEMP_DIR, \"credits_audio.wav\")\n    cmd_credits = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.15*sin(2*PI*146.83*t)+0.1*sin(2*PI*220*t)+0.08*sin(2*PI*293.66*t):s=44100:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=1,afade=t=out:st=3.5:d=1.5\",\n        \"-y\",\n        credits_audio\n    ]\n    print(\"Creating credits audio...\")\n    subprocess.run(cmd_credits)\n    \n    return {\n        'title': title_audio,\n        'credits': credits_audio\n    }\n\n# Concatenate videos\ndef concatenate_videos(title_file, video_file, credits_file):\n    concat_list = os.path.join(TEMP_DIR, \"concat.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\",\n        concat_video\n    ]\n    \n    print(\"Concatenating videos...\")\n    subprocess.run(cmd)\n    \n    return concat_video if os.path.exists(concat_video) else None\n\n# Prepare audio with timing adjustments\ndef create_final_audio_mix(mixed_voice, ambient_tracks, effect_sounds):\n    # Make voice start earlier\n    shifted_voice = os.path.join(TEMP_DIR, \"voice_shifted.wav\")\n    cmd_shift = [\n        \"ffmpeg\",\n        \"-i\", mixed_voice,\n        \"-af\", \"adelay=1000|1000\", # Start after 1 second\n        \"-y\",\n        shifted_voice\n    ]\n    print(\"Shifting voice timing...\")\n    subprocess.run(cmd_shift)\n    \n    # Now mix with ambient tracks\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Handle timing and mixing with simpler filter chains\n    filter_complex = [\n        # Format title audio \n        \"[1:0]atrim=0:5,asetpts=PTS-STARTPTS,volume=0.8[title]\",\n        # Format voice\n        \"[0:0]volume=1.0[voice]\",\n        # Format drone\n        \"[2:0]volume=0.25[drone]\",\n        # Format wind\n        \"[3:0]volume=0.15[wind]\",\n        # Format credits\n        \"[4:0]atrim=0:5,asetpts=PTS-STARTPTS,volume=0.7[credits]\",\n        # Mix title and voice\n        \"[title][voice]amix=inputs=2:duration=longest[tv]\",\n        # Mix with drone\n        \"[tv][drone]amix=inputs=2:duration=longest[tvd]\",\n        # Mix with wind\n        \"[tvd][wind]amix=inputs=2:duration=longest[tvdw]\",\n        # Add credits\n        \"[tvdw][credits]amix=inputs=2:duration=longest[aout]\"\n    ]\n    \n    cmd_mix = [\n        \"ffmpeg\",\n        \"-i\", shifted_voice,\n        \"-i\", effect_sounds['title'],\n        \"-i\", ambient_tracks['drone'],\n        \"-i\", ambient_tracks['wind'],\n        \"-i\", effect_sounds['credits'],\n        \"-filter_complex\", \";\".join(filter_complex),\n        \"-map\", \"[aout]\",\n        \"-y\",\n        final_mix\n    ]\n    \n    print(\"Creating final audio mix...\")\n    subprocess.run(cmd_mix)\n    \n    # Verify the mix length is correct (total length should be 110.91 seconds - 5+100.91+5)\n    if os.path.exists(final_mix):\n        cmd_duration = [\n            \"ffprobe\",\n            \"-v\", \"error\",\n            \"-show_entries\", \"format=duration\",\n            \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n            final_mix\n        ]\n        result = subprocess.run(cmd_duration, capture_output=True, text=True)\n        duration = float(result.stdout.strip())\n        print(f\"Audio mix duration: {duration:.2f} seconds\")\n    \n    return final_mix if os.path.exists(final_mix) else None\n\n# Add audio to video\ndef add_audio_to_video(video_file, audio_file):\n    cmd = [\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"192k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Final version with layered voices and improved timing\",\n        \"-y\",\n        OUTPUT_FILE\n    ]\n    \n    print(\"Adding final audio mix to video...\")\n    subprocess.run(cmd)\n    \n    return OUTPUT_FILE if os.path.exists(OUTPUT_FILE) else None\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING FINAL VERSION WITH LAYERED VOICES AND IMPROVED TIMING =====\\n\")\n    \n    # Find best voice track\n    voice_file = find_voice_track()\n    if not voice_file:\n        return\n    \n    # Create title and credits\n    title_file = create_title()\n    credits_file = create_credits()\n    \n    if not title_file or not credits_file:\n        print(\"Error creating title or credits\")\n        return\n    \n    # Extract video\n    video_only = extract_video(voice_file)\n    if not video_only:\n        print(\"Error extracting video\")\n        return\n    \n    # Create layered voice tracks\n    voice_layers = create_layered_voice(voice_file)\n    for key, file in voice_layers.items():\n        if not os.path.exists(file):\n            print(f\"Error creating {key} voice layer\")\n            return\n    \n    # Mix voice layers\n    mixed_voice = mix_voice_layers(voice_layers)\n    if not mixed_voice:\n        print(\"Error mixing voice layers\")\n        return\n    \n    # Create ambient tracks\n    ambient_tracks = create_ambient_tracks()\n    for key, file in ambient_tracks.items():\n        if not os.path.exists(file):\n            print(f\"Error creating {key} ambient track\")\n            return\n    \n    # Create title and credits audio\n    effect_sounds = create_title_credits_audio()\n    for key, file in effect_sounds.items():\n        if not os.path.exists(file):\n            print(f\"Error creating {key} effect sound\")\n            return\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(title_file, video_only, credits_file)\n    if not concat_video:\n        print(\"Error concatenating videos\")\n        return\n    \n    # Create final audio mix with proper timing\n    final_mix = create_final_audio_mix(mixed_voice, ambient_tracks, effect_sounds)\n    if not final_mix:\n        print(\"Error creating final audio mix\")\n        return\n    \n    # Add audio to video\n    final_output = add_audio_to_video(concat_video, final_mix)\n    if not final_output:\n        print(\"Error adding audio to video\")\n        return\n    \n    print(f\"\\n===== SUCCESS! =====\")\n    print(f\"Final layered video created: {OUTPUT_FILE}\")\n    print(\"Improvements:\")\n    print(\"- Voice starts early (1 second into title) rather than 20 seconds in\")\n    print(\"- Added layered voices (main, echo, reverb, lower octave)\")\n    print(\"- Added continuous ambient sounds (drone, wind)\")\n    print(\"- Shorter title (5s) and credits (5s) for better pacing\")\n    print(\"- Maintains exact 1:40 (100.91 second) timing for the main content\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_FINAL_LAYERED.mp4",
      "WhereYouGoWhenYouLeave_{voice}.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "title.mp4",
      "credits.mp4",
      "video_only.mp4",
      "main_voice.wav",
      "echo_voice.wav",
      "reverb_voice.wav",
      "low_voice.wav",
      "mixed_voice.wav",
      "drone.wav",
      "wind.wav",
      "title_audio.wav",
      "credits_audio.wav",
      "concatenated.mp4",
      "voice_shifted.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+60:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+30:",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+100:"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd_main"
      },
      {
        "type": "run",
        "snippet": "cmd_echo"
      },
      {
        "type": "run",
        "snippet": "cmd_reverb"
      },
      {
        "type": "run",
        "snippet": "cmd_low"
      },
      {
        "type": "run",
        "snippet": "mix_cmd"
      },
      {
        "type": "run",
        "snippet": "cmd_drone"
      },
      {
        "type": "run",
        "snippet": "cmd_wind"
      },
      {
        "type": "run",
        "snippet": "cmd_title"
      },
      {
        "type": "run",
        "snippet": "cmd_credits"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd_shift"
      },
      {
        "type": "run",
        "snippet": "cmd_mix"
      },
      {
        "type": "run",
        "snippet": "cmd_duration, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "time"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/voice_after_native.py",
    "size": 12850,
    "lines": 361,
    "source": "import os\nimport subprocess\nimport shutil\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_VOICE_AFTER\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_VOICE_AFTER_NATIVE.mp4\")\n\n# Timing constants\nINTRO_LENGTH = 7\nOUTRO_LENGTH = 8\nNATIVE_START_DELAY = 2  # Native audio starts 2 seconds after intro\nVOICE_AFTER_NATIVE = 2  # Voice starts 2 seconds after native\nMAIN_LENGTH = 100.91  # Must be exactly 1:40 (100.91s) to match video length\n\n# Clean and create temp directory\nif os.path.exists(TEMP_DIR):\n    shutil.rmtree(TEMP_DIR)\nos.makedirs(TEMP_DIR)\nprint(f\"Using directory: {TEMP_DIR}\")\n\n# Find source files\ndef find_source_files():\n    # Find voice track\n    voice_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\")\n    if os.path.exists(voice_file):\n        print(f\"Using voice track: {os.path.basename(voice_file)}\")\n    else:\n        print(\"Voice track not found\")\n        return None, None\n    \n    # Find original video\n    original_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\")\n    if not os.path.exists(original_file):\n        original_file = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    \n    if os.path.exists(original_file):\n        print(f\"Using original video: {os.path.basename(original_file)}\")\n    else:\n        print(\"Original video not found, using voice file\")\n        original_file = voice_file\n    \n    return voice_file, original_file\n\n# Create intro and outro\ndef create_intro_outro():\n    # Create intro\n    intro_file = os.path.join(TEMP_DIR, \"intro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=80:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", intro_file\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro\")\n    \n    # Create outro\n    outro_file = os.path.join(TEMP_DIR, \"outro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2\",\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", outro_file\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro\")\n    \n    return intro_file, outro_file\n\n# Extract audio components\ndef extract_audio_components(voice_file, original_file):\n    # Extract original voice audio without any processing\n    voice_audio = os.path.join(TEMP_DIR, \"voice_original.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ])\n    print(\"Extracted original voice audio without any processing\")\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ])\n    print(\"Extracted native audio\")\n    \n    # Extract video only\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ])\n    print(\"Extracted video without audio\")\n    \n    return voice_audio, native_audio, video_only\n\n# Create necessary audio effects\ndef create_audio_effects():\n    # Ambient background noise\n    total_length = INTRO_LENGTH + MAIN_LENGTH + OUTRO_LENGTH\n    ambient_file = os.path.join(TEMP_DIR, \"ambient.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=pink:amplitude=0.01:duration={total_length}\",\n        \"-af\", \"volume=0.1\",\n        \"-y\", ambient_file\n    ])\n    print(f\"Created {total_length}-second ambient background\")\n    \n    # Intro audio effect\n    intro_audio = os.path.join(TEMP_DIR, \"intro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.15\",\n        \"-y\", intro_audio\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro audio effect\")\n    \n    # Outro audio effect\n    outro_audio = os.path.join(TEMP_DIR, \"outro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.15\",\n        \"-y\", outro_audio\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro audio effect\")\n    \n    return ambient_file, intro_audio, outro_audio\n\n# Concatenate videos\ndef concatenate_videos(intro_file, main_file, outro_file):\n    concat_file = os.path.join(TEMP_DIR, \"concat.txt\")\n    with open(concat_file, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(main_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concat_video.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"concat\", \n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    print(\"Concatenated intro + main video + outro\")\n    \n    return concat_video\n\n# Position audio tracks with correct timing\ndef position_all_audio(voice_audio, native_audio):\n    # Calculate exact delays\n    native_delay = INTRO_LENGTH + NATIVE_START_DELAY  # 7s intro + 2s delay = 9s\n    voice_delay = native_delay + VOICE_AFTER_NATIVE  # native start + 2s = 11s\n    \n    # Create silence for native audio delay\n    native_silence = os.path.join(TEMP_DIR, \"native_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={native_delay}\",\n        \"-y\", native_silence\n    ])\n    print(f\"Created {native_delay}-second silence for native audio delay\")\n    \n    # Create silence for voice delay\n    voice_silence = os.path.join(TEMP_DIR, \"voice_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={voice_delay}\",\n        \"-y\", voice_silence\n    ])\n    print(f\"Created {voice_delay}-second silence for voice delay\")\n    \n    # Position native audio with delay\n    native_positioned = os.path.join(TEMP_DIR, \"native_positioned.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", native_silence,\n        \"-i\", native_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", native_positioned\n    ])\n    print(f\"Positioned native audio to start {native_delay}s after beginning\")\n    \n    # Position voice with delay\n    voice_positioned = os.path.join(TEMP_DIR, \"voice_positioned.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_silence,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_positioned\n    ])\n    print(f\"Positioned voice to start {voice_delay}s after beginning ({VOICE_AFTER_NATIVE}s after native)\")\n    \n    return voice_positioned, native_positioned\n\n# Position intro and outro effects\ndef position_intro_outro(intro_audio, outro_audio):\n    total_length = INTRO_LENGTH + MAIN_LENGTH + OUTRO_LENGTH\n    \n    # Position intro at beginning\n    intro_positioned = os.path.join(TEMP_DIR, \"intro_positioned.wav\")\n    intro_padding = os.path.join(TEMP_DIR, \"intro_padding.wav\")\n    intro_pad_length = total_length - INTRO_LENGTH\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={intro_pad_length}\",\n        \"-y\", intro_padding\n    ])\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", intro_audio,\n        \"-i\", intro_padding,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", intro_positioned\n    ])\n    print(\"Positioned intro audio at beginning\")\n    \n    # Position outro at end\n    outro_positioned = os.path.join(TEMP_DIR, \"outro_positioned.wav\")\n    outro_padding = os.path.join(TEMP_DIR, \"outro_padding.wav\")\n    outro_pad_length = total_length - OUTRO_LENGTH\n    subprocess.run([\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={outro_pad_length}\",\n        \"-y\", outro_padding\n    ])\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", outro_padding,\n        \"-i\", outro_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", outro_positioned\n    ])\n    print(\"Positioned outro audio at end\")\n    \n    return intro_positioned, outro_positioned\n\n# Create final audio mix\ndef create_audio_mix(voice_positioned, native_positioned, ambient, intro_positioned, outro_positioned):\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Mix all audio layers with proper volumes\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_positioned,\n        \"-i\", native_positioned,\n        \"-i\", ambient,\n        \"-i\", intro_positioned,\n        \"-i\", outro_positioned,\n        \"-filter_complex\",\n        \"[0:0]volume=1.0[voice];\" +\n        \"[1:0]volume=0.7[native];\" +\n        \"[2:0]volume=0.1[ambient];\" +\n        \"[3:0]volume=0.15[intro];\" +\n        \"[4:0]volume=0.15[outro];\" +\n        \"[voice][native]amix=inputs=2:duration=longest[content];\" +\n        \"[ambient][intro]amix=inputs=2:duration=longest[amb_fx1];\" +\n        \"[amb_fx1][outro]amix=inputs=2:duration=longest[amb_fx];\" +\n        \"[content][amb_fx]amix=inputs=2:duration=longest[out]\",\n        \"-map\", \"[out]\",\n        \"-y\", final_mix\n    ])\n    print(\"Created final audio mix\")\n    \n    return final_mix\n\n# Add audio to video\ndef add_audio_to_video(video_file, audio_file):\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Final version with voice starting 2s after native audio\",\n        \"-y\", OUTPUT_FILE\n    ])\n    print(f\"Added audio to video: {OUTPUT_FILE}\")\n    \n    if os.path.exists(OUTPUT_FILE):\n        cmd = [\n            \"ffprobe\", \"-v\", \"error\",\n            \"-show_entries\", \"format=duration\",\n            \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n            OUTPUT_FILE\n        ]\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        duration = float(result.stdout.strip())\n        print(f\"Final video duration: {duration:.2f} seconds\")\n        return True\n    else:\n        return False\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING VERSION WITH VOICE STARTING 2s AFTER NATIVE AUDIO =====\\n\")\n    \n    # Find source files\n    voice_file, original_file = find_source_files()\n    if not voice_file or not original_file:\n        print(\"Error: Source files not found\")\n        return False\n    \n    # Create intro and outro\n    intro_file, outro_file = create_intro_outro()\n    \n    # Extract audio and video components\n    voice_audio, native_audio, video_only = extract_audio_components(voice_file, original_file)\n    \n    # Create ambient and effect audio\n    ambient_audio, intro_audio, outro_audio = create_audio_effects()\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Position all audio with proper delays\n    voice_positioned, native_positioned = position_all_audio(voice_audio, native_audio)\n    \n    # Position intro and outro effects\n    intro_positioned, outro_positioned = position_intro_outro(intro_audio, outro_audio)\n    \n    # Create final audio mix\n    final_mix = create_audio_mix(voice_positioned, native_positioned, ambient_audio, intro_positioned, outro_positioned)\n    \n    # Add audio to video\n    success = add_audio_to_video(concat_video, final_mix)\n    \n    if success:\n        print(\"\\n===== SUCCESS! =====\")\n        print(f\"Final video created: {OUTPUT_FILE}\")\n        print(\"\\nFinal specifications:\")\n        print(\"- Original voice track preserved exactly as is (no processing)\")\n        print(f\"- Native audio starts {NATIVE_START_DELAY}s after the {INTRO_LENGTH}s intro\")\n        print(f\"- Voice starts {VOICE_AFTER_NATIVE}s after native audio begins\")\n        print(f\"- Total timing: intro ({INTRO_LENGTH}s) \u2192 native audio ({NATIVE_START_DELAY}s delay) \u2192 voice ({VOICE_AFTER_NATIVE}s after native)\")\n        print(f\"- Main content: {MAIN_LENGTH} seconds (1:40)\")\n        print(f\"- Outro length: {OUTRO_LENGTH} seconds\")\n        return True\n    else:\n        print(\"Error: Failed to create final video\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_VOICE_AFTER_NATIVE.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "intro.mp4",
      "outro.mp4",
      "voice_original.wav",
      "native_audio.wav",
      "video_only.mp4",
      "ambient.wav",
      "intro_audio.wav",
      "outro_audio.wav",
      "concat_video.mp4",
      "native_silence.wav",
      "voice_silence.wav",
      "native_positioned.wav",
      "voice_positioned.wav",
      "intro_positioned.wav",
      "intro_padding.wav",
      "outro_positioned.wav",
      "outro_padding.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=80:x=(w-text_w)/2:y=(h-text_h)/2",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text=",
      ":fontcolor=white:fontsize=60:x=(w-text_w)/2:y=(h-text_h)/2"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='WHERE YOU GO WHEN YOU L"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\",\n        \"-vf\", \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:text='\u00a9 2025 RESURRECTING ATL"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=pink:amplitude=0.01:duration={total_length}\",\n        \"-af\", \"volume=0.1\",\n        \"-y\", ambient_file\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.15\",\n        \"-y\", int"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.15\",\n        \"-y\", "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"concat\", \n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={native_delay}\",\n        \"-y\", native_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={voice_delay}\",\n        \"-y\", voice_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", native_silence,\n        \"-i\", native_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", native_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_silence,\n        \"-i\", voice_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", voice_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={intro_pad_length}\",\n        \"-y\", intro_padding\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", intro_audio,\n        \"-i\", intro_padding,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", intro_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={outro_pad_length}\",\n        \"-y\", outro_padding\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", outro_padding,\n        \"-i\", outro_audio,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\", outro_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_positioned,\n        \"-i\", native_positioned,\n        \"-i\", ambient,\n        \"-i\", intro_positioned,\n        \"-i\", outro_positioned,\n        \"-filter_complex\",\n "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-s"
      },
      {
        "type": "run",
        "snippet": "cmd, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/stretched_voice_final.py",
    "size": 18246,
    "lines": 511,
    "source": "import os\nimport subprocess\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_STRETCHED\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_STRETCHED_FINAL.mp4\")\n\n# Timing constants (in seconds)\nINTRO_LENGTH = 7\nOUTRO_LENGTH = 8\nVOICE_LENGTH = 110  # 1:50\nTOTAL_LENGTH = 115  # 1:55 (7 second intro + 100 second video + 8 second outro)\nVOICE_DELAY = 3     # Start voice after 3 seconds of ambience\n\n# Ensure temp directory exists\nos.makedirs(TEMP_DIR, exist_ok=True)\nprint(f\"Using temp directory: {TEMP_DIR}\")\n\n# Find source files\ndef find_source_files():\n    # Find best voice track\n    voice_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_BroadcastPerfection.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ConfidentPresence.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_CrystalClarity.mp4\")\n    ]\n    \n    for file in voice_files:\n        if os.path.exists(file):\n            print(f\"Using voice track: {os.path.basename(file)}\")\n            voice_file = file\n            break\n    else:\n        print(\"No suitable voice track found\")\n        return None, None\n    \n    # Find original video with native audio\n    original_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    ]\n    \n    for file in original_files:\n        if os.path.exists(file):\n            print(f\"Using original video: {os.path.basename(file)}\")\n            original_file = file\n            break\n    else:\n        print(\"No original video found, using voice track as video source\")\n        original_file = voice_file\n    \n    return voice_file, original_file\n\n# Extract audio from sources\ndef extract_audio(voice_file, video_file):\n    # Extract voice audio\n    voice_audio = os.path.join(TEMP_DIR, \"voice_original.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-ar\", \"48000\",\n        \"-y\", voice_audio\n    ])\n    print(\"Extracted voice audio\")\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", video_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-ar\", \"48000\",\n        \"-y\", native_audio\n    ])\n    print(\"Extracted native audio\")\n    \n    # Get voice duration\n    cmd_duration = [\n        \"ffprobe\", \"-v\", \"error\",\n        \"-show_entries\", \"format=duration\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        voice_audio\n    ]\n    voice_duration = float(subprocess.run(cmd_duration, capture_output=True, text=True).stdout.strip())\n    print(f\"Original voice duration: {voice_duration:.2f} seconds\")\n    \n    return voice_audio, native_audio, voice_duration\n\n# Stretch the voice to 1:50 (110 seconds)\ndef stretch_voice(voice_audio, original_duration):\n    # Calculate stretch factor to reach 110 seconds (1:50)\n    stretch_factor = VOICE_LENGTH / original_duration\n    print(f\"Stretching voice by factor: {stretch_factor:.4f} to reach {VOICE_LENGTH} seconds\")\n    \n    # Stretch voice using atempo filter with careful pitch preservation\n    stretched_voice = os.path.join(TEMP_DIR, \"voice_stretched.wav\")\n    \n    # If stretch factor is large, break it into smaller steps to maintain quality\n    if stretch_factor < 0.5 or stretch_factor > 2.0:\n        # We'll use multiple passes for extreme stretching\n        intermediate = os.path.join(TEMP_DIR, \"voice_intermediate.wav\")\n        first_factor = 0.8 if stretch_factor < 1 else 1.25\n        \n        # First pass\n        subprocess.run([\n            \"ffmpeg\", \"-i\", voice_audio,\n            \"-filter:a\", f\"atempo={first_factor}\",\n            \"-y\", intermediate\n        ])\n        \n        # Second pass\n        second_factor = stretch_factor / first_factor\n        subprocess.run([\n            \"ffmpeg\", \"-i\", intermediate,\n            \"-filter:a\", f\"atempo={second_factor}\",\n            \"-y\", stretched_voice\n        ])\n    else:\n        # Single pass for moderate stretching\n        subprocess.run([\n            \"ffmpeg\", \"-i\", voice_audio,\n            \"-filter:a\", f\"atempo={1/stretch_factor}\",  # Inverse for stretching\n            \"-y\", stretched_voice\n        ])\n    \n    # Verify new duration\n    cmd_duration = [\n        \"ffprobe\", \"-v\", \"error\",\n        \"-show_entries\", \"format=duration\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        stretched_voice\n    ]\n    new_duration = float(subprocess.run(cmd_duration, capture_output=True, text=True).stdout.strip())\n    print(f\"Stretched voice duration: {new_duration:.2f} seconds (target: {VOICE_LENGTH})\")\n    \n    return stretched_voice\n\n# Create intro and outro videos with longer durations\ndef create_intro_outro():\n    # Create intro - 7 seconds\n    intro_file = os.path.join(TEMP_DIR, \"intro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            f\"enable='between(t,0,{INTRO_LENGTH})':\"\n            f\"alpha='if(lt(t,1),t/1,if(gt(t,{INTRO_LENGTH-1}),({INTRO_LENGTH}-t)/1,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHEN YOU LEAVE':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+60:\"\n            f\"enable='between(t,0.5,{INTRO_LENGTH})':\"\n            f\"alpha='if(lt(t,1.5),(t-0.5)/1,if(gt(t,{INTRO_LENGTH-1}),({INTRO_LENGTH}-t)/1,1))'\"\n        ),\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", intro_file\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro animation\")\n    \n    # Create outro - 8 seconds\n    outro_file = os.path.join(TEMP_DIR, \"outro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=60:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            f\"enable='between(t,0,{OUTRO_LENGTH})':\"\n            f\"alpha='if(lt(t,1),t/1,if(gt(t,{OUTRO_LENGTH-1}),({OUTRO_LENGTH}-t)/1,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='A Multimedia Poetry Project':fontcolor=white:fontsize=40:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+30:\"\n            f\"enable='between(t,0.5,{OUTRO_LENGTH})':\"\n            f\"alpha='if(lt(t,1.5),(t-0.5)/1,if(gt(t,{OUTRO_LENGTH-1}),({OUTRO_LENGTH}-t)/1,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=30:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+100:\"\n            f\"enable='between(t,1,{OUTRO_LENGTH})':\"\n            f\"alpha='if(lt(t,2),(t-1)/1,if(gt(t,{OUTRO_LENGTH-1}),({OUTRO_LENGTH}-t)/1,1))'\"\n        ),\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", outro_file\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro animation\")\n    \n    return intro_file, outro_file\n\n# Create ambient audio for the full 1:55 length\ndef create_ambient_audio():\n    # Create continuous ambient background\n    ambient_audio = os.path.join(TEMP_DIR, \"ambient_full.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=brown:amplitude=0.02:duration={TOTAL_LENGTH}\",\n        \"-af\", \"aformat=sample_fmts=s16:channel_layouts=stereo:sample_rates=48000\",\n        \"-y\", ambient_audio\n    ])\n    print(f\"Created {TOTAL_LENGTH}-second ambient background\")\n    \n    # Create intro atmosphere audio with fade-in\n    intro_audio = os.path.join(TEMP_DIR, \"intro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0.1*sin(2*PI*220*t)+0.07*sin(2*PI*330*t)+0.05*sin(2*PI*440*t):s=48000:d={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.4\",\n        \"-y\", intro_audio\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro audio effect\")\n    \n    # Create outro atmosphere audio with fade-out\n    outro_audio = os.path.join(TEMP_DIR, \"outro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0.08*sin(2*PI*146.83*t)+0.05*sin(2*PI*220*t)+0.03*sin(2*PI*293.66*t):s=48000:d={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-3}:d=3,volume=0.35\",\n        \"-y\", outro_audio\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro audio effect\")\n    \n    return ambient_audio, intro_audio, outro_audio\n\n# Extract video without audio\ndef extract_video(video_file):\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", video_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ])\n    print(\"Extracted video without audio\")\n    \n    return video_only\n\n# Concatenate videos\ndef concatenate_videos(intro_file, video_file, outro_file):\n    concat_list = os.path.join(TEMP_DIR, \"concat.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    print(\"Concatenated videos (intro + main + outro)\")\n    \n    return concat_video\n\n# Position audio with precise timing\ndef position_audio_layers(stretched_voice, native_audio, ambient_audio, intro_audio, outro_audio):\n    # 1. Position the stretched voice with a 3-second delay\n    voice_positioned = os.path.join(TEMP_DIR, \"voice_positioned.wav\")\n    \n    # Create silence for delay\n    silence = os.path.join(TEMP_DIR, f\"silence_{VOICE_DELAY}s.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anullsrc=r=48000:cl=mono:d={VOICE_DELAY}\",\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", silence\n    ])\n    \n    # Concatenate silence + voice\n    voice_concat = os.path.join(TEMP_DIR, \"voice_concat.txt\")\n    with open(voice_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(silence)}'\\n\")\n        f.write(f\"file '{os.path.abspath(stretched_voice)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", voice_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", voice_positioned\n    ])\n    print(f\"Positioned voice track with {VOICE_DELAY}-second delay\")\n    \n    # 2. Position native audio with same 3-second delay as voice\n    native_positioned = os.path.join(TEMP_DIR, \"native_positioned.wav\")\n    native_concat = os.path.join(TEMP_DIR, \"native_concat.txt\")\n    with open(native_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(silence)}'\\n\")  # Same delay as voice\n        f.write(f\"file '{os.path.abspath(native_audio)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", native_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", native_positioned\n    ])\n    print(f\"Positioned native audio with {VOICE_DELAY}-second delay (same as voice)\")\n    \n    # 3. Position intro audio at the beginning\n    intro_positioned = os.path.join(TEMP_DIR, \"intro_positioned.wav\")\n    \n    # Create long silence after intro\n    long_silence = os.path.join(TEMP_DIR, \"long_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anullsrc=r=48000:cl=mono:d={TOTAL_LENGTH - INTRO_LENGTH}\",\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", long_silence\n    ])\n    \n    # Concat intro + silence\n    intro_concat = os.path.join(TEMP_DIR, \"intro_concat.txt\")\n    with open(intro_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_audio)}'\\n\")\n        f.write(f\"file '{os.path.abspath(long_silence)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", intro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", intro_positioned\n    ])\n    print(\"Positioned intro audio at beginning\")\n    \n    # 4. Position outro audio at the end\n    outro_positioned = os.path.join(TEMP_DIR, \"outro_positioned.wav\")\n    \n    # Create long silence before outro\n    pre_outro_silence = os.path.join(TEMP_DIR, \"pre_outro_silence.wav\")\n    pre_outro_length = TOTAL_LENGTH - OUTRO_LENGTH\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anullsrc=r=48000:cl=mono:d={pre_outro_length}\",\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", pre_outro_silence\n    ])\n    \n    # Concat silence + outro\n    outro_concat = os.path.join(TEMP_DIR, \"outro_concat.txt\")\n    with open(outro_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(pre_outro_silence)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_audio)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", outro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", outro_positioned\n    ])\n    print(\"Positioned outro audio at end\")\n    \n    return voice_positioned, native_positioned, intro_positioned, outro_positioned\n\n# Mix all audio layers\ndef mix_audio_layers(voice, native, ambient, intro, outro):\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Mix all layers with appropriate volume levels\n    filter_complex = [\n        # Ambient background (continuous through whole piece)\n        \"[0:0]volume=0.18[ambient]\",\n        \n        # Voice layer (starts after 3 seconds)\n        \"[1:0]volume=1.0[voice]\",\n        \n        # Native audio (starts after 3 seconds)\n        \"[2:0]volume=0.9[native]\",\n        \n        # Intro audio effects (at beginning)\n        \"[3:0]volume=0.3[intro_fx]\",\n        \n        # Outro audio effects (at end)\n        \"[4:0]volume=0.25[outro_fx]\",\n        \n        # Mix ambient with intro/outro effects\n        \"[ambient][intro_fx]amix=inputs=2:duration=longest[amb_intro]\",\n        \"[amb_intro][outro_fx]amix=inputs=2:duration=longest[amb_fx]\",\n        \n        # Mix native audio with voice\n        \"[native][voice]amix=inputs=2:duration=longest[content]\",\n        \n        # Mix everything together\n        \"[amb_fx][content]amix=inputs=2:duration=longest[final]\"\n    ]\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", ambient,\n        \"-i\", voice,\n        \"-i\", native,\n        \"-i\", intro,\n        \"-i\", outro,\n        \"-filter_complex\", \";\".join(filter_complex),\n        \"-map\", \"[final]\",\n        \"-y\", final_mix\n    ])\n    print(\"Created final audio mix with all layers\")\n    \n    return final_mix\n\n# Add audio to video\ndef add_audio_to_video(video_file, audio_file):\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",  # Higher quality audio\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Final version with stretched voice (1:55 total length)\",\n        \"-y\", OUTPUT_FILE\n    ])\n    print(f\"Created final video: {OUTPUT_FILE}\")\n    \n    return OUTPUT_FILE if os.path.exists(OUTPUT_FILE) else None\n\n# Main function\ndef main():\n    print(f\"\\n===== CREATING FINAL VERSION WITH STRETCHED VOICE ({VOICE_LENGTH}s) AND EXTENDED INTRO/OUTRO =====\\n\")\n    \n    # Find source files\n    voice_file, video_file = find_source_files()\n    if not voice_file or not video_file:\n        print(\"Error: Missing source files\")\n        return\n    \n    # Extract audio\n    voice_audio, native_audio, voice_duration = extract_audio(voice_file, video_file)\n    \n    # Stretch voice to 1:50 (110 seconds)\n    stretched_voice = stretch_voice(voice_audio, voice_duration)\n    \n    # Create intro and outro\n    intro_file, outro_file = create_intro_outro()\n    \n    # Create ambient audio\n    ambient_audio, intro_audio, outro_audio = create_ambient_audio()\n    \n    # Extract video without audio\n    video_only = extract_video(video_file)\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Position audio layers with precise timing\n    voice_positioned, native_positioned, intro_positioned, outro_positioned = position_audio_layers(\n        stretched_voice, native_audio, ambient_audio, intro_audio, outro_audio\n    )\n    \n    # Mix all audio layers\n    final_mix = mix_audio_layers(\n        ambient_audio, voice_positioned, native_positioned, intro_positioned, outro_positioned\n    )\n    \n    # Add audio to video\n    final_output = add_audio_to_video(concat_video, final_mix)\n    \n    if final_output:\n        print(\"\\n===== SUCCESS! =====\")\n        print(f\"Final video created: {final_output}\")\n        print(\"\\nNew timing structure:\")\n        print(f\"- Total length: {TOTAL_LENGTH} seconds (1:55)\")\n        print(f\"- Intro: {INTRO_LENGTH} seconds\")\n        print(f\"- Main content: 100 seconds (1:40)\")\n        print(f\"- Outro: {OUTRO_LENGTH} seconds\")\n        print(f\"- Voice track: Stretched to {VOICE_LENGTH} seconds (1:50)\")\n        print(f\"- Voice/video start: After {VOICE_DELAY} seconds of ambient intro\")\n        print(\"\\nAudio layers:\")\n        print(\"1. Continuous ambient background throughout\")\n        print(\"2. Voice starts simultaneously with native video audio\")\n        print(\"3. Enhanced intro/outro audio effects\")\n        print(\"4. All layers balanced for optimal listening experience\")\n        return True\n    else:\n        print(\"Error: Failed to create final video\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_STRETCHED_FINAL.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4",
      "WhereYouGoWhenYouLeave_BroadcastPerfection.mp4",
      "WhereYouGoWhenYouLeave_ConfidentPresence.mp4",
      "WhereYouGoWhenYouLeave_CrystalClarity.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "voice_original.wav",
      "native_audio.wav",
      "voice_stretched.wav",
      "voice_intermediate.wav",
      "intro.mp4",
      "outro.mp4",
      "ambient_full.wav",
      "intro_audio.wav",
      "outro_audio.wav",
      "video_only.mp4",
      "concatenated.mp4",
      "voice_positioned.wav",
      "silence_{VOICE_DELAY}s.wav",
      "native_positioned.wav",
      "intro_positioned.wav",
      "long_silence.wav",
      "outro_positioned.wav",
      "pre_outro_silence.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      ")\n    \n    return voice_audio, native_audio, voice_duration\n\n# Stretch the voice to 1:50 (110 seconds)\ndef stretch_voice(voice_audio, original_duration):\n    # Calculate stretch factor to reach 110 seconds (1:50)\n    stretch_factor = VOICE_LENGTH / original_duration\n    print(f",
      ", intermediate\n        ])\n        \n        # Second pass\n        second_factor = stretch_factor / first_factor\n        subprocess.run([\n            ",
      "atempo={1/stretch_factor}",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "if(lt(t,1),t/1,if(gt(t,{INTRO_LENGTH-1}),({INTRO_LENGTH}-t)/1,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+60:",
      "if(lt(t,1.5),(t-0.5)/1,if(gt(t,{INTRO_LENGTH-1}),({INTRO_LENGTH}-t)/1,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "if(lt(t,1),t/1,if(gt(t,{OUTRO_LENGTH-1}),({OUTRO_LENGTH}-t)/1,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+30:",
      "if(lt(t,1.5),(t-0.5)/1,if(gt(t,{OUTRO_LENGTH-1}),({OUTRO_LENGTH}-t)/1,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+100:",
      "if(lt(t,2),(t-1)/1,if(gt(t,{OUTRO_LENGTH-1}),({OUTRO_LENGTH}-t)/1,1))",
      ",\n        \n        # Mix ambient with intro/outro effects\n        ",
      "\\n===== CREATING FINAL VERSION WITH STRETCHED VOICE ({VOICE_LENGTH}s) AND EXTENDED INTRO/OUTRO =====\\n",
      "- Voice/video start: After {VOICE_DELAY} seconds of ambient intro",
      "3. Enhanced intro/outro audio effects"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-ar\", \"48000\",\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", video_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-ar\", \"48000\",\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "cmd_duration, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "[\n            \"ffmpeg\", \"-i\", voice_audio,\n            \"-filter:a\", f\"atempo={first_factor}\",\n            \"-y\", intermediate\n        ]"
      },
      {
        "type": "run",
        "snippet": "[\n            \"ffmpeg\", \"-i\", intermediate,\n            \"-filter:a\", f\"atempo={second_factor}\",\n            \"-y\", stretched_voice\n        ]"
      },
      {
        "type": "run",
        "snippet": "[\n            \"ffmpeg\", \"-i\", voice_audio,\n            \"-filter:a\", f\"atempo={1/stretch_factor}\",  # Inverse for stretching\n            \"-y\", stretched_voice\n        ]"
      },
      {
        "type": "run",
        "snippet": "cmd_duration, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n    "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n    "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=brown:amplitude=0.02:duration={TOTAL_LENGTH}\",\n        \"-af\", \"aformat=sample_fmts=s16:channel_layouts=stereo:sample_rates=48"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0.1*sin(2*PI*220*t"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0.08*sin(2*PI*146.83*t"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", video_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anullsrc=r=48000:cl=mono:d={VOICE_DELAY}\",\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", voice_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", voice_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", native_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", native_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anullsrc=r=48000:cl=mono:d={TOTAL_LENGTH - INTRO_LENGTH}\",\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", long_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", intro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", intro_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anullsrc=r=48000:cl=mono:d={pre_outro_length}\",\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", pre_outro_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", outro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", outro_positioned\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", ambient,\n        \"-i\", voice,\n        \"-i\", native,\n        \"-i\", intro,\n        \"-i\", outro,\n        \"-filter_complex\", \";\".join(filter_complex"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",  # Higher quality audio\n        \"-map\", \"0:v\",\n        \"-"
      }
    ],
    "imports": [
      "os",
      "subprocess"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/fix_pitch_and_length.py",
    "size": 14884,
    "lines": 422,
    "source": "import os\nimport subprocess\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_FIXED_PITCH\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FIXED_FINAL.mp4\")\n\n# Timing constants (in seconds)\nINTRO_LENGTH = 7\nOUTRO_LENGTH = 8\nMAIN_LENGTH = 100.91  # Exact 1:40 as required\nTOTAL_LENGTH = INTRO_LENGTH + MAIN_LENGTH + OUTRO_LENGTH  # Should be close to 1:55\nVOICE_DELAY = 3  # Delay before voice starts\n\n# Ensure temp directory exists\nos.makedirs(TEMP_DIR, exist_ok=True)\nprint(f\"Using directory: {TEMP_DIR}\")\n\n# Find source files\ndef find_source_files():\n    # Find best voice track\n    voice_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_BroadcastPerfection.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ConfidentPresence.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_CrystalClarity.mp4\")\n    ]\n    \n    for file in voice_files:\n        if os.path.exists(file):\n            print(f\"Using voice track: {os.path.basename(file)}\")\n            voice_file = file\n            break\n    else:\n        print(\"No suitable voice track found\")\n        return None, None\n    \n    # Find original video with native audio\n    original_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    ]\n    \n    for file in original_files:\n        if os.path.exists(file):\n            print(f\"Using original video: {os.path.basename(file)}\")\n            original_file = file\n            break\n    else:\n        print(\"No original video found, using voice track as video source\")\n        original_file = voice_file\n    \n    return voice_file, original_file\n\n# Create intro and outro videos\ndef create_intro_outro():\n    # Create intro\n    intro_file = os.path.join(TEMP_DIR, \"intro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60,\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHEN YOU LEAVE':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+60\"\n        ),\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", intro_file\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro\")\n    \n    # Create outro\n    outro_file = os.path.join(TEMP_DIR, \"outro.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=60:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60,\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='A Multimedia Poetry Project':fontcolor=white:fontsize=40:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+30,\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=30:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+100\"\n        ),\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", outro_file\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro\")\n    \n    return intro_file, outro_file\n\n# Create audio effects\ndef create_audio_effects():\n    # Intro audio\n    intro_audio = os.path.join(TEMP_DIR, \"intro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.3\",\n        \"-y\", intro_audio\n    ])\n    print(f\"Created {INTRO_LENGTH}-second intro audio\")\n    \n    # Outro audio\n    outro_audio = os.path.join(TEMP_DIR, \"outro_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.25\",\n        \"-y\", outro_audio\n    ])\n    print(f\"Created {OUTRO_LENGTH}-second outro audio\")\n    \n    # Ambient audio for whole track\n    ambient_audio = os.path.join(TEMP_DIR, \"ambient.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=pink:duration={TOTAL_LENGTH}:amplitude=0.02\",\n        \"-af\", \"volume=0.15\",\n        \"-y\", ambient_audio\n    ])\n    print(f\"Created {TOTAL_LENGTH}-second ambient audio\")\n    \n    return intro_audio, outro_audio, ambient_audio\n\n# Extract and process audio WITHOUT stretching\ndef process_audio(voice_file, original_file):\n    # Extract voice audio - preserve original pitch\n    voice_audio = os.path.join(TEMP_DIR, \"voice_original.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ])\n    print(\"Extracted voice audio (preserving original pitch)\")\n    \n    # Get duration of voice track\n    cmd_duration = [\n        \"ffprobe\", \"-v\", \"error\",\n        \"-show_entries\", \"format=duration\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        voice_audio\n    ]\n    voice_duration = float(subprocess.run(cmd_duration, capture_output=True, text=True).stdout.strip())\n    print(f\"Original voice duration: {voice_duration:.2f} seconds\")\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ])\n    print(\"Extracted native audio\")\n    \n    return voice_audio, native_audio\n\n# Extract video without audio\ndef extract_video(video_file):\n    video_only = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", video_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ])\n    print(\"Extracted video without audio\")\n    \n    return video_only\n\n# Concatenate videos\ndef concatenate_videos(intro_file, video_file, outro_file):\n    concat_list = os.path.join(TEMP_DIR, \"concat.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(intro_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    print(\"Concatenated videos (intro + main + outro)\")\n    \n    return concat_video\n\n# Position all audio with proper timing\ndef position_audio_layers(voice_audio, native_audio, intro_audio, outro_audio, ambient_audio):\n    # Create a 3-second silence file for delay\n    silence_file = os.path.join(TEMP_DIR, \"silence_3s.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=3\",\n        \"-y\", silence_file\n    ])\n    print(\"Created 3-second silence for delay\")\n    \n    # Add delay to voice - KEEPING ORIGINAL PITCH\n    voice_delayed = os.path.join(TEMP_DIR, \"voice_delayed.wav\")\n    voice_concat = os.path.join(TEMP_DIR, \"voice_concat.txt\")\n    with open(voice_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(silence_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(voice_audio)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", voice_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", voice_delayed\n    ])\n    print(\"Added 3-second delay to voice (preserving original pitch)\")\n    \n    # Add same delay to native audio\n    native_delayed = os.path.join(TEMP_DIR, \"native_delayed.wav\")\n    native_concat = os.path.join(TEMP_DIR, \"native_concat.txt\")\n    with open(native_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(silence_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(native_audio)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", native_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", native_delayed\n    ])\n    print(\"Added 3-second delay to native audio\")\n    \n    # Position intro audio at beginning\n    intro_extended = os.path.join(TEMP_DIR, \"intro_extended.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", intro_audio,\n        \"-af\", \"apad=pad_dur=108\",  # Pad to intro + main + outro length\n        \"-y\", intro_extended\n    ])\n    print(\"Extended intro audio to full length\")\n    \n    # Position outro audio at end\n    outro_extended = os.path.join(TEMP_DIR, \"outro_extended.wav\")\n    # Create a silence with exact length: intro + main length\n    pre_outro_silence = os.path.join(TEMP_DIR, \"pre_outro_silence.wav\")\n    pre_outro_length = INTRO_LENGTH + MAIN_LENGTH\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={pre_outro_length}\",\n        \"-y\", pre_outro_silence\n    ])\n    \n    # Concatenate pre-outro silence + outro audio\n    outro_concat = os.path.join(TEMP_DIR, \"outro_concat.txt\")\n    with open(outro_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(pre_outro_silence)}'\\n\")\n        f.write(f\"file '{os.path.abspath(outro_audio)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", outro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", outro_extended\n    ])\n    print(\"Positioned outro audio at end\")\n    \n    return voice_delayed, native_delayed, intro_extended, outro_extended\n\n# Mix all audio components\ndef mix_audio_layers(voice_delayed, native_delayed, ambient_audio, intro_extended, outro_extended):\n    # Mix all layers with proper volumes\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Create a filter complex for audio mixing\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", voice_delayed,         # Voice with delay\n        \"-i\", native_delayed,        # Native audio with delay\n        \"-i\", ambient_audio,         # Ambient background\n        \"-i\", intro_extended,        # Intro effects\n        \"-i\", outro_extended,        # Outro effects\n        \"-filter_complex\",\n        \"[0:0]volume=0.95[voice];\" +  # Voice layer\n        \"[1:0]volume=0.8[native];\" +  # Native audio layer\n        \"[2:0]volume=0.2[ambient];\" + # Ambient background\n        \"[3:0]volume=0.25[intro];\" +  # Intro effects\n        \"[4:0]volume=0.25[outro];\" +  # Outro effects\n        \"[voice][native]amix=inputs=2:duration=first[content];\" +\n        \"[ambient][intro]amix=inputs=2:duration=first[amb_intro];\" +\n        \"[amb_intro][outro]amix=inputs=2:duration=first[effects];\" +\n        \"[content][effects]amix=inputs=2:duration=first[out]\",\n        \"-map\", \"[out]\",\n        \"-y\", final_mix\n    ])\n    print(\"Created final audio mix with proper timing and volumes\")\n    \n    # Verify the final mix length\n    cmd_duration = [\n        \"ffprobe\", \"-v\", \"error\",\n        \"-show_entries\", \"format=duration\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        final_mix\n    ]\n    mix_duration = float(subprocess.run(cmd_duration, capture_output=True, text=True).stdout.strip())\n    print(f\"Final audio mix duration: {mix_duration:.2f} seconds\")\n    \n    return final_mix\n\n# Add audio to video\ndef add_audio_to_video(video_file, audio_file):\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",  # Higher quality audio\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",  # This ensures the video is not longer than needed\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Final version with original voice pitch and correct length\",\n        \"-y\", OUTPUT_FILE\n    ])\n    print(f\"Added audio to video: {OUTPUT_FILE}\")\n    \n    # Verify final output length\n    if os.path.exists(OUTPUT_FILE):\n        cmd_duration = [\n            \"ffprobe\", \"-v\", \"error\",\n            \"-show_entries\", \"format=duration\",\n            \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n            OUTPUT_FILE\n        ]\n        final_duration = float(subprocess.run(cmd_duration, capture_output=True, text=True).stdout.strip())\n        print(f\"Final output duration: {final_duration:.2f} seconds (target: ~{TOTAL_LENGTH} seconds)\")\n        return True\n    else:\n        return False\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING FINAL VERSION WITH ORIGINAL VOICE PITCH AND CORRECT LENGTH =====\\n\")\n    \n    # Find source files\n    voice_file, original_file = find_source_files()\n    if not voice_file or not original_file:\n        print(\"Error: Missing source files\")\n        return\n    \n    # Create intro and outro videos\n    intro_file, outro_file = create_intro_outro()\n    \n    # Create audio effects\n    intro_audio, outro_audio, ambient_audio = create_audio_effects()\n    \n    # Process audio files (preserving original pitch)\n    voice_audio, native_audio = process_audio(voice_file, original_file)\n    \n    # Extract video without audio\n    video_only = extract_video(original_file)\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(intro_file, video_only, outro_file)\n    \n    # Position audio layers with proper timing\n    voice_delayed, native_delayed, intro_extended, outro_extended = position_audio_layers(\n        voice_audio, native_audio, intro_audio, outro_audio, ambient_audio\n    )\n    \n    # Mix all audio layers\n    final_mix = mix_audio_layers(\n        voice_delayed, native_delayed, ambient_audio, intro_extended, outro_extended\n    )\n    \n    # Add audio to video\n    success = add_audio_to_video(concat_video, final_mix)\n    \n    if success:\n        print(\"\\n===== SUCCESS! =====\")\n        print(f\"Final video created: {OUTPUT_FILE}\")\n        print(\"\\nFinal specifications:\")\n        print(f\"- Total length: ~{TOTAL_LENGTH} seconds (1:55)\")\n        print(f\"- Intro: {INTRO_LENGTH} seconds\")\n        print(f\"- Main content: {MAIN_LENGTH} seconds (1:40)\")\n        print(f\"- Outro: {OUTRO_LENGTH} seconds\")\n        print(\"- Voice track: Original pitch preserved (no deepening)\")\n        print(f\"- Voice and native audio start: After {VOICE_DELAY} seconds of ambiance\")\n        return True\n    else:\n        print(\"Error: Failed to create final video\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_FIXED_FINAL.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4",
      "WhereYouGoWhenYouLeave_BroadcastPerfection.mp4",
      "WhereYouGoWhenYouLeave_ConfidentPresence.mp4",
      "WhereYouGoWhenYouLeave_CrystalClarity.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "intro.mp4",
      "outro.mp4",
      "intro_audio.wav",
      "outro_audio.wav",
      "ambient.wav",
      "voice_original.wav",
      "native_audio.wav",
      "video_only.mp4",
      "concatenated.mp4",
      "silence_3s.wav",
      "voice_delayed.wav",
      "native_delayed.wav",
      "intro_extended.wav",
      "outro_extended.wav",
      "pre_outro_silence.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60,",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+60",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60,",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+30,",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+100"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={INTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n    "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", f\"color=c=black:s=1920x1080:r=24:d={OUTRO_LENGTH}\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n    "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=220:duration={INTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={INTRO_LENGTH-2}:d=2,volume=0.3\",\n        \"-"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"sine=frequency=146.83:duration={OUTRO_LENGTH}\",\n        \"-af\", f\"afade=t=in:st=0:d=2,afade=t=out:st={OUTRO_LENGTH-2}:d=2,volume=0.25\",\n      "
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"anoisesrc=color=pink:duration={TOTAL_LENGTH}:amplitude=0.02\",\n        \"-af\", \"volume=0.15\",\n        \"-y\", ambient_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "cmd_duration, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", video_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_only\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0:d=3\",\n        \"-y\", silence_file\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", voice_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", voice_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", native_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", native_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", intro_audio,\n        \"-af\", \"apad=pad_dur=108\",  # Pad to intro + main + outro length\n        \"-y\", intro_extended\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", f\"aevalsrc=0:d={pre_outro_length}\",\n        \"-y\", pre_outro_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", outro_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", outro_extended\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", voice_delayed,         # Voice with delay\n        \"-i\", native_delayed,        # Native audio with delay\n        \"-i\", ambient_audio,         # Ambient background\n   "
      },
      {
        "type": "run",
        "snippet": "cmd_duration, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"256k\",  # Higher quality audio\n        \"-map\", \"0:v\",\n        \"-"
      },
      {
        "type": "run",
        "snippet": "cmd_duration, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "subprocess"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/create_three_layer_final.py",
    "size": 14730,
    "lines": 462,
    "source": "import os\nimport subprocess\nimport json\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_THREE_LAYER\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_THREE_LAYER.mp4\")\nVIDEO_DURATION = 100.91  # Exact 1:40 timing\n\n# Ensure temp directory exists\nif not os.path.exists(TEMP_DIR):\n    os.makedirs(TEMP_DIR)\n    print(f\"Created directory: {TEMP_DIR}\")\n\n# Find the best voice track and original video\ndef find_source_files():\n    # Voice track with professional voiceover\n    voice_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_BroadcastPerfection.mp4\")\n    ]\n    \n    for file in voice_files:\n        if os.path.exists(file):\n            print(f\"Using voice track: {os.path.basename(file)}\")\n            voice_file = file\n            break\n    else:\n        print(\"No suitable voice track found\")\n        return None, None\n    \n    # Original video with native audio\n    original_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    ]\n    \n    for file in original_files:\n        if os.path.exists(file):\n            print(f\"Using original video: {os.path.basename(file)}\")\n            original_file = file\n            break\n    else:\n        # If no original with audio found, use the voice file\n        print(\"No original video found, using voice track as video source\")\n        original_file = voice_file\n    \n    return voice_file, original_file\n\n# Create short title animation\ndef create_title():\n    title_file = os.path.join(TEMP_DIR, \"title.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            \"enable='between(t,0,5)':alpha='if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHEN YOU LEAVE':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+60:\"\n            \"enable='between(t,0.3,5)':alpha='if(lt(t,0.8),(t-0.3)/0.5,if(gt(t,4.5),(5-t)/0.5,1))'\"\n        ),\n        \"-c:v\", \"libx264\",\n        \"-pix_fmt\", \"yuv420p\",\n        \"-y\",\n        title_file\n    ]\n    \n    print(\"Creating title card...\")\n    subprocess.run(cmd)\n    \n    return title_file if os.path.exists(title_file) else None\n\n# Create end credits\ndef create_credits():\n    credits_file = os.path.join(TEMP_DIR, \"credits.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=60:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            \"enable='between(t,0,5)':alpha='if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='A Multimedia Poetry Project':fontcolor=white:fontsize=40:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+30:\"\n            \"enable='between(t,0.5,5)':alpha='if(lt(t,1.0),(t-0.5)/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=30:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+100:\"\n            \"enable='between(t,1,5)':alpha='if(lt(t,1.5),(t-1)/0.5,if(gt(t,4.5),(5-t)/0.5,1))'\"\n        ),\n        \"-c:v\", \"libx264\",\n        \"-pix_fmt\", \"yuv420p\",\n        \"-y\",\n        credits_file\n    ]\n    \n    print(\"Creating end credits...\")\n    subprocess.run(cmd)\n    \n    return credits_file if os.path.exists(credits_file) else None\n\n# Extract different audio components\ndef extract_audio(voice_file, original_file):\n    # 1. Extract voice audio\n    voice_audio = os.path.join(TEMP_DIR, \"voice_audio.wav\")\n    cmd_voice = [\n        \"ffmpeg\",\n        \"-i\", voice_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\",\n        voice_audio\n    ]\n    print(\"Extracting voice audio...\")\n    subprocess.run(cmd_voice)\n    \n    # 2. Extract original/native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    cmd_native = [\n        \"ffmpeg\",\n        \"-i\", original_file,\n        \"-vn\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\",\n        native_audio\n    ]\n    print(\"Extracting native audio...\")\n    subprocess.run(cmd_native)\n    \n    # 3. Get durations\n    voice_duration = get_duration(voice_audio)\n    native_duration = get_duration(native_audio)\n    \n    print(f\"Voice audio duration: {voice_duration:.2f} seconds\")\n    print(f\"Native audio duration: {native_duration:.2f} seconds\")\n    \n    return {\n        'voice': voice_audio, \n        'native': native_audio,\n        'voice_duration': voice_duration,\n        'native_duration': native_duration\n    }\n\n# Get audio duration\ndef get_duration(audio_file):\n    cmd = [\n        \"ffprobe\",\n        \"-v\", \"error\",\n        \"-show_entries\", \"format=duration\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        audio_file\n    ]\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    return float(result.stdout.strip())\n\n# Create title and credits sound effects\ndef create_special_audio():\n    # Title audio - starts quiet and builds\n    title_audio = os.path.join(TEMP_DIR, \"title_audio.wav\")\n    cmd_title = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.12*sin(2*PI*220*t)+0.08*sin(2*PI*330*t)+0.05*sin(2*PI*440*t):s=44100:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=1.5,afade=t=out:st=3:d=2,volume=0.4\",\n        \"-y\",\n        title_audio\n    ]\n    print(\"Creating title audio...\")\n    subprocess.run(cmd_title)\n    \n    # Credits audio - has fade in/out\n    credits_audio = os.path.join(TEMP_DIR, \"credits_audio.wav\")\n    cmd_credits = [\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.1*sin(2*PI*146.83*t)+0.07*sin(2*PI*220*t)+0.04*sin(2*PI*293.66*t):s=44100:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=1.5,afade=t=out:st=3:d=2,volume=0.35\",\n        \"-y\",\n        credits_audio\n    ]\n    print(\"Creating credits audio...\")\n    subprocess.run(cmd_credits)\n    \n    return {\n        'title': title_audio,\n        'credits': credits_audio\n    }\n\n# Prepare the voice audio with proper timing\ndef prepare_voice_audio(audio_info):\n    # Calculate speed up factor to match desired duration\n    source_duration = audio_info['voice_duration']\n    target_duration = VIDEO_DURATION  # 1:40 = 100.91 seconds\n    speed_factor = source_duration / target_duration\n    \n    print(f\"Speed factor for voice audio: {speed_factor:.4f}\")\n    \n    # Speed up the voice audio to match the video duration\n    speed_adjusted = os.path.join(TEMP_DIR, \"voice_speed_adjusted.wav\")\n    cmd_speed = [\n        \"ffmpeg\",\n        \"-i\", audio_info['voice'],\n        \"-filter:a\", f\"atempo={speed_factor}\",\n        \"-y\",\n        speed_adjusted\n    ]\n    print(\"Adjusting voice audio speed...\")\n    subprocess.run(cmd_speed)\n    \n    # Apply 2-second delay to voice audio\n    delayed_voice = os.path.join(TEMP_DIR, \"voice_delayed.wav\")\n    cmd_delay = [\n        \"ffmpeg\",\n        \"-i\", speed_adjusted,\n        \"-af\", \"adelay=2000|2000\",  # 2 second delay\n        \"-y\",\n        delayed_voice\n    ]\n    print(\"Adding 2-second delay to voice audio...\")\n    subprocess.run(cmd_delay)\n    \n    return delayed_voice\n\n# Extract video without audio\ndef extract_video(input_video):\n    video_file = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-i\", input_video,\n        \"-c:v\", \"copy\",\n        \"-an\",\n        \"-y\",\n        video_file\n    ]\n    \n    print(\"Extracting video without audio...\")\n    subprocess.run(cmd)\n    \n    return video_file if os.path.exists(video_file) else None\n\n# Concatenate videos\ndef concatenate_videos(title_file, video_file, credits_file):\n    concat_list = os.path.join(TEMP_DIR, \"concat.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    \n    cmd = [\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\",\n        concat_video\n    ]\n    \n    print(\"Concatenating videos...\")\n    subprocess.run(cmd)\n    \n    return concat_video if os.path.exists(concat_video) else None\n\n# Create the final audio mix with all three layers\ndef create_final_audio_mix(audio_info, special_audio, native_audio):\n    # Prepare the final audio mix with three layers\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    \n    # Create a silent audio buffer for the end credits\n    silence_buffer = os.path.join(TEMP_DIR, \"silence_buffer.wav\")\n    cmd_silence = [\n        \"ffmpeg\", \n        \"-f\", \"lavfi\", \n        \"-i\", \"anullsrc=r=44100:cl=mono\",\n        \"-t\", \"5\",  # 5 seconds of silence\n        \"-y\",\n        silence_buffer\n    ]\n    subprocess.run(cmd_silence)\n    \n    # Create title audio padding to make it full length\n    title_padded = os.path.join(TEMP_DIR, \"title_padded.wav\")\n    cmd_title_pad = [\n        \"ffmpeg\",\n        \"-i\", special_audio['title'],\n        \"-i\", silence_buffer,\n        \"-filter_complex\", \"[0:0][1:0]concat=n=2:v=0:a=1\",\n        \"-y\",\n        title_padded\n    ]\n    print(\"Creating padded title audio...\")\n    subprocess.run(cmd_title_pad)\n    \n    # Create credits audio with proper padding\n    credits_padded = os.path.join(TEMP_DIR, \"credits_padded.wav\")\n    cmd_credits_pad = [\n        \"ffmpeg\",\n        \"-i\", silence_buffer,  # Starting silence\n        \"-i\", special_audio['credits'],  # Credits sound\n        \"-filter_complex\", (\n            \"[0:0]atrim=0:\"+ str(VIDEO_DURATION + 5) + \"[silence];\" + \n            \"[silence][1:0]concat=n=2:v=0:a=1[out]\"\n        ),\n        \"-map\", \"[out]\",\n        \"-y\",\n        credits_padded\n    ]\n    print(\"Creating padded credits audio...\")\n    subprocess.run(cmd_credits_pad)\n    \n    # Create a filter complex to mix all audio together\n    filter_complex = [\n        # Layer 1: Native audio from original video\n        \"[0:0]volume=1.0[native]\",\n        \n        # Layer 2: Delayed voice audio\n        \"[1:0]volume=1.0[voice]\",\n        \n        # Layer 3: Title and credits audio combined\n        \"[2:0]volume=0.4[title_fx]\",\n        \"[3:0]volume=0.35[credits_fx]\",\n        \n        # Combine title effects and credits effects\n        \"[title_fx][credits_fx]amix=inputs=2:duration=longest[effects]\",\n        \n        # Mix all three layers\n        \"[native][voice]amix=inputs=2:duration=longest[main_mix]\",\n        \"[main_mix][effects]amix=inputs=2:duration=longest[final]\"\n    ]\n    \n    cmd_mix = [\n        \"ffmpeg\",\n        \"-i\", native_audio,\n        \"-i\", audio_info,\n        \"-i\", title_padded,\n        \"-i\", credits_padded,\n        \"-filter_complex\", \";\".join(filter_complex),\n        \"-map\", \"[final]\",\n        \"-y\",\n        final_mix\n    ]\n    \n    print(\"Creating final three-layer audio mix...\")\n    subprocess.run(cmd_mix)\n    \n    # Verify the mix length\n    if os.path.exists(final_mix):\n        final_duration = get_duration(final_mix)\n        print(f\"Final audio mix duration: {final_duration:.2f} seconds\")\n    \n    return final_mix if os.path.exists(final_mix) else None\n\n# Combine final video with final audio mix\ndef add_audio_to_video(video_file, audio_file):\n    cmd = [\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"192k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Three-layer audio mix with 2-second voice delay\",\n        \"-y\",\n        OUTPUT_FILE\n    ]\n    \n    print(\"Adding final audio mix to video...\")\n    subprocess.run(cmd)\n    \n    return OUTPUT_FILE if os.path.exists(OUTPUT_FILE) else None\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING THREE-LAYER AUDIO MIX WITH 2-SECOND VOICE DELAY =====\\n\")\n    \n    # Find source files\n    voice_file, original_file = find_source_files()\n    if not voice_file:\n        return\n    \n    # Create title and credits\n    title_file = create_title()\n    credits_file = create_credits()\n    \n    if not title_file or not credits_file:\n        print(\"Error creating title or credits\")\n        return\n    \n    # Extract audio components\n    audio_info = extract_audio(voice_file, original_file)\n    \n    # Create special audio effects\n    special_audio = create_special_audio()\n    \n    # Extract video only\n    video_only = extract_video(original_file)\n    if not video_only:\n        print(\"Error extracting video\")\n        return\n    \n    # Prepare voice audio with 2-second delay\n    processed_voice = prepare_voice_audio(audio_info)\n    if not os.path.exists(processed_voice):\n        print(\"Error processing voice audio\")\n        return\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(title_file, video_only, credits_file)\n    if not concat_video:\n        print(\"Error concatenating videos\")\n        return\n    \n    # Create final audio mix\n    final_mix = create_final_audio_mix(processed_voice, special_audio, audio_info['native'])\n    if not final_mix:\n        print(\"Error creating final audio mix\")\n        return\n    \n    # Add audio to video\n    final_output = add_audio_to_video(concat_video, final_mix)\n    if not final_output:\n        print(\"Error adding audio to video\")\n        return\n    \n    print(f\"\\n===== SUCCESS! =====\")\n    print(f\"Final three-layer video created: {OUTPUT_FILE}\")\n    print(\"Improvements:\")\n    print(\"- THREE LAYER AUDIO:\")\n    print(\"  1. Native audio from original video\")\n    print(\"  2. Voice audio starts 2 seconds after video begins\") \n    print(\"  3. Title/credits effects audio that blends beneath the other layers\")\n    print(\"- Voice audio speed-adjusted to match exactly 1:40 (100.91 seconds)\")\n    print(\"- Dynamic title (5s) and credits (5s) with fade in/out text\")\n    print(\"- Full multimedia experience with synchronized audio and visuals\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_THREE_LAYER.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4",
      "WhereYouGoWhenYouLeave_BroadcastPerfection.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "title.mp4",
      "credits.mp4",
      "voice_audio.wav",
      "native_audio.wav",
      "title_audio.wav",
      "credits_audio.wav",
      "voice_speed_adjusted.wav",
      "voice_delayed.wav",
      "video_only.mp4",
      "concatenated.mp4",
      "final_mix.wav",
      "silence_buffer.wav",
      "title_padded.wav",
      "credits_padded.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+60:",
      "if(lt(t,0.8),(t-0.3)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+30:",
      "if(lt(t,1.0),(t-0.5)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+100:",
      "if(lt(t,1.5),(t-1)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      ")\n    subprocess.run(cmd_voice)\n    \n    # 2. Extract original/native audio\n    native_audio = os.path.join(TEMP_DIR, ",
      ")\n    subprocess.run(cmd_title)\n    \n    # Credits audio - has fade in/out\n    credits_audio = os.path.join(TEMP_DIR, ",
      "]\n    target_duration = VIDEO_DURATION  # 1:40 = 100.91 seconds\n    speed_factor = source_duration / target_duration\n    \n    print(f",
      "  3. Title/credits effects audio that blends beneath the other layers",
      "- Dynamic title (5s) and credits (5s) with fade in/out text"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd_voice"
      },
      {
        "type": "run",
        "snippet": "cmd_native"
      },
      {
        "type": "run",
        "snippet": "cmd, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "cmd_title"
      },
      {
        "type": "run",
        "snippet": "cmd_credits"
      },
      {
        "type": "run",
        "snippet": "cmd_speed"
      },
      {
        "type": "run",
        "snippet": "cmd_delay"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd_silence"
      },
      {
        "type": "run",
        "snippet": "cmd_title_pad"
      },
      {
        "type": "run",
        "snippet": "cmd_credits_pad"
      },
      {
        "type": "run",
        "snippet": "cmd_mix"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/delay_voice_2_seconds.py",
    "size": 12861,
    "lines": 368,
    "source": "import os\nimport subprocess\nimport time\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nTEMP_DIR = os.path.join(LIZARD_DIR, \"TEMP_VOICE_DELAY\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FINAL_2SEC_DELAY.mp4\")\nVIDEO_DURATION = 100.91  # Exact 1:40 timing per project requirements\n\n# Ensure temp directory exists\nos.makedirs(TEMP_DIR, exist_ok=True)\nprint(f\"Using directory: {TEMP_DIR}\")\n\n# Find source files\ndef find_source_files():\n    # Find best voice track\n    voice_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ExecutiveVoice.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_BroadcastPerfection.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_ConfidentPresence.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_CrystalClarity.mp4\")\n    ]\n    \n    for file in voice_files:\n        if os.path.exists(file):\n            print(f\"Using voice track: {os.path.basename(file)}\")\n            voice_file = file\n            break\n    else:\n        print(\"No suitable voice track found\")\n        return None, None\n    \n    # Find original video with native audio\n    original_files = [\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_OriginalAudio.mp4\"),\n        os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_FinalAssembly.mp4\")\n    ]\n    \n    for file in original_files:\n        if os.path.exists(file):\n            print(f\"Using original video: {os.path.basename(file)}\")\n            original_file = file\n            break\n    else:\n        print(\"No original video with native audio found, using voice track\")\n        original_file = voice_file\n    \n    return voice_file, original_file\n\n# Extract audio and prepare files\ndef extract_and_prepare(voice_file, original_file):\n    # Extract video without audio\n    video_file = os.path.join(TEMP_DIR, \"video_only.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_file\n    ])\n    print(\"Extracted video without audio\")\n    \n    # Extract native audio\n    native_audio = os.path.join(TEMP_DIR, \"native_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ])\n    print(\"Extracted native audio\")\n    \n    # Extract voice audio\n    voice_audio = os.path.join(TEMP_DIR, \"voice_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ])\n    print(\"Extracted voice audio\")\n    \n    return video_file, native_audio, voice_audio\n\n# Create title and credits videos\ndef create_title_and_credits():\n    # Create title\n    title_file = os.path.join(TEMP_DIR, \"title.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            \"enable='between(t,0,5)':alpha='if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHEN YOU LEAVE':fontcolor=white:fontsize=90:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+60:\"\n            \"enable='between(t,0.3,5)':alpha='if(lt(t,0.8),(t-0.3)/0.5,if(gt(t,4.5),(5-t)/0.5,1))'\"\n        ),\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", title_file\n    ])\n    print(\"Created title animation\")\n    \n    # Create credits\n    credits_file = os.path.join(TEMP_DIR, \"credits.mp4\")\n    subprocess.run([\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='WHERE YOU GO WHEN YOU LEAVE':fontcolor=white:fontsize=60:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2-60:\"\n            \"enable='between(t,0,5)':alpha='if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='A Multimedia Poetry Project':fontcolor=white:fontsize=40:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+30:\"\n            \"enable='between(t,0.5,5)':alpha='if(lt(t,1.0),(t-0.5)/0.5,if(gt(t,4.5),(5-t)/0.5,1))',\"\n            \n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text='\u00a9 2025 RESURRECTING ATLANTIS':fontcolor=white:fontsize=30:\"\n            \"x=(w-text_w)/2:y=(h-text_h)/2+100:\"\n            \"enable='between(t,1,5)':alpha='if(lt(t,1.5),(t-1)/0.5,if(gt(t,4.5),(5-t)/0.5,1))'\"\n        ),\n        \"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\",\n        \"-y\", credits_file\n    ])\n    print(\"Created credits animation\")\n    \n    # Create title audio\n    title_audio = os.path.join(TEMP_DIR, \"title_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.12*sin(2*PI*220*t)+0.08*sin(2*PI*330*t)+0.05*sin(2*PI*440*t):s=44100:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=1.5,afade=t=out:st=3:d=2,volume=0.3\",\n        \"-y\", title_audio\n    ])\n    print(\"Created title audio\")\n    \n    # Create credits audio\n    credits_audio = os.path.join(TEMP_DIR, \"credits_audio.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.1*sin(2*PI*146.83*t)+0.07*sin(2*PI*220*t)+0.04*sin(2*PI*293.66*t):s=44100:d=5\",\n        \"-af\", \"afade=t=in:st=0:d=1.5,afade=t=out:st=3:d=2,volume=0.25\",\n        \"-y\", credits_audio\n    ])\n    print(\"Created credits audio\")\n    \n    return title_file, credits_file, title_audio, credits_audio\n\n# Process the voice audio\ndef process_voice(voice_audio):\n    # Get duration of voice audio\n    cmd_duration = [\n        \"ffprobe\", \"-v\", \"error\",\n        \"-show_entries\", \"format=duration\",\n        \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n        voice_audio\n    ]\n    voice_duration = float(subprocess.run(cmd_duration, capture_output=True, text=True).stdout.strip())\n    print(f\"Voice audio duration: {voice_duration:.2f} seconds\")\n    \n    # Speed-adjust if necessary to match 100.91 seconds\n    speed_factor = voice_duration / VIDEO_DURATION\n    print(f\"Speed adjustment factor: {speed_factor:.4f}\")\n    \n    adjusted_voice = os.path.join(TEMP_DIR, \"voice_adjusted.wav\")\n    if 0.98 <= speed_factor <= 1.02:\n        # If close enough to 1, skip speed adjustment\n        subprocess.run([\"cp\", voice_audio, adjusted_voice])\n        print(\"Voice duration is close to target, skipping speed adjustment\")\n    else:\n        subprocess.run([\n            \"ffmpeg\", \"-i\", voice_audio,\n            \"-filter:a\", f\"atempo={speed_factor}\",\n            \"-y\", adjusted_voice\n        ])\n        print(f\"Adjusted voice speed by factor of {speed_factor:.4f}\")\n    \n    # Create a 2-second silent buffer\n    silent_buffer = os.path.join(TEMP_DIR, \"silent_2s.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anullsrc=r=44100:cl=mono:d=2\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\", silent_buffer\n    ])\n    print(\"Created 2-second silence buffer\")\n    \n    # Concatenate silence with adjusted voice to create delay\n    voice_delayed = os.path.join(TEMP_DIR, \"voice_delayed.wav\")\n    concat_file = os.path.join(TEMP_DIR, \"concat.txt\")\n    \n    with open(concat_file, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(silent_buffer)}'\\n\")\n        f.write(f\"file '{os.path.abspath(adjusted_voice)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", voice_delayed\n    ])\n    print(\"Created voice track with exact 2-second delay\")\n    \n    return voice_delayed\n\n# Concatenate videos\ndef concatenate_videos(title_file, video_file, credits_file):\n    concat_list = os.path.join(TEMP_DIR, \"video_concat.txt\")\n    \n    with open(concat_list, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(title_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(video_file)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_file)}'\\n\")\n    \n    concat_video = os.path.join(TEMP_DIR, \"concatenated.mp4\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ])\n    print(\"Concatenated videos (title + main + credits)\")\n    \n    return concat_video\n\n# Create final audio mix\ndef create_audio_mix(native_audio, voice_delayed, title_audio, credits_audio):\n    # Create a filter to extend title audio to full length\n    title_extended = os.path.join(TEMP_DIR, \"title_extended.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", title_audio,\n        \"-af\", \"apad=pad_len=4410000\",  # Roughly 100 seconds at 44.1kHz\n        \"-y\", title_extended\n    ])\n    print(\"Extended title audio\")\n    \n    # Create silence + credits audio\n    credits_extended = os.path.join(TEMP_DIR, \"credits_extended.wav\")\n    \n    # Create a long silence\n    long_silence = os.path.join(TEMP_DIR, \"long_silence.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anullsrc=r=44100:cl=mono:d=105\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\", long_silence\n    ])\n    \n    # Concatenate with credits\n    credits_concat = os.path.join(TEMP_DIR, \"credits_concat.txt\")\n    with open(credits_concat, \"w\") as f:\n        f.write(f\"file '{os.path.abspath(long_silence)}'\\n\")\n        f.write(f\"file '{os.path.abspath(credits_audio)}'\\n\")\n    \n    subprocess.run([\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", credits_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", credits_extended\n    ])\n    print(\"Created extended credits audio\")\n    \n    # Mix all audio tracks\n    final_mix = os.path.join(TEMP_DIR, \"final_mix.wav\")\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", native_audio,\n        \"-i\", voice_delayed,\n        \"-i\", title_extended,\n        \"-i\", credits_extended,\n        \"-filter_complex\",\n        \"[0:0]volume=1.0[native];\" +\n        \"[1:0]volume=0.9[voice];\" +\n        \"[2:0]volume=0.25[title];\" +\n        \"[3:0]volume=0.25[credits];\" +\n        \"[native][voice]amix=inputs=2:duration=longest[main];\" +\n        \"[main][title]amix=inputs=2:duration=longest[main_title];\" +\n        \"[main_title][credits]amix=inputs=2:duration=longest[out]\",\n        \"-map\", \"[out]\",\n        \"-y\", final_mix\n    ])\n    print(\"Created final audio mix with all three layers\")\n    \n    return final_mix\n\n# Add audio to video\ndef add_audio_to_video(video_file, audio_file):\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"192k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-shortest\",\n        \"-metadata\", \"title=Where You Go When You Leave\",\n        \"-metadata\", \"comment=Final version with precise 2-second voice delay\",\n        \"-y\", OUTPUT_FILE\n    ])\n    print(f\"Added audio to video: {OUTPUT_FILE}\")\n    \n    return os.path.exists(OUTPUT_FILE)\n\n# Main function\ndef main():\n    print(\"\\n===== CREATING FINAL VERSION WITH PRECISE 2-SECOND VOICE DELAY =====\\n\")\n    \n    # Find source files\n    voice_file, original_file = find_source_files()\n    if not voice_file:\n        print(\"Error: No source files found\")\n        return False\n    \n    # Extract and prepare base files\n    video_file, native_audio, voice_audio = extract_and_prepare(voice_file, original_file)\n    \n    # Create title and credits\n    title_file, credits_file, title_audio, credits_audio = create_title_and_credits()\n    \n    # Process voice audio with exact 2-second delay\n    voice_delayed = process_voice(voice_audio)\n    \n    # Concatenate videos\n    concat_video = concatenate_videos(title_file, video_file, credits_file)\n    \n    # Create final audio mix\n    final_mix = create_audio_mix(native_audio, voice_delayed, title_audio, credits_audio)\n    \n    # Add audio to video\n    success = add_audio_to_video(concat_video, final_mix)\n    \n    if success:\n        print(\"\\n===== SUCCESS! =====\")\n        print(f\"Final video with precise timing created: {OUTPUT_FILE}\")\n        print(\"\\nKey improvements:\")\n        print(\"1. Voice audio starts EXACTLY 2 seconds after the video begins\")\n        print(\"2. Native audio starts immediately with the video\")\n        print(\"3. Title and credits audio blend beneath the main audio layers\")\n        print(\"4. All audio synchronized to the exact 1:40 (100.91s) video length\")\n        return True\n    else:\n        print(\"Error: Failed to create final video\")\n        return False\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_FINAL_2SEC_DELAY.mp4",
      "WhereYouGoWhenYouLeave_ExecutiveVoice.mp4",
      "WhereYouGoWhenYouLeave_UltraCrispDefinition.mp4",
      "WhereYouGoWhenYouLeave_BroadcastPerfection.mp4",
      "WhereYouGoWhenYouLeave_ConfidentPresence.mp4",
      "WhereYouGoWhenYouLeave_CrystalClarity.mp4",
      "WhereYouGoWhenYouLeave_OriginalAudio.mp4",
      "WhereYouGoWhenYouLeave_FinalAssembly.mp4",
      "video_only.mp4",
      "native_audio.wav",
      "voice_audio.wav",
      "title.mp4",
      "credits.mp4",
      "title_audio.wav",
      "credits_audio.wav",
      "voice_adjusted.wav",
      "silent_2s.wav",
      "voice_delayed.wav",
      "concatenated.mp4",
      "title_extended.wav",
      "credits_extended.wav",
      "long_silence.wav",
      "final_mix.wav",
      "/Users/gaia/resurrecting atlantis",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+60:",
      "if(lt(t,0.8),(t-0.3)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2-60:",
      "if(lt(t,0.5),t/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+30:",
      "if(lt(t,1.0),(t-0.5)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      "drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:",
      "x=(w-text_w)/2:y=(h-text_h)/2+100:",
      "if(lt(t,1.5),(t-1)/0.5,if(gt(t,4.5),(5-t)/0.5,1))",
      ")\n    \n    # Speed-adjust if necessary to match 100.91 seconds\n    speed_factor = voice_duration / VIDEO_DURATION\n    print(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-c:v\", \"copy\", \"-an\",\n        \"-y\", video_file\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", original_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", native_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \"-i\", voice_file,\n        \"-vn\", \"-acodec\", \"pcm_s16le\",\n        \"-y\", voice_audio\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text="
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\", \n        \"-f\", \"lavfi\",\n        \"-i\", \"color=c=black:s=1920x1080:r=24:d=5\",\n        \"-vf\", (\n            \"drawtext=fontfile=/System/Library/Fonts/Helvetica.ttc:\"\n            \"text="
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.12*sin(2*PI*220*t"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"aevalsrc=0.1*sin(2*PI*146.83*t"
      },
      {
        "type": "run",
        "snippet": "cmd_duration, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "[\"cp\", voice_audio, adjusted_voice]"
      },
      {
        "type": "run",
        "snippet": "[\n            \"ffmpeg\", \"-i\", voice_audio,\n            \"-filter:a\", f\"atempo={speed_factor}\",\n            \"-y\", adjusted_voice\n        ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anullsrc=r=44100:cl=mono:d=2\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\", silent_buffer\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_file,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", voice_delayed\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list,\n        \"-c:v\", \"copy\",\n        \"-y\", concat_video\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", title_audio,\n        \"-af\", \"apad=pad_len=4410000\",  # Roughly 100 seconds at 44.1kHz\n        \"-y\", title_extended\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"lavfi\",\n        \"-i\", \"anullsrc=r=44100:cl=mono:d=105\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-y\", long_silence\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", credits_concat,\n        \"-c:a\", \"pcm_s16le\",\n        \"-y\", credits_extended\n    ]"
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", native_audio,\n        \"-i\", voice_delayed,\n        \"-i\", title_extended,\n        \"-i\", credits_extended,\n        \"-filter_complex\",\n        \"[0:0]volume=1.0[native];\""
      },
      {
        "type": "run",
        "snippet": "[\n        \"ffmpeg\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-c:v\", \"copy\",\n        \"-c:a\", \"aac\",\n        \"-b:a\", \"192k\",\n        \"-map\", \"0:v\",\n        \"-map\", \"1:a\",\n        \"-s"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "time"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "LIZARD/WhereYouGoWhenYouLeave_TechnicalCodex.py",
    "size": 5558,
    "lines": 141,
    "source": "import os\nimport json\nimport re\nfrom collections import OrderedDict\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nCONTENT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_PoemContent.md\")\nOUTPUT_JSON = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_Codex.json\")\nOUTPUT_PLAIN = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_Codex.txt\")\n\n# Poem sequence for reference\nPOEM_SEQUENCE = [\n    {\"id\": \"01\", \"code\": \"SH\", \"title\": \"Out of Life\", \"timestamp\": \"000000\"},\n    {\"id\": \"02\", \"code\": \"FL\", \"title\": \"Flashing Lights\", \"timestamp\": \"021100\"},\n    {\"id\": \"03\", \"code\": \"HT\", \"title\": \"How to Break Off an Engagement\", \"timestamp\": \"042200\"},\n    {\"id\": \"04\", \"code\": \"NM\", \"title\": \"Nevermore\", \"timestamp\": \"063300\"},\n    {\"id\": \"05\", \"code\": \"BE\", \"title\": \"Bloodline\", \"timestamp\": \"084400\"},\n    {\"id\": \"06\", \"code\": \"AT\", \"title\": \"Resurrecting Atlantis\", \"timestamp\": \"105500\"},\n    {\"id\": \"07\", \"code\": \"DJ\", \"title\": \"DJ Turn Me Up\", \"timestamp\": \"130600\"},\n    {\"id\": \"08\", \"code\": \"NS\", \"title\": \"Newly Single\", \"timestamp\": \"151700\"},\n    {\"id\": \"09\", \"code\": \"YH\", \"title\": \"Yet Heard\", \"timestamp\": \"172800\"},\n    {\"id\": \"10\", \"code\": \"MR\", \"title\": \"Magic Ride\", \"timestamp\": \"193900\"},\n    {\"id\": \"12\", \"code\": \"RU\", \"title\": \"Reunion\", \"timestamp\": \"215000\"},\n    {\"id\": \"13\", \"code\": \"HW\", \"title\": \"How to Win My Heart\", \"timestamp\": \"240100\"},\n    {\"id\": \"14\", \"code\": \"HM\", \"title\": \"Hot Minute\", \"timestamp\": \"261200\"}\n]\n\ndef parse_video_sequence():\n    \"\"\"Parse the video sequence from the content file.\"\"\"\n    with open(CONTENT_FILE, 'r') as f:\n        content = f.read()\n        \n    # Extract entries\n    entries = []\n    \n    # Pattern to match section headers\n    section_pattern = r'## (\\d+)_([A-Z]{2})_([^\\n]+)'\n    \n    # Pattern to match entry headers and content\n    entry_pattern = r'### ([A-Z]{2}\\d{3}) \\[(\\d{2}:\\d{2}:\\d{2})\\]\\s+\\*\\*Content:\\*\\* ([^\\n]+)\\s+\\*\\*Syntagma Type:\\*\\* ([^\\n]+)\\s+\\*\\*Ekphrasis:\\*\\* ([^\\n]+)'\n    \n    # Find all matches\n    for match in re.finditer(entry_pattern, content):\n        entry_id = match.group(1)\n        timestamp = match.group(2)\n        content_text = match.group(3)\n        syntagma_type = match.group(4)\n        ekphrasis = match.group(5)\n        \n        # Find which section this belongs to\n        section_code = entry_id[:2]\n        section_num = None\n        section_title = None\n        \n        for poem in POEM_SEQUENCE:\n            if poem[\"code\"] == section_code:\n                section_num = poem[\"id\"]\n                section_title = poem[\"title\"]\n                break\n        \n        entries.append({\n            \"id\": entry_id,\n            \"timestamp\": timestamp,\n            \"content\": content_text,\n            \"syntagma_type\": syntagma_type,\n            \"ekphrasis\": ekphrasis,\n            \"section_code\": section_code,\n            \"section_num\": section_num,\n            \"section_title\": section_title\n        })\n    \n    return entries\n\ndef create_technical_codex():\n    \"\"\"Create a technical codex format that serves as a precise sequencing reference.\"\"\"\n    entries = parse_video_sequence()\n    \n    if not entries:\n        print(\"No entries found.\")\n        return\n    \n    # Create a dictionary of section timelines\n    sections = OrderedDict()\n    for poem in POEM_SEQUENCE:\n        sections[poem[\"code\"]] = {\n            \"num\": poem[\"id\"],\n            \"title\": poem[\"title\"],\n            \"baseline_timestamp\": poem[\"timestamp\"],\n            \"entries\": []\n        }\n    \n    # Populate sections with entries\n    for entry in entries:\n        section_code = entry[\"section_code\"]\n        if section_code in sections:\n            sections[section_code][\"entries\"].append(entry)\n    \n    # Write the JSON output\n    with open(OUTPUT_JSON, 'w') as f:\n        json.dump({\n            \"title\": \"WHERE YOU GO WHEN YOU LEAVE\",\n            \"type\": \"poetic_sequence\",\n            \"methodology\": \"Metz's Syntagmatic Analysis + Deleuze's Cineosis\",\n            \"sections\": sections,\n            \"entries\": entries\n        }, f, indent=2)\n    \n    # Create the plain text codex\n    with open(OUTPUT_PLAIN, 'w') as f:\n        f.write(\"WHERE YOU GO WHEN YOU LEAVE - TECHNICAL CODEX\\n\")\n        f.write(\"===========================================\\n\\n\")\n        f.write(\"FORMAT: [TIMECODE] [ID] :: [CONTENT] <-- [SYNTAGMA] --> [EKPHRASIS]\\n\\n\")\n        \n        # Add each entry in order\n        for entry in entries:\n            section_num = entry[\"section_num\"] or \"??\"\n            section_title = entry[\"section_title\"] or entry[\"section_code\"]\n            \n            # Only print section header before first entry of each section\n            is_first_in_section = True\n            for prev_entry in entries:\n                if prev_entry == entry:\n                    break\n                if prev_entry[\"section_code\"] == entry[\"section_code\"]:\n                    is_first_in_section = False\n            \n            if is_first_in_section:\n                f.write(f\"\\n{section_num}_{entry['section_code']}_{section_title.replace(' ', '')}\\n\")\n                f.write(\"-\" * 60 + \"\\n\")\n            \n            # Format the entry in a literal codex style\n            f.write(f\"[{entry['timestamp']}] {entry['id']} :: \\\"{entry['content']}\\\" <-- {entry['syntagma_type']} --> \\\"{entry['ekphrasis']}\\\"\\n\")\n    \n    print(f\"Technical codex created at: {OUTPUT_JSON} and {OUTPUT_PLAIN}\")\n\nif __name__ == \"__main__\":\n    create_technical_codex()\n",
    "file_references": [
      "WhereYouGoWhenYouLeave_Codex.json",
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "re",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "LIZARD/WhereYouGoWhenYouLeave_Codex.py",
    "size": 5531,
    "lines": 155,
    "source": "import os\nimport json\nimport re\n\n# Configuration\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nLIZARD_DIR = os.path.join(BASE_DIR, \"LIZARD\")\nCONTENT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_PoemContent.md\")\nOUTPUT_FILE = os.path.join(LIZARD_DIR, \"WhereYouGoWhenYouLeave_Codex.md\")\nVIDEO_REPORT = os.path.join(LIZARD_DIR, \"video_sequence_report.md\")\n\ndef parse_content_file():\n    \"\"\"Parse the poem content file to extract all relevant data.\"\"\"\n    if not os.path.exists(CONTENT_FILE):\n        print(f\"Error: Content file not found at {CONTENT_FILE}\")\n        return []\n        \n    with open(CONTENT_FILE, 'r') as f:\n        content = f.read()\n        \n    # Extract entries\n    entries = []\n    \n    # Pattern to match section headers\n    section_pattern = r'## (\\d+)_([A-Z]{2})_([^\\n]+)'\n    \n    # Pattern to match entry headers and content\n    entry_pattern = r'### ([A-Z]{2}\\d{3}) \\[(\\d{2}:\\d{2}:\\d{2})\\]\\s+\\*\\*Content:\\*\\* ([^\\n]+)\\s+\\*\\*Syntagma Type:\\*\\* ([^\\n]+)\\s+\\*\\*Ekphrasis:\\*\\* ([^\\n]+)'\n    \n    # Find all section headers\n    sections = []\n    for match in re.finditer(section_pattern, content):\n        section_num = match.group(1)\n        section_code = match.group(2)\n        section_title = match.group(3)\n        sections.append({\n            'num': section_num,\n            'code': section_code,\n            'title': section_title,\n            'start_pos': match.end(),\n            'entries': []\n        })\n    \n    # Sort sections by their position in the file\n    sections.sort(key=lambda x: x['start_pos'])\n    \n    # For each section, find all entries\n    for i, section in enumerate(sections):\n        section_start = section['start_pos']\n        section_end = len(content)\n        \n        if i < len(sections) - 1:\n            section_end = sections[i+1]['start_pos']\n            \n        section_content = content[section_start:section_end]\n        \n        for match in re.finditer(entry_pattern, section_content):\n            entry_id = match.group(1)\n            timestamp = match.group(2)\n            content_text = match.group(3)\n            syntagma_type = match.group(4)\n            ekphrasis = match.group(5)\n            \n            section['entries'].append({\n                'id': entry_id,\n                'timestamp': timestamp,\n                'content': content_text,\n                'syntagma_type': syntagma_type,\n                'ekphrasis': ekphrasis\n            })\n    \n    return sections\n\ndef find_video_filenames():\n    \"\"\"Extract original video filenames from the video sequence report.\"\"\"\n    if not os.path.exists(VIDEO_REPORT):\n        print(f\"Warning: Video report not found at {VIDEO_REPORT}\")\n        return {}\n        \n    with open(VIDEO_REPORT, 'r') as f:\n        report = f.read()\n        \n    video_files = {}\n    pattern = r'\\[(\\d{2}:\\d{2}:\\d{2})\\] ([A-Z]{2}\\d{3}) - `([^`]+)`'\n    \n    for match in re.finditer(pattern, report):\n        timestamp = match.group(1)\n        video_id = match.group(2)\n        filename = match.group(3)\n        \n        if video_id not in video_files:\n            video_files[video_id] = []\n            \n        video_files[video_id].append({\n            'timestamp': timestamp,\n            'filename': filename\n        })\n        \n    return video_files\n\ndef create_codex():\n    \"\"\"Create a literal codex of the poem sequence.\"\"\"\n    sections = parse_content_file()\n    video_files = find_video_filenames()\n    \n    if not sections:\n        print(\"No content sections found.\")\n        return\n        \n    with open(OUTPUT_FILE, 'w') as outfile:\n        outfile.write(\"# WHERE YOU GO WHEN YOU LEAVE - CODEX SEQUENCER\\n\\n\")\n        outfile.write(\"*A literal codex of poem content and corresponding video files*\\n\\n\")\n        outfile.write(\"| Entry # | Time Code | ID | Content | Syntagma Type | Ekphrasis | Video File |\\n\")\n        outfile.write(\"|---------|-----------|----|---------|--------------|-----------|-----------|\\n\")\n        \n        entry_count = 1\n        \n        for section in sections:\n            section_num = section['num']\n            section_code = section['code']\n            section_title = section['title']\n            \n            # Add section header row\n            outfile.write(f\"| **{section_num}** | **{section_code}** | **{section_title}** | | | | |\\n\")\n            \n            # Add entries\n            for entry in section['entries']:\n                entry_id = entry['id']\n                timestamp = entry['timestamp']\n                content_text = entry['content']\n                syntagma_type = entry['syntagma_type']\n                ekphrasis = entry['ekphrasis']\n                \n                # Find corresponding video filename\n                video_filename = \"\"\n                if entry_id in video_files and video_files[entry_id]:\n                    video_filename = video_files[entry_id][0]['filename']\n                    \n                # Write entry row\n                outfile.write(f\"| {entry_count:02d} | {timestamp} | {entry_id} | {content_text} | {syntagma_type} | {ekphrasis} | `{video_filename}` |\\n\")\n                \n                entry_count += 1\n        \n        # Add a JSON data structure at the end for programmatic access\n        outfile.write(\"\\n## JSON Data Structure\\n\\n\")\n        outfile.write(\"```json\\n\")\n        outfile.write(json.dumps(sections, indent=2))\n        outfile.write(\"\\n```\\n\")\n        \n    print(f\"Codex created at: {OUTPUT_FILE}\")\n\nif __name__ == \"__main__\":\n    create_codex()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "re"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "scripts/poem_to_codex.py",
    "size": 13161,
    "lines": 245,
    "source": "#!/usr/bin/env python3\n\"\"\"\npoem_to_codex.py\n\nConvert a Notion-like poem HTML export into a codex-style HTML similar to\n`0_CHANG-E/olo-fl-00-good.html`.\n\nUsage:\n  python poem_to_codex.py -i /path/to/poem_1_out_of_life.html -o /path/to/output.html \\\n    --title \"1. Out of Life\" --goal \"<goal sentence>\"\n\nHeuristics:\n- Title: from --title, else from first <h1>, else <title> tag, else filename stem.\n- Description/Prescription: tries to find labeled rows; else uses first/second paragraph.\n- Content paragraphs: remaining paragraphs are placed into numbered content cards.\n- Mermaid diagram & analysis definition: scaffolded placeholders you can edit later.\n\nThis script does not try to infer the slip-column poem layout; it focuses on\nmatching the codex structure (header, intro rows, diagram block, analysis sentence,\ncontent cards) so you can iterate the content.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport pathlib\nfrom bs4 import BeautifulSoup\nimport json\nimport re\n\n\ndef read_html(path: pathlib.Path) -> BeautifulSoup:\n    with path.open('r', encoding='utf-8') as f:\n        html = f.read()\n    return BeautifulSoup(html, 'html.parser')\n\n\ndef first_text(el) -> str:\n    if not el:\n        return ''\n    txt = el.get_text(\" \", strip=True)\n    return txt\n\n\ndef guess_title(soup: BeautifulSoup, fallback: str) -> str:\n    # Prefer explicit <h1>, then <title>, then fallback\n    h1 = soup.find('h1')\n    if h1:\n        t = first_text(h1)\n        if t:\n            return t\n    tt = soup.find('title')\n    if tt:\n        t = first_text(tt)\n        if t:\n            return t\n    return fallback\n\n\ndef extract_paragraphs(soup: BeautifulSoup) -> list[str]:\n    # Collect semantic paragraphs; ignore headers and nav.\n    paras: list[str] = []\n    for p in soup.find_all(['p', 'li']):\n        t = first_text(p)\n        if t:\n            paras.append(t)\n    # As a fallback, also consider plain text blocks under body\n    if not paras:\n        body = soup.find('body')\n        if body:\n            raw = first_text(body)\n            if raw:\n                # split on double newlines as rough paragraphs\n                for chunk in [c.strip() for c in raw.split('\\n\\n') if c.strip()]:\n                    paras.append(chunk)\n    return paras\n\n_SPLIT_DELIMS = [\n    ' \u2014 ', ' \u2013 ', ' - ', ' \u2014', ' \u2013', ' -', '\u2014', '\u2013', ' - ', ' | ', ' / ', '; ', ';'\n]\n\ndef split_two_clauses(text: str) -> tuple[str, str]:\n    \"\"\"Try to split a numbered line's payload into (description, prescription).\n    Uses a set of common delimiters. Falls back to first sentence split.\n    \"\"\"\n    t = text.strip()\n    for d in _SPLIT_DELIMS:\n        if d in t:\n            a, b = t.split(d, 1)\n            return a.strip().rstrip('.,;:'), b.strip()\n    # Fallback: split by sentence end\n    m = re.split(r\"(?<=[.!?])\\s+\", t, maxsplit=1)\n    if len(m) == 2:\n        return m[0].strip(), m[1].strip()\n    return t, ''\n\ndef extract_numbered_items(soup: BeautifulSoup) -> list[dict]:\n    \"\"\"Extract items like '1. foo \u2014 bar' from <li>, <p>, or plain text.\n    Returns list of dicts: {n:int, desc:str, presc:str, raw:str}\n    \"\"\"\n    items: list[dict] = []\n    candidates = []\n    candidates.extend(soup.find_all(['li', 'p']))\n    # As a last resort scan text nodes under body\n    body = soup.find('body')\n    if body:\n        body_text = body.get_text('\\n', strip=True)\n        for line in body_text.split('\\n'):\n            candidates.append(BeautifulSoup(f\"<p>{line}</p>\", 'html.parser').p)\n    seen_payloads = set()\n    for el in candidates:\n        line = first_text(el)\n        if not line:\n            continue\n        m = re.match(r\"^\\s*(\\d+)[\\)\\.:\\-]?\\s+(.*)$\", line)\n        if not m:\n            continue\n        n = int(m.group(1))\n        payload = m.group(2).strip()\n        if not payload or payload in seen_payloads:\n            continue\n        seen_payloads.add(payload)\n        a, b = split_two_clauses(payload)\n        items.append({\"n\": n, \"desc\": a, \"presc\": b, \"raw\": line})\n    # Deduplicate by n, keep first occurrence, and order by n\n    by_n = {}\n    for it in items:\n        by_n.setdefault(it['n'], it)\n    return [by_n[k] for k in sorted(by_n.keys())]\n\n\ndef extract_desc_prescription(soup: BeautifulSoup, paras: list[str]) -> tuple[str, str]:\n    # Look for labels like \"Description\" / \"Prescription\"\n    # Heuristic: find elements containing those exact words and read the sibling text\n    description = ''\n    prescription = ''\n    # Try by heading labels nearby\n    labels = soup.find_all(['div', 'span', 'strong', 'b'])\n    for lab in labels:\n        name = first_text(lab).lower()\n        if not name:\n            continue\n        if ('description' in name) and not description:\n            sibling_text = first_text(lab.parent)\n            if sibling_text and len(sibling_text) > len(name):\n                # remove label word from the combined line\n                description = sibling_text.replace(first_text(lab), '').strip(': -\u2014\u2022\\u00A0').strip()\n        if ('prescription' in name) and not prescription:\n            sibling_text = first_text(lab.parent)\n            if sibling_text and len(sibling_text) > len(name):\n                prescription = sibling_text.replace(first_text(lab), '').strip(': -\u2014\u2022\\u00A0').strip()\n        if description and prescription:\n            break\n    # Fallbacks from paragraphs\n    if not description and paras:\n        description = paras[0]\n    if not prescription and len(paras) > 1:\n        prescription = paras[1]\n    return description, prescription\n\n\ndef build_codex_html(title: str, goal: str, description: str, prescription: str, body_paras: list[str]) -> str:\n    # Use a minimal codex structure similar to olo-fl-00-good.html\n    # Mermaid scaffold: edit later to fit the poem\n    mermaid = (\n        \"flowchart TD\\n\"\n        \"classDef default fill:#fff,stroke:#000,stroke-width:1px,color:#000,rx:0,ry:0;\\n\"\n        \"classDef rootNode fill:#fff,stroke:#000,stroke-width:2px,color:#000,rx:0,ry:0;\\n\"\n        \"classDef categoryNode fill:#fff,stroke:#000,stroke-width:1.5px,color:#000,rx:0,ry:0;\\n\"\n        \"classDef leafNode fill:#fff,stroke:#000,stroke-width:1px,color:#000,rx:0,ry:0;\\n\"\n        \"A[Out of Life]:::rootNode\\n\"\n        \"A-->B1[Symbols]\\nA-->B2[Moods/Motivations]\\nA-->B3[Conceptions of Order]\\nA-->B4[Aura of Factuality]\\nA-->B5[Uniquely Realistic]\\n\"\n        \"class A rootNode;\\nclass B1,B2,B3,B4,B5 categoryNode;\\n\"\n    )\n\n    # Make simple content cards from remaining paragraphs (up to 5)\n    cards_html = []\n    for idx, para in enumerate(body_paras[:5], start=1):\n        cards_html.append(f\"\"\"\n        <section class=\\\"content-card\\\" id=\\\"fl-{idx}\\\">\n          <div class=\\\"section-header\\\"><div class=\\\"section-number\\\">{idx}</div><h2 class=\\\"section-title\\\">Section {idx}</h2></div>\n          <div class=\\\"model-description\\\">{para}</div>\n        </section>\n        \"\"\")\n\n    cards = \"\\n\".join(cards_html)\n\n    # Full HTML\n    html = f\"\"\"<!DOCTYPE html>\n<html lang=\\\"en\\\">\\n<head>\\n  <meta charset=\\\"UTF-8\\\" />\\n  <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\" />\\n  <title>CULTURAL SYSTEM FOR POEMS \u2014 Codex from Source</title>\\n  <link href=\\\"https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Pro:ital,wght@0,400;0,600;1,400&display=swap\\\" rel=\\\"stylesheet\\\" />\\n  <link rel=\\\"stylesheet\\\" href=\\\"/0_CHANG-E/poem.css\\\" />\\n  <style>\\n    /* Minimal helpers if poem.css unavailable */\\n    body{{font-family:'Inter',sans-serif;color:#000;background:#fff;margin:0}}\\n    .header{{border-bottom:1px solid #000;padding:12px 16px}}\\n    .container{{max-width:960px;margin:0 auto;padding:0 12px}}\\n    .title-section{{margin:16px 0}}\\n    .poem-goal{{font-family:'Crimson Pro',serif;border:1px solid #000;padding:8px;margin-top:8px}}\\n    .intro-single{{border:1px solid #000}}\\n    .intro-row{{display:grid;grid-template-columns:140px 1fr;border-top:1px solid #000}}\\n    .intro-row:first-child{{border-top:none}}\\n    .intro-label{{border-right:1px solid #000;padding:8px;font-weight:600}}\\n    .intro-text{{padding:8px}}\\n    .diagram-section{{border:1px solid #000;margin-top:12px;padding:8px}}\\n    .analysis-definition{{border:1px solid #000;margin-top:12px;padding:8px}}\\n    .content-card{{border:1px solid #000;margin-top:12px;padding:8px}}\\n    .section-header{{display:flex;align-items:center;gap:8px;border-bottom:1px solid #000;padding-bottom:6px;margin-bottom:6px}}\\n    .section-number{{border:1px solid #000;padding:2px 6px;font-weight:700}}\\n    .section-title{{font-family:'Crimson Pro',serif;margin:0}}\\n    pre.mermaid{{white-space:pre-wrap;}}\\n  </style>\\n  <script src=\\\"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.1/mermaid.min.js\\\"></script>\\n  <script>mermaid.initialize({{startOnLoad:true, securityLevel:'loose', theme:'base', themeVariables:{{primaryColor:'#fff',primaryTextColor:'#000',primaryBorderColor:'#000',lineColor:'#000',background:'#fff',mainBkg:'#fff'}}}});</script>\\n</head>\\n<body>\\n  <header class=\\\"header\\\">\\n    <div class=\\\"container\\\">\\n      <div class=\\\"header-content\\\">\\n        <div class=\\\"logo\\\">CULTURAL SYSTEM FOR POEMS</div>\\n        <div class=\\\"breadcrumb\\\"><a href=\\\"#\\\">Home</a> / <a href=\\\"#\\\">Poems</a> / Codex</div>\\n      </div>\\n    </div>\\n  </header>\\n  <div class=\\\"container\\\">\\n    <div class=\\\"main-content\\\">\\n      <main class=\\\"content-area\\\">\\n        <div class=\\\"title-section\\\">\\n          <h1 class=\\\"poem-title\\\">{title}</h1>\\n          {f'<div class=\\\\\"poem-goal\\\\\"><strong>GOAL:</strong> {goal}</div>' if goal else ''}\\n        </div>\\n        <div class=\\\"intro-single\\\">\\n          <div class=\\\"intro-row\\\">\\n            <div class=\\\"intro-label\\\">Description</div>\\n            <div class=\\\"intro-text\\\">{description}</div>\\n          </div>\\n          <div class=\\\"intro-row\\\">\\n            <div class=\\\"intro-label\\\">Prescription</div>\\n            <div class=\\\"intro-text\\\">{prescription}</div>\\n          </div>\\n        </div>\\n        <div class=\\\"diagram-section\\\">\\n          <div class=\\\"goal-bar\\\">{(goal or '').strip()}</div>\\n          <pre class=\\\"mermaid\\\">{mermaid}</pre>\\n        </div>\\n        <div class=\\\"analysis-definition\\\">\\n          <span data-target-id=\\\"fl-1\\\" data-mermaid-node=\\\"B1\\\">(1) a system of symbols \u2026</span> which acts to\\n          <span data-target-id=\\\"fl-2\\\" data-mermaid-node=\\\"B2\\\">(2) establish moods/motivations \u2026</span> in individuals by\\n          <span data-target-id=\\\"fl-3\\\" data-mermaid-node=\\\"B3\\\">(3) formulating conceptions of order \u2026</span> and\\n          <span data-target-id=\\\"fl-4\\\" data-mermaid-node=\\\"B4\\\">(4) clothing these conceptions with an aura of factuality \u2026</span> that\\n          <span data-target-id=\\\"fl-5\\\" data-mermaid-node=\\\"B5\\\">(5) makes them seem uniquely realistic.</span>\\n        </div>\\n        {cards}\\n      </main>\\n    </div>\\n  </div>\\n</body>\\n</html>\\n\"\"\"\n    return html\n\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument('-i', '--input', required=True, help='Input HTML file')\n    ap.add_argument('-o', '--output', required=True, help='Output HTML file')\n    ap.add_argument('--title', default='', help='Override title')\n    ap.add_argument('--goal', default='', help='Goal statement to include')\n    args = ap.parse_args()\n\n    in_path = pathlib.Path(args.input)\n    out_path = pathlib.Path(args.output)\n    soup = read_html(in_path)\n\n    title = args.title.strip() or guess_title(soup, in_path.stem)\n    paras = extract_paragraphs(soup)\n    items = extract_numbered_items(soup)\n    # If numbered items exist, build description/prescription from their clauses\n    if items:\n        description = \"; \".join([it['desc'] for it in items if it['desc']]).strip()\n        prescription = \"; \".join([it['presc'] for it in items if it['presc']]).strip()\n    else:\n        description, prescription = extract_desc_prescription(soup, paras)\n\n    # Remove the first two paras if used as desc/prescription; rest become content cards\n    remaining = paras[:]\n    # Don't try to remove if built from numbered items; otherwise keep prior behavior\n    if not items:\n        if description and remaining and description == remaining[0]:\n            remaining = remaining[1:]\n        if prescription and remaining and prescription == remaining[0]:\n            remaining = remaining[1:]\n\n    html = build_codex_html(title, args.goal.strip(), description, prescription, remaining)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    out_path.write_text(html, encoding='utf-8')\n    print(f\"Wrote codex HTML to: {out_path}\")\n\n    # Emit a sibling JSON representation for scaling pipelines\n    json_path = out_path.with_suffix('.json')\n    model = {\n        \"title\": title,\n        \"goal\": args.goal.strip(),\n        \"description_join\": description,\n        \"prescription_join\": prescription,\n        \"items\": items,  # list of {n, desc, presc, raw}\n        \"raw_paragraphs\": paras,\n        \"source\": str(in_path)\n    }\n    json_path.write_text(json.dumps(model, ensure_ascii=False, indent=2), encoding='utf-8')\n    print(f\"Wrote model JSON to: {json_path}\")\n\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      "\npoem_to_codex.py\n\nConvert a Notion-like poem HTML export into a codex-style HTML similar to\n`0_CHANG-E/olo-fl-00-good.html`.\n\nUsage:\n  python poem_to_codex.py -i /path/to/poem_1_out_of_life.html -o /path/to/output.html \\\n    --title ",
      "\n\nHeuristics:\n- Title: from --title, else from first <h1>, else <title> tag, else filename stem.\n- Description/Prescription: tries to find labeled rows; else uses first/second paragraph.\n- Content paragraphs: remaining paragraphs are placed into numbered content cards.\n- Mermaid diagram & analysis definition: scaffolded placeholders you can edit later.\n\nThis script does not try to infer the slip-column poem layout; it focuses on\nmatching the codex structure (header, intro rows, diagram block, analysis sentence,\ncontent cards) so you can iterate the content.\n",
      " / ",
      "<p>{line}</p>",
      " / ",
      "A-->B1[Symbols]\\nA-->B2[Moods/Motivations]\\nA-->B3[Conceptions of Order]\\nA-->B4[Aura of Factuality]\\nA-->B5[Uniquely Realistic]\\n",
      ">{idx}</div><h2 class=\\",
      ">Section {idx}</h2></div>\n          <div class=\\",
      ">{para}</div>\n        </section>\n        ",
      " />\\n  <meta name=\\",
      " />\\n  <title>CULTURAL SYSTEM FOR POEMS \u2014 Codex from Source</title>\\n  <link href=\\",
      " />\\n  <link rel=\\",
      "/0_CHANG-E/poem.css\\",
      ",serif;margin:0}}\\n    pre.mermaid{{white-space:pre-wrap;}}\\n  </style>\\n  <script src=\\",
      "></script>\\n  <script>mermaid.initialize({{startOnLoad:true, securityLevel:",
      "}}}});</script>\\n</head>\\n<body>\\n  <header class=\\",
      ">CULTURAL SYSTEM FOR POEMS</div>\\n        <div class=\\",
      ">Home</a> / <a href=\\",
      ">Poems</a> / Codex</div>\\n      </div>\\n    </div>\\n  </header>\\n  <div class=\\",
      ">{title}</h1>\\n          {f",
      "><strong>GOAL:</strong> {goal}</div>",
      "}\\n        </div>\\n        <div class=\\",
      ">Description</div>\\n            <div class=\\",
      ">{description}</div>\\n          </div>\\n          <div class=\\",
      ">Prescription</div>\\n            <div class=\\",
      ">{prescription}</div>\\n          </div>\\n        </div>\\n        <div class=\\",
      ").strip()}</div>\\n          <pre class=\\",
      ">{mermaid}</pre>\\n        </div>\\n        <div class=\\",
      ">(1) a system of symbols \u2026</span> which acts to\\n          <span data-target-id=\\",
      ">(2) establish moods/motivations \u2026</span> in individuals by\\n          <span data-target-id=\\",
      ">(3) formulating conceptions of order \u2026</span> and\\n          <span data-target-id=\\",
      ">(4) clothing these conceptions with an aura of factuality \u2026</span> that\\n          <span data-target-id=\\",
      ">(5) makes them seem uniquely realistic.</span>\\n        </div>\\n        {cards}\\n      </main>\\n    </div>\\n  </div>\\n</body>\\n</html>\\n",
      ")\n    args = ap.parse_args()\n\n    in_path = pathlib.Path(args.input)\n    out_path = pathlib.Path(args.output)\n    soup = read_html(in_path)\n\n    title = args.title.strip() or guess_title(soup, in_path.stem)\n    paras = extract_paragraphs(soup)\n    items = extract_numbered_items(soup)\n    # If numbered items exist, build description/prescription from their clauses\n    if items:\n        description = ",
      "]]).strip()\n    else:\n        description, prescription = extract_desc_prescription(soup, paras)\n\n    # Remove the first two paras if used as desc/prescription; rest become content cards\n    remaining = paras[:]\n    # Don"
    ],
    "subprocess_calls": [],
    "imports": [
      "__future__",
      "argparse",
      "pathlib",
      "bs4",
      "json",
      "re"
    ],
    "generates": [],
    "reads": [
      "reads_data"
    ],
    "docstring": "poem_to_codex.py\n\nConvert a Notion-like poem HTML export into a codex-style HTML similar to\n`0_CHANG-E/olo-fl-00-good.html`.\n\nUsage:\n  python poem_to_codex.py -i /path/to/poem_1_out_of_life.html -o /path/to/output.html     --title \"1. Out of Life\" --goal \"<goal sentence>\"\n\nHeuristics:\n- Title: from --title, else from first <h1>, else <title> tag, else filename stem.\n- Description/Prescription: tries to find labeled rows; else uses first/second paragraph.\n- Content paragraphs: remaining paragraphs are placed into numbered content cards.\n- Mermaid diagram & analysis definition: scaffolded placeholders you can edit later.\n\nThis script does not try to infer the slip-column poem layout; it focuses on\nmatching the codex structure (header, intro rows, diagram block, analysis sentence,\ncontent cards) so you can iterate the content."
  },
  {
    "path": "HONEYBADGER/generate_symbolic_genome.py",
    "size": 8728,
    "lines": 248,
    "source": "#!/usr/bin/env python3\n\"\"\"\nGenerate Symbolic Genome Visualizations for Poetry Timeline\n\nThis script creates a visual representation of each poem's \"symbolic genome\"\nusing the defined glyphs for syntagma types, image types, and cineosis functions.\n\"\"\"\n\nimport json\nimport os\nfrom collections import defaultdict\n\n# Define glyph mappings\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2b32\",\n    \"Recollection-Image\": \"\u2b31\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load the normalized timeline data\"\"\"\n    with open(filepath, 'r') as f:\n        return json.load(f)\n\ndef organize_by_poem(timeline):\n    \"\"\"Group and sort entries by poem\"\"\"\n    poem_entries = defaultdict(list)\n    \n    # Group by poem\n    for entry in timeline:\n        poem_entries[entry['poem']].append(entry)\n    \n    # Sort entries within each poem\n    for poem in poem_entries:\n        # Sort by ID which should reflect sequence\n        poem_entries[poem].sort(key=lambda e: e['id'])\n    \n    return poem_entries\n\ndef get_glyph_or_default(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for key or return default if not found\"\"\"\n    return mapping.get(key, default)\n\ndef generate_poem_genome(entries, width=80):\n    \"\"\"Generate symbolic genome visualization for a poem\"\"\"\n    # Generate the three tracks (syntagma, image, cineosis)\n    syntagma_track = []\n    image_track = []\n    cineosis_track = []\n    id_track = []\n    \n    for entry in entries:\n        syntagma_glyph = get_glyph_or_default(SYNTAGMA_GLYPHS, entry.get('syntagmaType', ''))\n        image_glyph = get_glyph_or_default(IMAGE_GLYPHS, entry.get('imageType', ''))\n        cineosis_glyph = get_glyph_or_default(CINEOSIS_GLYPHS, entry.get('cineosisFunction', ''))\n        \n        syntagma_track.append(syntagma_glyph)\n        image_track.append(image_glyph)\n        cineosis_track.append(cineosis_glyph)\n        \n        # Store abbreviated ID (last 2-3 chars)\n        id_part = entry.get('id', '??')[-3:] if len(entry.get('id', '??')) > 3 else entry.get('id', '??')\n        id_track.append(id_part)\n    \n    # Create the visualization\n    lines = []\n    \n    # Add header\n    lines.append(\"\u250c\" + \"\u2500\" * (len(entries) * 2 - 1) + \"\u2510\")\n    \n    # Add syntagma track\n    syntagma_line = \"\u2502\" + \" \".join(syntagma_track) + \"\u2502 S\"\n    lines.append(syntagma_line)\n    \n    # Add image track\n    image_line = \"\u2502\" + \" \".join(image_track) + \"\u2502 I\"\n    lines.append(image_line)\n    \n    # Add cineosis track\n    cineosis_line = \"\u2502\" + \" \".join(cineosis_track) + \"\u2502 C\"\n    lines.append(cineosis_line)\n    \n    # Add footer\n    lines.append(\"\u2514\" + \"\u2500\" * (len(entries) * 2 - 1) + \"\u2518\")\n    \n    # Add ID track below\n    id_spacing = []\n    for id_str in id_track:\n        # Center align within 3 chars\n        spaces = (3 - len(id_str)) // 2\n        id_spacing.append(\" \" * spaces + id_str + \" \" * (3 - len(id_str) - spaces))\n    \n    id_line = \" \" + \"\".join(id_spacing)\n    lines.append(id_line)\n    \n    return \"\\n\".join(lines)\n\ndef generate_poem_stats(entries):\n    \"\"\"Generate statistics about the distribution of types in a poem\"\"\"\n    total = len(entries)\n    if total == 0:\n        return \"No entries found for this poem.\"\n    \n    # Count frequencies\n    syntagma_counts = defaultdict(int)\n    image_counts = defaultdict(int)\n    cineosis_counts = defaultdict(int)\n    \n    for entry in entries:\n        syntagma_counts[entry.get('syntagmaType', 'Unknown')] += 1\n        image_counts[entry.get('imageType', 'Unknown')] += 1\n        cineosis_counts[entry.get('cineosisFunction', 'Unknown')] += 1\n    \n    # Generate report lines\n    lines = [\"### Distribution Statistics\", \"\"]\n    \n    # Syntagma distribution\n    lines.append(\"**Syntagma Types:**\")\n    for syntagma, count in sorted(syntagma_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(SYNTAGMA_GLYPHS, syntagma)\n        percentage = (count / total) * 100\n        lines.append(f\"- {glyph} {syntagma}: {count} ({percentage:.1f}%)\")\n    lines.append(\"\")\n    \n    # Image distribution\n    lines.append(\"**Image Types:**\")\n    for image, count in sorted(image_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(IMAGE_GLYPHS, image)\n        percentage = (count / total) * 100\n        lines.append(f\"- {glyph} {image}: {count} ({percentage:.1f}%)\")\n    lines.append(\"\")\n    \n    # Cineosis distribution\n    lines.append(\"**Cineosis Functions:**\")\n    for cineosis, count in sorted(cineosis_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(CINEOSIS_GLYPHS, cineosis)\n        percentage = (count / total) * 100\n        lines.append(f\"- {glyph} {cineosis}: {count} ({percentage:.1f}%)\")\n    \n    return \"\\n\".join(lines)\n\ndef generate_report(poem_entries):\n    \"\"\"Generate a comprehensive report with all poem genomes\"\"\"\n    report_lines = [\"# Symbolic Genome Visualization\", \n                    \"\",\n                    \"This document visualizes the symbolic genome of each poem using the following notation:\",\n                    \"\",\n                    \"## Notation Key\",\n                    \"\",\n                    \"### Syntagma Types (S)\",\n                    \"- \u2591  Autonomous Syntagma (AS)\",\n                    \"- \u2592  Chronological Syntagma (CS)\",\n                    \"- \u2662  Crystal Syntagma (XS)\",\n                    \"- \u259e  Descriptive Syntagma (DS)\",\n                    \"- \u2599  Flashback Syntagma (FS)\",\n                    \"- \u2588  Thematic Montage (TM)\",\n                    \"\",\n                    \"### Image Types (I)\",\n                    \"- \u25ba  Action-Image\",\n                    \"- \u2639  Affection-Image\",\n                    \"- \u2b27  Crystal-Image\",\n                    \"- \u263c  Descriptive Image\",\n                    \"- \u2a00  Opsign\",\n                    \"- \u2b32  Perception-Image\",\n                    \"- \u2b31  Recollection-Image\",\n                    \"- \u266c  Sonsign\",\n                    \"- \u2263  Thematic Montage\",\n                    \"\",\n                    \"### Cineosis Functions (C)\",\n                    \"- \u266a  Aural-Echo Extension\",\n                    \"- \u2794  Causal Motion Trigger\",\n                    \"- \u2661  Emotion Relay\",\n                    \"- \u2016  Event Pause Invocation\",\n                    \"- \u21bb  Memory Storage Retrieval\",\n                    \"- \u25ff  Mood Environment Stabilizer\",\n                    \"- \u2726  Narrative Modifier\",\n                    \"- \u223f  Subjective Frame Recalibration\",\n                    \"- \u2318  Temporal Reflection Loop\",\n                    \"\",\n                    \"## Poem Genomes\",\n                    \"\"]\n    \n    # Generate visualization for each poem\n    for poem, entries in sorted(poem_entries.items()):\n        report_lines.append(f\"## {poem}\")\n        report_lines.append(f\"*{len(entries)} timeline entries*\")\n        report_lines.append(\"\")\n        report_lines.append(\"```\")\n        report_lines.append(generate_poem_genome(entries))\n        report_lines.append(\"```\")\n        report_lines.append(\"\")\n        report_lines.append(generate_poem_stats(entries))\n        report_lines.append(\"\")\n        report_lines.append(\"---\")\n        report_lines.append(\"\")\n    \n    # Write the report\n    output_path = '/Users/gaia/resurrecting atlantis/HONEYBADGER/NOTATION/SYMBOLIC_GENOME_REPORT.md'\n    with open(output_path, 'w') as f:\n        f.write(\"\\n\".join(report_lines))\n    \n    return output_path\n\ndef main():\n    print(\"Generating symbolic genome visualizations...\")\n    \n    # Load the normalized timeline\n    timeline = load_timeline('/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json')\n    print(f\"Loaded timeline with {len(timeline)} entries\")\n    \n    # Organize by poem\n    poem_entries = organize_by_poem(timeline)\n    print(f\"Organized entries into {len(poem_entries)} poems\")\n    \n    # Generate comprehensive report\n    output_path = generate_report(poem_entries)\n    print(f\"Generated symbolic genome report at {output_path}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json",
      ")\n    \n    # Add ID track below\n    id_spacing = []\n    for id_str in id_track:\n        # Center align within 3 chars\n        spaces = (3 - len(id_str)) // 2\n        id_spacing.append(",
      ")\n    for syntagma, count in sorted(syntagma_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(SYNTAGMA_GLYPHS, syntagma)\n        percentage = (count / total) * 100\n        lines.append(f",
      ")\n    for image, count in sorted(image_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(IMAGE_GLYPHS, image)\n        percentage = (count / total) * 100\n        lines.append(f",
      ")\n    for cineosis, count in sorted(cineosis_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(CINEOSIS_GLYPHS, cineosis)\n        percentage = (count / total) * 100\n        lines.append(f",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NOTATION/SYMBOLIC_GENOME_REPORT.md",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Generate Symbolic Genome Visualizations for Poetry Timeline\n\nThis script creates a visual representation of each poem's \"symbolic genome\"\nusing the defined glyphs for syntagma types, image types, and cineosis functions."
  },
  {
    "path": "HONEYBADGER/syntagma_transitions.py",
    "size": 12666,
    "lines": 317,
    "source": "#!/usr/bin/env python3\n\"\"\"\nSyntagma Transition Analysis\n\nGenerates contextual co-occurrence matrices showing how often each syntagmaType \nis followed by each other type, revealing syntagmatic chains or \"story rhythms.\"\n\nUses Tufte-inspired minimalist visualizations with high data-to-ink ratio.\n\"\"\"\n\nimport json\nimport os\nfrom datetime import datetime\nfrom collections import Counter, defaultdict\nimport math\n\n# Base directory\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\n# Input and output paths\nINPUT_JSON_PATH = os.path.join(BASE_DIR, \"HONEYBADGER\", \"COMPLETE-TIMELINE.json\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"HONEYBADGER\", \"TIMELINE_ANALYSIS\")\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef log(message):\n    \"\"\"Print log message with timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"{timestamp}: {message}\")\n\ndef sort_timeline_entries(entries):\n    \"\"\"Sort timeline entries by poem and timestamp.\"\"\"\n    # Group entries by poem\n    entries_by_poem = defaultdict(list)\n    for entry in entries:\n        poem = entry.get(\"poem\", \"\")\n        if poem:\n            entries_by_poem[poem].append(entry)\n    \n    # Sort each poem's entries by timestamp\n    for poem, poem_entries in entries_by_poem.items():\n        entries_by_poem[poem] = sorted(poem_entries, key=lambda x: x.get(\"timestamp\", \"00:00:00\"))\n    \n    return entries_by_poem\n\ndef build_transition_matrix(entries_by_poem):\n    \"\"\"Build a transition matrix showing how often each syntagmaType is followed by another.\"\"\"\n    all_syn_types = set()\n    transitions = defaultdict(Counter)\n    \n    # First collect all syntagma types\n    for poem_entries in entries_by_poem.values():\n        for entry in poem_entries:\n            syn_type = entry.get(\"syntagmaType\", \"\")\n            if syn_type:\n                all_syn_types.add(syn_type)\n    \n    # Then build transition counts for each poem\n    for poem, poem_entries in entries_by_poem.items():\n        for i in range(len(poem_entries) - 1):\n            from_type = poem_entries[i].get(\"syntagmaType\", \"\")\n            to_type = poem_entries[i + 1].get(\"syntagmaType\", \"\")\n            if from_type and to_type:\n                transitions[from_type][to_type] += 1\n    \n    # Sort for consistent display\n    all_syn_types = sorted(all_syn_types)\n    \n    return all_syn_types, transitions\n\ndef create_tufte_matrix(syn_types, transitions, output_path):\n    \"\"\"Create a Tufte-inspired transition matrix visualization.\"\"\"\n    with open(output_path, \"w\") as f:\n        f.write(\"# Syntagma Type Transition Matrix\\n\\n\")\n        f.write(\"*How often each syntagma type (row) is followed by another type (column)*\\n\\n\")\n        \n        # Find maximum transition count for scaling\n        max_count = 0\n        for from_type in transitions:\n            for to_type, count in transitions[from_type].items():\n                max_count = max(max_count, count)\n        \n        # Create short abbreviations for column headers\n        abbrevs = {}\n        for i, syn_type in enumerate(syn_types):\n            if syn_type == \"Descriptive Syntagma (DS)\":\n                abbrevs[syn_type] = \"DS\"\n            elif syn_type == \"Crystal Syntagma (CS)\":\n                abbrevs[syn_type] = \"CS\"\n            elif syn_type == \"Chronological Syntagma (CS)\":\n                abbrevs[syn_type] = \"ChS\"\n            elif syn_type == \"Thematic Montage (TM)\":\n                abbrevs[syn_type] = \"TM\"\n            else:\n                # Extract first letter of each word\n                abbrevs[syn_type] = ''.join([word[0] for word in syn_type.split('-')])\n        \n        # Write column headers\n        f.write(\"FROM \u2192 TO | \" + \" | \".join(abbrevs[t] for t in syn_types) + \" | Total\\n\")\n        f.write(\"|\".join([\"---\"] * (len(syn_types) + 2)) + \"\\n\")\n        \n        # Write transition counts\n        for from_type in syn_types:\n            total_transitions = sum(transitions[from_type].values())\n            row = [abbrevs[from_type]]\n            \n            for to_type in syn_types:\n                count = transitions[from_type].get(to_type, 0)\n                \n                # Use unicode block characters for mini bar chart\n                if max_count > 0 and count > 0:\n                    # Scale to 0-8 range for block characters\n                    intensity = min(8, math.ceil(8 * count / max_count))\n                    # Unicode block characters from 1/8 to full block\n                    blocks = \" \u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588\"\n                    bar = blocks[intensity]\n                    cell = f\"{count}{bar}\" if count > 0 else \" \"\n                else:\n                    cell = \" \" if count == 0 else str(count)\n                \n                row.append(cell)\n            \n            row.append(str(total_transitions))\n            f.write(\" | \".join(row) + \"\\n\")\n        \n        f.write(\"\\n\\n\")\n\ndef create_transition_diagrams(syn_types, transitions, entries_by_poem, output_path):\n    \"\"\"Create transition diagrams showing common patterns.\"\"\"\n    with open(output_path, \"w\") as f:\n        f.write(\"# Syntagma Type Transition Patterns\\n\\n\")\n        \n        # Find top transitions overall\n        all_transitions = []\n        for from_type in transitions:\n            for to_type, count in transitions[from_type].items():\n                all_transitions.append((from_type, to_type, count))\n        \n        top_transitions = sorted(all_transitions, key=lambda x: x[2], reverse=True)[:25]\n        \n        f.write(\"## Top 25 Most Common Transitions\\n\\n\")\n        f.write(\"| From \u2192 To | Count | Examples |\\n\")\n        f.write(\"|-----------|-------|----------|\\n\")\n        \n        for from_type, to_type, count in top_transitions:\n            # Find example sequences\n            examples = []\n            for poem, entries in entries_by_poem.items():\n                for i in range(len(entries) - 1):\n                    if entries[i].get(\"syntagmaType\") == from_type and entries[i+1].get(\"syntagmaType\") == to_type:\n                        examples.append(f\"{entries[i].get('id')}\u2192{entries[i+1].get('id')}\")\n                        if len(examples) >= 3:\n                            break\n                if len(examples) >= 3:\n                    break\n            \n            example_str = \", \".join(examples[:3])\n            f.write(f\"| {from_type} \u2192 {to_type} | {count} | {example_str} |\\n\")\n        \n        f.write(\"\\n## Transition Patterns by Poem\\n\\n\")\n        \n        # Create poem-specific transition matrices\n        for poem, entries in entries_by_poem.items():\n            poem_transitions = defaultdict(Counter)\n            for i in range(len(entries) - 1):\n                from_type = entries[i].get(\"syntagmaType\", \"\")\n                to_type = entries[i + 1].get(\"syntagmaType\", \"\")\n                if from_type and to_type:\n                    poem_transitions[from_type][to_type] += 1\n            \n            if not poem_transitions:\n                continue\n                \n            f.write(f\"### {poem}\\n\\n\")\n            \n            # Find poem's common transitions\n            poem_counts = []\n            for from_type in poem_transitions:\n                for to_type, count in poem_transitions[from_type].items():\n                    poem_counts.append((from_type, to_type, count))\n            \n            top_poem_transitions = sorted(poem_counts, key=lambda x: x[2], reverse=True)[:5]\n            \n            if top_poem_transitions:\n                f.write(\"Top transitions:\\n\\n\")\n                for from_type, to_type, count in top_poem_transitions:\n                    percentage = (count / len(entries)) * 100\n                    f.write(f\"- {from_type} \u2192 {to_type}: {count} ({percentage:.1f}%)\\n\")\n                \n                # Create a mini transition chain diagram\n                f.write(\"\\nCommon chains:\\n\\n```\\n\")\n                \n                # Find chains of 3+ transitions\n                chains = find_common_chains(entries, 3)\n                for chain, count in chains[:3]:\n                    f.write(f\"{count}\u00d7 {' \u2192 '.join(chain)}\\n\")\n                \n                f.write(\"```\\n\\n\")\n\ndef find_common_chains(entries, min_length):\n    \"\"\"Find common chains of syntagma types of specified minimum length.\"\"\"\n    all_chains = []\n    \n    for length in range(min_length, min_length + 3):  # Look for chains of length 3-5\n        chains = Counter()\n        \n        for i in range(len(entries) - length + 1):\n            chain = []\n            for j in range(length):\n                syn_type = entries[i + j].get(\"syntagmaType\", \"\")\n                if not syn_type:\n                    break\n                chain.append(syn_type)\n            \n            if len(chain) == length:\n                chains[tuple(chain)] += 1\n        \n        # Add most common chains of this length\n        for chain, count in chains.most_common(3):\n            if count > 1:  # Only include chains that appear more than once\n                all_chains.append((chain, count))\n    \n    return sorted(all_chains, key=lambda x: x[1], reverse=True)\n\ndef analyze_syntagma_transitions():\n    \"\"\"Analyze syntagma type transitions and generate visualizations.\"\"\"\n    log(\"=== Analyzing Syntagma Transitions ===\")\n    \n    # Load the JSON\n    try:\n        with open(INPUT_JSON_PATH, \"r\") as f:\n            timeline_data = json.load(f)\n    except Exception as e:\n        log(f\"Error loading JSON: {e}\")\n        return\n        \n    log(f\"Loaded timeline with {len(timeline_data)} entries\")\n    \n    # Sort entries by poem and timestamp\n    entries_by_poem = sort_timeline_entries(timeline_data)\n    log(f\"Sorted entries for {len(entries_by_poem)} poems\")\n    \n    # Build transition matrix\n    syn_types, transitions = build_transition_matrix(entries_by_poem)\n    log(f\"Built transition matrix for {len(syn_types)} syntagma types\")\n    \n    # Create visualizations\n    matrix_path = os.path.join(OUTPUT_DIR, \"SYNTAGMA_TRANSITION_MATRIX.md\")\n    create_tufte_matrix(syn_types, transitions, matrix_path)\n    log(f\"Created transition matrix visualization at {matrix_path}\")\n    \n    patterns_path = os.path.join(OUTPUT_DIR, \"SYNTAGMA_TRANSITION_PATTERNS.md\")\n    create_transition_diagrams(syn_types, transitions, entries_by_poem, patterns_path)\n    log(f\"Created transition patterns visualization at {patterns_path}\")\n    \n    # Create ASCII sparkline visualization\n    sparkline_path = os.path.join(OUTPUT_DIR, \"SYNTAGMA_FLOW.md\")\n    with open(sparkline_path, \"w\") as f:\n        f.write(\"# Syntagma Flow Visualization\\n\\n\")\n        f.write(\"*Sequence of syntagma types in each poem*\\n\\n\")\n        \n        # Sort poems by length for visual clarity\n        sorted_poems = sorted(entries_by_poem.items(), key=lambda x: len(x[1]), reverse=True)\n        \n        for poem, entries in sorted_poems:\n            # Skip poems with too few entries\n            if len(entries) < 3:\n                continue\n                \n            f.write(f\"## {poem} ({len(entries)} entries)\\n\\n\")\n            \n            # Create syntagma type codes for compact display\n            syn_codes = {\n                \"Descriptive Syntagma (DS)\": \"DS\",\n                \"Affection-Image\": \"AF\",\n                \"Action-Image\": \"AC\",\n                \"Crystal Syntagma (CS)\": \"CS\",\n                \"Sonsign\": \"SN\",\n                \"Perception-Image\": \"PI\",\n                \"Recollection-Image\": \"RI\",\n                \"Chronological Syntagma (CS)\": \"CH\",\n                \"Thematic Montage (TM)\": \"TM\"\n            }\n            \n            # Generate flow\n            flow = []\n            for entry in entries:\n                syn_type = entry.get(\"syntagmaType\", \"\")\n                code = syn_codes.get(syn_type, \"??\")\n                flow.append(f\"{code}\")\n            \n            # Calculate distribution\n            syn_counts = Counter([entry.get(\"syntagmaType\", \"\") for entry in entries])\n            total = len(entries)\n            \n            # Print flow\n            f.write(\"```\\n\")\n            f.write(\" \u2192 \".join(flow) + \"\\n\")\n            f.write(\"```\\n\\n\")\n            \n            # Print distribution\n            f.write(\"**Distribution:**\\n\\n\")\n            for syn_type, count in syn_counts.most_common():\n                percentage = (count / total) * 100\n                code = syn_codes.get(syn_type, \"??\")\n                f.write(f\"- {code} ({syn_type}): {count}/{total} ({percentage:.1f}%)\\n\")\n            \n            f.write(\"\\n---\\n\\n\")\n    \n    log(f\"Created syntagma flow visualization at {sparkline_path}\")\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    analyze_syntagma_transitions()\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis",
      ")\n        \n        # Write transition counts\n        for from_type in syn_types:\n            total_transitions = sum(transitions[from_type].values())\n            row = [abbrevs[from_type]]\n            \n            for to_type in syn_types:\n                count = transitions[from_type].get(to_type, 0)\n                \n                # Use unicode block characters for mini bar chart\n                if max_count > 0 and count > 0:\n                    # Scale to 0-8 range for block characters\n                    intensity = min(8, math.ceil(8 * count / max_count))\n                    # Unicode block characters from 1/8 to full block\n                    blocks = ",
      ")\n                for from_type, to_type, count in top_poem_transitions:\n                    percentage = (count / len(entries)) * 100\n                    f.write(f",
      ")\n            for syn_type, count in syn_counts.most_common():\n                percentage = (count / total) * 100\n                code = syn_codes.get(syn_type, ",
      "- {code} ({syn_type}): {count}/{total} ({percentage:.1f}%)\\n"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "datetime",
      "collections",
      "math"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Syntagma Transition Analysis\n\nGenerates contextual co-occurrence matrices showing how often each syntagmaType \nis followed by each other type, revealing syntagmatic chains or \"story rhythms.\"\n\nUses Tufte-inspired minimalist visualizations with high data-to-ink ratio."
  },
  {
    "path": "HONEYBADGER/complete_timeline.py",
    "size": 18963,
    "lines": 451,
    "source": "#!/usr/bin/env python3\n\"\"\"\nComplete Timeline Generator\n\nThis script updates the SIMPLE-TOTAL-TIMELINE.json file by incorporating missing data\nfor the NM section from NM_assembly.json and NM_prompts.md, creating a COMPLETE-TIMELINE.json file.\n\"\"\"\n\nimport os\nimport json\nimport re\nfrom datetime import datetime\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"HONEYBADGER\")\n\n# Source files\nSIMPLE_TIMELINE_JSON = os.path.join(OUTPUT_DIR, \"SIMPLE-TOTAL-TIMELINE.json\")\nNM_ASSEMBLY_JSON = os.path.join(TIGER_DIR, \"NM\", \"NM_assembly.json\")\n\n# Prompt files - all sections\nNM_PROMPTS_MD = os.path.join(TIGER_DIR, \"NM\", \"NM_prompts.md\")\nAT_PROMPTS_MD = os.path.join(TIGER_DIR, \"AT\", \"AT_prompts.md\")\nDJ_PROMPTS_MD = os.path.join(TIGER_DIR, \"DJ\", \"DJ_prompts.md\")\nNS_PROMPTS_MD = os.path.join(TIGER_DIR, \"NS\", \"NS_prompts.md\")\nYH_PROMPTS_MD = os.path.join(TIGER_DIR, \"YH\", \"YH_prompts.md\")\nBE_PROMPTS_MD = os.path.join(TIGER_DIR, \"BE\", \"BE_prompts.md\")\nBL_PROMPTS_MD = os.path.join(TIGER_DIR, \"BL\", \"BL_prompts.md\")\nFL_PROMPTS_MD = os.path.join(TIGER_DIR, \"FL\", \"FL_prompts.md\")\nHM_PROMPTS_MD = os.path.join(TIGER_DIR, \"HM\", \"HM_prompts.md\")\nHT_PROMPTS_MD = os.path.join(TIGER_DIR, \"HT\", \"HT_prompts.md\")\nMR_PROMPTS_MD = os.path.join(TIGER_DIR, \"MR\", \"MR_prompts.md\")\nRU_PROMPTS_MD = os.path.join(TIGER_DIR, \"RU\", \"RU_prompts.md\")\nSH_PROMPTS_MD = os.path.join(TIGER_DIR, \"SH\", \"SH_prompts.md\")\n\n# Output file\nCOMPLETE_TIMELINE_JSON = os.path.join(OUTPUT_DIR, \"COMPLETE-TIMELINE.json\")\n\ndef log(message):\n    \"\"\"Print log message with timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"{timestamp}: {message}\")\n\ndef extract_prompts_from_markdown(md_file, prefix):\n    \"\"\"\n    Extract prompts from prompts.md file.\n    Returns a dictionary mapping shot IDs to complete prompt strings.\n    Format: \"CATEGORY \u00b7 FUNCTION \u00b7 DESCRIPTION \u00b7 PROMPT_TEXT\"\n    \n    Args:\n        md_file: Path to the markdown file containing prompts\n        prefix: Shot ID prefix (e.g., \"NM\" or \"AT\")\n    \"\"\"\n    prompts = {}\n    \n    try:\n        with open(md_file, 'r') as f:\n            content = f.read()\n        \n        # Pattern to match prompts\n        # Format: \"NM001 \u00b7 DS \u00b7 Mood Environment Stabilizer \u00b7 Title set in raven-black calligraphy over twilight \u00b7 Indigo sky bleeds behind...\"\n        pattern = r'(' + prefix + r'\\d{3})\\s*\u00b7\\s*([A-Z]+)\\s*\u00b7\\s*([^\u00b7]+)\u00b7\\s*([^\u00b7]+)\u00b7\\s*(.+?)(?=\\n' + prefix + r'\\d{3}|\\n\\n|\\Z)'\n        \n        matches = re.finditer(pattern, content, re.DOTALL)\n        \n        for match in matches:\n            shot_id = match.group(1)\n            category = match.group(2).strip()\n            function = match.group(3).strip()\n            description = match.group(4).strip()\n            prompt_text = match.group(5).strip() if len(match.groups()) > 4 else \"\"\n            \n            # Compress any newlines to create a single line\n            description = description.replace('\\n', ' ').strip()\n            prompt_text = prompt_text.replace('\\n', ' ').strip()\n            \n            # Format exactly like in the Codex.md\n            full_prompt = f\"{category} \u00b7 {function} \u00b7 {description} \u00b7 {prompt_text}\"\n            \n            prompts[shot_id] = full_prompt\n            \n        log(f\"Extracted {len(prompts)} prompts from {os.path.basename(md_file)}\")\n        return prompts\n    \n    except Exception as e:\n        log(f\"Error extracting NM prompts from markdown: {e}\")\n        return {}\n\ndef load_nm_assembly_data(assembly_file):\n    \"\"\"\n    Load assembly data from NM_assembly.json file.\n    Returns a dictionary mapping shot IDs to assembly data.\n    \"\"\"\n    try:\n        with open(assembly_file, 'r') as f:\n            assembly_data = json.load(f)\n        \n        # Convert to dictionary with shot_id as key\n        assembly_dict = {item['id']: item for item in assembly_data}\n        \n        log(f\"Loaded {len(assembly_dict)} entries from NM_assembly.json\")\n        return assembly_dict\n    \n    except Exception as e:\n        log(f\"Error loading NM assembly data: {e}\")\n        return {}\n\ndef create_complete_timeline():\n    \"\"\"\n    Create a complete timeline by updating the SIMPLE-TOTAL-TIMELINE.json with NM data.\n    \"\"\"\n    log(\"Starting complete timeline generation...\")\n    \n    # Load simple timeline JSON\n    try:\n        with open(SIMPLE_TIMELINE_JSON, 'r') as f:\n            timeline_data = json.load(f)\n        \n        log(f\"Loaded simple timeline JSON with {len(timeline_data)} entries\")\n    except Exception as e:\n        log(f\"Error loading simple timeline JSON: {e}\")\n        return False\n    \n    # Load NM assembly data\n    nm_assembly = load_nm_assembly_data(NM_ASSEMBLY_JSON)\n    \n    # Load prompts for all sections\n    nm_prompts = extract_prompts_from_markdown(NM_PROMPTS_MD, \"NM\")\n    at_prompts = extract_prompts_from_markdown(AT_PROMPTS_MD, \"AT\")\n    dj_prompts = extract_prompts_from_markdown(DJ_PROMPTS_MD, \"DJ\")\n    ns_prompts = extract_prompts_from_markdown(NS_PROMPTS_MD, \"NS\")\n    yh_prompts = extract_prompts_from_markdown(YH_PROMPTS_MD, \"YH\")\n    be_prompts = extract_prompts_from_markdown(BE_PROMPTS_MD, \"BE\")\n    bl_prompts = extract_prompts_from_markdown(BL_PROMPTS_MD, \"BL\")\n    fl_prompts = extract_prompts_from_markdown(FL_PROMPTS_MD, \"FL\")\n    hm_prompts = extract_prompts_from_markdown(HM_PROMPTS_MD, \"HM\")\n    ht_prompts = extract_prompts_from_markdown(HT_PROMPTS_MD, \"HT\")\n    mr_prompts = extract_prompts_from_markdown(MR_PROMPTS_MD, \"MR\")\n    ru_prompts = extract_prompts_from_markdown(RU_PROMPTS_MD, \"RU\")\n    sh_prompts = extract_prompts_from_markdown(SH_PROMPTS_MD, \"SH\")\n    \n    # Update entries in the timeline\n    updated_nm_count = 0\n    updated_at_count = 0\n    updated_dj_count = 0\n    updated_ns_count = 0\n    updated_yh_count = 0\n    updated_be_count = 0\n    updated_bl_count = 0\n    updated_fl_count = 0\n    updated_hm_count = 0\n    updated_ht_count = 0\n    updated_mr_count = 0\n    updated_ru_count = 0\n    updated_sh_count = 0\n    \n    for i, entry in enumerate(timeline_data):\n        shot_id = entry.get('id', '')\n        \n        # Check if this is an NM entry with missing data\n        if shot_id.startswith('NM') and entry.get('missing_assembly_data', False):\n            # Get assembly data\n            assembly_entry = nm_assembly.get(shot_id)\n            nm_prompt = nm_prompts.get(shot_id)\n            \n            if assembly_entry:\n                # Create a new entry with fields in the desired order\n                new_entry = {\n                    \"timestamp\": entry.get(\"timestamp\", \"\"),\n                    \"image_path\": entry.get(\"image_path\", \"\"),\n                    \"id\": shot_id,\n                    \"poem\": assembly_entry.get(\"poem\", \"\"),\n                    \"content\": assembly_entry.get(\"content\", \"\"),\n                    \"syntagmaType\": assembly_entry.get(\"syntagmaType\", \"\"),\n                    \"operativeEkphrasis\": assembly_entry.get(\"operativeEkphrasis\", \"\"),\n                    \"imageType\": assembly_entry.get(\"imageType\", \"\"),\n                    \"cineosisFunction\": assembly_entry.get(\"cineosisFunction\", \"\")\n                }\n                \n                # Add prompt as the last field if available\n                if nm_prompt:\n                    new_entry[\"full_prompt\"] = nm_prompt\n                \n                # Replace the entry with our new ordered entry\n                timeline_data[i] = new_entry\n                updated_nm_count += 1\n        \n        # Check if this is an AT entry that needs a prompt\n        elif shot_id.startswith('AT') and ('full_prompt' not in entry or not entry['full_prompt']):\n            at_prompt = at_prompts.get(shot_id)\n            \n            if at_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = at_prompt\n                updated_at_count += 1\n        \n        # Check if this is a DJ entry that needs a prompt\n        elif shot_id.startswith('DJ') and ('full_prompt' not in entry or not entry['full_prompt']):\n            dj_prompt = dj_prompts.get(shot_id)\n            \n            if dj_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = dj_prompt\n                updated_dj_count += 1\n        \n        # Check if this is an NS entry that needs a prompt\n        elif shot_id.startswith('NS') and ('full_prompt' not in entry or not entry['full_prompt']):\n            ns_prompt = ns_prompts.get(shot_id)\n            \n            if ns_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = ns_prompt\n                updated_ns_count += 1\n                \n        # Check if this is a YH entry that needs a prompt\n        elif shot_id.startswith('YH') and ('full_prompt' not in entry or not entry['full_prompt']):\n            yh_prompt = yh_prompts.get(shot_id)\n            \n            if yh_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = yh_prompt\n                updated_yh_count += 1\n                \n        # Check if this is a BE entry that needs a prompt\n        elif shot_id.startswith('BE') and ('full_prompt' not in entry or not entry['full_prompt']):\n            be_prompt = be_prompts.get(shot_id)\n            \n            if be_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = be_prompt\n                updated_be_count += 1\n                \n        # Check if this is a BL entry that needs a prompt\n        elif shot_id.startswith('BL') and ('full_prompt' not in entry or not entry['full_prompt']):\n            bl_prompt = bl_prompts.get(shot_id)\n            \n            if bl_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = bl_prompt\n                updated_bl_count += 1\n                \n        # Check if this is an FL entry that needs a prompt\n        elif shot_id.startswith('FL') and ('full_prompt' not in entry or not entry['full_prompt']):\n            fl_prompt = fl_prompts.get(shot_id)\n            \n            if fl_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = fl_prompt\n                updated_fl_count += 1\n                \n        # Check if this is an HM entry that needs a prompt\n        elif shot_id.startswith('HM') and ('full_prompt' not in entry or not entry['full_prompt']):\n            hm_prompt = hm_prompts.get(shot_id)\n            \n            if hm_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = hm_prompt\n                updated_hm_count += 1\n                \n        # Check if this is an HT entry that needs a prompt\n        elif shot_id.startswith('HT') and ('full_prompt' not in entry or not entry['full_prompt']):\n            ht_prompt = ht_prompts.get(shot_id)\n            \n            if ht_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = ht_prompt\n                updated_ht_count += 1\n                \n        # Check if this is an MR entry that needs a prompt\n        elif shot_id.startswith('MR') and ('full_prompt' not in entry or not entry['full_prompt']):\n            mr_prompt = mr_prompts.get(shot_id)\n            \n            if mr_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = mr_prompt\n                updated_mr_count += 1\n                \n        # Check if this is an RU entry that needs a prompt\n        elif shot_id.startswith('RU') and ('full_prompt' not in entry or not entry['full_prompt']):\n            ru_prompt = ru_prompts.get(shot_id)\n            \n            if ru_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = ru_prompt\n                updated_ru_count += 1\n                \n        # Check if this is an SH entry that needs a prompt\n        elif shot_id.startswith('SH') and ('full_prompt' not in entry or not entry['full_prompt']):\n            sh_prompt = sh_prompts.get(shot_id)\n            \n            if sh_prompt:\n                # Add the prompt while preserving all other fields\n                timeline_data[i]['full_prompt'] = sh_prompt\n                updated_sh_count += 1\n    \n    log(f\"Updated {updated_nm_count} NM entries with assembly data\")\n    log(f\"Added prompts to {updated_at_count} AT entries where available\")\n    log(f\"Added prompts to {updated_dj_count} DJ entries where available\")\n    log(f\"Added prompts to {updated_ns_count} NS entries where available\")\n    log(f\"Added prompts to {updated_yh_count} YH entries where available\")\n    log(f\"Added prompts to {updated_be_count} BE entries where available\")\n    log(f\"Added prompts to {updated_bl_count} BL entries where available\")\n    log(f\"Added prompts to {updated_fl_count} FL entries where available\")\n    log(f\"Added prompts to {updated_hm_count} HM entries where available\")\n    log(f\"Added prompts to {updated_ht_count} HT entries where available\")\n    log(f\"Added prompts to {updated_mr_count} MR entries where available\")\n    log(f\"Added prompts to {updated_ru_count} RU entries where available\")\n    log(f\"Added prompts to {updated_sh_count} SH entries where available\")\n    \n    # Fix HT entries where content is incorrectly in the poem field\n    updated_ht_structure_count = 0\n    for i, entry in enumerate(timeline_data):\n        shot_id = entry.get('id', '')\n        \n        # Check for HT entries with missing content but content in poem field\n        if shot_id.startswith('HT') and shot_id != 'HT031' and shot_id != 'HT032':\n            poem = entry.get('poem', '')\n            content = entry.get('content', '')\n            \n            # If content is empty and poem doesn't match \"How To Win My Heart\"\n            if not content and poem and poem != \"How To Win My Heart\":\n                # Move what's in poem field to content field\n                timeline_data[i]['content'] = poem\n                # Set correct poem name\n                timeline_data[i]['poem'] = \"How To Win My Heart\"\n                updated_ht_structure_count += 1\n    \n    log(f\"Fixed {updated_ht_structure_count} HT entries with incorrect field structure\")\n    \n    # Ensure consistent field order for all entries\n    for i, entry in enumerate(timeline_data):\n        # Create a new ordered dictionary with fields in proper sequence\n        ordered_entry = {}\n        \n        # Add fields in the desired order\n        for field in ['timestamp', 'image_path', 'id', 'poem', 'content']:\n            if field in entry:\n                ordered_entry[field] = entry[field]\n        \n        # Add any remaining fields except full_prompt\n        for field, value in entry.items():\n            if field not in ordered_entry and field != 'full_prompt':\n                ordered_entry[field] = value\n        \n        # Add full_prompt as the last field if it exists\n        if 'full_prompt' in entry:\n            ordered_entry['full_prompt'] = entry['full_prompt']\n        \n        # Replace the original entry with the ordered one\n        timeline_data[i] = ordered_entry\n        \n    log(\"Standardized field order for all entries: timestamp, image_path, id, poem, content, etc.\")\n\n    \n    # Save complete timeline JSON\n    try:\n        with open(COMPLETE_TIMELINE_JSON, 'w') as f:\n            json.dump(timeline_data, f, indent=2)\n        \n        log(f\"Saved COMPLETE-TIMELINE.json with {len(timeline_data)} entries\")\n        return True\n    except Exception as e:\n        log(f\"Error saving complete timeline JSON: {e}\")\n        return False\n\ndef validate_complete_timeline():\n    \"\"\"\n    Validate the complete timeline JSON.\n    \"\"\"\n    log(\"Validating complete timeline...\")\n    \n    try:\n        with open(COMPLETE_TIMELINE_JSON, 'r') as f:\n            data = json.load(f)\n        \n        total = len(data)\n        complete_entries = 0\n        incomplete_entries = 0\n        nm_entries = 0\n        nm_complete = 0\n        \n        # Check for completeness of essential fields\n        for entry in data:\n            shot_id = entry.get('id', '')\n            is_nm = shot_id.startswith('NM')\n            \n            if is_nm:\n                nm_entries += 1\n                \n            # Check if critical fields are present\n            has_id = 'id' in entry and entry['id']\n            has_image = 'image_path' in entry and entry['image_path']\n            has_poem = 'poem' in entry and entry['poem']\n            has_content = 'content' in entry and entry['content']\n            has_metadata = ('syntagmaType' in entry and 'imageType' in entry and \n                           'cineosisFunction' in entry)\n            has_prompt = 'full_prompt' in entry and entry['full_prompt']\n            \n            # Also check if full_prompt is the last field (proper order)\n            keys = list(entry.keys())\n            correct_order = len(keys) > 0 and keys[-1] == 'full_prompt'\n            \n            if has_id and has_image and has_poem and has_content and has_metadata:\n                complete_entries += 1\n                \n                if is_nm:\n                    nm_complete += 1\n            else:\n                incomplete_entries += 1\n        \n        log(f\"Total entries: {total}\")\n        log(f\"Complete entries (with all metadata): {complete_entries}\")\n        log(f\"Incomplete entries: {incomplete_entries}\")\n        log(f\"NM section entries: {nm_entries}\")\n        log(f\"Complete NM entries: {nm_complete}\")\n        \n        # Show sample NM entries for verification\n        nm_samples = []\n        for entry in data:\n            if entry.get('id', '').startswith('NM') and len(nm_samples) < 5:\n                nm_samples.append(entry)\n        \n        if nm_samples:\n            log(f\"Sample NM entries for verification:\")\n            for entry in nm_samples:\n                shot_id = entry.get('id', '')\n                poem = entry.get('poem', '')\n                content = entry.get('content', '')\n                prompt = entry.get('full_prompt', '')[:50] + '...' if entry.get('full_prompt') else 'MISSING'\n                \n                log(f\"- {shot_id}: Poem: '{poem}', Content: '{content}', Prompt: {prompt}\")\n        \n        return True\n    except Exception as e:\n        log(f\"Error validating complete timeline: {e}\")\n        return False\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    log(\"=== Complete Timeline Generator ===\")\n    \n    if create_complete_timeline():\n        validate_complete_timeline()\n    \n    log(\"=== Complete Timeline Generation Complete ===\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SIMPLE-TOTAL-TIMELINE.json",
      "NM_assembly.json",
      "COMPLETE-TIMELINE.json",
      "Loaded {len(assembly_dict)} entries from NM_assembly.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "re",
      "datetime"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Complete Timeline Generator\n\nThis script updates the SIMPLE-TOTAL-TIMELINE.json file by incorporating missing data\nfor the NM section from NM_assembly.json and NM_prompts.md, creating a COMPLETE-TIMELINE.json file."
  },
  {
    "path": "HONEYBADGER/fix_final_inconsistency.py",
    "size": 685,
    "lines": 24,
    "source": "#!/usr/bin/env python3\n\"\"\"Fix the final syntagma type inconsistency\"\"\"\n\nimport json\n\n# Load the timeline\nwith open('/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json', 'r') as f:\n    data = json.load(f)\n\n# Fix any remaining \"Autonomous Syntagma\" entries\nfixed = 0\nfor entry in data:\n    if entry['syntagmaType'] == 'Autonomous Syntagma':\n        entry['syntagmaType'] = 'Autonomous Syntagma (AS)'\n        fixed += 1\n\nprint(f\"Fixed {fixed} remaining inconsistencies\")\n\n# Save the updated timeline\nwith open('/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json', 'w') as f:\n    json.dump(data, f, indent=2)\n\nprint(\"Timeline updated successfully\")\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Fix the final syntagma type inconsistency"
  },
  {
    "path": "HONEYBADGER/improved_codex_overlay.py",
    "size": 6337,
    "lines": 154,
    "source": "#!/usr/bin/env python3\n\"\"\"\nImproved Codex Overlay Generator\nCreates a single test frame with enhanced codex overlay that:\n1. Makes poem content larger and properly right-aligned\n2. Includes the operativeEkphrasis field \n3. Fixes spacing issues with longer poem names\n4. Improves overall layout and readability\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"HONEYBADGER\", \"output\")\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Test sample from TimelineAssemblage.json\nTEST_ASSEMBLY = {\n    \"timestamp\": \"08:47:16\",\n    \"image_path\": \"BE/BE002__CS__Temporal_Reflection_Loop__Infinity_symbo_88dd3cf8-c745-40a6-8ad0-80ba0735fbcb_0.png\",\n    \"id\": \"BE002\",\n    \"poem\": \"How to break off an engagement\",\n    \"content\": \"Ode to forever\u2014\",\n    \"syntagmaType\": \"Crystal Syntagma (CS)\",\n    \"operativeEkphrasis\": \"Infinity symbol made of pale light, slowly fraying at both ends.\",\n    \"imageType\": \"Crystal-Image\",\n    \"cineosisFunction\": \"Temporal Reflection Loop\"\n}\n\n# Define colors and fonts\nBLACK = \"#000000\"\nWHITE = \"#FFFFFF\"\nRED = \"#FF0000\"\nBLUE = \"#0000FF\"\nCREAM = \"#FFF8E0\"\nTRANSPARENT_BLACK = (0, 0, 0, 180)  # RGBA with alpha for transparency\n\ndef create_improved_codex_overlay(assembly_data, output_path):\n    \"\"\"\n    Create a test image with improved codex overlay\n    \"\"\"\n    # Load image\n    image_path = os.path.join(TIGER_DIR, assembly_data[\"image_path\"])\n    if not os.path.exists(image_path):\n        # Create a blank canvas if image doesn't exist\n        pil_img = Image.new('RGB', (1920, 1080), color=(100, 100, 100))  # Gray background\n    else:\n        pil_img = Image.open(image_path)\n        # Resize if needed\n        if pil_img.width != 1920 or pil_img.height != 1080:\n            pil_img = pil_img.resize((1920, 1080))\n    \n    # Create a transparent overlay for the codex data\n    overlay = Image.new('RGBA', pil_img.size, (0, 0, 0, 0))\n    draw = ImageDraw.Draw(overlay)\n    \n    # Draw semi-transparent background\n    draw.rectangle([(0, 0), (pil_img.width, 240)], fill=(0, 0, 0, 180))\n\n    # Load fonts - use default if custom fonts are not available\n    try:\n        title_font = ImageFont.truetype(\"Arial Bold\", 36)\n        large_font = ImageFont.truetype(\"Arial Bold\", 30)\n        medium_font = ImageFont.truetype(\"Arial\", 26)\n        small_font = ImageFont.truetype(\"Arial\", 22)\n    except IOError:\n        title_font = ImageFont.load_default()\n        large_font = ImageFont.load_default()\n        medium_font = ImageFont.load_default()\n        small_font = ImageFont.load_default()\n\n    # Draw shot ID and timestamp at the top left\n    draw.text((30, 15), f\"{assembly_data['id']} [{assembly_data['timestamp']}]\", \n              fill=WHITE, font=title_font)\n    \n    # Draw poem name - shortened if too long\n    poem_name = assembly_data['poem']\n    if len(poem_name) > 35:\n        poem_name = poem_name[:32] + \"...\"\n    draw.text((30, 60), poem_name, fill=WHITE, font=medium_font)\n    \n    # Draw the TYPE value at the top right\n    syntagma_type = assembly_data.get('syntagmaType', '').split(' ')[0] if assembly_data.get('syntagmaType') else \"TYPE\"\n    type_text = f\"T: {syntagma_type}\"\n    type_width = large_font.getbbox(type_text)[2]\n    draw.text((pil_img.width - type_width - 30, 15), type_text, \n              fill=WHITE, font=large_font)\n    \n    # Draw the FUNC value at the top right second line\n    func_abbr = ''.join([word[0] for word in assembly_data.get('cineosisFunction', '').split() if word])\n    func_text = f\"F: {func_abbr}\"\n    func_width = large_font.getbbox(func_text)[2]\n    draw.text((pil_img.width - func_width - 30, 55), func_text, \n              fill=WHITE, font=large_font)\n    \n    # Draw the SYNT value at the top right third line\n    image_type = assembly_data.get('imageType', '').split('-')[0] if assembly_data.get('imageType') else \"SYNT\"\n    synt_text = f\"S: {image_type}\"\n    synt_width = large_font.getbbox(synt_text)[2]\n    draw.text((pil_img.width - synt_width - 30, 95), synt_text, \n              fill=WHITE, font=large_font)\n    \n    # Draw poem content - larger and right-aligned but not overlapping F value\n    content = assembly_data.get('content', '')\n    content_width = large_font.getbbox(content)[2]\n    # Position content so it doesn't overlap with the F value\n    content_x = min(pil_img.width - content_width - 30, pil_img.width - func_width - 200)\n    draw.text((content_x, 140), content, fill=WHITE, font=large_font)\n    \n    # Draw operative ekphrasis - multi-line if needed\n    ekphrasis = assembly_data.get('operativeEkphrasis', '')\n    wrapped_ekphrasis = textwrap.fill(ekphrasis, width=80)\n    draw.text((30, 140), wrapped_ekphrasis, fill=WHITE, font=small_font)\n    \n    # Combine the image with the overlay\n    result = Image.alpha_composite(pil_img.convert('RGBA'), overlay)\n    \n    # Save result\n    result.convert('RGB').save(output_path)\n    print(f\"Created improved codex overlay test frame: {output_path}\")\n\ndef main():\n    \"\"\"Main function to create test images\"\"\"\n    # Create test with sample assembly data\n    output_file = os.path.join(OUTPUT_DIR, \"improved_codex_test.jpg\")\n    create_improved_codex_overlay(TEST_ASSEMBLY, output_file)\n    \n    # Optionally load more samples from TimelineAssemblage.json\n    assemblage_file = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json\")\n    if os.path.exists(assemblage_file):\n        try:\n            with open(assemblage_file, 'r') as f:\n                assemblage_data = json.load(f)\n                \n            # Test with another example that has a long poem name\n            for item in assemblage_data:\n                if len(item.get('poem', '')) > 30 and 'operativeEkphrasis' in item:\n                    output_file = os.path.join(OUTPUT_DIR, f\"improved_codex_test_long_name_{item['id']}.jpg\")\n                    create_improved_codex_overlay(item, output_file)\n                    print(f\"Created test with long poem name: {item['poem']}\")\n                    break\n        except Exception as e:\n            print(f\"Error loading TimelineAssemblage.json: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "BE/BE002__CS__Temporal_Reflection_Loop__Infinity_symbo_88dd3cf8-c745-40a6-8ad0-80ba0735fbcb_0.png",
      "improved_codex_test.jpg",
      "COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json",
      "]}.jpg",
      "BE/BE002__CS__Temporal_Reflection_Loop__Infinity_symbo_88dd3cf8-c745-40a6-8ad0-80ba0735fbcb_0.png"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Improved Codex Overlay Generator\nCreates a single test frame with enhanced codex overlay that:\n1. Makes poem content larger and properly right-aligned\n2. Includes the operativeEkphrasis field \n3. Fixes spacing issues with longer poem names\n4. Improves overall layout and readability"
  },
  {
    "path": "HONEYBADGER/fix_syntagma_types.py",
    "size": 11738,
    "lines": 317,
    "source": "#!/usr/bin/env python3\n\"\"\"\nSyntagma Type Correction Script\n\nThis script fixes entries in the COMPLETE-TIMELINE.json file where imageType and syntagmaType\nare identical, applying a rule-based approach derived from film theory principles.\n\nIt implements a multi-level framework that combines:\n1. Direct theoretical mappings (Metz to Deleuze correspondence)\n2. Contextual sequence analysis\n3. Content-based heuristics\n4. Structural position analysis\n\"\"\"\n\nimport json\nimport os\nfrom collections import defaultdict\n\n# Canonical syntagma types\nCANONICAL_SYNTAGMA_TYPES = {\n    \"Descriptive Syntagma\",\n    \"Chronological Syntagma\",\n    \"Autonomous Syntagma\", \n    \"Flashback Syntagma\",\n    \"Thematic Montage\",\n    \"Crystal Syntagma\"\n}\n\ndef load_timeline():\n    \"\"\"Load the complete timeline data\"\"\"\n    with open('/Users/gaia/resurrecting atlantis/HONEYBADGER/COMPLETE-TIMELINE.json', 'r') as f:\n        return json.load(f)\n\ndef save_timeline(timeline, filename='UPDATED-TIMELINE.json'):\n    \"\"\"Save the updated timeline\"\"\"\n    output_path = f'/Users/gaia/resurrecting atlantis/HONEYBADGER/{filename}'\n    with open(output_path, 'w') as f:\n        json.dump(timeline, f, indent=2)\n    print(f\"Updated timeline saved to {output_path}\")\n\ndef is_problematic_entry(entry):\n    \"\"\"Check if entry has the same imageType and syntagmaType\"\"\"\n    return entry['imageType'] == entry['syntagmaType']\n\ndef extract_entry_id_num(entry_id):\n    \"\"\"Extract the numeric part from an entry ID for sorting\"\"\"\n    # Entries typically have format like \"FL001\", \"NS023\"\n    try:\n        return int(entry_id[2:])\n    except:\n        return 0\n\ndef organize_entries_by_poem(timeline):\n    \"\"\"Group and sort entries by poem\"\"\"\n    poem_entries = defaultdict(list)\n    for entry in timeline:\n        poem_entries[entry['poem']].append(entry)\n    \n    # Sort entries within each poem by ID\n    for poem, entries in poem_entries.items():\n        poem_entries[poem] = sorted(entries, key=lambda e: extract_entry_id_num(e['id']))\n    \n    return poem_entries\n\ndef get_entry_position_in_poem(entry, poem_entries):\n    \"\"\"Get the position of an entry in its poem (start, middle, end)\"\"\"\n    entries = poem_entries[entry['poem']]\n    entry_index = next((i for i, e in enumerate(entries) if e['id'] == entry['id']), -1)\n    \n    if entry_index == -1:\n        return \"unknown\"\n    \n    total_entries = len(entries)\n    if entry_index < 3:  # First 3 entries\n        return \"start\"\n    elif entry_index >= total_entries - 3:  # Last 3 entries\n        return \"end\"\n    else:\n        return \"middle\"\n\ndef get_entry_context(entry, poem_entries, window=2):\n    \"\"\"Get preceding and following entries for context\"\"\"\n    entries = poem_entries[entry['poem']]\n    entry_index = next((i for i, e in enumerate(entries) if e['id'] == entry['id']), -1)\n    \n    if entry_index == -1:\n        return [], []\n    \n    context_before = entries[max(0, entry_index-window):entry_index]\n    context_after = entries[entry_index+1:min(len(entries), entry_index+window+1)]\n    \n    return context_before, context_after\n\ndef is_in_sequence(entry, context_before, context_after):\n    \"\"\"Check if entry is part of a sequence of same imageType\"\"\"\n    # Check if entry is preceded by 2 entries of same imageType\n    if len(context_before) >= 2:\n        if all(e['imageType'] == entry['imageType'] for e in context_before[-2:]):\n            return True\n    \n    # Check if entry is followed by 2 entries of same imageType\n    if len(context_after) >= 2:\n        if all(e['imageType'] == entry['imageType'] for e in context_after[:2]):\n            return True\n    \n    return False\n\ndef apply_level1_direct_mappings(entry):\n    \"\"\"Apply Level 1 direct theoretical mappings\"\"\"\n    if not is_problematic_entry(entry):\n        return entry['syntagmaType'], 0\n    \n    # Direct mappings based on image type and cineosis function\n    if entry['imageType'] == 'Action-Image' and entry['cineosisFunction'] == 'Causal Motion Trigger':\n        return 'Chronological Syntagma', 1\n        \n    elif entry['imageType'] == 'Recollection-Image' and 'Memory' in entry['cineosisFunction']:\n        return 'Flashback Syntagma', 1\n        \n    elif entry['imageType'] in ['Affection-Image', 'Sonsign', 'Perception-Image']:\n        return 'Autonomous Syntagma', 1\n    \n    # Default - no change yet\n    return entry['syntagmaType'], 0\n\ndef apply_level2_contextual_rules(entry, context_before, context_after, position):\n    \"\"\"Apply Level 2 contextual rules based on sequence analysis\"\"\"\n    if not is_problematic_entry(entry):\n        return entry['syntagmaType'], 0\n    \n    # Check for sequences (3+ consecutive entries of same type)\n    if is_in_sequence(entry, context_before, context_after):\n        if entry['imageType'] == 'Affection-Image':\n            return 'Descriptive Syntagma', 2\n        elif entry['imageType'] == 'Action-Image':\n            return 'Chronological Syntagma', 2\n        elif entry['imageType'] == 'Perception-Image':\n            return 'Descriptive Syntagma', 2\n        elif entry['imageType'] == 'Sonsign' and len(context_before) >= 1 and context_before[0]['imageType'] == 'Sonsign':\n            return 'Thematic Montage', 2\n    \n    return entry['syntagmaType'], 0\n\ndef apply_level3_position_rules(entry, position):\n    \"\"\"Apply Level 3 rules based on structural position in the poem\"\"\"\n    if not is_problematic_entry(entry):\n        return entry['syntagmaType'], 0\n    \n    if position == \"start\":\n        return 'Descriptive Syntagma', 3\n    \n    elif position == \"end\":\n        if entry['imageType'] == 'Affection-Image':\n            return 'Descriptive Syntagma', 3\n        else:\n            return 'Autonomous Syntagma', 3\n    \n    return entry['syntagmaType'], 0\n\ndef determine_syntagma_type(entry, poem_entries):\n    \"\"\"Apply all rule levels in order to determine the correct syntagmaType\"\"\"\n    if not is_problematic_entry(entry):\n        return entry['syntagmaType'], 0, \"Not problematic\"\n    \n    # Get context and position\n    context_before, context_after = get_entry_context(entry, poem_entries)\n    position = get_entry_position_in_poem(entry, poem_entries)\n    \n    # Apply rules in order of precedence\n    new_type, level = apply_level1_direct_mappings(entry)\n    if level > 0:\n        return new_type, level, \"Direct theoretical mapping\"\n    \n    new_type, level = apply_level2_contextual_rules(entry, context_before, context_after, position)\n    if level > 0:\n        return new_type, level, \"Contextual sequence analysis\"\n    \n    new_type, level = apply_level3_position_rules(entry, position)\n    if level > 0:\n        return new_type, level, \"Structural position analysis\"\n    \n    # Default fallback for any remaining problematic entries\n    if entry['imageType'] == 'Action-Image':\n        return 'Chronological Syntagma', 4, \"Default fallback for Action-Image\"\n    elif entry['imageType'] == 'Descriptive Image':\n        return 'Descriptive Syntagma', 4, \"Default fallback for Descriptive Image\"\n    else:\n        return 'Autonomous Syntagma', 4, \"Default fallback\"\n\ndef fix_syntagma_types(timeline):\n    \"\"\"Process the entire timeline and fix syntagmaType values\"\"\"\n    # Organize entries by poem for contextual analysis\n    poem_entries = organize_entries_by_poem(timeline)\n    \n    # Track changes\n    changes = []\n    \n    # Process each entry\n    for entry in timeline:\n        if is_problematic_entry(entry):\n            old_syntagma = entry['syntagmaType']\n            new_syntagma, rule_level, rule_description = determine_syntagma_type(entry, poem_entries)\n            \n            if new_syntagma != old_syntagma:\n                # Update the entry\n                entry['syntagmaType'] = new_syntagma\n                \n                # Record the change\n                changes.append({\n                    'id': entry['id'],\n                    'poem': entry['poem'],\n                    'old_syntagma': old_syntagma,\n                    'new_syntagma': new_syntagma,\n                    'imageType': entry['imageType'],\n                    'cineosisFunction': entry['cineosisFunction'],\n                    'content': entry.get('content', ''),\n                    'rule_level': rule_level,\n                    'rule_description': rule_description\n                })\n    \n    return changes\n\ndef generate_changes_report(changes):\n    \"\"\"Generate a detailed report of changes made\"\"\"\n    report_lines = [\"# Syntagma Type Correction Report\\n\"]\n    report_lines.append(f\"**Total entries corrected:** {len(changes)}\\n\")\n    \n    # Group changes by original syntagma type\n    by_original = defaultdict(list)\n    for change in changes:\n        by_original[change['old_syntagma']].append(change)\n    \n    report_lines.append(\"## Changes by Original Type\\n\")\n    \n    for orig_type, items in sorted(by_original.items()):\n        report_lines.append(f\"### {orig_type}\\n\")\n        report_lines.append(f\"Total changed: {len(items)}\\n\")\n        \n        # Group by new syntagma type\n        by_new = defaultdict(list)\n        for item in items:\n            by_new[item['new_syntagma']].append(item)\n        \n        for new_type, subitems in sorted(by_new.items()):\n            report_lines.append(f\"#### {orig_type} \u2192 {new_type}\\n\")\n            report_lines.append(f\"Count: {len(subitems)}\\n\")\n            \n            # Group by rule applied\n            by_rule = defaultdict(list)\n            for item in subitems:\n                by_rule[item['rule_description']].append(item)\n            \n            for rule, rule_items in sorted(by_rule.items()):\n                report_lines.append(f\"- **{rule}:** {len(rule_items)} entries\\n\")\n                report_lines.append(\"  - Examples: \")\n                example_ids = [item['id'] for item in rule_items[:5]]\n                report_lines.append(\", \".join(example_ids))\n                report_lines.append(\"\\n\")\n            \n            report_lines.append(\"\\n\")\n    \n    # Add poem-specific analysis\n    report_lines.append(\"## Changes by Poem\\n\")\n    \n    by_poem = defaultdict(list)\n    for change in changes:\n        by_poem[change['poem']].append(change)\n    \n    for poem, items in sorted(by_poem.items()):\n        report_lines.append(f\"### {poem}\\n\")\n        report_lines.append(f\"Total changed: {len(items)}\\n\")\n        \n        # Summarize changes for this poem\n        type_counts = defaultdict(int)\n        for item in items:\n            type_counts[item['new_syntagma']] += 1\n        \n        report_lines.append(\"Distribution of new syntagma types:\\n\")\n        for new_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n            percentage = (count / len(items)) * 100\n            report_lines.append(f\"- {new_type}: {count} ({percentage:.1f}%)\\n\")\n        \n        report_lines.append(\"\\n\")\n    \n    # Write report to file\n    report_path = '/Users/gaia/resurrecting atlantis/HONEYBADGER/SYNTAGMA_CORRECTION_REPORT.md'\n    with open(report_path, 'w') as f:\n        f.write('\\n'.join(report_lines))\n    \n    print(f\"Change report written to {report_path}\")\n\ndef main():\n    print(\"Starting syntagma type correction...\")\n    \n    # Load timeline\n    timeline = load_timeline()\n    print(f\"Loaded timeline with {len(timeline)} entries\")\n    \n    # Identify problematic entries\n    problematic = [entry for entry in timeline if is_problematic_entry(entry)]\n    print(f\"Found {len(problematic)} entries with matching imageType and syntagmaType\")\n    \n    # Fix syntagma types\n    changes = fix_syntagma_types(timeline)\n    print(f\"Applied corrections to {len(changes)} entries\")\n    \n    # Generate report\n    generate_changes_report(changes)\n    \n    # Save updated timeline\n    save_timeline(timeline)\n    \n    print(\"Process completed successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/COMPLETE-TIMELINE.json",
      "UPDATED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/COMPLETE-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/{filename}",
      ")\n        for new_type, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n            percentage = (count / len(items)) * 100\n            report_lines.append(f",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/SYNTAGMA_CORRECTION_REPORT.md"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Syntagma Type Correction Script\n\nThis script fixes entries in the COMPLETE-TIMELINE.json file where imageType and syntagmaType\nare identical, applying a rule-based approach derived from film theory principles.\n\nIt implements a multi-level framework that combines:\n1. Direct theoretical mappings (Metz to Deleuze correspondence)\n2. Contextual sequence analysis\n3. Content-based heuristics\n4. Structural position analysis"
  },
  {
    "path": "HONEYBADGER/sequence_fix.py",
    "size": 6766,
    "lines": 198,
    "source": "#!/usr/bin/env python3\n\"\"\"\nSequence-Based Syntagma Correction\n\nThis script specifically addresses the overuse of Autonomous Syntagma\nby identifying sequences of related shots and converting them to appropriate\nsyntagma types based on their collective narrative function.\n\"\"\"\n\nimport json\nimport os\nfrom collections import defaultdict\n\ndef load_timeline(filepath):\n    \"\"\"Load the timeline data\"\"\"\n    with open(filepath, 'r') as f:\n        return json.load(f)\n\ndef organize_by_poem(timeline):\n    \"\"\"Group and sort entries by poem\"\"\"\n    poem_entries = defaultdict(list)\n    \n    # Group by poem\n    for entry in timeline:\n        poem_entries[entry['poem']].append(entry)\n    \n    # Sort entries within each poem\n    for poem in poem_entries:\n        # Sort by ID which should reflect sequence\n        poem_entries[poem].sort(key=lambda e: e['id'])\n    \n    return poem_entries\n\ndef identify_autonomous_sequences(entries, min_seq_length=3):\n    \"\"\"Find sequences of Autonomous Syntagma entries\"\"\"\n    sequences = []\n    current_seq = []\n    \n    for i, entry in enumerate(entries):\n        if entry['syntagmaType'] == 'Autonomous Syntagma':\n            current_seq.append((i, entry))\n        else:\n            # If we have a sequence of sufficient length, record it\n            if len(current_seq) >= min_seq_length:\n                sequences.append(current_seq)\n            current_seq = []\n    \n    # Check if the last sequence meets the minimum length\n    if len(current_seq) >= min_seq_length:\n        sequences.append(current_seq)\n    \n    return sequences\n\ndef analyze_sequence_type(sequence, entries):\n    \"\"\"Determine the appropriate syntagma type for a sequence\"\"\"\n    # Extract the sequence entries\n    seq_entries = [entry for _, entry in sequence]\n    \n    # Get indices for context\n    start_idx = sequence[0][0]\n    end_idx = sequence[-1][0]\n    \n    # Check imageType patterns\n    image_types = [e['imageType'] for e in seq_entries]\n    \n    # Check if majority are Affection-Images\n    affection_count = image_types.count('Affection-Image')\n    if affection_count > len(seq_entries) / 2:\n        return \"Descriptive Syntagma\"\n    \n    # Check if majority are Action-Images\n    action_count = image_types.count('Action-Image')\n    if action_count > len(seq_entries) / 2:\n        return \"Chronological Syntagma\"\n    \n    # Check if there's a mix of image types (suggests Thematic Montage)\n    unique_types = set(image_types)\n    if len(unique_types) >= 3:\n        return \"Thematic Montage\"\n    \n    # Check position in narrative\n    if start_idx < 5:  # Near beginning of poem\n        return \"Descriptive Syntagma\"\n    \n    # Default to the original but with a warning\n    return \"REVIEW_NEEDED\"\n\ndef fix_autonomous_sequences(poem_entries):\n    \"\"\"Fix sequences of Autonomous Syntagma entries\"\"\"\n    changes = []\n    \n    for poem, entries in poem_entries.items():\n        # Find sequences\n        sequences = identify_autonomous_sequences(entries)\n        \n        # Process each sequence\n        for sequence in sequences:\n            # Determine appropriate type\n            new_type = analyze_sequence_type(sequence, entries)\n            \n            # Skip sequences that need review\n            if new_type == \"REVIEW_NEEDED\":\n                continue\n            \n            # Apply the change\n            for idx, entry in sequence:\n                old_type = entries[idx]['syntagmaType']\n                entries[idx]['syntagmaType'] = new_type\n                \n                # Record the change\n                changes.append({\n                    'id': entry['id'],\n                    'poem': poem,\n                    'old_type': old_type,\n                    'new_type': new_type,\n                    'imageType': entry['imageType'],\n                    'in_sequence_with': [e[1]['id'] for e in sequence]\n                })\n    \n    return changes\n\ndef generate_report(changes):\n    \"\"\"Generate a report of the changes made\"\"\"\n    report_lines = [\"# Sequence-Based Syntagma Corrections\\n\"]\n    \n    # Add overview\n    report_lines.append(f\"## Overview\\n\")\n    report_lines.append(f\"Total entries corrected: {len(changes)}\\n\")\n    \n    # Group by poem\n    by_poem = defaultdict(list)\n    for change in changes:\n        by_poem[change['poem']].append(change)\n    \n    report_lines.append(\"## Changes by Poem\\n\")\n    \n    for poem, poem_changes in sorted(by_poem.items()):\n        report_lines.append(f\"### {poem}\\n\")\n        report_lines.append(f\"Entries corrected: {len(poem_changes)}\\n\")\n        \n        # Group by transition type\n        by_transition = defaultdict(list)\n        for change in poem_changes:\n            key = f\"{change['old_type']} \u2192 {change['new_type']}\"\n            by_transition[key].append(change)\n        \n        for transition, trans_changes in sorted(by_transition.items()):\n            report_lines.append(f\"#### {transition}: {len(trans_changes)} entries\\n\")\n            \n            # Show examples of sequences\n            sequences = {}\n            for change in trans_changes:\n                seq_key = ','.join(change['in_sequence_with'])\n                if seq_key not in sequences:\n                    sequences[seq_key] = change['in_sequence_with']\n            \n            report_lines.append(\"Sequences identified:\\n\")\n            for seq in list(sequences.values())[:3]:  # Show up to 3 sequence examples\n                report_lines.append(f\"- {', '.join(seq)}\\n\")\n            \n            report_lines.append(\"\\n\")\n    \n    # Write the report\n    with open('/Users/gaia/resurrecting atlantis/HONEYBADGER/SEQUENCE_CORRECTIONS.md', 'w') as f:\n        f.write('\\n'.join(report_lines))\n\ndef main():\n    print(\"Starting sequence-based syntagma correction...\")\n    \n    # Load the timeline with our previous corrections\n    timeline = load_timeline('/Users/gaia/resurrecting atlantis/HONEYBADGER/UPDATED-TIMELINE.json')\n    print(f\"Loaded timeline with {len(timeline)} entries\")\n    \n    # Organize by poem\n    poem_entries = organize_by_poem(timeline)\n    print(f\"Organized entries into {len(poem_entries)} poems\")\n    \n    # Fix autonomous sequences\n    changes = fix_autonomous_sequences(poem_entries)\n    print(f\"Applied corrections to {len(changes)} entries in sequences\")\n    \n    # Generate a report\n    generate_report(changes)\n    print(\"Generated correction report\")\n    \n    # Rebuild the flat timeline\n    corrected_timeline = []\n    for entries in poem_entries.values():\n        corrected_timeline.extend(entries)\n    \n    # Save the corrected timeline\n    with open('/Users/gaia/resurrecting atlantis/HONEYBADGER/SEQUENCE-FIXED-TIMELINE.json', 'w') as f:\n        json.dump(corrected_timeline, f, indent=2)\n    print(\"Saved corrected timeline\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/UPDATED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/SEQUENCE-FIXED-TIMELINE.json",
      ")\n    if affection_count > len(seq_entries) / 2:\n        return ",
      ")\n    if action_count > len(seq_entries) / 2:\n        return ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/SEQUENCE_CORRECTIONS.md",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/UPDATED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/SEQUENCE-FIXED-TIMELINE.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Sequence-Based Syntagma Correction\n\nThis script specifically addresses the overuse of Autonomous Syntagma\nby identifying sequences of related shots and converting them to appropriate\nsyntagma types based on their collective narrative function."
  },
  {
    "path": "HONEYBADGER/normalize_syntagma_format.py",
    "size": 3401,
    "lines": 98,
    "source": "#!/usr/bin/env python3\n\"\"\"\nNormalize Syntagma Type Formatting\n\nThis script standardizes the formatting of syntagma types in the timeline data,\nensuring consistent naming conventions and abbreviation usage.\n\"\"\"\n\nimport json\nimport os\n\n# Define canonical syntagma types with their abbreviations\nCANONICAL_SYNTAGMA_TYPES = {\n    \"Descriptive Syntagma\": \"DS\",\n    \"Chronological Syntagma\": \"CS\",\n    \"Autonomous Syntagma\": \"AS\",\n    \"Flashback Syntagma\": \"FS\",\n    \"Thematic Montage\": \"TM\", \n    \"Crystal Syntagma\": \"XS\"\n}\n\ndef normalize_syntagma(syntagma_type):\n    \"\"\"Normalize a syntagma type to the canonical format with abbreviation\"\"\"\n    # Handle the edge case of \"Affection-Image\" that wasn't fixed\n    if syntagma_type == \"Affection-Image\":\n        return \"Autonomous Syntagma\"  # Convert remaining Affection-Image to Autonomous\n    \n    # Extract base syntagma type (remove any existing abbreviations)\n    base_type = syntagma_type\n    if \"(\" in syntagma_type and \")\" in syntagma_type:\n        base_type = syntagma_type.split(\"(\")[0].strip()\n    \n    # Find matching canonical type\n    for canonical, abbrev in CANONICAL_SYNTAGMA_TYPES.items():\n        if canonical.lower() == base_type.lower() or canonical == base_type:\n            return f\"{canonical} ({abbrev})\"\n    \n    # If no match found, return the original with a warning\n    print(f\"WARNING: No canonical match for '{syntagma_type}', keeping as-is\")\n    return syntagma_type\n\ndef main():\n    print(\"Starting syntagma type normalization...\")\n    \n    # Load sequence-fixed timeline\n    input_path = '/Users/gaia/resurrecting atlantis/HONEYBADGER/SEQUENCE-FIXED-TIMELINE.json'\n    with open(input_path, 'r') as f:\n        timeline = json.load(f)\n    \n    print(f\"Loaded timeline with {len(timeline)} entries\")\n    \n    # Track unique types before normalization\n    before_types = set(entry['syntagmaType'] for entry in timeline)\n    print(f\"Found {len(before_types)} different syntagma type formats before normalization:\")\n    for t in sorted(before_types):\n        print(f\"  - {t}\")\n    \n    # Normalize all syntagma types\n    changes = 0\n    for entry in timeline:\n        old_type = entry['syntagmaType']\n        new_type = normalize_syntagma(old_type)\n        \n        if old_type != new_type:\n            entry['syntagmaType'] = new_type\n            changes += 1\n    \n    print(f\"Normalized {changes} entries\")\n    \n    # Track unique types after normalization\n    after_types = set(entry['syntagmaType'] for entry in timeline)\n    print(f\"Reduced to {len(after_types)} canonical syntagma types:\")\n    for t in sorted(after_types):\n        print(f\"  - {t}\")\n    \n    # Generate type counts\n    type_counts = {}\n    for entry in timeline:\n        syntagma = entry['syntagmaType']\n        if syntagma not in type_counts:\n            type_counts[syntagma] = 0\n        type_counts[syntagma] += 1\n    \n    print(\"\\nFinal syntagma type distribution:\")\n    for t, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n        percentage = (count / len(timeline)) * 100\n        print(f\"  - {t}: {count} ({percentage:.1f}%)\")\n    \n    # Save normalized timeline\n    output_path = '/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json'\n    with open(output_path, 'w') as f:\n        json.dump(timeline, f, indent=2)\n    \n    print(f\"\\nSaved normalized timeline to {output_path}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/SEQUENCE-FIXED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/SEQUENCE-FIXED-TIMELINE.json",
      ")\n    for t, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n        percentage = (count / len(timeline)) * 100\n        print(f",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Normalize Syntagma Type Formatting\n\nThis script standardizes the formatting of syntagma types in the timeline data,\nensuring consistent naming conventions and abbreviation usage."
  },
  {
    "path": "HONEYBADGER/create_simple_timeline.py",
    "size": 8499,
    "lines": 245,
    "source": "#!/usr/bin/env python3\n\"\"\"\nSimple Timeline Data Generator\n\nCreates a SIMPLE-TOTAL-TIMELINE.json file with consolidated data where:\n1. All metadata from the original TimelineAssemblage.json is preserved\n2. Prompt data is formatted as a single string in exactly the format from Codex.md:\n   \"CATEGORY \u00b7 FUNCTION \u00b7 DESCRIPTION \u00b7 PROMPT_TEXT\"\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"HONEYBADGER\")\n\n# Source files\nTIMELINE_ASSEMBLAGE_JSON = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json\")\nPROMPTS_MD = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_prompts.md\")\nCODEX_MD = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Codex.md\")\n\n# Output file\nSIMPLE_TIMELINE_JSON = os.path.join(OUTPUT_DIR, \"SIMPLE-TOTAL-TIMELINE.json\")\n\ndef log(message):\n    \"\"\"Print log message with timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"{timestamp}: {message}\")\n\ndef extract_prompts_from_markdown(md_file):\n    \"\"\"\n    Extract prompts from markdown file.\n    Returns a dictionary mapping shot IDs to complete prompt strings.\n    Format: \"CATEGORY \u00b7 FUNCTION \u00b7 DESCRIPTION \u00b7 PROMPT_TEXT\"\n    \"\"\"\n    prompts = {}\n    \n    with open(md_file, 'r') as f:\n        content = f.read()\n    \n    # Pattern to match shot entries with the full prompt as a single capture group\n    # Format: \"SH001 [00:01:52] \u00b7 DS \u00b7 Mood Environment Stabilizer \u00b7 Description \u00b7 Full prompt text\"\n    pattern = r'([A-Z]{2}\\d{3})\\s*(?:\\[([\\d:]+)\\])?\\s*\u00b7\\s*([A-Z]+)\\s*\u00b7\\s*([^\u00b7]+)\u00b7\\s*([^\u00b7]+)\u00b7\\s*(.+?)(?=\\n[A-Z]{2}\\d{3}|\\n\\n|\\n\\s*---|$)'\n    \n    matches = re.finditer(pattern, content, re.DOTALL)\n    \n    for match in matches:\n        shot_id = match.group(1)\n        category = match.group(3).strip()\n        function = match.group(4).strip()\n        description = match.group(5).strip() if len(match.groups()) > 4 else \"\"\n        prompt_text = match.group(6).strip() if len(match.groups()) > 5 else \"\"\n        \n        # Compress any newlines within components to create a single line\n        description = description.replace('\\n', ' ').strip()\n        prompt_text = prompt_text.replace('\\n', ' ').strip()\n        \n        # Format exactly like in the Codex.md\n        full_prompt = f\"{category} \u00b7 {function} \u00b7 {description} \u00b7 {prompt_text}\"\n        \n        prompts[shot_id] = full_prompt\n    \n    return prompts\n\ndef extract_prompts_from_codex(md_file):\n    \"\"\"\n    Extract prompts directly from Codex.md which already has the exact format we want.\n    Returns a dictionary mapping shot IDs to complete prompt strings as they appear in Codex.\n    \"\"\"\n    prompts = {}\n    \n    with open(md_file, 'r') as f:\n        content = f.read()\n    \n    # Pattern to match codex entries\n    # Format: \"### ID [TIMESTAMP]\" followed by \"**Prompt:** FULL_PROMPT\"\n    pattern = r'### ([A-Z]{2}\\d{3}).*?\\*\\*Prompt:\\*\\* (.*?)(?=\\n---|\\n###|\\Z)'\n    \n    matches = re.finditer(pattern, content, re.DOTALL)\n    \n    for match in matches:\n        shot_id = match.group(1)\n        prompt = match.group(2).strip()\n        \n        # Clean up the prompt - remove any newlines\n        prompt = prompt.replace('\\n', ' ').strip()\n        \n        prompts[shot_id] = prompt\n    \n    return prompts\n\ndef create_simple_timeline():\n    \"\"\"\n    Create a simplified consolidated timeline JSON.\n    \"\"\"\n    log(f\"Starting simple timeline generation...\")\n    \n    # Load timeline assemblage JSON\n    try:\n        with open(TIMELINE_ASSEMBLAGE_JSON, 'r') as f:\n            timeline_data = json.load(f)\n        \n        log(f\"Loaded timeline assemblage JSON with {len(timeline_data)} entries\")\n    except Exception as e:\n        log(f\"Error loading timeline assemblage JSON: {e}\")\n        return False\n    \n    # Try to extract prompts from Codex.md first (preferred source)\n    try:\n        prompts = extract_prompts_from_codex(CODEX_MD)\n        log(f\"Extracted {len(prompts)} prompts from Codex.md\")\n    except Exception as e:\n        log(f\"Error extracting prompts from Codex.md: {e}\")\n        log(f\"Falling back to prompts.md...\")\n        \n        # Fallback to prompts.md\n        try:\n            prompts = extract_prompts_from_markdown(PROMPTS_MD)\n            log(f\"Extracted {len(prompts)} prompts from prompts.md\")\n        except Exception as e:\n            log(f\"Error extracting prompts from prompts.md: {e}\")\n            return False\n    \n    # Track stats\n    total_entries = len(timeline_data)\n    matched_prompts = 0\n    missing_prompts = 0\n    \n    # Create simple timeline data\n    simple_timeline = []\n    \n    for entry in timeline_data:\n        shot_id = entry.get('id', '')\n        \n        # Create a clean entry with all original fields\n        simple_entry = entry.copy()\n        \n        if shot_id in prompts:\n            # Add prompt as a single field\n            simple_entry['full_prompt'] = prompts[shot_id]\n            matched_prompts += 1\n        else:\n            # Add empty prompt for missing entries\n            simple_entry['full_prompt'] = \"\"\n            missing_prompts += 1\n            log(f\"Warning: No prompt found for shot {shot_id}\")\n        \n        simple_timeline.append(simple_entry)\n    \n    # Save simple consolidated JSON\n    try:\n        with open(SIMPLE_TIMELINE_JSON, 'w') as f:\n            json.dump(simple_timeline, f, indent=2)\n        \n        log(f\"Saved SIMPLE-TOTAL-TIMELINE.json with {total_entries} entries\")\n        log(f\"Matched prompts: {matched_prompts}, Missing prompts: {missing_prompts}\")\n        \n        return True\n    except Exception as e:\n        log(f\"Error saving simple timeline JSON: {e}\")\n        return False\n\ndef validate_simple_timeline():\n    \"\"\"\n    Validate the simple timeline JSON.\n    \"\"\"\n    log(f\"Validating simple timeline...\")\n    \n    try:\n        with open(SIMPLE_TIMELINE_JSON, 'r') as f:\n            data = json.load(f)\n        \n        total = len(data)\n        complete = 0\n        incomplete = 0\n        \n        # Check for completeness of essential fields\n        for entry in data:\n            # Check if critical fields are present\n            has_id = 'id' in entry and entry['id']\n            has_image = 'image_path' in entry and entry['image_path']\n            has_prompt = 'full_prompt' in entry and entry['full_prompt']\n            \n            if has_id and has_image and has_prompt:\n                complete += 1\n            else:\n                incomplete += 1\n        \n        log(f\"Total entries: {total}\")\n        log(f\"Complete entries (id, image_path, full_prompt): {complete}\")\n        log(f\"Incomplete entries: {incomplete}\")\n        \n        # Show sample prompts for verification\n        if complete > 0:\n            sample_entries = []\n            sections = set()\n            \n            # Get one sample from each section\n            for entry in data:\n                shot_id = entry.get('id', '')\n                if shot_id and len(shot_id) >= 2:\n                    section = shot_id[:2]\n                    if section not in sections and 'full_prompt' in entry and entry['full_prompt']:\n                        sections.add(section)\n                        sample_entries.append((shot_id, entry['full_prompt']))\n                        if len(sample_entries) >= 5:  # Limit to 5 samples\n                            break\n            \n            log(f\"Sample prompts for verification:\")\n            for shot_id, prompt in sample_entries:\n                log(f\"- {shot_id}: {prompt[:80]}...\")\n        \n        # Count entries by section\n        section_counts = {}\n        for entry in data:\n            shot_id = entry.get('id', '')\n            if shot_id and len(shot_id) >= 2:\n                section = shot_id[:2]\n                section_counts[section] = section_counts.get(section, 0) + 1\n        \n        log(\"Entry counts by section:\")\n        for section, count in sorted(section_counts.items()):\n            log(f\"- {section}: {count} entries\")\n        \n        return True\n    except Exception as e:\n        log(f\"Error validating simple timeline: {e}\")\n        return False\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    log(\"=== Simple Timeline Data Generator ===\")\n    \n    if create_simple_timeline():\n        validate_simple_timeline()\n    \n    log(\"=== Simple Timeline Generation Complete ===\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json",
      "SIMPLE-TOTAL-TIMELINE.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "json",
      "datetime"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Simple Timeline Data Generator\n\nCreates a SIMPLE-TOTAL-TIMELINE.json file with consolidated data where:\n1. All metadata from the original TimelineAssemblage.json is preserved\n2. Prompt data is formatted as a single string in exactly the format from Codex.md:\n   \"CATEGORY \u00b7 FUNCTION \u00b7 DESCRIPTION \u00b7 PROMPT_TEXT\""
  },
  {
    "path": "HONEYBADGER/analyze_timeline_types.py",
    "size": 10241,
    "lines": 229,
    "source": "#!/usr/bin/env python3\n\"\"\"\nTimeline Type Analysis\n\nThis script analyzes the COMPLETE-TIMELINE.json file to:\n1. Create an index of all unique imageType, syntagmaType, and cineosisFunction values\n2. Generate statistics on the frequency of each value\n3. Analyze distribution of values across different poems\n\"\"\"\n\nimport json\nimport os\nfrom datetime import datetime\nfrom collections import Counter, defaultdict\n\n# Base directory\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\n# Input and output paths\nINPUT_JSON_PATH = os.path.join(BASE_DIR, \"HONEYBADGER\", \"COMPLETE-TIMELINE.json\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"HONEYBADGER\", \"TIMELINE_ANALYSIS\")\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef log(message):\n    \"\"\"Print log message with timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"{timestamp}: {message}\")\n\ndef analyze_timeline():\n    \"\"\"\n    Analyze timeline entries to generate index and statistics.\n    \"\"\"\n    log(\"=== Timeline Type Analysis ===\")\n    \n    # Load the JSON\n    try:\n        with open(INPUT_JSON_PATH, \"r\") as f:\n            timeline_data = json.load(f)\n    except Exception as e:\n        log(f\"Error loading JSON: {e}\")\n        return\n        \n    log(f\"Loaded timeline with {len(timeline_data)} entries\")\n    \n    # Extract all unique values\n    image_types = []\n    syntagma_types = []\n    cineosis_functions = []\n    poems = []\n    \n    # Create data structures for analysis\n    entries_by_poem = defaultdict(list)\n    \n    for entry in timeline_data:\n        image_type = entry.get(\"imageType\", \"\")\n        syntagma_type = entry.get(\"syntagmaType\", \"\")\n        cineosis_function = entry.get(\"cineosisFunction\", \"\")\n        poem = entry.get(\"poem\", \"\")\n        \n        if image_type:\n            image_types.append(image_type)\n        if syntagma_type:\n            syntagma_types.append(syntagma_type)\n        if cineosis_function:\n            cineosis_functions.append(cineosis_function)\n        if poem:\n            poems.append(poem)\n            entries_by_poem[poem].append(entry)\n            \n    # Count occurrences\n    image_type_counts = Counter(image_types)\n    syntagma_type_counts = Counter(syntagma_types)\n    cineosis_function_counts = Counter(cineosis_functions)\n    poem_counts = Counter(poems)\n    \n    # Calculate total entries for percentage calculations\n    total_entries = len(timeline_data)\n    \n    # Create index of unique values\n    unique_image_types = sorted(image_type_counts.keys())\n    unique_syntagma_types = sorted(syntagma_type_counts.keys())\n    unique_cineosis_functions = sorted(cineosis_function_counts.keys())\n    unique_poems = sorted(poem_counts.keys())\n    \n    log(f\"Found {len(unique_image_types)} unique imageType values\")\n    log(f\"Found {len(unique_syntagma_types)} unique syntagmaType values\")\n    log(f\"Found {len(unique_cineosis_functions)} unique cineosisFunction values\")\n    log(f\"Found {len(unique_poems)} unique poem values\")\n    \n    # Generate markdown report\n    report_path = os.path.join(OUTPUT_DIR, \"TIMELINE_TYPES_REPORT.md\")\n    \n    with open(report_path, \"w\") as report:\n        report.write(\"# Timeline Type Analysis Report\\n\\n\")\n        report.write(f\"Analysis of {total_entries} timeline entries.\\n\\n\")\n        \n        # Summary section\n        report.write(\"## Summary\\n\\n\")\n        report.write(f\"- Total entries: {total_entries}\\n\")\n        report.write(f\"- Unique poems: {len(unique_poems)}\\n\")\n        report.write(f\"- Unique imageType values: {len(unique_image_types)}\\n\")\n        report.write(f\"- Unique syntagmaType values: {len(unique_syntagma_types)}\\n\")\n        report.write(f\"- Unique cineosisFunction values: {len(unique_cineosis_functions)}\\n\\n\")\n        \n        # ImageType index and statistics\n        report.write(\"## ImageType Values\\n\\n\")\n        report.write(\"| ImageType | Count | Percentage |\\n\")\n        report.write(\"|-----------|-------|------------|\\n\")\n        \n        for image_type, count in sorted(image_type_counts.items(), key=lambda x: x[1], reverse=True):\n            percentage = (count / total_entries) * 100\n            report.write(f\"| {image_type} | {count} | {percentage:.2f}% |\\n\")\n            \n        # SyntagmaType index and statistics\n        report.write(\"\\n## SyntagmaType Values\\n\\n\")\n        report.write(\"| SyntagmaType | Count | Percentage |\\n\")\n        report.write(\"|-------------|-------|------------|\\n\")\n        \n        for syntagma_type, count in sorted(syntagma_type_counts.items(), key=lambda x: x[1], reverse=True):\n            percentage = (count / total_entries) * 100\n            report.write(f\"| {syntagma_type} | {count} | {percentage:.2f}% |\\n\")\n            \n        # CineosisFunction index and statistics\n        report.write(\"\\n## CineosisFunction Values\\n\\n\")\n        report.write(\"| CineosisFunction | Count | Percentage |\\n\")\n        report.write(\"|------------------|-------|------------|\\n\")\n        \n        for function, count in sorted(cineosis_function_counts.items(), key=lambda x: x[1], reverse=True):\n            percentage = (count / total_entries) * 100\n            report.write(f\"| {function} | {count} | {percentage:.2f}% |\\n\")\n            \n        # Poem statistics\n        report.write(\"\\n## Poem Distribution\\n\\n\")\n        report.write(\"| Poem | Entry Count | Percentage |\\n\")\n        report.write(\"|------|-------------|------------|\\n\")\n        \n        for poem, count in sorted(poem_counts.items(), key=lambda x: x[1], reverse=True):\n            percentage = (count / total_entries) * 100\n            report.write(f\"| {poem} | {count} | {percentage:.2f}% |\\n\")\n            \n        # Distribution by poem\n        report.write(\"\\n## Type Distribution by Poem\\n\\n\")\n        \n        for poem in unique_poems:\n            poem_entries = entries_by_poem[poem]\n            report.write(f\"### {poem}\\n\\n\")\n            report.write(f\"Total entries: {len(poem_entries)}\\n\\n\")\n            \n            # ImageType distribution in this poem\n            poem_image_types = [entry.get(\"imageType\", \"\") for entry in poem_entries if \"imageType\" in entry]\n            poem_image_type_counts = Counter(poem_image_types)\n            \n            report.write(\"#### ImageType Distribution\\n\\n\")\n            report.write(\"| ImageType | Count | Percentage |\\n\")\n            report.write(\"|-----------|-------|------------|\\n\")\n            \n            for image_type, count in sorted(poem_image_type_counts.items(), key=lambda x: x[1], reverse=True):\n                percentage = (count / len(poem_entries)) * 100\n                report.write(f\"| {image_type} | {count} | {percentage:.2f}% |\\n\")\n                \n            # SyntagmaType distribution in this poem\n            poem_syntagma_types = [entry.get(\"syntagmaType\", \"\") for entry in poem_entries if \"syntagmaType\" in entry]\n            poem_syntagma_type_counts = Counter(poem_syntagma_types)\n            \n            report.write(\"\\n#### SyntagmaType Distribution\\n\\n\")\n            report.write(\"| SyntagmaType | Count | Percentage |\\n\")\n            report.write(\"|-------------|-------|------------|\\n\")\n            \n            for syntagma_type, count in sorted(poem_syntagma_type_counts.items(), key=lambda x: x[1], reverse=True):\n                percentage = (count / len(poem_entries)) * 100\n                report.write(f\"| {syntagma_type} | {count} | {percentage:.2f}% |\\n\")\n                \n            # CineosisFunction distribution in this poem\n            poem_cineosis_functions = [entry.get(\"cineosisFunction\", \"\") for entry in poem_entries if \"cineosisFunction\" in entry]\n            poem_cineosis_function_counts = Counter(poem_cineosis_functions)\n            \n            report.write(\"\\n#### CineosisFunction Distribution\\n\\n\")\n            report.write(\"| CineosisFunction | Count | Percentage |\\n\")\n            report.write(\"|------------------|-------|------------|\\n\")\n            \n            for function, count in sorted(poem_cineosis_function_counts.items(), key=lambda x: x[1], reverse=True):\n                percentage = (count / len(poem_entries)) * 100\n                report.write(f\"| {function} | {count} | {percentage:.2f}% |\\n\")\n                \n            report.write(\"\\n\")\n            \n    log(f\"Generated markdown report at {report_path}\")\n    \n    # Generate JSON index files\n    image_type_index = {t: {\"count\": image_type_counts[t], \"percentage\": (image_type_counts[t] / total_entries) * 100} for t in unique_image_types}\n    syntagma_type_index = {t: {\"count\": syntagma_type_counts[t], \"percentage\": (syntagma_type_counts[t] / total_entries) * 100} for t in unique_syntagma_types}\n    cineosis_function_index = {t: {\"count\": cineosis_function_counts[t], \"percentage\": (cineosis_function_counts[t] / total_entries) * 100} for t in unique_cineosis_functions}\n    \n    with open(os.path.join(OUTPUT_DIR, \"IMAGE_TYPES.json\"), \"w\") as f:\n        json.dump(image_type_index, f, indent=2)\n        \n    with open(os.path.join(OUTPUT_DIR, \"SYNTAGMA_TYPES.json\"), \"w\") as f:\n        json.dump(syntagma_type_index, f, indent=2)\n        \n    with open(os.path.join(OUTPUT_DIR, \"CINEOSIS_FUNCTIONS.json\"), \"w\") as f:\n        json.dump(cineosis_function_index, f, indent=2)\n        \n    log(\"Generated JSON index files\")\n    \n    # Create a mapping table for different types\n    type_mapping = defaultdict(list)\n    for entry in timeline_data:\n        image_type = entry.get(\"imageType\", \"\")\n        syntagma_type = entry.get(\"syntagmaType\", \"\")\n        if image_type and syntagma_type:\n            type_mapping[(image_type, syntagma_type)].append(entry.get(\"id\", \"\"))\n    \n    with open(os.path.join(OUTPUT_DIR, \"TYPE_MAPPING.md\"), \"w\") as f:\n        f.write(\"# ImageType to SyntagmaType Mapping\\n\\n\")\n        f.write(\"| ImageType | SyntagmaType | Count | Example IDs |\\n\")\n        f.write(\"|-----------|-------------|-------|-------------|\\n\")\n        \n        for (img_type, syn_type), ids in sorted(type_mapping.items()):\n            f.write(f\"| {img_type} | {syn_type} | {len(ids)} | {', '.join(ids[:5])}{'...' if len(ids) > 5 else ''} |\\n\")\n    \n    log(\"Generated type mapping report\")\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    analyze_timeline()\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "IMAGE_TYPES.json",
      "SYNTAGMA_TYPES.json",
      "CINEOSIS_FUNCTIONS.json",
      "/Users/gaia/resurrecting atlantis",
      ")\n        \n        for image_type, count in sorted(image_type_counts.items(), key=lambda x: x[1], reverse=True):\n            percentage = (count / total_entries) * 100\n            report.write(f",
      ")\n        \n        for syntagma_type, count in sorted(syntagma_type_counts.items(), key=lambda x: x[1], reverse=True):\n            percentage = (count / total_entries) * 100\n            report.write(f",
      ")\n        \n        for function, count in sorted(cineosis_function_counts.items(), key=lambda x: x[1], reverse=True):\n            percentage = (count / total_entries) * 100\n            report.write(f",
      ")\n        \n        for poem, count in sorted(poem_counts.items(), key=lambda x: x[1], reverse=True):\n            percentage = (count / total_entries) * 100\n            report.write(f",
      ")\n            \n            for image_type, count in sorted(poem_image_type_counts.items(), key=lambda x: x[1], reverse=True):\n                percentage = (count / len(poem_entries)) * 100\n                report.write(f",
      ")\n            \n            for syntagma_type, count in sorted(poem_syntagma_type_counts.items(), key=lambda x: x[1], reverse=True):\n                percentage = (count / len(poem_entries)) * 100\n                report.write(f",
      ")\n            \n            for function, count in sorted(poem_cineosis_function_counts.items(), key=lambda x: x[1], reverse=True):\n                percentage = (count / len(poem_entries)) * 100\n                report.write(f",
      ": (image_type_counts[t] / total_entries) * 100} for t in unique_image_types}\n    syntagma_type_index = {t: {",
      ": (syntagma_type_counts[t] / total_entries) * 100} for t in unique_syntagma_types}\n    cineosis_function_index = {t: {",
      ": (cineosis_function_counts[t] / total_entries) * 100} for t in unique_cineosis_functions}\n    \n    with open(os.path.join(OUTPUT_DIR, "
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "datetime",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Timeline Type Analysis\n\nThis script analyzes the COMPLETE-TIMELINE.json file to:\n1. Create an index of all unique imageType, syntagmaType, and cineosisFunction values\n2. Generate statistics on the frequency of each value\n3. Analyze distribution of values across different poems"
  },
  {
    "path": "HONEYBADGER/find_matching_types.py",
    "size": 3684,
    "lines": 106,
    "source": "#!/usr/bin/env python3\n\"\"\"\nFind Matching Types Script\n\nThis script identifies timeline entries where imageType and syntagmaType values are identical,\nthen saves those entries to a new file along with a summary.\n\"\"\"\n\nimport json\nimport os\nfrom datetime import datetime\nfrom collections import Counter\n\n# Base directory\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\n# Input and output paths\nINPUT_JSON_PATH = os.path.join(BASE_DIR, \"HONEYBADGER\", \"COMPLETE-TIMELINE.json\")\nOUTPUT_JSON_PATH = os.path.join(BASE_DIR, \"HONEYBADGER\", \"MATCHING_TYPES_ENTRIES.json\")\nSUMMARY_PATH = os.path.join(BASE_DIR, \"HONEYBADGER\", \"MATCHING_TYPES_SUMMARY.md\")\n\ndef log(message):\n    \"\"\"Print log message with timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"{timestamp}: {message}\")\n\ndef find_matching_types():\n    \"\"\"\n    Find entries where imageType and syntagmaType are identical.\n    \"\"\"\n    log(\"=== Finding Entries with Matching Types ===\")\n    \n    # Load the JSON\n    try:\n        with open(INPUT_JSON_PATH, \"r\") as f:\n            timeline_data = json.load(f)\n    except Exception as e:\n        log(f\"Error loading JSON: {e}\")\n        return\n        \n    log(f\"Loaded timeline with {len(timeline_data)} entries\")\n    \n    # Find matching entries\n    matching_entries = []\n    type_pairs = []  # To track what types were matched\n    \n    for entry in timeline_data:\n        image_type = entry.get(\"imageType\", \"\")\n        syntagma_type = entry.get(\"syntagmaType\", \"\")\n        \n        # Check if both fields exist and are identical\n        if image_type and syntagma_type and image_type == syntagma_type:\n            matching_entries.append(entry)\n            type_pairs.append((image_type, syntagma_type))\n            \n    log(f\"Found {len(matching_entries)} entries with matching imageType and syntagmaType\")\n    \n    # Save matching entries to a file\n    try:\n        with open(OUTPUT_JSON_PATH, \"w\") as f:\n            json.dump(matching_entries, f, indent=2)\n        log(f\"Saved matching entries to {OUTPUT_JSON_PATH}\")\n    except Exception as e:\n        log(f\"Error saving matching entries: {e}\")\n        \n    # Generate summary information\n    type_counts = Counter(type_pairs)\n    most_common_types = type_counts.most_common()\n    \n    # Create a summary markdown file\n    try:\n        with open(SUMMARY_PATH, \"w\") as f:\n            f.write(\"# Timeline Entries with Matching Types\\n\\n\")\n            f.write(f\"**Total matching entries:** {len(matching_entries)}\\n\\n\")\n            \n            f.write(\"## Type Breakdown\\n\\n\")\n            f.write(\"| Type | Count |\\n\")\n            f.write(\"|------|-------|\\n\")\n            for (image_type, _), count in most_common_types:\n                f.write(f\"| {image_type} | {count} |\\n\")\n                \n            f.write(\"\\n## Entry IDs by Type\\n\\n\")\n            # Group entries by type\n            entries_by_type = {}\n            for entry in matching_entries:\n                type_name = entry.get(\"imageType\", \"Unknown\")\n                if type_name not in entries_by_type:\n                    entries_by_type[type_name] = []\n                entries_by_type[type_name].append(entry.get(\"id\", \"Unknown\"))\n                \n            # Write each type's entries\n            for type_name, entries in entries_by_type.items():\n                f.write(f\"### {type_name}\\n\\n\")\n                f.write(\", \".join(sorted(entries)))\n                f.write(\"\\n\\n\")\n                \n        log(f\"Saved summary to {SUMMARY_PATH}\")\n    except Exception as e:\n        log(f\"Error creating summary: {e}\")\n        \ndef main():\n    \"\"\"Main function.\"\"\"\n    find_matching_types()\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "MATCHING_TYPES_ENTRIES.json",
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "datetime",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Find Matching Types Script\n\nThis script identifies timeline entries where imageType and syntagmaType values are identical,\nthen saves those entries to a new file along with a summary."
  },
  {
    "path": "HONEYBADGER/generate_symbolic_genome_updated.py",
    "size": 10790,
    "lines": 261,
    "source": "#!/usr/bin/env python3\n\"\"\"\nGenerate Symbolic Genome Visualizations for Poetry Timeline\n\nThis script creates a visual representation of each poem's \"symbolic genome\"\nusing the defined glyphs for syntagma types, image types, and cineosis functions.\n\"\"\"\n\nimport json\nimport os\nfrom collections import defaultdict\n\n# Define glyph mappings with updated symbols\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2756\",  # Updated\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",  # Updated\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",  # Updated\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",  # Updated\n    \"Event Pause Invocation\": \"\u275a\",  # Updated\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",  # Updated\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load the normalized timeline data\"\"\"\n    with open(filepath, 'r') as f:\n        return json.load(f)\n\ndef organize_by_poem(timeline):\n    \"\"\"Group and sort entries by poem\"\"\"\n    poem_entries = defaultdict(list)\n    \n    # Group by poem\n    for entry in timeline:\n        poem_entries[entry['poem']].append(entry)\n    \n    # Sort entries within each poem\n    for poem in poem_entries:\n        # Sort by ID which should reflect sequence\n        poem_entries[poem].sort(key=lambda e: e['id'])\n    \n    return poem_entries\n\ndef get_glyph_or_default(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for key or return default if not found\"\"\"\n    return mapping.get(key, default)\n\ndef generate_poem_genome(entries, width=80):\n    \"\"\"Generate symbolic genome visualization for a poem and raw track data.\"\"\"\n    syntagma_track_glyphs = []\n    image_track_glyphs = []\n    cineosis_track_glyphs = []\n    id_track_parts = []\n    \n    for entry in entries:\n        syntagma_glyph = get_glyph_or_default(SYNTAGMA_GLYPHS, entry.get('syntagmaType', ''))\n        image_glyph = get_glyph_or_default(IMAGE_GLYPHS, entry.get('imageType', ''))\n        cineosis_glyph = get_glyph_or_default(CINEOSIS_GLYPHS, entry.get('cineosisFunction', ''))\n        \n        syntagma_track_glyphs.append(syntagma_glyph)\n        image_track_glyphs.append(image_glyph)\n        cineosis_track_glyphs.append(cineosis_glyph)\n        \n        id_part = entry.get('id', '??')[-3:] if len(entry.get('id', '??')) > 3 else entry.get('id', '??')\n        id_track_parts.append(id_part)\n    \n    s_line_str = \" \".join(syntagma_track_glyphs)\n    i_line_str = \" \".join(image_track_glyphs)\n    c_line_str = \" \".join(cineosis_track_glyphs)\n\n    # Create the markdown visualization\n    markdown_lines = []\n    if not entries: # Handle empty entries case for markdown\n        markdown_lines.append(\"\u250c\u2510\")\n        markdown_lines.append(\"\u2502\u2502 S\")\n        markdown_lines.append(\"\u2502\u2502 I\")\n        markdown_lines.append(\"\u2502\u2502 C\")\n        markdown_lines.append(\"\u2514\u2518\")\n        markdown_lines.append(\"No timeline entries for this poem.\")\n    else:\n        markdown_lines.append(\"\u250c\" + \"\u2500\" * (len(entries) * 2 - 1) + \"\u2510\")\n        markdown_lines.append(f\"\u2502{s_line_str}\u2502 S\")\n        markdown_lines.append(f\"\u2502{i_line_str}\u2502 I\")\n        markdown_lines.append(f\"\u2502{c_line_str}\u2502 C\")\n        markdown_lines.append(\"\u2514\" + \"\u2500\" * (len(entries) * 2 - 1) + \"\u2518\")\n        \n        id_spacing = []\n        for id_str in id_track_parts:\n            spaces = (3 - len(id_str)) // 2\n            id_spacing.append(\" \" * spaces + id_str + \" \" * (3 - len(id_str) - spaces))\n        markdown_lines.append(\" \" + \"\".join(id_spacing))\n    \n    return {\n        \"markdown\": \"\\n\".join(markdown_lines),\n        \"s_line\": s_line_str,\n        \"i_line\": i_line_str,\n        \"c_line\": c_line_str\n    }\n\ndef generate_poem_stats(entries):\n    \"\"\"Generate statistics about the distribution of types in a poem\"\"\"\n    total = len(entries)\n    if total == 0:\n        return \"No entries found for this poem.\"\n    \n    # Count frequencies\n    syntagma_counts = defaultdict(int)\n    image_counts = defaultdict(int)\n    cineosis_counts = defaultdict(int)\n    \n    for entry in entries:\n        syntagma_counts[entry.get('syntagmaType', 'Unknown')] += 1\n        image_counts[entry.get('imageType', 'Unknown')] += 1\n        cineosis_counts[entry.get('cineosisFunction', 'Unknown')] += 1\n    \n    # Generate report lines\n    lines = [\"### Distribution Statistics\", \"\"]\n    \n    # Syntagma distribution\n    lines.append(\"**Syntagma Types:**\")\n    for syntagma, count in sorted(syntagma_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(SYNTAGMA_GLYPHS, syntagma)\n        percentage = (count / total) * 100\n        lines.append(f\"- {glyph} {syntagma}: {count} ({percentage:.1f}%)\")\n    lines.append(\"\")\n    \n    # Image distribution\n    lines.append(\"**Image Types:**\")\n    for image, count in sorted(image_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(IMAGE_GLYPHS, image)\n        percentage = (count / total) * 100\n        lines.append(f\"- {glyph} {image}: {count} ({percentage:.1f}%)\")\n    lines.append(\"\")\n    \n    # Cineosis distribution\n    lines.append(\"**Cineosis Functions:**\")\n    for cineosis, count in sorted(cineosis_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(CINEOSIS_GLYPHS, cineosis)\n        percentage = (count / total) * 100\n        lines.append(f\"- {glyph} {cineosis}: {count} ({percentage:.1f}%)\")\n    \n    return \"\\n\".join(lines)\n\ndef generate_report(poem_entries):\n    \"\"\"Generate a comprehensive markdown report and a list of poem data for JSON.\"\"\"\n    report_lines = [\"# Symbolic Genome Visualization\", \n                    \"\",\n                    \"This document visualizes the symbolic genome of each poem using the following notation:\",\n                    \"\",\n                    \"## Notation Key\",\n                    \"\",\n                    \"### Syntagma Types (S)\",\n                    f\"- {SYNTAGMA_GLYPHS['Autonomous Syntagma (AS)']}  Autonomous Syntagma (AS)\",\n                    f\"- {SYNTAGMA_GLYPHS['Chronological Syntagma (CS)']}  Chronological Syntagma (CS)\",\n                    f\"- {SYNTAGMA_GLYPHS['Crystal Syntagma (XS)']}  Crystal Syntagma (XS)\",\n                    f\"- {SYNTAGMA_GLYPHS['Descriptive Syntagma (DS)']}  Descriptive Syntagma (DS)\",\n                    f\"- {SYNTAGMA_GLYPHS['Flashback Syntagma (FS)']}  Flashback Syntagma (FS)\",\n                    f\"- {SYNTAGMA_GLYPHS['Thematic Montage (TM)']}  Thematic Montage (TM)\",\n                    \"\",\n                    \"### Image Types (I)\",\n                    f\"- {IMAGE_GLYPHS['Action-Image']}  Action-Image\",\n                    f\"- {IMAGE_GLYPHS['Affection-Image']}  Affection-Image\",\n                    f\"- {IMAGE_GLYPHS['Crystal-Image']}  Crystal-Image\",\n                    f\"- {IMAGE_GLYPHS['Descriptive Image']}  Descriptive Image\",\n                    f\"- {IMAGE_GLYPHS['Opsign']}  Opsign\",\n                    f\"- {IMAGE_GLYPHS['Perception-Image']}  Perception-Image\",\n                    f\"- {IMAGE_GLYPHS['Recollection-Image']}  Recollection-Image\",\n                    f\"- {IMAGE_GLYPHS['Sonsign']}  Sonsign\",\n                    f\"- {IMAGE_GLYPHS['Thematic Montage']}  Thematic Montage\",\n                    \"\",\n                    \"### Cineosis Functions (C)\",\n                    f\"- {CINEOSIS_GLYPHS['Aural-Echo Extension']}  Aural-Echo Extension\",\n                    f\"- {CINEOSIS_GLYPHS['Causal Motion Trigger']}  Causal Motion Trigger\",\n                    f\"- {CINEOSIS_GLYPHS['Emotion Relay']}  Emotion Relay\",\n                    f\"- {CINEOSIS_GLYPHS['Event Pause Invocation']}  Event Pause Invocation\",\n                    f\"- {CINEOSIS_GLYPHS['Memory Storage Retrieval']}  Memory Storage Retrieval\",\n                    f\"- {CINEOSIS_GLYPHS['Mood Environment Stabilizer']}  Mood Environment Stabilizer\",\n                    f\"- {CINEOSIS_GLYPHS['Narrative Modifier']}  Narrative Modifier\",\n                    f\"- {CINEOSIS_GLYPHS['Subjective Frame Recalibration']}  Subjective Frame Recalibration\",\n                    f\"- {CINEOSIS_GLYPHS['Temporal Reflection Loop']}  Temporal Reflection Loop\",\n                    \"\", \n                    \"## Poem Genomes\",\n                    \"\"]\n    \n    poems_data_for_json = []\n\n    for poem_title, entries in sorted(poem_entries.items()):\n        report_lines.append(f\"## {poem_title}\")\n        report_lines.append(f\"*{len(entries)} timeline entries*\\n\")\n        \n        genome_data = generate_poem_genome(entries)\n        report_lines.append(genome_data[\"markdown\"])\n        report_lines.append(\"\\n\")\n        report_lines.append(generate_poem_stats(entries))\n        report_lines.append(\"\\n\")\n        \n        current_poem_json_data = {\n            \"title\": poem_title,\n            \"timeline_entries_count\": len(entries),\n            \"s_line\": genome_data[\"s_line\"],\n            \"i_line\": genome_data[\"i_line\"],\n            \"c_line\": genome_data[\"c_line\"]\n        }\n        poems_data_for_json.append(current_poem_json_data)\n        \n    return \"\\n\".join(report_lines), poems_data_for_json\n\ndef main():\n    print(\"Generating symbolic genome visualizations with updated glyphs...\")\n    \n    timeline_file = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json\"\n    markdown_output_file = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/NOTATION/SYMBOLIC_GENOME_REPORT.md\"\n    json_output_file = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/symbolic_genome_data.json\"\n    \n    timeline_data = load_timeline(timeline_file)\n    poem_entries = organize_by_poem(timeline_data)\n    \n    markdown_report_content, json_data_list = generate_report(poem_entries)\n    \n    # Write Markdown report\n    try:\n        with open(markdown_output_file, 'w', encoding='utf-8') as f_md:\n            f_md.write(markdown_report_content)\n        print(f\"Symbolic Genome Report (Markdown) generated at {markdown_output_file}\")\n    except IOError as e:\n        print(f\"Error writing Markdown file: {e}\")\n\n    # Write JSON data\n    try:\n        with open(json_output_file, 'w', encoding='utf-8') as f_json:\n            json.dump(json_data_list, f_json, ensure_ascii=False, indent=4)\n        print(f\"Symbolic Genome Data (JSON) generated at {json_output_file}\")\n    except IOError as e:\n        print(f\"Error writing JSON file: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/symbolic_genome_data.json",
      ")\n        \n        id_spacing = []\n        for id_str in id_track_parts:\n            spaces = (3 - len(id_str)) // 2\n            id_spacing.append(",
      ")\n    for syntagma, count in sorted(syntagma_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(SYNTAGMA_GLYPHS, syntagma)\n        percentage = (count / total) * 100\n        lines.append(f",
      ")\n    for image, count in sorted(image_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(IMAGE_GLYPHS, image)\n        percentage = (count / total) * 100\n        lines.append(f",
      ")\n    for cineosis, count in sorted(cineosis_counts.items(), key=lambda x: -x[1]):\n        glyph = get_glyph_or_default(CINEOSIS_GLYPHS, cineosis)\n        percentage = (count / total) * 100\n        lines.append(f",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NORMALIZED-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NOTATION/SYMBOLIC_GENOME_REPORT.md",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/symbolic_genome_data.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Generate Symbolic Genome Visualizations for Poetry Timeline\n\nThis script creates a visual representation of each poem's \"symbolic genome\"\nusing the defined glyphs for syntagma types, image types, and cineosis functions."
  },
  {
    "path": "HONEYBADGER/consolidate_timeline_data.py",
    "size": 6630,
    "lines": 198,
    "source": "#!/usr/bin/env python3\n\"\"\"\nTimeline Data Consolidator\n\nThis script creates a consolidated TOTAL-TIMELINE.json file that integrates:\n1. All metadata from COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json\n2. Prompt data from COLLECTION_WhereYouGoWhenYouLeave_prompts.md\n\nFor each shot ID in the assemblage JSON, it looks up the corresponding prompt\nfrom the prompts.md file and adds it as a new field. This handles duplicates\nautomatically, preserving all entries in the original assemblage JSON.\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"HONEYBADGER\")\n\n# Source files\nTIMELINE_ASSEMBLAGE_JSON = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json\")\nPROMPTS_MD = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_prompts.md\")\n\n# Output file\nTOTAL_TIMELINE_JSON = os.path.join(OUTPUT_DIR, \"TOTAL-TIMELINE.json\")\n\ndef log(message):\n    \"\"\"Print log message with timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"{timestamp}: {message}\")\n\ndef extract_prompts_from_markdown(md_file):\n    \"\"\"\n    Extract prompts from markdown file.\n    Returns a dictionary mapping shot IDs to prompt data.\n    \"\"\"\n    prompts = {}\n    \n    with open(md_file, 'r') as f:\n        content = f.read()\n    \n    # Pattern to match shot entries - handles both formats:\n    # Format 1: \"SH001 [00:01:52] \u00b7 DS \u00b7 Mood Environment Stabilizer \u00b7 Description \u00b7 Prompt\"\n    # Format 2: \"HM001 \u00b7 DS \u00b7 Mood Environment Stabilizer \u00b7 Description \u00b7 Prompt\"\n    pattern = r'([A-Z]{2}\\d{3})\\s*(?:\\[([\\d:]+)\\])?\\s*\u00b7\\s*([A-Z]+)\\s*\u00b7\\s*([^\u00b7]+)\u00b7\\s*([^\u00b7]+)\u00b7\\s*(.+?)(?=\\n[A-Z]{2}\\d{3}|\\n\\n|\\n\\s*---|$)'\n    \n    matches = re.finditer(pattern, content, re.DOTALL)\n    \n    for match in matches:\n        shot_id = match.group(1)\n        timestamp = match.group(2) if match.group(2) else \"\"\n        category = match.group(3).strip()\n        function = match.group(4).strip()\n        description = match.group(5).strip() if len(match.groups()) > 4 else \"\"\n        prompt = match.group(6).strip() if len(match.groups()) > 5 else \"\"\n        \n        prompts[shot_id] = {\n            \"timestamp\": timestamp,\n            \"category\": category,\n            \"function\": function,\n            \"description\": description,\n            \"prompt\": prompt\n        }\n    \n    return prompts\n\ndef consolidate_timeline_data():\n    \"\"\"\n    Consolidate data from timeline assemblage JSON and prompts markdown.\n    \"\"\"\n    log(f\"Starting data consolidation...\")\n    \n    # Load timeline assemblage JSON\n    try:\n        with open(TIMELINE_ASSEMBLAGE_JSON, 'r') as f:\n            timeline_data = json.load(f)\n        \n        log(f\"Loaded timeline assemblage JSON with {len(timeline_data)} entries\")\n    except Exception as e:\n        log(f\"Error loading timeline assemblage JSON: {e}\")\n        return False\n    \n    # Extract prompts from markdown\n    try:\n        prompts = extract_prompts_from_markdown(PROMPTS_MD)\n        log(f\"Extracted {len(prompts)} prompts from markdown\")\n    except Exception as e:\n        log(f\"Error extracting prompts from markdown: {e}\")\n        return False\n    \n    # Track stats\n    total_entries = len(timeline_data)\n    matched_prompts = 0\n    missing_prompts = 0\n    \n    # Add prompt data to timeline JSON\n    for entry in timeline_data:\n        shot_id = entry.get('id', '')\n        \n        if shot_id in prompts:\n            # Add prompt data to the entry\n            entry['prompt_data'] = prompts[shot_id]\n            matched_prompts += 1\n        else:\n            # Add an empty placeholder for missing prompts\n            entry['prompt_data'] = {\n                \"timestamp\": \"\",\n                \"category\": \"\",\n                \"function\": \"\",\n                \"description\": \"\",\n                \"prompt\": \"\"\n            }\n            missing_prompts += 1\n            log(f\"Warning: No prompt found for shot {shot_id}\")\n    \n    # Save consolidated JSON\n    try:\n        with open(TOTAL_TIMELINE_JSON, 'w') as f:\n            json.dump(timeline_data, f, indent=2)\n        \n        log(f\"Saved consolidated TOTAL-TIMELINE.json with {total_entries} entries\")\n        log(f\"Matched prompts: {matched_prompts}, Missing prompts: {missing_prompts}\")\n        \n        return True\n    except Exception as e:\n        log(f\"Error saving consolidated JSON: {e}\")\n        return False\n\ndef validate_consolidated_data():\n    \"\"\"\n    Validate the consolidated JSON for completeness and integrity.\n    \"\"\"\n    log(f\"Validating consolidated data...\")\n    \n    try:\n        with open(TOTAL_TIMELINE_JSON, 'r') as f:\n            data = json.load(f)\n        \n        total = len(data)\n        complete = 0\n        incomplete = 0\n        missing_fields = {}\n        \n        required_fields = ['id', 'timestamp', 'image_path', 'poem', 'content', \n                          'syntagmaType', 'operativeEkphrasis', 'imageType', \n                          'cineosisFunction', 'prompt_data']\n        \n        for entry in data:\n            entry_id = entry.get('id', 'unknown')\n            missing = [field for field in required_fields if field not in entry or not entry[field]]\n            \n            if not missing:\n                complete += 1\n            else:\n                incomplete += 1\n                missing_fields[entry_id] = missing\n        \n        log(f\"Total entries: {total}\")\n        log(f\"Complete entries: {complete}\")\n        log(f\"Incomplete entries: {incomplete}\")\n        \n        if incomplete > 0:\n            log(\"Entries with missing fields:\")\n            for entry_id, missing in missing_fields.items():\n                log(f\"- {entry_id}: Missing {', '.join(missing)}\")\n        \n        section_counts = {}\n        for entry in data:\n            shot_id = entry.get('id', '')\n            if shot_id:\n                section = shot_id[:2]\n                section_counts[section] = section_counts.get(section, 0) + 1\n        \n        log(\"Entry counts by section:\")\n        for section, count in sorted(section_counts.items()):\n            log(f\"- {section}: {count} entries\")\n        \n        return True\n    except Exception as e:\n        log(f\"Error validating consolidated JSON: {e}\")\n        return False\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    log(\"=== Timeline Data Consolidator ===\")\n    \n    if consolidate_timeline_data():\n        validate_consolidated_data()\n    \n    log(\"=== Timeline Data Consolidation Complete ===\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json",
      "TOTAL-TIMELINE.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "the",
      "os",
      "re",
      "json",
      "datetime"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Timeline Data Consolidator\n\nThis script creates a consolidated TOTAL-TIMELINE.json file that integrates:\n1. All metadata from COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json\n2. Prompt data from COLLECTION_WhereYouGoWhenYouLeave_prompts.md\n\nFor each shot ID in the assemblage JSON, it looks up the corresponding prompt\nfrom the prompts.md file and adds it as a new field. This handles duplicates\nautomatically, preserving all entries in the original assemblage JSON."
  },
  {
    "path": "HONEYBADGER/validate_timeline.py",
    "size": 13026,
    "lines": 308,
    "source": "#!/usr/bin/env python3\n\"\"\"\nTimeline Validation Scanner\n\nThis script analyzes the COMPLETE-TIMELINE.json file to identify discrepancies,\ninconsistencies, or missing elements that could affect downstream video generation.\n\"\"\"\n\nimport json\nimport os\nimport re\nfrom datetime import datetime\nfrom collections import defaultdict, Counter\n\n# Base directory\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\n# Timeline JSON path\nTIMELINE_JSON_PATH = os.path.join(BASE_DIR, \"HONEYBADGER\", \"COMPLETE-TIMELINE.json\")\n\ndef log(message):\n    \"\"\"Print log message with timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"{timestamp}: {message}\")\n\ndef load_timeline_data():\n    \"\"\"Load the timeline JSON file.\"\"\"\n    try:\n        with open(TIMELINE_JSON_PATH, \"r\") as f:\n            return json.load(f)\n    except (json.JSONDecodeError, FileNotFoundError) as e:\n        log(f\"Error loading timeline JSON: {e}\")\n        return []\n\ndef validate_timeline():\n    \"\"\"\n    Perform comprehensive validation of the timeline entries.\n    Checks for inconsistencies, discrepancies, and missing elements.\n    \"\"\"\n    log(\"=== Timeline Validation Scanner ===\")\n    timeline_data = load_timeline_data()\n    entry_count = len(timeline_data)\n    log(f\"Analyzing {entry_count} timeline entries...\")\n    \n    # Track various issues\n    missing_fields_entries = []\n    syntagma_type_mismatch = []\n    field_order_issues = []\n    poem_name_inconsistencies = []\n    section_prefix_inconsistencies = []\n    missing_prompt_entries = []\n    operativeEkphrasis_inconsistencies = []\n    content_issues = []\n\n    # Reference data\n    expected_fields = [\n        \"timestamp\", \"image_path\", \"id\", \"poem\", \"content\",\n        \"syntagmaType\", \"operativeEkphrasis\", \"imageType\", \"cineosisFunction\", \"full_prompt\"\n    ]\n    \n    # Expected syntagmaType to imageType mappings\n    syntagma_image_map = {\n        \"Descriptive Syntagma (DS)\": \"Descriptive Image\",\n        \"Chronological Syntagma (CS)\": \"Crystal-Image\",\n        \"Perception-Image (PI)\": \"Perception-Image\", \n        \"Affection-Image (AF)\": \"Affection-Image\",\n        \"Action-Image (AC)\": \"Action-Image\",\n        \"Relation-Image (RI)\": \"Relation-Image\",\n        \"Sonsign (SS)\": \"Sonsign\"\n    }\n    \n    # Expected section prefix to poem name mappings\n    expected_poem_names = {\n        \"NM\": \"Nevermore\",\n        \"AT\": \"Resurrecting Atlantis\",\n        \"DJ\": \"Dear John\",\n        \"NS\": \"Newly Single\",\n        \"YH\": \"You're Home\",\n        \"BE\": \"How to break off an engagement\",\n        \"BL\": \"Blue Light\",\n        \"FL\": \"Flashing Lights\",\n        \"HM\": \"Hotel Monteleone\",\n        \"HT\": \"How To Win My Heart\",\n        \"MR\": \"Myocardial Reperfusion\",\n        \"RU\": \"Re-up\",\n        \"SH\": \"Out of Life\"\n    }\n    \n    # Collect statistics\n    sections = defaultdict(int)\n    field_presence = defaultdict(int)\n    \n    # Previous entry tracking\n    prev_entry = None\n    \n    # Analyze each entry\n    for i, entry in enumerate(timeline_data):\n        # Track section statistics\n        shot_id = entry.get(\"id\", \"\")\n        prefix = shot_id[:2] if shot_id and len(shot_id) >= 2 else \"\"\n        sections[prefix] += 1\n        \n        # Check field presence\n        missing_fields = [field for field in expected_fields if field not in entry]\n        if missing_fields:\n            missing_fields_entries.append({\n                \"id\": shot_id,\n                \"missing_fields\": missing_fields\n            })\n            \n        # Track field presence\n        for field in entry:\n            field_presence[field] += 1\n            \n        # Check field ordering (content should follow poem)\n        keys_list = list(entry.keys())\n        if \"poem\" in keys_list and \"content\" in keys_list:\n            poem_pos = keys_list.index(\"poem\")\n            content_pos = keys_list.index(\"content\")\n            if content_pos != poem_pos + 1:\n                field_order_issues.append(shot_id)\n        \n        # Check syntagma type vs image type consistency\n        syntagma = entry.get(\"syntagmaType\", \"\")\n        image_type = entry.get(\"imageType\", \"\")\n        \n        # Some special handling for variations\n        normalized_syntagma = syntagma\n        if prefix in [\"PI\", \"AF\", \"AC\", \"RI\", \"SS\"] or syntagma in [\"PI\", \"AF\", \"AC\", \"RI\", \"SS\"]:\n            # Handle abbreviated forms\n            abbrev_map = {\n                \"PI\": \"Perception-Image\",\n                \"AF\": \"Affection-Image\", \n                \"AC\": \"Action-Image\",\n                \"RI\": \"Relation-Image\",\n                \"SS\": \"Sonsign\"\n            }\n            \n            # Try to normalize based on prefix or abbreviation\n            for abbr, full in abbrev_map.items():\n                if syntagma == abbr:\n                    normalized_syntagma = f\"{full} ({abbr})\"\n                    break\n        \n        # Check for mismatch\n        if normalized_syntagma in syntagma_image_map and image_type != syntagma_image_map[normalized_syntagma]:\n            syntagma_type_mismatch.append({\n                \"id\": shot_id,\n                \"syntagmaType\": syntagma,\n                \"imageType\": image_type,\n                \"expected_imageType\": syntagma_image_map.get(normalized_syntagma, \"Unknown\")\n            })\n        \n        # Check section prefix vs poem name consistency\n        poem_name = entry.get(\"poem\", \"\")\n        if prefix in expected_poem_names and poem_name != expected_poem_names[prefix]:\n            poem_name_inconsistencies.append({\n                \"id\": shot_id,\n                \"actual_poem\": poem_name,\n                \"expected_poem\": expected_poem_names[prefix]\n            })\n            \n        # Check if image path prefix matches ID prefix\n        image_path = entry.get(\"image_path\", \"\")\n        if image_path and prefix:\n            image_prefix_match = image_path.startswith(f\"{prefix}/\") or image_path.startswith(f\"{prefix}_\")\n            if not image_prefix_match:\n                section_prefix_inconsistencies.append({\n                    \"id\": shot_id,\n                    \"image_path\": image_path\n                })\n                \n        # Check for missing prompt\n        if \"full_prompt\" not in entry:\n            missing_prompt_entries.append(shot_id)\n            \n        # Check operative ekphrasis consistency with full_prompt\n        oe = entry.get(\"operativeEkphrasis\", \"\")\n        full_prompt = entry.get(\"full_prompt\", \"\")\n        if oe and full_prompt and \" \u00b7 \" in full_prompt:\n            prompt_parts = full_prompt.split(\" \u00b7 \")\n            if len(prompt_parts) >= 3:\n                prompt_oe = prompt_parts[2].strip()\n                if prompt_oe != oe and prompt_oe.rstrip(\".\") != oe.rstrip(\".\"):\n                    operativeEkphrasis_inconsistencies.append({\n                        \"id\": shot_id,\n                        \"entry_oe\": oe,\n                        \"prompt_oe\": prompt_oe\n                    })\n                    \n        # Check if content field has unusual patterns or seems misplaced\n        content = entry.get(\"content\", \"\")\n        if content:\n            # Check if content seems like it might be a prompt part\n            if \" \u00b7 \" in content or content.startswith((\"DS \", \"CS \", \"PI \", \"AF \")):\n                content_issues.append({\n                    \"id\": shot_id,\n                    \"issue\": \"content field may contain prompt text\",\n                    \"content\": content\n                })\n            # Check if content is unusually long (potential field confusion)\n            elif len(content) > 100:\n                content_issues.append({\n                    \"id\": shot_id,\n                    \"issue\": \"unusually long content field\",\n                    \"content\": content[:50] + \"...\"\n                })\n                \n        # Sequential analysis with previous entry\n        if prev_entry and \"timestamp\" in entry and \"timestamp\" in prev_entry:\n            # Any sequential checks would go here\n            pass\n            \n        prev_entry = entry\n\n    # Report findings\n    log(f\"\\nAnalysis Complete. Found {len(sections)} different section prefixes:\")\n    for prefix, count in sorted(sections.items()):\n        log(f\"- Section {prefix}: {count} entries\")\n    \n    if missing_fields_entries:\n        log(f\"\\n\u26a0\ufe0f Found {len(missing_fields_entries)} entries with missing fields:\")\n        for entry in missing_fields_entries[:5]:  # Show first 5 examples\n            log(f\"  - ID: {entry['id']}, Missing: {', '.join(entry['missing_fields'])}\")\n        if len(missing_fields_entries) > 5:\n            log(f\"    (and {len(missing_fields_entries) - 5} more entries...)\")\n    else:\n        log(\"\\n\u2705 All entries have the expected fields\")\n        \n    if field_order_issues:\n        log(f\"\\n\u26a0\ufe0f Found {len(field_order_issues)} entries with field ordering issues (content not immediately after poem):\")\n        for entry_id in field_order_issues[:5]:  # Show first 5 examples\n            log(f\"  - ID: {entry_id}\")\n        if len(field_order_issues) > 5:\n            log(f\"    (and {len(field_order_issues) - 5} more entries...)\")\n    else:\n        log(\"\\n\u2705 All entries have correct field ordering (content follows poem)\")\n        \n    if syntagma_type_mismatch:\n        log(f\"\\n\u26a0\ufe0f Found {len(syntagma_type_mismatch)} entries with syntagmaType/imageType mismatches:\")\n        for entry in syntagma_type_mismatch[:5]:  # Show first 5 examples\n            log(f\"  - ID: {entry['id']}, syntagmaType: {entry['syntagmaType']}, imageType: {entry['imageType']}, expected: {entry['expected_imageType']}\")\n        if len(syntagma_type_mismatch) > 5:\n            log(f\"    (and {len(syntagma_type_mismatch) - 5} more entries...)\")\n    else:\n        log(\"\\n\u2705 All entries have matching syntagmaType and imageType values\")\n        \n    if poem_name_inconsistencies:\n        log(f\"\\n\u26a0\ufe0f Found {len(poem_name_inconsistencies)} entries with poem name inconsistencies:\")\n        for entry in poem_name_inconsistencies[:5]:  # Show first 5 examples\n            log(f\"  - ID: {entry['id']}, actual: '{entry['actual_poem']}', expected: '{entry['expected_poem']}'\")\n        if len(poem_name_inconsistencies) > 5:\n            log(f\"    (and {len(poem_name_inconsistencies) - 5} more entries...)\")\n    else:\n        log(\"\\n\u2705 All entries have correct poem names matching their section\")\n        \n    if section_prefix_inconsistencies:\n        log(f\"\\n\u26a0\ufe0f Found {len(section_prefix_inconsistencies)} entries with image_path prefix inconsistencies:\")\n        for entry in section_prefix_inconsistencies[:5]:  # Show first 5 examples\n            log(f\"  - ID: {entry['id']}, image_path: {entry['image_path']}\")\n        if len(section_prefix_inconsistencies) > 5:\n            log(f\"    (and {len(section_prefix_inconsistencies) - 5} more entries...)\")\n    else:\n        log(\"\\n\u2705 All entries have image paths that match their ID prefix\")\n        \n    if missing_prompt_entries:\n        log(f\"\\n\u26a0\ufe0f Found {len(missing_prompt_entries)} entries missing full_prompt field:\")\n        for entry_id in missing_prompt_entries[:5]:  # Show first 5 examples\n            log(f\"  - ID: {entry_id}\")\n        if len(missing_prompt_entries) > 5:\n            log(f\"    (and {len(missing_prompt_entries) - 5} more entries...)\")\n    else:\n        log(\"\\n\u2705 All entries have full_prompt field\")\n        \n    if operativeEkphrasis_inconsistencies:\n        log(f\"\\n\u26a0\ufe0f Found {len(operativeEkphrasis_inconsistencies)} entries with operativeEkphrasis inconsistencies:\")\n        for entry in operativeEkphrasis_inconsistencies[:5]:  # Show first 5 examples\n            log(f\"  - ID: {entry['id']}, entry: '{entry['entry_oe']}', prompt: '{entry['prompt_oe']}'\")\n        if len(operativeEkphrasis_inconsistencies) > 5:\n            log(f\"    (and {len(operativeEkphrasis_inconsistencies) - 5} more entries...)\")\n    else:\n        log(\"\\n\u2705 All entries have matching operativeEkphrasis values\")\n        \n    if content_issues:\n        log(f\"\\n\u26a0\ufe0f Found {len(content_issues)} entries with potential content field issues:\")\n        for entry in content_issues[:5]:  # Show first 5 examples\n            log(f\"  - ID: {entry['id']}, issue: {entry['issue']}, content: '{entry['content']}'\")\n        if len(content_issues) > 5:\n            log(f\"    (and {len(content_issues) - 5} more entries...)\")\n    else:\n        log(\"\\n\u2705 All entries have proper content fields\")\n\n    # Additional statistics\n    uncommon_fields = [field for field, count in field_presence.items() if count < entry_count * 0.9]  # Fields present in less than 90% of entries\n    if uncommon_fields:\n        log(f\"\\n\u26a0\ufe0f Found {len(uncommon_fields)} uncommon fields (present in <90% of entries):\")\n        for field in uncommon_fields:\n            presence_percent = field_presence[field] / entry_count * 100\n            log(f\"  - '{field}' present in {field_presence[field]} entries ({presence_percent:.1f}%)\")\n    \n    log(\"\\n=== Timeline Validation Complete ===\")\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    validate_timeline()\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis",
      "\\n\u26a0\ufe0f Found {len(syntagma_type_mismatch)} entries with syntagmaType/imageType mismatches:",
      ")\n        for field in uncommon_fields:\n            presence_percent = field_presence[field] / entry_count * 100\n            log(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "re",
      "datetime",
      "collections"
    ],
    "generates": [],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Timeline Validation Scanner\n\nThis script analyzes the COMPLETE-TIMELINE.json file to identify discrepancies,\ninconsistencies, or missing elements that could affect downstream video generation."
  },
  {
    "path": "HONEYBADGER/syntagma_flow_analysis.py",
    "size": 15812,
    "lines": 404,
    "source": "#!/usr/bin/env python3\n\"\"\"\nSyntagma Flow Analysis Script\n\nThis script analyzes the corrected timeline data to identify:\n1. Syntagma transition patterns\n2. Potential logical contradictions in narrative flow\n3. Emergent structures and rhythms in the syntagma sequences\n4. Relationship between syntagma types and other metadata fields\n\nOutput is a comprehensive markdown report with Tufte-inspired visualizations.\n\"\"\"\n\nimport json\nimport os\nfrom collections import defaultdict, Counter\n\n# Constants for analysis\nCANONICAL_SYNTAGMA_TYPES = {\n    \"Descriptive Syntagma\",\n    \"Chronological Syntagma\",\n    \"Autonomous Syntagma\",\n    \"Flashback Syntagma\",\n    \"Thematic Montage\",\n    \"Crystal Syntagma\"\n}\n\n# Shorter codes for visualization\nSYNTAGMA_CODES = {\n    \"Descriptive Syntagma\": \"DS\",\n    \"Chronological Syntagma\": \"CS\",\n    \"Autonomous Syntagma\": \"AS\",\n    \"Flashback Syntagma\": \"FS\",\n    \"Thematic Montage\": \"TM\",\n    \"Crystal Syntagma\": \"XS\"\n}\n\n# Unicode symbols for visualization\nSYNTAGMA_SYMBOLS = {\n    \"Descriptive Syntagma\": \"\u25c9\",  # Circle\n    \"Chronological Syntagma\": \"\u2192\",  # Arrow\n    \"Autonomous Syntagma\": \"\u25c7\",  # Diamond\n    \"Flashback Syntagma\": \"\u2190\",  # Left arrow\n    \"Thematic Montage\": \"\u2261\",  # Triple bar\n    \"Crystal Syntagma\": \"\u229b\"   # Asterisk\n}\n\ndef extract_entry_id_num(entry_id):\n    \"\"\"Extract the numeric part from an entry ID for sorting\"\"\"\n    try:\n        return int(entry_id[2:])\n    except:\n        return 0\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file\"\"\"\n    with open(filepath, 'r') as f:\n        return json.load(f)\n\ndef organize_entries_by_poem(timeline):\n    \"\"\"Group and sort entries by poem\"\"\"\n    poem_entries = defaultdict(list)\n    for entry in timeline:\n        poem_entries[entry['poem']].append(entry)\n    \n    # Sort entries within each poem by ID\n    for poem, entries in poem_entries.items():\n        poem_entries[poem] = sorted(entries, key=lambda e: extract_entry_id_num(e['id']))\n    \n    return poem_entries\n\ndef normalize_syntagma_type(syntagma_type):\n    \"\"\"Extract the base syntagma type from potentially formatted strings\"\"\"\n    # Handle cases with parenthetical codes like \"Crystal Syntagma (CS)\"\n    if \"(\" in syntagma_type and \")\" in syntagma_type:\n        return syntagma_type.split(\"(\")[0].strip()\n    return syntagma_type\n\ndef analyze_transitions(poem_entries):\n    \"\"\"Analyze transitions between syntagma types within poems\"\"\"\n    results = {}\n    \n    # For each poem, analyze its syntagma transitions\n    for poem, entries in poem_entries.items():\n        transitions = []\n        transition_counts = defaultdict(int)\n        \n        # Build transition pairs\n        for i in range(len(entries) - 1):\n            current = normalize_syntagma_type(entries[i]['syntagmaType'])\n            next_type = normalize_syntagma_type(entries[i + 1]['syntagmaType'])\n            \n            transition = (current, next_type)\n            transitions.append((entries[i]['id'], entries[i+1]['id'], transition))\n            transition_counts[transition] += 1\n        \n        results[poem] = {\n            'transitions': transitions,\n            'counts': dict(transition_counts)\n        }\n    \n    return results\n\ndef build_global_transition_matrix(poem_transitions):\n    \"\"\"Build a global transition matrix across all poems\"\"\"\n    # Get unique syntagma types from all transitions\n    all_types = set()\n    for poem_data in poem_transitions.values():\n        for (from_type, to_type) in poem_data['counts'].keys():\n            all_types.add(from_type)\n            all_types.add(to_type)\n    \n    # Create sorted list of types for consistent ordering\n    all_types = sorted(all_types)\n    \n    # Initialize transition matrix\n    matrix = {from_type: {to_type: 0 for to_type in all_types} for from_type in all_types}\n    \n    # Fill transition matrix\n    for poem_data in poem_transitions.values():\n        for (from_type, to_type), count in poem_data['counts'].items():\n            matrix[from_type][to_type] += count\n    \n    return matrix, all_types\n\ndef identify_rare_transitions(matrix, threshold=0.05):\n    \"\"\"Identify unusually rare transitions (potential logical contradictions)\"\"\"\n    rare_transitions = []\n    \n    # Calculate total transitions from each type\n    totals = {from_type: sum(counts.values()) for from_type, counts in matrix.items()}\n    \n    # Find rare transitions\n    for from_type, counts in matrix.items():\n        if totals[from_type] == 0:\n            continue\n            \n        for to_type, count in counts.items():\n            if count > 0:  # Only consider transitions that actually occur\n                proportion = count / totals[from_type]\n                if proportion < threshold:\n                    rare_transitions.append({\n                        'from': from_type,\n                        'to': to_type,\n                        'count': count,\n                        'proportion': proportion,\n                        'total_from': totals[from_type]\n                    })\n    \n    # Sort by proportion (rarest first)\n    rare_transitions.sort(key=lambda x: x['proportion'])\n    \n    return rare_transitions\n\ndef identify_common_patterns(poem_transitions, pattern_length=3):\n    \"\"\"Identify common syntagma patterns of specified length\"\"\"\n    all_patterns = Counter()\n    \n    for poem, data in poem_transitions.items():\n        transitions = data['transitions']\n        \n        # Skip poems with too few transitions for pattern detection\n        if len(transitions) < pattern_length:\n            continue\n        \n        # Extract sequences\n        for i in range(len(transitions) - pattern_length + 2):\n            # Get sequence of syntagma types\n            sequence = tuple(normalize_syntagma_type(transitions[i+j][2][1]) for j in range(pattern_length-1))\n            all_patterns[sequence] += 1\n    \n    return all_patterns\n\ndef analyze_syntagma_context(poem_entries):\n    \"\"\"Analyze the contextual relationship between syntagma types and other fields\"\"\"\n    # Track relationships between syntagma types and other fields\n    type_to_image = defaultdict(Counter)\n    type_to_function = defaultdict(Counter)\n    \n    for poem, entries in poem_entries.items():\n        for entry in entries:\n            syntagma = normalize_syntagma_type(entry['syntagmaType'])\n            type_to_image[syntagma][entry['imageType']] += 1\n            type_to_function[syntagma][entry['cineosisFunction']] += 1\n    \n    return {\n        'syntagma_to_image': {k: dict(v) for k, v in type_to_image.items()},\n        'syntagma_to_function': {k: dict(v) for k, v in type_to_function.items()}\n    }\n\ndef check_theoretical_consistency(poem_entries):\n    \"\"\"Check for theoretical inconsistencies in syntagma assignments\"\"\"\n    inconsistencies = []\n    \n    for poem, entries in poem_entries.items():\n        for entry in entries:\n            syntagma = normalize_syntagma_type(entry['syntagmaType'])\n            \n            # Check for potential inconsistencies based on theory\n            if (syntagma == \"Chronological Syntagma\" and \n                entry['imageType'] == \"Affection-Image\" and\n                not \"Motion\" in entry['cineosisFunction']):\n                inconsistencies.append({\n                    'id': entry['id'],\n                    'poem': poem,\n                    'syntagmaType': syntagma,\n                    'imageType': entry['imageType'],\n                    'cineosisFunction': entry['cineosisFunction'],\n                    'reason': \"Chronological Syntagma typically implies causality or motion, but this is an Affection-Image without motion cineosis\"\n                })\n            \n            if (syntagma == \"Flashback Syntagma\" and\n                entry['imageType'] not in [\"Recollection-Image\", \"Time-Image\"] and\n                not any(x in entry['cineosisFunction'] for x in [\"Memory\", \"Recall\", \"Past\", \"Temporal\"])):\n                inconsistencies.append({\n                    'id': entry['id'],\n                    'poem': poem,\n                    'syntagmaType': syntagma,\n                    'imageType': entry['imageType'],\n                    'cineosisFunction': entry['cineosisFunction'],\n                    'reason': \"Flashback Syntagma lacks supporting memory/temporal qualities in imageType or cineosisFunction\"\n                })\n    \n    return inconsistencies\n\ndef visualize_poem_flow(poem, entries):\n    \"\"\"Create a Tufte-inspired visualization of syntagma flow for a poem\"\"\"\n    lines = []\n    \n    # First line: Entry IDs\n    id_line = \"IDs:   \"\n    for entry in entries:\n        id_line += f\"{entry['id']} \"\n    lines.append(id_line)\n    \n    # Second line: Syntagma flow with symbols\n    flow_line = \"Flow:  \"\n    for entry in entries:\n        syntagma = normalize_syntagma_type(entry['syntagmaType'])\n        symbol = SYNTAGMA_SYMBOLS.get(syntagma, \"\u25cf\")\n        flow_line += f\"{symbol} \"\n    lines.append(flow_line)\n    \n    # Third line: Syntagma codes\n    code_line = \"Types: \"\n    for entry in entries:\n        syntagma = normalize_syntagma_type(entry['syntagmaType'])\n        code = SYNTAGMA_CODES.get(syntagma, \"??\")\n        code_line += f\"{code} \"\n    lines.append(code_line)\n    \n    return lines\n\ndef generate_report(results):\n    \"\"\"Generate a comprehensive markdown report of the analysis results\"\"\"\n    report_lines = [\"# Syntagma Flow Analysis Report\\n\"]\n    \n    # Add overview section\n    report_lines.append(\"## Overview\\n\")\n    total_entries = sum(len(entries) for entries in results['poem_entries'].values())\n    report_lines.append(f\"- Total entries analyzed: {total_entries}\")\n    report_lines.append(f\"- Number of poems: {len(results['poem_entries'])}\")\n    \n    syntagma_counts = Counter()\n    for poem, entries in results['poem_entries'].items():\n        for entry in entries:\n            syntagma_counts[normalize_syntagma_type(entry['syntagmaType'])] += 1\n    \n    report_lines.append(\"\\n### Syntagma Type Distribution\\n\")\n    for syntagma, count in sorted(syntagma_counts.items(), key=lambda x: -x[1]):\n        percentage = (count / total_entries) * 100\n        report_lines.append(f\"- {syntagma}: {count} ({percentage:.1f}%)\")\n    \n    # Add transition matrix section\n    report_lines.append(\"\\n## Transition Matrix\\n\")\n    report_lines.append(\"This matrix shows how frequently each syntagma type transitions to every other type:\\n\")\n    \n    matrix, types = results['transition_matrix']\n    \n    # Table header\n    header = \"| FROM \u2192 TO | \" + \" | \".join(f\"{SYNTAGMA_CODES.get(t, t[:2])}\" for t in types) + \" |\"\n    report_lines.append(header)\n    \n    # Table separator\n    separator = \"|\" + \"---|\" * (len(types) + 1)\n    report_lines.append(separator)\n    \n    # Table rows\n    for from_type in types:\n        row = f\"| {SYNTAGMA_CODES.get(from_type, from_type[:2])} |\"\n        for to_type in types:\n            row += f\" {matrix[from_type][to_type]} |\"\n        report_lines.append(row)\n    \n    # Add rare transitions section\n    report_lines.append(\"\\n## Potential Logical Contradictions\\n\")\n    report_lines.append(\"These are unusually rare transitions that might indicate logical contradictions in narrative flow:\\n\")\n    \n    if results['rare_transitions']:\n        for trans in results['rare_transitions'][:10]:  # Show top 10 rarest\n            report_lines.append(f\"- {trans['from']} \u2192 {trans['to']}: {trans['count']} occurrences ({trans['proportion']:.1%} of all transitions from {trans['from']})\")\n    else:\n        report_lines.append(\"No rare transitions identified at the specified threshold.\")\n    \n    # Add common patterns section\n    report_lines.append(\"\\n## Emergent Patterns\\n\")\n    report_lines.append(\"These are common sequences of 3 consecutive syntagma types that appear across poems:\\n\")\n    \n    common_patterns = results['common_patterns'].most_common(10)\n    if common_patterns:\n        for pattern, count in common_patterns:\n            pattern_str = \" \u2192 \".join(SYNTAGMA_CODES.get(p, p[:2]) for p in pattern)\n            report_lines.append(f\"- {pattern_str}: {count} occurrences\")\n    else:\n        report_lines.append(\"No common patterns identified.\")\n    \n    # Add theoretical inconsistencies section\n    report_lines.append(\"\\n## Theoretical Inconsistencies\\n\")\n    report_lines.append(\"These entries may have theoretical inconsistencies between syntagma type and other metadata:\\n\")\n    \n    if results['inconsistencies']:\n        for inconsistency in results['inconsistencies']:\n            report_lines.append(f\"- **{inconsistency['id']}** ({inconsistency['poem']}):\")\n            report_lines.append(f\"  - SyntagmaType: {inconsistency['syntagmaType']}\")\n            report_lines.append(f\"  - ImageType: {inconsistency['imageType']}\")\n            report_lines.append(f\"  - CineosisFunction: {inconsistency['cineosisFunction']}\")\n            report_lines.append(f\"  - Issue: {inconsistency['reason']}\")\n            report_lines.append(\"\")\n    else:\n        report_lines.append(\"No theoretical inconsistencies identified.\")\n    \n    # Add poem-specific flow visualizations\n    report_lines.append(\"\\n## Poem-Specific Flow Visualizations\\n\")\n    for poem, entries in results['poem_entries'].items():\n        report_lines.append(f\"\\n### {poem}\\n\")\n        report_lines.append(\"```\")\n        for line in visualize_poem_flow(poem, entries):\n            report_lines.append(line)\n        report_lines.append(\"```\\n\")\n        \n        # Add syntagma distribution for this poem\n        poem_syntagma_counts = Counter(normalize_syntagma_type(e['syntagmaType']) for e in entries)\n        distribution = []\n        for syntagma, count in sorted(poem_syntagma_counts.items(), key=lambda x: -x[1]):\n            percentage = (count / len(entries)) * 100\n            distribution.append(f\"{syntagma}: {count} ({percentage:.1f}%)\")\n        \n        report_lines.append(\"Distribution: \" + \", \".join(distribution))\n    \n    # Write report to file\n    report_path = '/Users/gaia/resurrecting atlantis/HONEYBADGER/SYNTAGMA_FLOW_ANALYSIS.md'\n    with open(report_path, 'w') as f:\n        f.write('\\n'.join(report_lines))\n    \n    return report_path\n\ndef main():\n    print(\"Starting syntagma flow analysis...\")\n    \n    # Load updated timeline\n    timeline = load_timeline('/Users/gaia/resurrecting atlantis/HONEYBADGER/UPDATED-TIMELINE.json')\n    print(f\"Loaded timeline with {len(timeline)} entries\")\n    \n    # Organize entries by poem\n    poem_entries = organize_entries_by_poem(timeline)\n    print(f\"Organized entries into {len(poem_entries)} poems\")\n    \n    # Analyze transitions\n    poem_transitions = analyze_transitions(poem_entries)\n    print(\"Analyzed syntagma transitions\")\n    \n    # Build global transition matrix\n    transition_matrix = build_global_transition_matrix(poem_transitions)\n    print(\"Built global transition matrix\")\n    \n    # Identify rare transitions\n    rare_transitions = identify_rare_transitions(transition_matrix[0])\n    print(f\"Identified {len(rare_transitions)} potentially problematic rare transitions\")\n    \n    # Identify common patterns\n    common_patterns = identify_common_patterns(poem_transitions)\n    print(f\"Identified recurring syntagma patterns\")\n    \n    # Check for theoretical inconsistencies\n    inconsistencies = check_theoretical_consistency(poem_entries)\n    print(f\"Found {len(inconsistencies)} potential theoretical inconsistencies\")\n    \n    # Compile results\n    results = {\n        'poem_entries': poem_entries,\n        'poem_transitions': poem_transitions,\n        'transition_matrix': transition_matrix,\n        'rare_transitions': rare_transitions,\n        'common_patterns': common_patterns,\n        'inconsistencies': inconsistencies\n    }\n    \n    # Generate report\n    report_path = generate_report(results)\n    print(f\"Analysis complete! Report written to {report_path}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/UPDATED-TIMELINE.json",
      "\n    rare_transitions = []\n    \n    # Calculate total transitions from each type\n    totals = {from_type: sum(counts.values()) for from_type, counts in matrix.items()}\n    \n    # Find rare transitions\n    for from_type, counts in matrix.items():\n        if totals[from_type] == 0:\n            continue\n            \n        for to_type, count in counts.items():\n            if count > 0:  # Only consider transitions that actually occur\n                proportion = count / totals[from_type]\n                if proportion < threshold:\n                    rare_transitions.append({\n                        ",
      "Flashback Syntagma lacks supporting memory/temporal qualities in imageType or cineosisFunction",
      ")\n    for syntagma, count in sorted(syntagma_counts.items(), key=lambda x: -x[1]):\n        percentage = (count / total_entries) * 100\n        report_lines.append(f",
      "]) for e in entries)\n        distribution = []\n        for syntagma, count in sorted(poem_syntagma_counts.items(), key=lambda x: -x[1]):\n            percentage = (count / len(entries)) * 100\n            distribution.append(f",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/SYNTAGMA_FLOW_ANALYSIS.md",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/UPDATED-TIMELINE.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Syntagma Flow Analysis Script\n\nThis script analyzes the corrected timeline data to identify:\n1. Syntagma transition patterns\n2. Potential logical contradictions in narrative flow\n3. Emergent structures and rhythms in the syntagma sequences\n4. Relationship between syntagma types and other metadata fields\n\nOutput is a comprehensive markdown report with Tufte-inspired visualizations."
  },
  {
    "path": "HONEYBADGER/create_clean_timeline.py",
    "size": 7076,
    "lines": 210,
    "source": "#!/usr/bin/env python3\n\"\"\"\nClean Timeline Data Generator\n\nCreates a CLEAN-TOTAL-TIMELINE.json file with consolidated data where:\n1. All metadata from the original TimelineAssemblage.json is preserved\n2. Prompt data is added as a single field rather than multiple fields\n3. Duplicates are properly handled\n\"\"\"\n\nimport os\nimport re\nimport json\nfrom datetime import datetime\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"HONEYBADGER\")\n\n# Source files\nTIMELINE_ASSEMBLAGE_JSON = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json\")\nPROMPTS_MD = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_prompts.md\")\n\n# Output file\nCLEAN_TIMELINE_JSON = os.path.join(OUTPUT_DIR, \"CLEAN-TOTAL-TIMELINE.json\")\n\ndef log(message):\n    \"\"\"Print log message with timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"{timestamp}: {message}\")\n\ndef extract_prompts_from_markdown(md_file):\n    \"\"\"\n    Extract prompts from markdown file.\n    Returns a dictionary mapping shot IDs to complete prompt strings.\n    \"\"\"\n    prompts = {}\n    \n    with open(md_file, 'r') as f:\n        content = f.read()\n    \n    # Pattern to match shot entries with the full prompt as a single capture group\n    # Format: \"SH001 [00:01:52] \u00b7 DS \u00b7 Mood Environment Stabilizer \u00b7 Description \u00b7 Full prompt text\"\n    pattern = r'([A-Z]{2}\\d{3})\\s*(?:\\[([\\d:]+)\\])?\\s*\u00b7\\s*([A-Z]+)\\s*\u00b7\\s*([^\u00b7]+)\u00b7\\s*([^\u00b7]+)\u00b7\\s*(.+?)(?=\\n[A-Z]{2}\\d{3}|\\n\\n|\\n\\s*---|$)'\n    \n    matches = re.finditer(pattern, content, re.DOTALL)\n    \n    for match in matches:\n        shot_id = match.group(1)\n        timestamp = match.group(2) if match.group(2) else \"\"\n        category = match.group(3).strip()\n        function = match.group(4).strip()\n        description = match.group(5).strip() if len(match.groups()) > 4 else \"\"\n        prompt_text = match.group(6).strip() if len(match.groups()) > 5 else \"\"\n        \n        # Compress any newlines within the prompt to create a single line\n        prompt_text = prompt_text.replace('\\n', ' ').strip()\n        \n        prompts[shot_id] = {\n            \"timestamp\": timestamp,\n            \"category\": category,\n            \"function\": function,\n            \"full_prompt\": prompt_text  # Single field for the full prompt\n        }\n    \n    return prompts\n\ndef create_clean_timeline():\n    \"\"\"\n    Create a clean consolidated timeline JSON.\n    \"\"\"\n    log(f\"Starting clean timeline generation...\")\n    \n    # Load timeline assemblage JSON\n    try:\n        with open(TIMELINE_ASSEMBLAGE_JSON, 'r') as f:\n            timeline_data = json.load(f)\n        \n        log(f\"Loaded timeline assemblage JSON with {len(timeline_data)} entries\")\n    except Exception as e:\n        log(f\"Error loading timeline assemblage JSON: {e}\")\n        return False\n    \n    # Extract prompts from markdown\n    try:\n        prompts = extract_prompts_from_markdown(PROMPTS_MD)\n        log(f\"Extracted {len(prompts)} prompts from markdown\")\n    except Exception as e:\n        log(f\"Error extracting prompts from markdown: {e}\")\n        return False\n    \n    # Track stats\n    total_entries = len(timeline_data)\n    matched_prompts = 0\n    missing_prompts = 0\n    \n    # Create clean timeline data\n    clean_timeline = []\n    \n    for entry in timeline_data:\n        shot_id = entry.get('id', '')\n        \n        # Create a clean entry with all original fields\n        clean_entry = entry.copy()\n        \n        if shot_id in prompts:\n            # Add prompt as a single field\n            clean_entry['prompt'] = prompts[shot_id]['full_prompt']\n            clean_entry['prompt_category'] = prompts[shot_id]['category']\n            clean_entry['prompt_function'] = prompts[shot_id]['function']\n            matched_prompts += 1\n        else:\n            # Add empty prompt for missing entries\n            clean_entry['prompt'] = \"\"\n            clean_entry['prompt_category'] = \"\"\n            clean_entry['prompt_function'] = \"\"\n            missing_prompts += 1\n            log(f\"Warning: No prompt found for shot {shot_id}\")\n        \n        clean_timeline.append(clean_entry)\n    \n    # Save clean consolidated JSON\n    try:\n        with open(CLEAN_TIMELINE_JSON, 'w') as f:\n            json.dump(clean_timeline, f, indent=2)\n        \n        log(f\"Saved CLEAN-TOTAL-TIMELINE.json with {total_entries} entries\")\n        log(f\"Matched prompts: {matched_prompts}, Missing prompts: {missing_prompts}\")\n        \n        return True\n    except Exception as e:\n        log(f\"Error saving clean timeline JSON: {e}\")\n        return False\n\ndef validate_clean_timeline():\n    \"\"\"\n    Validate the clean timeline JSON.\n    \"\"\"\n    log(f\"Validating clean timeline...\")\n    \n    try:\n        with open(CLEAN_TIMELINE_JSON, 'r') as f:\n            data = json.load(f)\n        \n        total = len(data)\n        complete = 0\n        incomplete = 0\n        \n        # Check for completeness of essential fields\n        for entry in data:\n            # Check if critical fields are present\n            has_id = 'id' in entry and entry['id']\n            has_image = 'image_path' in entry and entry['image_path']\n            has_prompt = 'prompt' in entry and entry['prompt']\n            \n            if has_id and has_image and has_prompt:\n                complete += 1\n            else:\n                incomplete += 1\n        \n        log(f\"Total entries: {total}\")\n        log(f\"Complete entries (id, image_path, prompt): {complete}\")\n        log(f\"Incomplete entries: {incomplete}\")\n        \n        # Check for duplicate shot IDs\n        shot_ids = {}\n        for entry in data:\n            shot_id = entry.get('id', '')\n            if shot_id:\n                if shot_id not in shot_ids:\n                    shot_ids[shot_id] = 1\n                else:\n                    shot_ids[shot_id] += 1\n        \n        # Find shot IDs that appear multiple times\n        duplicates = {shot_id: count for shot_id, count in shot_ids.items() if count > 1}\n        \n        log(f\"Found {len(duplicates)} shot IDs with multiple entries\")\n        for shot_id, count in duplicates.items():\n            log(f\"- {shot_id} appears {count} times\")\n        \n        section_counts = {}\n        for entry in data:\n            shot_id = entry.get('id', '')\n            if shot_id and len(shot_id) >= 2:\n                section = shot_id[:2]\n                section_counts[section] = section_counts.get(section, 0) + 1\n        \n        log(\"Entry counts by section:\")\n        for section, count in sorted(section_counts.items()):\n            log(f\"- {section}: {count} entries\")\n        \n        return True\n    except Exception as e:\n        log(f\"Error validating clean timeline: {e}\")\n        return False\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    log(\"=== Clean Timeline Data Generator ===\")\n    \n    if create_clean_timeline():\n        validate_clean_timeline()\n    \n    log(\"=== Clean Timeline Generation Complete ===\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COLLECTION_WhereYouGoWhenYouLeave_TimelineAssemblage.json",
      "CLEAN-TOTAL-TIMELINE.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "json",
      "datetime"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Clean Timeline Data Generator\n\nCreates a CLEAN-TOTAL-TIMELINE.json file with consolidated data where:\n1. All metadata from the original TimelineAssemblage.json is preserved\n2. Prompt data is added as a single field rather than multiple fields\n3. Duplicates are properly handled"
  },
  {
    "path": "HONEYBADGER/extract_style_conditioning.py",
    "size": 4867,
    "lines": 140,
    "source": "#!/usr/bin/env python3\n\"\"\"\nExtract Style Conditioning Script\n\nThis script extracts the style conditioning part from each entry's full_prompt field\n(everything after the last \\u00b7 character) and adds it as a new field called styleConditioning.\n\"\"\"\n\nimport json\nimport os\nfrom datetime import datetime\n\n# Base directory\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\n# Input and output paths\nJSON_PATH = os.path.join(BASE_DIR, \"HONEYBADGER\", \"COMPLETE-TIMELINE.json\")\nBACKUP_PATH = os.path.join(BASE_DIR, \"HONEYBADGER\", \"COMPLETE-TIMELINE_backup.json\")\n\ndef log(message):\n    \"\"\"Print log message with timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"{timestamp}: {message}\")\n\ndef extract_style_conditioning():\n    \"\"\"\n    Extract style conditioning from each timeline entry's full_prompt\n    and add as a new field.\n    \"\"\"\n    log(\"=== Style Conditioning Extractor ===\")\n    \n    # Create a backup of the original file\n    try:\n        with open(JSON_PATH, \"r\") as f:\n            timeline_data = json.load(f)\n        \n        # Save backup\n        with open(BACKUP_PATH, \"w\") as f:\n            json.dump(timeline_data, f, indent=2)\n        log(f\"Created backup at {BACKUP_PATH}\")\n    except Exception as e:\n        log(f\"Error creating backup: {e}\")\n        return\n        \n    log(f\"Processing {len(timeline_data)} entries\")\n    \n    # Process each entry\n    entries_processed = 0\n    style_found = 0\n    \n    for entry in timeline_data:\n        entries_processed += 1\n        \n        # Skip if no full_prompt\n        if \"full_prompt\" not in entry:\n            continue\n            \n        full_prompt = entry[\"full_prompt\"]\n        \n        # Extract style conditioning (everything after the last \\u00b7)\n        parts = full_prompt.split(\"\\u00b7\")\n        if len(parts) >= 2:\n            # Style is the last part, trimmed\n            style_conditioning = parts[-1].strip()\n            style_found += 1\n            \n            # Create a new ordered entry with fields in specified order\n            ordered_entry = {}\n            \n            # First: timestamp, image_path, id, poem\n            for field in [\"timestamp\", \"image_path\", \"id\", \"poem\"]:\n                if field in entry:\n                    ordered_entry[field] = entry[field]\n            \n            # Next: imageType, syntagmaType\n            for field in [\"imageType\", \"syntagmaType\"]:\n                if field in entry:\n                    ordered_entry[field] = entry[field]\n                    \n            # Then: content\n            if \"content\" in entry:\n                ordered_entry[\"content\"] = entry[\"content\"]\n                \n            # Next: cineosisFunction\n            if \"cineosisFunction\" in entry:\n                ordered_entry[\"cineosisFunction\"] = entry[\"cineosisFunction\"]\n            \n            # Then: operativeEkphrasis\n            if \"operativeEkphrasis\" in entry:\n                ordered_entry[\"operativeEkphrasis\"] = entry[\"operativeEkphrasis\"]\n            \n            # Add styleConditioning field\n            ordered_entry[\"styleConditioning\"] = style_conditioning\n            \n            # Add any remaining fields except full_prompt\n            for field, value in entry.items():\n                if field not in ordered_entry and field != \"full_prompt\":\n                    ordered_entry[field] = value\n            \n            # Add full_prompt as the last field\n            if \"full_prompt\" in entry:\n                ordered_entry[\"full_prompt\"] = entry[\"full_prompt\"]\n            \n            # Replace the original entry with the ordered one\n            timeline_data[timeline_data.index(entry)] = ordered_entry\n            \n            # Progress reporting\n            if entries_processed % 100 == 0:\n                log(f\"Processed {entries_processed} entries\")\n    \n    log(f\"Added styleConditioning field to {style_found} entries\")\n    \n    # Save the updated timeline\n    try:\n        with open(JSON_PATH, \"w\") as f:\n            json.dump(timeline_data, f, indent=2)\n        log(f\"Successfully saved updated timeline to {JSON_PATH}\")\n    except Exception as e:\n        log(f\"Error saving updated timeline: {e}\")\n        \n    # Verify the output\n    log(\"Verifying output file...\")\n    try:\n        with open(JSON_PATH, \"r\") as f:\n            verified_data = json.load(f)\n        log(f\"Verification successful. Output contains {len(verified_data)} entries.\")\n        \n        # Check how many entries have the styleConditioning field\n        style_count = sum(1 for entry in verified_data if \"styleConditioning\" in entry)\n        log(f\"\u2705 Output file contains {style_count} entries with styleConditioning field\")\n            \n    except Exception as e:\n        log(f\"Verification failed: {e}\")\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    extract_style_conditioning()\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "COMPLETE-TIMELINE_backup.json",
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "datetime"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Extract Style Conditioning Script\n\nThis script extracts the style conditioning part from each entry's full_prompt field\n(everything after the last \u00b7 character) and adds it as a new field called styleConditioning."
  },
  {
    "path": "HONEYBADGER/fix_field_order.py",
    "size": 5450,
    "lines": 153,
    "source": "#!/usr/bin/env python3\n\"\"\"\nTimeline Field Order Fixer\n\nThis script takes the SIMPLE-TOTAL-TIMELINE.json and creates a new version with consistent field ordering,\nensuring 'content' always follows 'poem' and other fields are in a standardized order.\n\"\"\"\n\nimport json\nimport os\nfrom datetime import datetime\n\n# Base directory\nBASE_DIR = \"/Users/gaia/resurrecting atlantis\"\n# Input and output paths\nINPUT_JSON_PATH = os.path.join(BASE_DIR, \"HONEYBADGER\", \"SIMPLE-TOTAL-TIMELINE.json\")\nOUTPUT_JSON_PATH = os.path.join(BASE_DIR, \"HONEYBADGER\", \"COMPLETE-TIMELINE.json\")\n\ndef log(message):\n    \"\"\"Print log message with timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"{timestamp}: {message}\")\n\ndef fix_field_order():\n    \"\"\"\n    Fix field order in timeline entries to ensure consistent structure.\n    \"\"\"\n    log(\"=== Timeline Field Order Fixer ===\")\n    \n    # Load the input JSON\n    try:\n        with open(INPUT_JSON_PATH, \"r\") as f:\n            timeline_data = json.load(f)\n    except Exception as e:\n        log(f\"Error loading input JSON: {e}\")\n        return\n        \n    log(f\"Loaded source timeline with {len(timeline_data)} entries\")\n    \n    # Fix each entry\n    fixed_count = 0\n    missing_content_fixed = 0\n    duplicates_fixed = 0\n    \n    # Track unique entries by ID and frame number\n    seen_entries = {}\n    deduplicated_timeline = []\n    \n    for entry in timeline_data:\n        shot_id = entry.get(\"id\", \"\")\n        image_path = entry.get(\"image_path\", \"\")\n        \n        # Create a unique key for the entry\n        frame_number = None\n        if image_path:\n            # Extract frame number from image path (last digit before extension)\n            parts = image_path.split(\"_\")\n            if len(parts) > 1:\n                frame = parts[-1]\n                if frame.endswith(\".png\"):\n                    frame_number = frame.replace(\".png\", \"\")\n        \n        entry_key = f\"{shot_id}_{frame_number}\"\n        \n        # Skip if we've seen this exact entry before\n        if entry_key in seen_entries:\n            duplicates_fixed += 1\n            continue\n            \n        seen_entries[entry_key] = True\n            \n        # Create a new ordered entry\n        ordered_entry = {}\n        \n        # Standard field order\n        primary_fields = [\"timestamp\", \"image_path\", \"id\", \"poem\", \"content\"]\n        \n        # Add primary fields in order\n        for field in primary_fields:\n            if field in entry:\n                ordered_entry[field] = entry[field]\n            elif field == \"content\" and field not in entry:\n                # For missing content field, add an empty one to maintain structure\n                ordered_entry[field] = \"\"\n                missing_content_fixed += 1\n        \n        # Add all remaining fields except full_prompt\n        for field, value in entry.items():\n            if field not in primary_fields and field != \"full_prompt\":\n                ordered_entry[field] = value\n                \n        # Add full_prompt as the last field if it exists\n        if \"full_prompt\" in entry:\n            ordered_entry[\"full_prompt\"] = entry[\"full_prompt\"]\n            \n        # Special fix for HT entries where poem field might be content\n        if shot_id.startswith(\"HT\") and \"poem\" in ordered_entry:\n            poem_value = ordered_entry.get(\"poem\", \"\")\n            content_value = ordered_entry.get(\"content\", \"\")\n            \n            # If poem isn't \"How To Win My Heart\" and content is empty, swap them\n            if poem_value and poem_value != \"How To Win My Heart\" and not content_value:\n                ordered_entry[\"content\"] = poem_value\n                ordered_entry[\"poem\"] = \"How To Win My Heart\"\n                \n        deduplicated_timeline.append(ordered_entry)\n        fixed_count += 1\n            \n    log(f\"Fixed field order in {fixed_count} entries\")\n    log(f\"Added missing 'content' field to {missing_content_fixed} entries\")\n    log(f\"Removed {duplicates_fixed} duplicate entries\")\n    \n    # Save the updated timeline\n    try:\n        with open(OUTPUT_JSON_PATH, \"w\") as f:\n            json.dump(deduplicated_timeline, f, indent=2)\n        log(f\"Successfully saved fixed timeline to {OUTPUT_JSON_PATH}\")\n    except Exception as e:\n        log(f\"Error saving fixed timeline: {e}\")\n        \n    # Verify the output\n    log(\"Verifying output file...\")\n    try:\n        with open(OUTPUT_JSON_PATH, \"r\") as f:\n            verified_data = json.load(f)\n        log(f\"Verification successful. Output contains {len(verified_data)} entries.\")\n        \n        # Check that content follows poem in all entries\n        content_after_poem = True\n        for entry in verified_data:\n            keys = list(entry.keys())\n            if \"poem\" in keys and \"content\" in keys:\n                poem_pos = keys.index(\"poem\") \n                content_pos = keys.index(\"content\")\n                if content_pos != poem_pos + 1:\n                    content_after_poem = False\n                    break\n                \n        if content_after_poem:\n            log(\"\u2705 Field ordering verified: 'content' consistently follows 'poem' in all entries\")\n        else:\n            log(\"\u274c Field ordering issue found: 'content' does not always follow 'poem'\")\n            \n    except Exception as e:\n        log(f\"Verification failed: {e}\")\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    fix_field_order()\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SIMPLE-TOTAL-TIMELINE.json",
      "COMPLETE-TIMELINE.json",
      "/Users/gaia/resurrecting atlantis"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "datetime"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Timeline Field Order Fixer\n\nThis script takes the SIMPLE-TOTAL-TIMELINE.json and creates a new version with consistent field ordering,\nensuring 'content' always follows 'poem' and other fields are in a standardized order."
  },
  {
    "path": "ELEPHANT/menlo_genome_pattern_tester.py",
    "size": 7723,
    "lines": 175,
    "source": "from PIL import Image, ImageDraw, ImageFont\nimport json # Added for JSON parsing\nimport os\nimport re # Still used by old code, will be removed if not needed after full refactor, but safe for now\n\n# --- Configuration ---\nJSON_DATA_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"symbolic_genome_data.json\") # Changed from GENOME_REPORT_PATH\nOUTPUT_IMAGE_NAME = \"menlo_genome_pattern.png\"\nOUTPUT_DIR = os.path.dirname(os.path.abspath(__file__))\n\nFONT_NAME_PRIMARY = \"Menlo\"\nFONT_NAME_FALLBACK = \"Courier New\" # Fallback if Menlo is not found\nFONT_SIZE = 14\nTITLE_FONT_SIZE = 18\n\nPADDING = 20\nLINE_SPACING = 5       # Vertical space between S, I, C lines\nSECTION_SPACING = 15   # Vertical space between poem blocks\nTITLE_BOTTOM_MARGIN = 8 # Space below a poem title\n\nBACKGROUND_COLOR = (30, 30, 30)    # Dark gray\nTEXT_COLOR = (220, 220, 220)     # Light gray\nTITLE_COLOR = (100, 180, 255)    # Light blue\nERROR_COLOR = (255, 100, 100)    # Light red\nBOX_BORDER_COLOR = (80, 80, 80)  # For the genome box\n\nMAX_IMAGE_WIDTH = 3000 # To prevent extremely wide images if a line is very long\n\ndef get_font(font_name, fallback_name, size):\n    \"\"\"Attempts to load the primary font, then fallback, then Pillow default.\"\"\"\n    try:\n        return ImageFont.truetype(font_name, size)\n    except IOError:\n        print(f\"Warning: Font '{font_name}' not found.\")\n        try:\n            return ImageFont.truetype(fallback_name, size)\n        except IOError:\n            print(f\"Warning: Fallback font '{fallback_name}' not found. Using Pillow's default font.\")\n            return ImageFont.load_default()\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    output_path = os.path.join(OUTPUT_DIR, OUTPUT_IMAGE_NAME)\n\n    font = get_font(FONT_NAME_PRIMARY, FONT_NAME_FALLBACK, FONT_SIZE)\n    title_font = get_font(FONT_NAME_PRIMARY, FONT_NAME_FALLBACK, TITLE_FONT_SIZE)\n\n    poems_data = load_genome_data_from_json(JSON_DATA_PATH)\n    if not poems_data:\n        print(\"Exiting: No data to render.\")\n        return\n\n    # Calculate image dimensions\n    total_height = PADDING * 2\n    max_line_width = 0\n\n    # Create a dummy draw object to measure text\n    dummy_img = Image.new(\"RGB\", (1, 1))\n    draw_context = ImageDraw.Draw(dummy_img)\n\n    for i, poem in enumerate(poems_data):\n        if i > 0:\n            total_height += SECTION_SPACING\n        \n        # Title height and width\n        try:\n            title_bbox = draw_context.textbbox((0,0), poem['title'], font=title_font)\n            title_width = title_bbox[2] - title_bbox[0]\n            title_height = title_bbox[3] - title_bbox[1]\n            max_line_width = max(max_line_width, title_width)\n            total_height += title_height + TITLE_BOTTOM_MARGIN\n        except UnicodeEncodeError:\n            # Handle cases where title might have unrenderable chars, though less likely\n            error_msg_bbox = draw_context.textbbox((0,0), \"[Title Error]\", font=font)\n            max_line_width = max(max_line_width, error_msg_bbox[2] - error_msg_bbox[0])\n            total_height += (error_msg_bbox[3] - error_msg_bbox[1]) + TITLE_BOTTOM_MARGIN\n\n        # Genome lines height and width\n        for line_key in ['s_line', 'i_line', 'c_line']:\n            line_content = poem[line_key]\n            try:\n                line_bbox = draw_context.textbbox((0,0), line_content, font=font)\n                line_width = line_bbox[2] - line_bbox[0]\n                line_height = line_bbox[3] - line_bbox[1]\n            except UnicodeEncodeError:\n                # This can happen if the font doesn't support some glyphs\n                # We'll draw an error message in its place later\n                error_msg = f\"[Unrenderable chars in {line_key}]\"\n                line_bbox = draw_context.textbbox((0,0), error_msg, font=font)\n                line_width = line_bbox[2] - line_bbox[0]\n                line_height = line_bbox[3] - line_bbox[1]\n            \n            max_line_width = max(max_line_width, line_width)\n            total_height += line_height\n            if line_key != 'c_line': # Add spacing for S and I lines\n                total_height += LINE_SPACING\n        total_height += LINE_SPACING # Extra spacing after C line before box bottom\n\n    img_width = min(max_line_width + PADDING * 2, MAX_IMAGE_WIDTH)\n    img_height = total_height\n\n    if img_width <= PADDING * 2 or img_height <= PADDING * 2:\n        print(\"Error: Calculated image dimensions are too small. Check parsing and font metrics.\")\n        return\n\n    image = Image.new(\"RGB\", (int(img_width), int(img_height)), BACKGROUND_COLOR)\n    draw = ImageDraw.Draw(image)\n\n    current_y = PADDING\n\n    for poem in poems_data:\n        # Draw title\n        try:\n            title_bbox = draw.textbbox((PADDING, current_y), poem['title'], font=title_font)\n            draw.text((PADDING, current_y), poem['title'], font=title_font, fill=TITLE_COLOR)\n            current_y += (title_bbox[3] - title_bbox[1]) + TITLE_BOTTOM_MARGIN\n        except UnicodeEncodeError:\n            error_msg = \"[Title Error]\"\n            error_bbox = draw.textbbox((PADDING, current_y), error_msg, font=font)\n            draw.text((PADDING, current_y), error_msg, font=font, fill=ERROR_COLOR)\n            current_y += (error_bbox[3] - error_bbox[1]) + TITLE_BOTTOM_MARGIN\n\n        # Draw genome lines (S, I, C)\n        genome_block_start_y = current_y\n        max_genome_line_width_for_box = 0\n\n        for line_key in ['s_line', 'i_line', 'c_line']:\n            line_content = poem[line_key] + f\" | {line_key[0].upper()}\"\n            try:\n                line_bbox = draw.textbbox((PADDING + 5, current_y), line_content, font=font)\n                draw.text((PADDING + 5, current_y), line_content, font=font, fill=TEXT_COLOR)\n                max_genome_line_width_for_box = max(max_genome_line_width_for_box, line_bbox[2] - line_bbox[0])\n                current_y += (line_bbox[3] - line_bbox[1]) + LINE_SPACING\n            except UnicodeEncodeError:\n                error_msg = f\"[Unrenderable chars in {line_key}]\"\n                error_bbox = draw.textbbox((PADDING + 5, current_y), error_msg, font=font)\n                draw.text((PADDING + 5, current_y), error_msg, font=font, fill=ERROR_COLOR)\n                max_genome_line_width_for_box = max(max_genome_line_width_for_box, error_bbox[2] - error_bbox[0])\n                current_y += (error_bbox[3] - error_bbox[1]) + LINE_SPACING\n        \n        # Draw box around the genome lines\n        box_padding = 5\n        draw.rectangle([\n            PADDING - box_padding,\n            genome_block_start_y - box_padding,\n            PADDING + max_genome_line_width_for_box + 5 + box_padding, # +5 for the initial offset of text\n            current_y - LINE_SPACING + box_padding # Adjust to enclose last line properly\n        ], outline=BOX_BORDER_COLOR, width=1)\n\n        current_y += SECTION_SPACING - LINE_SPACING # Adjust for next section\n\n    try:\n        image.save(output_path)\n        print(f\"Successfully generated image: {output_path}\")\n    except Exception as e:\n        print(f\"Error saving image: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "symbolic_genome_data.json",
      "menlo_genome_pattern.png"
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "json",
      "os",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "ELEPHANT/render_cards_clapperboard.py",
    "size": 9929,
    "lines": 278,
    "source": "#!/usr/bin/env python3\n\"\"\"\nRender Timeline Data Cards as Clapperboard-style Headers\n\nThis script renders timeline entries with a structured clapperboard-style header\nthat sits above a 16:9 image area, creating an overall 3:2 aspect ratio.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants (based on user requirements)\nASPECT_16_9 = 16/9  # Image area\nASPECT_3_2 = 3/2    # Overall frame\n\n# Calculate dimensions to maintain proper aspect ratios\nBASE_WIDTH = 1200\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height of the 16:9 image area\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)   # Total height for 3:2 aspect ratio\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT   # Height of the header area\n\n# Card dimensions\nCARD_WIDTH = BASE_WIDTH\nCARD_HEIGHT = TOTAL_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nDARK_GRAY = (40, 40, 40)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\nAMBER = (255, 191, 0)\nTEAL = (0, 128, 128)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(SCRIPT_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths (system fonts - we'll use these initially)\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\" \n\n# Define glyphs for different types\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card with a clapperboard-style header above a 16:9 image area.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (CARD_WIDTH+8, CARD_HEIGHT+8), BLUE_BORDER)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (CARD_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font_lg = find_font(HEADER_FONT_PATH, 32)\n    header_font = find_font(HEADER_FONT_PATH, 28)\n    body_font = find_font(BODY_FONT_PATH, 24)\n    symbol_font = find_font(SYMBOL_FONT_PATH, 28)\n    \n    # Fill the header area black\n    draw.rectangle([(0, 0), (CARD_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    \n    # Fill the image area light gray (placeholder)\n    draw.rectangle([(0, HEADER_HEIGHT), (CARD_WIDTH, CARD_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Add text to the image area (placeholder)\n    draw.text((CARD_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=header_font, fill=BLACK, anchor=\"mm\")\n    \n    # Draw horizontal dividing line\n    draw.line([(0, HEADER_HEIGHT), (CARD_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # Layout the header like a clapperboard\n    \n    # Top row - ID and timestamp\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = \"FRM: (---)\"\n    \n    top_y = 20\n    draw.text((40, top_y), id_text, font=header_font_lg, fill=WHITE)\n    draw.text((400, top_y), time_text, font=header_font_lg, fill=WHITE)\n    draw.text((800, top_y), frame_text, font=header_font_lg, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, top_y+40), (CARD_WIDTH, top_y+40)], fill=WHITE, width=1)\n    \n    # Second row - Poem and Line\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_text = f\"LINE: \\\"{entry.get('content', '---')}\\\"\"\n    \n    row2_y = top_y + 50\n    draw.text((40, row2_y), poem_text, font=header_font, fill=WHITE)\n    draw.text((500, row2_y), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, row2_y+40), (CARD_WIDTH, row2_y+40)], fill=WHITE, width=1)\n    \n    # Third row - Syntagma Type, Image Type, and Cineosis Function\n    # This row needs careful alignment and proper spacing\n    syntagma_type = entry.get('syntagmaType', '---')\n    image_type = entry.get('imageType', '---')\n    cineosis_func = entry.get('cineosisFunction', '---')\n    \n    syntagma_glyph = get_glyph(SYNTAGMA_GLYPHS, syntagma_type, \"\u25a1\")\n    image_glyph = get_glyph(IMAGE_GLYPHS, image_type, \"\u25a1\")\n    cineosis_glyph = get_glyph(CINEOSIS_GLYPHS, cineosis_func, \"\u25ef\")\n    \n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # Split the row into clear sections\n    row3_y = row2_y + 50\n    \n    # Section 1: Syntagma Type - First 33% of width\n    draw.text((40, row3_y), f\"SY: {syntagma_glyph} {syntagma_abbrev}\", font=header_font, fill=WHITE)\n    \n    # Section 2: Image Type - Middle 33% of width\n    image_text = f\"IT: {image_glyph} {image_type.split('-')[0].upper()}\"\n    draw.text((CARD_WIDTH//3 + 40, row3_y), image_text, font=header_font, fill=WHITE)\n    \n    # Section 3: Cineosis Function - Last 33% of width\n    cf_text = f\"CF: {cineosis_glyph}\"\n    cf_name = cineosis_func.split(' ')[0].upper()\n    if len(cineosis_func.split(' ')) > 1:\n        cf_name += \" \" + cineosis_func.split(' ')[1].upper()\n    draw.text((2*CARD_WIDTH//3 + 40, row3_y), cf_text + \" \" + cf_name, font=header_font, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, row3_y+40), (CARD_WIDTH, row3_y+40)], fill=WHITE, width=1)\n    \n    # Fourth row - Raw prompt\n    prompt_label = \"RAW PROMPT (MACHINE INPUT):\"\n    \n    row4_y = row3_y + 50\n    draw.rectangle([(0, row4_y-10), (CARD_WIDTH, row4_y+30)], fill=LIGHT_GRAY)\n    draw.text((40, row4_y), prompt_label, font=header_font, fill=BLACK)\n    \n    # Format the prompt (shortened to fit in the remaining header space)\n    prompt_text = f\"{syntagma_abbrev} \u00b7 {cineosis_func} \u00b7 {entry.get('operativeEkphrasis', '---')}\"\n    \n    # Truncate the prompt if it's too long\n    if len(prompt_text) > 90:\n        prompt_text = prompt_text[:87] + \"...\"\n    \n    # Draw the prompt text\n    draw.text((40, row4_y+40), prompt_text, font=body_font, fill=WHITE)\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (4, 4))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_card.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    \n    # Render first 10 entries as samples\n    render_all_entries(timeline, OUTPUT_DIR, 1)\n    \n    # Also render a demo card showing the layout with dimensions\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"DEMO\",\n        \"timestamp\": \"00:00:00\",\n        \"poem\": \"Layout Example\",\n        \"content\": \"3:2 overall with 16:9 image\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"This is a demo card showing the layout structure with proper aspect ratios\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_layout.png\")\n    render_card(demo_entry, demo_path)\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {CARD_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {CARD_WIDTH}x{CARD_HEIGHT}px (3:2)\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_card.png",
      "demo_layout.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants (based on user requirements)\nASPECT_16_9 = 16/9  # Image area\nASPECT_3_2 = 3/2    # Overall frame\n\n# Calculate dimensions to maintain proper aspect ratios\nBASE_WIDTH = 1200\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height of the 16:9 image area\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)   # Total height for 3:2 aspect ratio\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT   # Height of the header area\n\n# Card dimensions\nCARD_WIDTH = BASE_WIDTH\nCARD_HEIGHT = TOTAL_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nDARK_GRAY = (40, 40, 40)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\nAMBER = (255, 191, 0)\nTEAL = (0, 128, 128)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(SCRIPT_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ", (CARD_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font_lg = find_font(HEADER_FONT_PATH, 32)\n    header_font = find_font(HEADER_FONT_PATH, 28)\n    body_font = find_font(BODY_FONT_PATH, 24)\n    symbol_font = find_font(SYMBOL_FONT_PATH, 28)\n    \n    # Fill the header area black\n    draw.rectangle([(0, 0), (CARD_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    \n    # Fill the image area light gray (placeholder)\n    draw.rectangle([(0, HEADER_HEIGHT), (CARD_WIDTH, CARD_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Add text to the image area (placeholder)\n    draw.text((CARD_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), ",
      "\n    draw.text((CARD_WIDTH//3 + 40, row3_y), image_text, font=header_font, fill=WHITE)\n    \n    # Section 3: Cineosis Function - Last 33% of width\n    cf_text = f",
      ")[1].upper()\n    draw.text((2*CARD_WIDTH//3 + 40, row3_y), cf_text + ",
      "Rendered {i+1}/{len(entries_to_render)} cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Render Timeline Data Cards as Clapperboard-style Headers\n\nThis script renders timeline entries with a structured clapperboard-style header\nthat sits above a 16:9 image area, creating an overall 3:2 aspect ratio."
  },
  {
    "path": "ELEPHANT/clapper_genome_exp.py",
    "size": 22657,
    "lines": 434,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER_GENOME_EXP - Experimental Clapperboard with Genome Report Display\n- Adds a top section to display content from SYMBOLIC_GENOME_REPORT.md.\n- Initially displays a fixed portion of the genome report (first poem's genome).\n- Based on CLAPPER33 (glyphs, color-coded prompts).\n- Processes a limited batch of cards for faster testing.\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(BASE_DIR, \"COMPLETE-TIMELINE.json\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_genome_exp\")\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR))\nGENOME_REPORT_PATH = os.path.join(BASE_DIR, \"SYMBOLIC_GENOME_REPORT.md\")\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Global variable to store the pre-loaded genome report lines\nPRELOADED_GENOME_LINES = []\n\ndef load_fixed_genome_report_snippet():\n    \"\"\"Loads the first poem's genome block from the report file.\"\"\"\n    global PRELOADED_GENOME_LINES\n    try:\n        with open(GENOME_REPORT_PATH, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        # Regex to find the first genome block (S, I, C lines)\n        # This looks for the start marker, captures the three lines, and stops at the end marker.\n        match = re.search(r\"\\u250c[\\u2500\\u252c]+\\u2510\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 S\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 I\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 C\\s*\\n\\s*\\u2514\", content, re.MULTILINE)\n        if match:\n            PRELOADED_GENOME_LINES = [\n                match.group(1).strip() + \" S\", \n                match.group(2).strip() + \" I\", \n                match.group(3).strip() + \" C\"\n            ]\n        else:\n            PRELOADED_GENOME_LINES = [\"Genome report snippet not found.\", \"Please check GENOME_REPORT_PATH and regex.\", \"\"]\n            print(f\"Warning: Could not find genome block in {GENOME_REPORT_PATH}\")\n            \n    except FileNotFoundError:\n        PRELOADED_GENOME_LINES = [\"SYMBOLIC_GENOME_REPORT.md not found.\", \"\", \"\"]\n        print(f\"Error: {GENOME_REPORT_PATH} not found.\")\n    except Exception as e:\n        PRELOADED_GENOME_LINES = [f\"Error loading genome report: {e}\", \"\", \"\"]\n        print(f\"Error loading genome report: {e}\")\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        # Attempt to find a bold version. OS/font specific. Common: \"FontName Bold.ttf\"\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n    except IOError:\n        try:\n            return ImageFont.truetype(font_name_to_try + \".otf\", size)\n        except IOError:\n            if preferred_name != fallback_name : # Avoid redundant warnings if preferred is already fallback\n                print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n            \n            fallback_font_name_to_try = fallback_name\n            if is_bold_fallback:\n                 fallback_font_name_to_try = fallback_name + \" Bold\"\n            try:\n                return ImageFont.truetype(fallback_font_name_to_try + \".ttf\", size)\n            except IOError:\n                try:\n                    return ImageFont.truetype(fallback_font_name_to_try + \".otf\", size)\n                except IOError:\n                    print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef render_card(entry, output_path, fonts):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    genome_line_height_small = genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1] + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    genome_line_height_tiny = genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1] + 1\n\n    total_genome_text_height_small = len(PRELOADED_GENOME_LINES) * genome_line_height_small\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = genome_line_height_small\n\n    if total_genome_text_height_small > GENOME_REPORT_HEIGHT - 8: # 8 for padding\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = genome_line_height_tiny\n        total_genome_text_height_tiny = len(PRELOADED_GENOME_LINES) * genome_line_height_tiny\n        if total_genome_text_height_tiny > GENOME_REPORT_HEIGHT -8:\n            print(f\"Warning: Genome text still too tall for {entry.get('id')} even with tiny font.\")\n\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - (len(PRELOADED_GENOME_LINES) * chosen_genome_line_height)) // 2 # Center vertically\n    for line in PRELOADED_GENOME_LINES:\n        # Attempt to center text, or left align if too wide\n        line_width = chosen_genome_font.getbbox(line)[2]\n        line_x = 10\n        if line_width < BASE_WIDTH - 20:\n            line_x = (BASE_WIDTH - line_width) // 2\n        draw.text((line_x, genome_y_start), line, font=chosen_genome_font, fill=WHITE)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, IMAGE TYPE, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    image_text = f\"IMAGE: {image_glyph} {image_type_str}\"\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), image_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. POEM AND LINE CONTENT ROW\n    poem_line_content_y = current_y_offset\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    draw.text((line_start_x, poem_line_content_y), line_prefix, font=header_font, fill=WHITE)\n    \n    line_content_start_x = line_start_x + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_start_x - 10\n    \n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_start_x, poem_line_content_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER_GENOME_EXP card generation...\")\n    load_fixed_genome_report_snippet() # Load the genome snippet once\n\n    fonts = {\n        'header': get_font(\"DejaVu Sans Mono\", \"Courier New\", 16, is_bold_preferred=True, is_bold_fallback=True),\n        'text':   get_font(\"DejaVu Sans Mono\", \"Courier New\", 16),\n        'text_bold': get_font(\"DejaVu Sans Mono\", \"Courier New\", 16, is_bold_preferred=True, is_bold_fallback=True),\n        'genome_small': get_font(\"DejaVu Sans Mono\", \"Courier New\", 12),\n        'genome_tiny':  get_font(\"DejaVu Sans Mono\", \"Courier New\", 10)\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_filename = f\"{entry_id}_genome_exp.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        render_card(entry, output_path, fonts)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {OUTPUT_DIR}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_genome_exp.png",
      ")\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        # Attempt to find a bold version. OS/font specific. Common: ",
      ")\n\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - (len(PRELOADED_GENOME_LINES) * chosen_genome_line_height)) // 2 # Center vertically\n    for line in PRELOADED_GENOME_LINES:\n        # Attempt to center text, or left align if too wide\n        line_width = chosen_genome_font.getbbox(line)[2]\n        line_x = 10\n        if line_width < BASE_WIDTH - 20:\n            line_x = (BASE_WIDTH - line_width) // 2\n        draw.text((line_x, genome_y_start), line, font=chosen_genome_font, fill=WHITE)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, IMAGE TYPE, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = ",
      ":\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER_GENOME_EXP - Experimental Clapperboard with Genome Report Display\n- Adds a top section to display content from SYMBOLIC_GENOME_REPORT.md.\n- Initially displays a fixed portion of the genome report (first poem's genome).\n- Based on CLAPPER33 (glyphs, color-coded prompts).\n- Processes a limited batch of cards for faster testing."
  },
  {
    "path": "ELEPHANT/clapper32.py",
    "size": 21412,
    "lines": 449,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER32 - Film Clapperboard Style with Color-Coded Prompts (from clapper31)\n- Image integration from image_path field in timeline data \n- Poem and Line Content on same line\n- Updated header labels: \"IMAGE\" and \"FRAME\"\n- Color-coded prompt components: syntagma, cineosis, ekphrasis (bold amber), style.\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\n\n# Constants\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 200  # Increased height for text\nCARD_HEIGHT = HEADER_HEIGHT + 576  # 576 is the height for a 16:9 aspect ratio of 1024 width\nIMAGE_HEIGHT = 576  # 16:9 aspect ratio\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\n# Colors for different prompt components - from clapper31, for readability\nSYNTAGMA_COLOR = (100, 200, 255)  # Brighter blue\nCINEOSIS_COLOR = (230, 150, 255)  # Lighter purple\nEKPHRASIS_COLOR = AMBER           # Same as LINE color (amber), for operative ekphrasis\nSTYLE_COLOR = (150, 255, 180)     # Lighter green\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(BASE_DIR, \"COMPLETE-TIMELINE.json\") # Using the complete timeline with image paths\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper32\")\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Base directory for relative image paths\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Helper function to wrap text\ndef wrap_text(text, font, max_width):\n    \"\"\"Wrap text to fit within a given width.\"\"\"\n    words = text.split()\n    wrapped_lines = []\n    current_line = []\n    \n    for word in words:\n        # Test with current word added\n        test_line = ' '.join(current_line + [word])\n        line_width = font.getbbox(test_line)[2]\n        \n        if line_width <= max_width:\n            current_line.append(word)\n        else:\n            # If the current line has words, complete it\n            if current_line:\n                wrapped_lines.append(' '.join(current_line))\n                current_line = [word]\n            else:\n                # If the word itself is too long, force it on its own line\n                wrapped_lines.append(word)\n                current_line = []\n    \n    # Add the last line if there's anything left\n    if current_line:\n        wrapped_lines.append(' '.join(current_line))\n    \n    return wrapped_lines\n\n# Helper function to draw colored text and return the new position (adapted from clapper31)\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    \"\"\"Draw text with specified color and font, handling wrapping. text_font_for_metrics is used for width calculation if provided.\"\"\"\n    if not text or text.strip() == '---' or text.strip() == '':  # Also check for placeholder\n        return y\n\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n\n    words = text.split()\n    current_line = []\n    current_width = 0\n    start_x = x\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line else 0\n\n        if max_width is not None and current_line and current_width + word_width + space_width > max_width:\n            line_text = ' '.join(current_line)\n            draw.text((start_x, y), line_text, font=font, fill=color)\n            y += 16  # Move to next line (assuming 16px line height)\n            current_line = [word]\n            current_width = word_width\n        else:\n            current_line.append(word)\n            current_width += word_width + space_width\n\n    if current_line:\n        line_text = ' '.join(current_line)\n        draw.text((start_x, y), line_text, font=font, fill=color)\n        y += 16  # Add space after this component\n\n    return y\n\ndef get_image_path(entry):\n    \"\"\"Get the image path from the entry data or return None.\"\"\"\n    # Check if image_path exists in the entry\n    if 'image_path' in entry and entry['image_path']:\n        # Construct absolute path from relative path in the timeline data\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        # Try TIGER directory as alternative\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    \n    # Fallback: search in TIGER directory by ID\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        # Try to find any file with ID prefix\n        for root, _, files in os.walk(base_dir):\n            for file in files:\n                if file.startswith(entry_id + '__') and file.lower().endswith('.png'):\n                    return os.path.join(root, file)\n    \n    return None\n\ndef render_card(entry, output_path):\n    \"\"\"Render a single data card with the entry information.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH+8, CARD_HEIGHT+8), BLUE_BORDER)\n    \n    # Create inner black image (main card)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    \n    draw = ImageDraw.Draw(img)\n    \n    # Load fonts\n    try:\n        header_font = ImageFont.truetype(\"Courier New Bold.ttf\", 16)  \n        text_font = ImageFont.truetype(\"Courier New.ttf\", 16)\n    except IOError:\n        # Fallback to default font if Courier New is not available\n        header_font = ImageFont.load_default()\n        text_font = ImageFont.load_default()\n        print(\"Warning: Courier New font not found, using default font.\")\n    \n    top_row_height = 30  # Height of the top row\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    \n    # Get frame position information if available\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    \n    # Image type without the frame number\n    image_type = entry.get('imageType', 'Descriptive Image')\n    image_text = f\"IMAGE: {image_type}\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), image_text, font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(4, top_row_height), (BASE_WIDTH+4, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line with proper wrapping for long content\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    \n    # Calculate poem width\n    poem_width = header_font.getbbox(poem_text)[2]\n    \n    # Draw poem name\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # LINE CONTENT: Highlighted with amber to give it emphasis\n    # Calculate start position for line content (after poem with some spacing)\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)  # Either after poem or at half width\n    \n    # Draw the line content with amber highlighting\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    # Draw prefix in white\n    draw.text((line_start_x, top_row_height + 8), line_prefix, font=header_font, fill=WHITE)\n    \n    # Available width for line content\n    available_width = BASE_WIDTH - line_start_x - line_prefix_width - 20\n    \n    # Wrap the line content if needed\n    wrapped_content = wrap_text(line_content, header_font, available_width)\n    \n    for i, line_part in enumerate(wrapped_content):\n        y_pos = top_row_height + 8 + (i * 16)\n        \n        # Add closing quote to last line\n        if i == len(wrapped_content) - 1:\n            line_part += \"\\\"\"\n        \n        # Draw the line content in amber\n        draw.text((line_start_x + line_prefix_width, y_pos), line_part, font=header_font, fill=AMBER)\n    \n    # Calculate actual height used by the line content\n    line_content_actual_height = 20 + max(1, len(wrapped_content)) * 16  # Base + line height\n    \n    # Draw horizontal divider after line content\n    divider_y = top_row_height + line_content_actual_height\n    draw.line([(4, divider_y), (BASE_WIDTH+4, divider_y)], fill=WHITE, width=1)\n    \n    # PROMPT ROW: Color-coded components (adapted from clapper31)\n    try:\n        bold_font = ImageFont.truetype(\"Courier New Bold.ttf\", 16)\n    except IOError:\n        bold_font = header_font # Fallback to header_font if bold not available\n\n    # Extract or construct prompt components\n    if 'full_prompt' in entry and entry['full_prompt']:\n        parts = entry['full_prompt'].split(' \u00b7 ')\n        if len(parts) >= 4:\n            syntagma_abbr = parts[0]\n            operative_ekphrasis = parts[2] # Ekphrasis is 3rd part in original construction\n            cineosis_function = parts[1]   # Cineosis is 2nd part\n            style_conditioning = parts[3]\n        elif len(parts) == 3: # Assuming older format: syntagma, ekphrasis, style\n            syntagma_abbr = parts[0]\n            operative_ekphrasis = parts[1]\n            cineosis_function = entry.get('cineosisFunction', '---') # Get from field if missing\n            style_conditioning = parts[2]\n        else: # Fallback if parts aren't properly separated or too few\n            syntagma_type_full = entry.get('syntagmaType', '')\n            if '(' in syntagma_type_full and ')' in syntagma_type_full:\n                syntagma_abbr = syntagma_type_full.split('(')[1].split(')')[0]\n            else: syntagma_abbr = ''.join(word[0] for word in syntagma_type_full.split() if word) or 'SS'\n            operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n            cineosis_function = entry.get('cineosisFunction', '---')\n            style_conditioning = entry.get('styleConditioning', '---')\n    else:\n        syntagma_type_full = entry.get('syntagmaType', '')\n        if '(' in syntagma_type_full and ')' in syntagma_type_full:\n            syntagma_abbr = syntagma_type_full.split('(')[1].split(')')[0]\n        else: syntagma_abbr = ''.join(word[0] for word in syntagma_type_full.split() if word) or 'SS'\n        operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n        cineosis_function = entry.get('cineosisFunction', '---')\n        style_conditioning = entry.get('styleConditioning', '---')\n\n    current_y = divider_y + 8\n    left_margin = 10\n    prompt_max_width = BASE_WIDTH - (2 * left_margin)\n\n    # Draw PROMPT: label\n    draw.text((left_margin, current_y), \"PROMPT:\", font=header_font, fill=WHITE)\n    current_y += 18 # Give space after PROMPT: label\n\n    # Draw Syntagma Abbreviation\n    current_y = draw_colored_text(draw, f\"Syntagma: {syntagma_abbr}\", left_margin, current_y, SYNTAGMA_COLOR, text_font, prompt_max_width, text_font_for_metrics=text_font)\n    # Draw Cineosis Function\n    current_y = draw_colored_text(draw, f\"Cineosis: {cineosis_function}\", left_margin, current_y, CINEOSIS_COLOR, text_font, prompt_max_width, text_font_for_metrics=text_font)\n    # Draw Operative Ekphrasis (bold amber)\n    current_y = draw_colored_text(draw, f\"Ekphrasis: {operative_ekphrasis}\", left_margin, current_y, EKPHRASIS_COLOR, bold_font, prompt_max_width, text_font_for_metrics=bold_font)\n    # Draw Style Conditioning\n    current_y = draw_colored_text(draw, f\"Style: {style_conditioning}\", left_margin, current_y, STYLE_COLOR, text_font, prompt_max_width, text_font_for_metrics=text_font)\n\n    prompt_actual_height = current_y - (divider_y + 8) # Total height of the prompt section content\n\n    # Calculate Y offset for the image, ensuring it's below all text\n    image_y_offset = divider_y + prompt_actual_height + 4 # Small padding after prompt section\n    \n    # Ensure image_y_offset is at least HEADER_HEIGHT, or if text is too long, cap it.\n    if image_y_offset < HEADER_HEIGHT:\n        image_y_offset = HEADER_HEIGHT\n    elif image_y_offset > CARD_HEIGHT - IMAGE_HEIGHT - 4: # Cap to prevent image going off card\n        image_y_offset = CARD_HEIGHT - IMAGE_HEIGHT - 4 \n        # print(f\"Warning: Text content for entry {entry.get('id')} too long, image might be clipped or prompt truncated.\")\n\n\n    # Get the actual image file path from the entry\n    image_path = get_image_path(entry)\n    \n    if image_path and os.path.exists(image_path):\n        try:\n            card_image = Image.open(image_path)\n            # Resize image to fit BASE_WIDTH x IMAGE_HEIGHT while maintaining aspect ratio\n            card_image.thumbnail((BASE_WIDTH, IMAGE_HEIGHT), Image.Resampling.LANCZOS)\n            \n            # Create a black background for the image area\n            img_bg = Image.new('RGB', (BASE_WIDTH, IMAGE_HEIGHT), BLACK)\n            \n            # Calculate position to center the image\n            pos_x = (BASE_WIDTH - card_image.width) // 2\n            pos_y = (IMAGE_HEIGHT - card_image.height) // 2\n            \n            img_bg.paste(card_image, (pos_x, pos_y))\n            img.paste(img_bg, (4, image_y_offset + 4)) # Paste into main image, considering border\n        except Exception as e:\n            print(f\"Error loading or processing image {image_path} for entry {entry.get('id', 'N/A')}: {e}\")\n            # Draw a placeholder if image fails to load\n            draw.rectangle([4, image_y_offset + 4, BASE_WIDTH + 4, image_y_offset + IMAGE_HEIGHT + 4], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_y_offset + IMAGE_HEIGHT // 2), \"Image Error\", fill=BLACK, font=header_font)\n    else:\n        # Draw a placeholder if no image path or image doesn't exist\n        draw.rectangle([4, image_y_offset + 4, BASE_WIDTH + 4, image_y_offset + IMAGE_HEIGHT + 4], fill=LIGHT_GRAY)\n        no_image_text = \"No Image Available\"\n        text_bbox = draw.textbbox((0,0), no_image_text, font=header_font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        draw.text(((BASE_WIDTH - text_width) // 2 + 4, image_y_offset + (IMAGE_HEIGHT - text_height) // 2 + 4), no_image_text, fill=BLACK, font=header_font)\n\n    # Save the image\n    img.save(output_path)\n    # print(f\"Rendered card: {output_path}\")\n\ndef render_demo_card():\n    \"\"\"Render a demo card with sample data.\"\"\"\n    demo_entry = {\n        \"id\": \"DEMO001\",\n        \"timestamp\": \"00:00:00\",\n        \"frame_position\": 1,\n        \"frame_total\": 1,\n        \"imageType\": \"Synthesized Image\",\n        \"poem\": \"The Great Work\",\n        \"content\": \"This is a demonstration line of poetic text for the clapperboard card.\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"operativeEkphrasis\": \"A lone figure stands on a neon-lit street corner, rain reflecting the city lights.\",\n        \"cineosisFunction\": \"Establishing Shot - Urban Despair\",\n        \"styleConditioning\": \"Shot on Kodak Vision3 500T, anamorphic lens, moody, atmospheric, Blade Runner aesthetic\",\n        \"image_path\": None # No image for demo\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_clapper32.png\")\n    render_card(demo_entry, demo_output_path)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\n\ndef render_extreme_test_case():\n    \"\"\"Render a test card with the longest entries to test layout boundaries.\"\"\"\n    pass # Ensure correct indentation for the first line of the function body\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        # Create a dummy longest entry if file not found\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Very Long Image Type Description That Pushes Boundaries\",\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Ultra Detailed Complex Syntagma Type (UDCST)\",\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista of an alien megacity at twilight, with towering, bio-luminescent skyscrapers, intricate sky-bridges teeming with exotic vehicles, and a sky filled with multiple moons and nebulae. The level of detail is immense, capturing every nuance of this fantastical urban landscape, from the smallest architectural features to the grand scale of the celestial backdrop. This description aims to be as long as possible to truly stress test the system.\",\n            \"cineosisFunction\": \"Establishing Shot - Grand Scale Alien Metropolis - Introduction to World Setting and Atmosphere - Evoking Wonder and Awe\",\n            \"styleConditioning\": \"Rendered in Unreal Engine 5 with Lumen and Nanite, 8K resolution, cinematic depth of field, volumetric lighting, ray-traced reflections and shadows, color graded with a rich, vibrant palette, inspired by the works of Syd Mead and Moebius, aiming for photorealistic yet fantastical quality.\",\n            \"image_path\": None\n        }\n    else:\n        with open(longest_entry_path, 'r') as f:\n            longest_entry = json.load(f)\n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_CLAPPER32.png\")\n    render_card(longest_entry, output_path)\n    print(f\"Rendered extreme test card: {output_path}\")\n\n\ndef calculate_frame_counts(timeline_data):\n    \"\"\"Calculate the total number of frames per poem and frame position for each entry.\"\"\"\n    poem_frames = {}\n    # First pass: count total frames for each poem\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name:\n            poem_frames[poem_name] = poem_frames.get(poem_name, 0) + 1\n            \n    # Second pass: assign frame_total and frame_position\n    current_poem_counts = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name:\n            entry['frame_total'] = poem_frames[poem_name]\n            current_poem_counts[poem_name] = current_poem_counts.get(poem_name, 0) + 1\n            entry['frame_position'] = current_poem_counts[poem_name]\n        else:\n            # Handle entries without a poem name (e.g., global metadata)\n            entry['frame_total'] = 0\n            entry['frame_position'] = 0\n    return timeline_data\n\ndef main():\n    \"\"\"Main function to load timeline and render all cards.\"\"\"\n    # Load timeline data\n    if not os.path.exists(TIMELINE_PATH):\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\")\n        # Create a dummy timeline if not found\n        timeline_data = [\n            {\n                \"id\": \"DUMMY001\", \"timestamp\": \"00:00:10\", \"imageType\": \"Placeholder Image\",\n                \"poem\": \"Dummy Poem\", \"content\": \"This is a line from a dummy poem.\",\n                \"syntagmaType\": \"Generic Syntagma (GS)\", \"operativeEkphrasis\": \"A simple placeholder scene.\",\n                \"cineosisFunction\": \"Filler\", \"styleConditioning\": \"Basic style\", \"image_path\": None\n            },\n            {\n                \"id\": \"DUMMY002\", \"timestamp\": \"00:00:20\", \"imageType\": \"Another Placeholder\",\n                \"poem\": \"Dummy Poem\", \"content\": \"Another line from the same dummy poem.\",\n                \"syntagmaType\": \"Generic Syntagma (GS)\", \"operativeEkphrasis\": \"A different placeholder scene.\",\n                \"cineosisFunction\": \"More Filler\", \"styleConditioning\": \"Slightly different basic style\", \"image_path\": None\n            }\n        ]\n        print(\"Using dummy timeline data as fallback.\")\n    else:\n        with open(TIMELINE_PATH, 'r') as f:\n            timeline_data = json.load(f)\n\n    # Calculate frame counts\n    timeline_data = calculate_frame_counts(timeline_data)\n\n    # Render demo card\n    # render_demo_card()  # Commented out for single image test\n    \n    # Render extreme test card\n    # render_extreme_test_case() # Commented out for single image test\n\n    # Render cards for each entry in the timeline\n    total_entries = len(timeline_data)\n    if total_entries > 0:\n        entry = timeline_data[0] # Process only the first entry\n        i = 0 # for logging consistency if used\n        entry_id = entry.get('id', f'unknown_id_{i+1}')\n        output_filename = f\"{entry_id}_clapper32.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        \n        # print(f\"Rendering card {i+1}/{total_entries}: {output_filename}\") # Verbose logging\n        render_card(entry, output_path)\n    \n    if total_entries > 0:\n        print(f\"\\nFirst card rendered successfully in {OUTPUT_DIR}\")\n    else:\n        print(\"No entries in timeline to render.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "_DEMO_CARD_clapper32.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_CLAPPER32.png",
      "{entry_id}_clapper32.png",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    \n    # Calculate poem width\n    poem_width = header_font.getbbox(poem_text)[2]\n    \n    # Draw poem name\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # LINE CONTENT: Highlighted with amber to give it emphasis\n    # Calculate start position for line content (after poem with some spacing)\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)  # Either after poem or at half width\n    \n    # Draw the line content with amber highlighting\n    line_prefix = ",
      ", (BASE_WIDTH, IMAGE_HEIGHT), BLACK)\n            \n            # Calculate position to center the image\n            pos_x = (BASE_WIDTH - card_image.width) // 2\n            pos_y = (IMAGE_HEIGHT - card_image.height) // 2\n            \n            img_bg.paste(card_image, (pos_x, pos_y))\n            img.paste(img_bg, (4, image_y_offset + 4)) # Paste into main image, considering border\n        except Exception as e:\n            print(f",
      "N/A",
      ")\n            # Draw a placeholder if image fails to load\n            draw.rectangle([4, image_y_offset + 4, BASE_WIDTH + 4, image_y_offset + IMAGE_HEIGHT + 4], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_y_offset + IMAGE_HEIGHT // 2), ",
      "\n        text_bbox = draw.textbbox((0,0), no_image_text, font=header_font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        draw.text(((BASE_WIDTH - text_width) // 2 + 4, image_y_offset + (IMAGE_HEIGHT - text_height) // 2 + 4), no_image_text, fill=BLACK, font=header_font)\n\n    # Save the image\n    img.save(output_path)\n    # print(f",
      "Rendering card {i+1}/{total_entries}: {output_filename}"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER32 - Film Clapperboard Style with Color-Coded Prompts (from clapper31)\n- Image integration from image_path field in timeline data \n- Poem and Line Content on same line\n- Updated header labels: \"IMAGE\" and \"FRAME\"\n- Color-coded prompt components: syntagma, cineosis, ekphrasis (bold amber), style."
  },
  {
    "path": "ELEPHANT/TRUNK/clapper62singleshot.py",
    "size": 40034,
    "lines": 778,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER62SINGLESHOT - Generates images for the longest poem for video creation.\n- Uses UPDATED-TIMELINE.json from the HONEYBADGER directory.\n- Identifies the longest poem by number of entries (frames).\n- Renders all cards for that specific poem.\n- Outputs images to HONEYBADGER/LONGEST_POEM_OUTPUT/\n- Based on clapper61prime.py.\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES - Mapped from discovered data to glyphs.md symbols\nSYNTAGMA_GLYPHS = {\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (CS)\": \"\u2756\", # Data uses \"CS\", glyphs.md uses \"XS\" for \u2756\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n    # Explicitly add other known syntagma types from glyphs.md for completeness, even if not in current data sample\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    # Image types found in syntagma data (e.g., \"Action-Image\") will correctly get '?'\n}  \n\n# IMAGE TYPES - Mapped from discovered data to glyphs.md symbols\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n    # Any other image types from data not listed here will get '?'\n}\n\n# CINEOSIS FUNCTIONS - Mapped from discovered data to glyphs.md symbols\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n    # Any other cineosis functions from data not listed here will get '?'\n}  \n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n# TIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nTIMELINE_PATH = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), \"HONEYBADGER\", \"UPDATED-TIMELINE.json\") # Correctly points to HONEYBADGER/UPDATED-TIMELINE.json\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.normpath(os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), \"HONEYBADGER\", \"LONGEST_POEM_OUTPUT\"))\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef discover_unique_prompt_values(timeline_data_path):\n    print(f\"\\nDiscovering unique prompt values from: {timeline_data_path}\")\n    try:\n        with open(timeline_data_path, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline data file not found at {timeline_data_path}\")\n        return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {timeline_data_path}\")\n        return\n\n    unique_image_types = set()\n    unique_syntagma_types = set()\n    unique_cineosis_functions = set()\n\n    for entry in data:\n        image_type = entry.get('imageType')\n        if image_type is not None:\n            unique_image_types.add(image_type)\n        \n        syntagma_type = entry.get('syntagmaType')\n        if syntagma_type is not None:\n            unique_syntagma_types.add(syntagma_type)\n\n        cineosis_function = entry.get('cineosisFunction')\n        if cineosis_function is not None:\n            unique_cineosis_functions.add(cineosis_function)\n\n    print(\"\\n--- Unique Image Types ---\")\n    for item in sorted(list(unique_image_types)):\n        print(item) # Simplified print\n    \n    print(\"\\n--- Unique Syntagma Types ---\")\n    for item in sorted(list(unique_syntagma_types)):\n        print(item) # Simplified print\n\n    print(\"\\n--- Unique Cineosis Functions ---\")\n    for item in sorted(list(unique_cineosis_functions)):\n        print(item) # Simplified print\n    print(\"\\nDiscovery complete.\")\n\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    \n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    # Construct the full path to the image\n    # Assumes entry['image_path'] is relative to the 'resurrecting atlantis' directory\n    # BASE_IMAGE_DIR is already defined as os.path.dirname(os.path.dirname(BASE_DIR))\n    \n    relative_path = entry.get('image_path')\n    if not relative_path:\n        # This case should ideally be handled by the caller (e.g., render_card)\n        # but as a safeguard:\n        print(f\"Warning: 'image_path' missing in entry for get_image_path: {entry.get('id', 'Unknown ID')}\")\n        # Returning None or an invalid path will likely cause FileNotFoundError later, which is handled.\n        return os.path.join(BASE_IMAGE_DIR, \"MISSING_IMAGE_PATH.png\") \n\n    # Specific adjustment for 'Magic ride' images located in TIGER/MR/\n    # If the original path in JSON is like \"MR/some_image.png\"\n    if relative_path.startswith(\"MR/\"):\n        # Check if poem title is 'Magic ride' for specificity, though startswith(\"MR/\") is likely sufficient given current context\n        # For now, we assume if it starts with MR/, it's for 'Magic ride' and needs TIGER prefix.\n        adjusted_relative_path = os.path.join(\"TIGER\", relative_path)\n        # print(f\"Adjusted image path from '{relative_path}' to '{adjusted_relative_path}' for an MR entry.\") # Optional debug print\n    else:\n        adjusted_relative_path = relative_path\n\n    full_image_path = os.path.join(BASE_IMAGE_DIR, adjusted_relative_path)\n    return os.path.normpath(full_image_path)\n\ndef load_genome_data_from_json(file_path):\n    # Loads poem genome data from a JSON file.\n    # Original file is a list of objects, e.g., [{\"title\": \"Poem1\", \"s_line\": \"...\", ...}, ...]\n    # This function transforms it into the expected dictionary structure:\n    # {\"Poem Title\": {\"S\": \"...\", \"I\": \"...\", \"C\": \"...\"}}\n    print(f\"Loading and transforming genome data from: {file_path}\")\n    transformed_genome_map = {}\n    try:\n        with open(file_path, 'r') as f:\n            genome_data_list = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Genome data file not found at {file_path}\")\n        return {}\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}\")\n        return {}\n\n    if not isinstance(genome_data_list, list):\n        print(f\"Error: Genome data in {file_path} is not a list of poem objects as expected.\")\n        # Attempt to handle if it's already in the old dictionary format for backward compatibility (optional)\n        if isinstance(genome_data_list, dict):\n            print(f\"Info: Genome data in {file_path} is a dictionary. Assuming it's in the target format.\")\n            # Perform validation for the dictionary format\n            for poem_title, genomes in genome_data_list.items():\n                if not isinstance(genomes, dict) or not all(key in genomes for key in ['S', 'I', 'C']):\n                    print(f\"Warning: Genome data for poem '{poem_title}' in {file_path} (dictionary format) is not structured as expected (missing S, I, or C keys).\")\n            return genome_data_list # Return as is if it's already a dict\n        return {}\n\n    for poem_entry in genome_data_list:\n        if not isinstance(poem_entry, dict):\n            print(f\"Warning: Skipping an item in genome data list as it's not a dictionary: {poem_entry}\")\n            continue\n        \n        poem_title = poem_entry.get('title')\n        s_line = poem_entry.get('s_line')\n        i_line = poem_entry.get('i_line')\n        c_line = poem_entry.get('c_line')\n\n        if not poem_title:\n            print(f\"Warning: Skipping genome entry due to missing 'title': {poem_entry}\")\n            continue\n        \n        if s_line is None or i_line is None or c_line is None:\n            print(f\"Warning: Genome data for poem '{poem_title}' is incomplete (missing s_line, i_line, or c_line). It will be recorded with missing parts.\")\n\n        transformed_genome_map[poem_title] = {\n            'S': s_line if s_line is not None else \"\",\n            'I': i_line if i_line is not None else \"\",\n            'C': c_line if c_line is not None else \"\"\n        }\n    \n    if not transformed_genome_map:\n        print(\"Warning: No valid genome data was transformed. The resulting map is empty.\")\n    else:\n        print(f\"Successfully loaded and transformed genome data for {len(transformed_genome_map)} poems.\")\n    return transformed_genome_map\n\n# Helper function to calculate the 0-based index of text[original_char_pos] among non-space characters up to that point.\ndef get_non_space_char_index(text, original_char_pos):\n    if original_char_pos < 0 or original_char_pos >= len(text):\n        return -1 # Invalid original_char_pos\n\n    non_space_count = 0\n    for i in range(original_char_pos + 1):\n        if text[i] != ' ':\n            non_space_count += 1\n    return non_space_count -1 # Return 0-based index\n\ndef render_card(entry, output_path, fonts, genome_map):\n    # Create a new image with a white background\n    card = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), WHITE)\n    draw = ImageDraw.Draw(card)\n\n    # --- Load the main image ---\n    image_path_relative = entry.get('image_path')\n    if not image_path_relative:\n        print(f\"Skipping entry {entry.get('id', 'Unknown ID')} due to missing 'image_path'.\")\n        return\n    \n    full_image_path = get_image_path(entry)\n\n    try:\n        img = Image.open(full_image_path)\n        img = img.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n    except FileNotFoundError:\n        print(f\"Error: Image not found at {full_image_path} for entry {entry.get('id')}. Creating placeholder.\")\n        img = Image.new('RGB', (BASE_WIDTH, IMAGE_DISPLAY_HEIGHT), (100, 100, 100)) # Dark gray placeholder\n        draw_placeholder = ImageDraw.Draw(img)\n        placeholder_text = f\"Image Not Found:\\n{os.path.basename(full_image_path)}\"\n        # Use a basic font for placeholder text if others aren't loaded yet\n        try:\n            placeholder_font = fonts.get('id_poem', ImageFont.load_default(size=24))\n        except:\n            placeholder_font = ImageFont.load_default(size=24)\n        \n        text_bbox = draw_placeholder.textbbox((0,0), placeholder_text, font=placeholder_font, align='center')\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        text_x = (BASE_WIDTH - text_width) / 2\n        text_y = (IMAGE_DISPLAY_HEIGHT - text_height) / 2\n        draw_placeholder.text((text_x, text_y), placeholder_text, font=placeholder_font, fill=WHITE, align='center')\n\n    # Paste the image onto the card\n    card.paste(img, (0, HEADER_HEIGHT + GENOME_REPORT_HEIGHT)) # Shifted down by GENOME_REPORT_HEIGHT\n\n    # --- Draw border (optional) ---\n    # draw.rectangle([0, 0, BASE_WIDTH - 1, CARD_HEIGHT - 1], outline=BLUE_BORDER, width=1)\n\n    # --- Text drawing setup ---\n    padding = 10\n    current_y = padding + GENOME_REPORT_HEIGHT # Start below genome report section\n\n    # --- Draw ID and Poem --- \n    id_text = f\"ID: {entry.get('id', 'N/A')}\"\n    poem_text = f\"POEM: {entry.get('poem', 'N/A')}\"\n    draw.text((padding, current_y), id_text, font=fonts['id_poem'], fill=BLACK)\n    id_bbox = fonts['id_poem'].getbbox(id_text)\n    current_y += (id_bbox[3] - id_bbox[1]) + 5\n    draw.text((padding, current_y), poem_text, font=fonts['id_poem'], fill=BLACK)\n    poem_bbox = fonts['id_poem'].getbbox(poem_text)\n    current_y += (poem_bbox[3] - poem_bbox[1]) + 10\n\n    # --- Prompt Section --- \n    prompt_section_y_start = current_y\n    prompt_label_x = padding\n    # value_start_x = padding + 180 # Fixed start for values, adjust as needed\n    max_glyph_width = 0\n    placeholder_char_width = 0 # Initialize here\n\n    # Calculate max_glyph_width using chosen_genome_font for prompt glyphs\n    # chosen_genome_font = fonts.get('genome_small', ImageFont.load_default(size=12))\n    chosen_genome_font = fonts.get('genome_12', ImageFont.load_default(size=12)) # Use genome_12 for prompt glyphs\n\n    # Ensure placeholder_char_width is calculated\n    if chosen_genome_font:\n        placeholder_char_bbox = chosen_genome_font.getbbox(\"?\")\n        placeholder_char_width = placeholder_char_bbox[2] - placeholder_char_bbox[0]\n    else: # Fallback if font isn't loaded, though it should be\n        placeholder_char_width = 10 # Arbitrary fallback\n\n    for glyph_dict in [IMAGE_TYPE_GLYPHS, SYNTAGMA_GLYPHS, CINEOSIS_FUNCTION_GLYPHS]:\n        for glyph in glyph_dict.values():\n            glyph_bbox = chosen_genome_font.getbbox(glyph)\n            glyph_width = glyph_bbox[2] - glyph_bbox[0]\n            if glyph_width > max_glyph_width:\n                max_glyph_width = glyph_width\n    # Also consider the fallback glyph '?'\n    fallback_glyph_bbox = chosen_genome_font.getbbox(\"?\")\n    fallback_glyph_width = fallback_glyph_bbox[2] - fallback_glyph_bbox[0]\n    if fallback_glyph_width > max_glyph_width:\n        max_glyph_width = fallback_glyph_width\n\n    glyph_column_x = padding + 10 # Where glyphs are drawn\n    gap_after_glyph = 15\n    dynamic_value_start_x = glyph_column_x + max_glyph_width + gap_after_glyph\n\n    # Image Type\n    image_type_label = \"IMAGE TYPE:\"\n    image_type_val = entry.get('imageType', '---')\n    image_type_glyph = IMAGE_TYPE_GLYPHS.get(image_type_val, \"?\")\n    draw.text((glyph_column_x, current_y), image_type_glyph, font=chosen_genome_font, fill=IMAGE_TYPE_COLOR)\n    draw.text((dynamic_value_start_x, current_y), f\"{image_type_label} {image_type_val}\", font=fonts['prompt_label'], fill=IMAGE_TYPE_COLOR)\n    label_bbox = fonts['prompt_label'].getbbox(image_type_label)\n    current_y += (label_bbox[3] - label_bbox[1]) + 5\n\n    # Syntagma Type\n    syntagma_label = \"SYNT.\" # Abbreviated for space\n    syntagma_val = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_val, \"?\")\n    draw.text((glyph_column_x, current_y), syntagma_glyph, font=chosen_genome_font, fill=SYNTAGMA_COLOR)\n    draw.text((dynamic_value_start_x, current_y), f\"{syntagma_label} TYPE: {syntagma_val}\", font=fonts['prompt_label'], fill=SYNTAGMA_COLOR)\n    label_bbox = fonts['prompt_label'].getbbox(syntagma_label)\n    current_y += (label_bbox[3] - label_bbox[1]) + 5\n\n    # Cineosis Function\n    cineosis_label = \"CINEOSIS:\"\n    cineosis_val = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_val, \"?\")\n    draw.text((glyph_column_x, current_y), cineosis_glyph, font=chosen_genome_font, fill=CINEOSIS_COLOR)\n    draw.text((dynamic_value_start_x, current_y), f\"{cineosis_label} {cineosis_val}\", font=fonts['prompt_label'], fill=CINEOSIS_COLOR)\n    label_bbox = fonts['prompt_label'].getbbox(cineosis_label)\n    current_y += (label_bbox[3] - label_bbox[1]) + 10 # More space after this section\n\n    # --- Operative Ekphrasis, Style Conditioning, Full Prompt ---\n    text_area_x = padding\n    # prompt_value_max_width = BASE_WIDTH - value_start_x - padding # Max width for the text values\n    prompt_value_max_width = BASE_WIDTH - dynamic_value_start_x - padding # Max width for the text values\n\n    current_y = draw_colored_text(draw, f\"LINE: {entry.get('content', '---')}\", dynamic_value_start_x, prompt_section_y_start - 20, BLACK, fonts['prompt_text'], max_width=prompt_value_max_width)\n    current_y = prompt_section_y_start # Reset Y for the actual prompt details, it was just for LINE: placement\n    current_y += (fonts['prompt_label'].getbbox(\"A\")[3] - fonts['prompt_label'].getbbox(\"A\")[1] + 5) * 3 + 10 # Est. height of 3 lines + spacing\n\n    current_y = draw_colored_text(draw, f\"EKPHRASIS: {entry.get('operativeEkphrasis', '---')}\", text_area_x, current_y, EKPHRASIS_COLOR, fonts['prompt_text'], max_width=BASE_WIDTH - (2*padding))\n    current_y = draw_colored_text(draw, f\"STYLE: {entry.get('styleConditioning', '---')}\", text_area_x, current_y, STYLE_COLOR, fonts['prompt_text'], max_width=BASE_WIDTH - (2*padding))\n    # current_y = draw_colored_text(draw, f\"PROMPT: {entry.get('full_prompt', '---')}\", text_area_x, current_y, BLACK, fonts['prompt_text'], max_width=BASE_WIDTH - (2*padding))\n\n    # --- Genome Barcode Section --- \n    GENOME_SECTION_Y_ON_IMG = 10 # Y position for the genome barcode section on the card image\n    genome_padding = 10\n    genome_line_height = 15 # Approximate height for each genome line\n    # genome_font = fonts.get('genome', ImageFont.load_default(size=12)) # Default to 12pt if 'genome' not found\n    # genome_font_small = fonts.get('genome_small', ImageFont.load_default(size=12)) # Menlo Regular 12pt\n    # genome_font_bold = fonts.get('genome_bold', ImageFont.load_default(size=12)) # Menlo Bold 12pt or fallback\n    genome_font_small = fonts.get('genome_12', ImageFont.load_default(size=12))\n    genome_font_bold = fonts.get('genome_12_bold', ImageFont.load_default(size=12))\n\n\n    poem_title_for_genome = entry.get('poem')\n    s_line_data_full = \"S: Not found in genome data.\"\n    i_line_data_full = \"I: Not found in genome data.\"\n    c_line_data_full = \"C: Not found in genome data.\"\n    frame_number = entry.get('frameNumber', 0) # Default to 0 if not present\n\n    if poem_title_for_genome and genome_map and poem_title_for_genome in genome_map:\n        poem_genomes = genome_map[poem_title_for_genome]\n        s_line_data_full = f\"S: {poem_genomes.get('S', '')}\"\n        i_line_data_full = f\"I: {poem_genomes.get('I', '')}\"\n        c_line_data_full = f\"C: {poem_genomes.get('C', '')}\"\n    else:\n        # print(f\"Warning: Genome data for poem '{poem_title_for_genome}' not found. Using default message.\")\n        pass # Already handled by default messages\n\n    # --- Genome Line Rendering with Scrolling and Highlighting --- \n    genome_lines_config = [\n        {'label': 'S', 'full_data': s_line_data_full, 'color': SYNTAGMA_COLOR},\n        {'label': 'I', 'full_data': i_line_data_full, 'color': IMAGE_TYPE_COLOR},\n        {'label': 'C', 'full_data': c_line_data_full, 'color': CINEOSIS_COLOR}\n    ]\n\n    # Max characters visible calculation\n    available_width_for_genome = BASE_WIDTH - (2 * genome_padding)\n    # char_width_approx = genome_font_small.getbbox('A')[2] # Approximate width of a single character\n    char_width_approx = 0\n    if genome_font_small:\n        char_bbox = genome_font_small.getbbox('A')\n        char_width_approx = char_bbox[2] - char_bbox[0]\n    else: # Fallback if font isn't loaded\n        char_width_approx = 7 # Arbitrary fallback based on 12pt Menlo approx\n    \n    if char_width_approx == 0: char_width_approx = 7 # Ensure not zero to avoid division error\n    max_visible_chars = int(available_width_for_genome / char_width_approx)\n\n    for i, line_config in enumerate(genome_lines_config):\n        y_pos = GENOME_SECTION_Y_ON_IMG + (i * genome_line_height)\n        full_genome_str = line_config['full_data'][3:] # Remove 'S: ', 'I: ', 'C: '\n        \n        # Calculate start and end indices for visible part of the genome string\n        # Highlighted char should be roughly 25% into the visible window\n        highlight_offset_in_window = max_visible_chars // 4\n        \n        # Ensure frame_number is within bounds of the actual genome string (non-space characters)\n        # content_char_index = get_non_space_char_index(entry.get('content', ''), frame_number)\n        # For genome, frame_number directly maps to genome characters (no spaces in genome data)\n        # However, frame_number is 1-based from JSON, convert to 0-based for string indexing\n        current_char_genome_index = frame_number -1 \n        if current_char_genome_index < 0: current_char_genome_index = 0 # Ensure non-negative\n        if current_char_genome_index >= len(full_genome_str):\n            current_char_genome_index = len(full_genome_str) - 1 if full_genome_str else 0\n\n        start_index = max(0, current_char_genome_index - highlight_offset_in_window)\n        end_index = start_index + max_visible_chars\n\n        # Adjust window if it extends beyond the genome length\n        if end_index > len(full_genome_str):\n            end_index = len(full_genome_str)\n            start_index = max(0, end_index - max_visible_chars)\n        \n        visible_genome_segment = full_genome_str[start_index:end_index]\n\n        # Prefix to indicate scrolling (e.g., \"[10] S: ...\")\n        scroll_prefix = f\"[{start_index+1}] \" if start_index > 0 else \"\"\n        line_label_prefix = f\"{line_config['label']}: \"\n        full_prefix = scroll_prefix + line_label_prefix\n        \n        # Draw prefix\n        draw.text((genome_padding, y_pos), full_prefix, font=genome_font_small, fill=line_config['color'])\n        prefix_bbox = genome_font_small.getbbox(full_prefix)\n        current_x_for_genome = genome_padding + (prefix_bbox[2] - prefix_bbox[0])\n\n        # Draw each character of the visible segment, highlighting the current one\n        for j, char_genome in enumerate(visible_genome_segment):\n            actual_genome_index = start_index + j\n            char_font = genome_font_bold if actual_genome_index == current_char_genome_index else genome_font_small\n            char_color = WHITE if actual_genome_index == current_char_genome_index else line_config['color']\n            bg_color = line_config['color'] if actual_genome_index == current_char_genome_index else None\n\n            char_bbox = char_font.getbbox(char_genome)\n            char_display_width = char_bbox[2] - char_bbox[0]\n\n            if bg_color:\n                # Slightly larger background for better visibility\n                bg_x0 = current_x_for_genome -1 \n                bg_y0 = y_pos -1 \n                bg_x1 = current_x_for_genome + char_display_width + 1\n                bg_y1 = y_pos + genome_line_height -1 \n                draw.rectangle([bg_x0, bg_y0, bg_x1, bg_y1], fill=bg_color)\n\n            draw.text((current_x_for_genome, y_pos), char_genome, font=char_font, fill=char_color)\n            current_x_for_genome += char_display_width\n\n        # Ellipsis if more content to the right\n        if end_index < len(full_genome_str):\n            draw.text((current_x_for_genome, y_pos), \"...\", font=genome_font_small, fill=line_config['color'])\n            current_x_for_genome += genome_font_small.getbbox(\"...\")[2] - genome_font_small.getbbox(\"...\")[0]\n        \n        # Optional: Fill remaining space with a subtle line or dots for alignment\n        # remaining_width = BASE_WIDTH - current_x_for_genome - genome_padding\n        # if remaining_width > 10:\n        #     draw.line([(current_x_for_genome + 2, y_pos + genome_line_height // 2),\n        #                (BASE_WIDTH - genome_padding -2, y_pos + genome_line_height // 2)], \n        #               fill=(220,220,220), width=1) # Light gray line\n\n    # Save the image\n    try:\n        card.save(output_path)\n        # print(f\"Saved card: {output_path}\")\n    except Exception as e:\n        print(f\"Error saving card {output_path}: {e}\")\n\ndef render_demo_card(fonts):\n    # Create a dummy entry for demonstration\n    demo_entry = {\n        \"id\": \"DEMO_001\",\n        \"poem\": \"Demo Poem Title\",\n        \"imageType\": \"Action-Image\",\n        \"syntagmaType\": \"Chronological Syntagma (CS)\",\n        \"cineosisFunction\": \"Causal Motion Trigger\",\n        \"operativeEkphrasis\": \"This is a demonstration of the card layout with various text elements and styles. It shows how different sections are rendered.\",\n        \"styleConditioning\": \"A vibrant and dynamic style with bold colors and sharp contrasts, aiming for a futuristic yet retro feel.\",\n        \"full_prompt\": \"A full prompt example that might be quite long and wrap over multiple lines to test the text wrapping capabilities of the rendering function.\",\n        \"image_path\": \"\", # No actual image for demo, placeholder will be used\n        \"content\": \"This is the content line for the demo.\",\n        \"frameNumber\": 15 # Example frame number for genome highlighting\n    }\n    demo_genome_map = {\n        \"Demo Poem Title\": {\n            \"S\": \"ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\",\n            \"I\": \"abcdefghijklmnopqrstuvwxyz!@#$%^&*()\",\n            \"C\": \"ZYXWVUTSRQPONMLKJIHGFEDCBA0987654321\"\n        }\n    }\n    output_path = os.path.join(OUTPUT_DIR, \"_demo_card.png\")\n    render_card(demo_entry, output_path, fonts, demo_genome_map)\n    print(f\"Demo card saved to {output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name=\"cards_extreme_test_cases_output\") :\n    extreme_output_dir = os.path.join(BASE_DIR, output_dir_name)\n    os.makedirs(extreme_output_dir, exist_ok=True)\n\n    entry_long_text = {\n        \"id\": \"EXTREME_LONG_001\", \"poem\": \"The Poem of Endless Words Without Pauses\",\n        \"imageType\": \"Descriptive Image\", \"syntagmaType\": \"Descriptive Syntagma (DS)\", \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"LoremipsumdolorsitametconsecteturadipiscingelitPellentesquehabitantmorbitristiquesenectusetnetusetmalesuadafamesacturpisegestasDonecvelurnaerosFuscealiquamtemporconvallisPraesentsitametvariusnuncSedvitaeantequissemrhoncusultriciesPhasellusfinibusipsumatexportaelementumMaecenaspharetrajustoacmetusvolutpatidvariusnisltinciduntNuncvitaeeratsitametnisifaucibushendreritAliquameratvolutpatSuspendissepotentiFusceefficituranteacmetuslaciniaidconsequatpurusmaximusPellentesquehabitantmorbitristiquesenectusetnetusetmalesuadafamesacturpisegestas\",\n        \"styleConditioning\": \"Short style.\", \"full_prompt\": \"This is a very long prompt designed to test the wrapping capabilities of the text rendering functions. It should ideally wrap across multiple lines gracefully without overflowing or causing errors in the layout. We are checking how it handles extremely long strings of text that have no natural breaking points like spaces or hyphens to see if the wrapping logic is robust enough for such edge cases.\",\n        \"image_path\": \"\", \"content\": \"This is the content line for the extreme long text test case to see how it is handled by the rendering engine.\",\n        \"frameNumber\": 1\n    }\n    entry_missing_fields = {\n        \"id\": \"EXTREME_MISSING_002\", \"poem\": None, # Poem is None\n        \"image_path\": \"\", \"frameNumber\": 1\n        # Other fields intentionally missing\n    }\n    entry_special_chars = {\n        \"id\": \"EXTREME_SPECIAL_003\", \"poem\": \"Poem with !@#$%^&*()_+[]{};':\\\",./<>?`~ emojis \ud83d\ude0a\ud83d\ude02\ud83d\udc4d\ud83c\udf89\",\n        \"imageType\": \"S\u00f8n\u0219ign\", # Special chars in value\n        \"syntagmaType\": \"Cr\u00a5stal Syntagma (CS)\",\n        \"cineosisFunction\": \"Mem\u00f8ry St\u00f8rage Retriev\u00e5l\",\n        \"operativeEkphrasis\": \"Ekphrasis with newlines\\nand tabs\\t and unicode characters like \u00fc\u00f6\u00e4\u00e9\u00ed\u00f3\u00fa\u00f1\u00e7.\",\n        \"styleConditioning\": \"Style with \\\"quotes\\\" and 'apostrophes'.\",\n        \"full_prompt\": \"Full prompt with a mix of everything: !@#$%^&*()_+[]{};':\\\",./<>?`~ \ud83d\ude0a\ud83d\ude02\ud83d\udc4d\ud83c\udf89 and newlines\\nand tabs\\t.\",\n        \"image_path\": \"\", \"content\": \"Content: Special chars & emojis \ud83d\ude0a\ud83d\ude02\ud83d\udc4d\ud83c\udf89\", \"frameNumber\": 5\n    }\n\n    test_entries = [entry_long_text, entry_missing_fields, entry_special_chars]\n    test_genome_map = {\n        entry_long_text[\"poem\"]: {\"S\": \"S\"*100, \"I\": \"I\"*100, \"C\": \"C\"*100},\n        # No genome for None poem title\n        entry_special_chars[\"poem\"]: {\"S\": \"S!@#\", \"I\": \"I\ud83d\ude0a\ud83d\ude02\", \"C\": \"C\ud83d\udc4d\ud83c\udf89\"}\n    }\n\n    for i, entry in enumerate(test_entries):\n        output_path = os.path.join(extreme_output_dir, f\"_extreme_test_card_{i+1}_{entry['id']}.png\")\n        render_card(entry, output_path, fonts, test_genome_map)\n        print(f\"Extreme test card saved to {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    \"\"\"Calculates the number of frames (occurrences) for each unique poem title.\"\"\"\n    poem_frame_counts = {}\n    for entry in timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            poem_frame_counts[poem_title] = poem_frame_counts.get(poem_title, 0) + 1\n    return poem_frame_counts\n\ndef slugify(text):\n    \"\"\"\n    Convert a string into a slug.\n    Removes special characters, converts to lowercase, and replaces spaces with hyphens.\n    \"\"\"\n    if text is None:\n        return \"none\"\n    text = str(text).lower()\n    text = re.sub(r'[^\\w\\s-]', '', text)  # Remove non-alphanumeric, non-space, non-hyphen\n    text = re.sub(r'[-\\s]+', '-', text).strip('-') # Replace spaces/hyphens with single hyphen\n    return text if text else \"untitled\"\n\ndef main():\n    print(\"Starting CLAPPER62SINGLESHOT: Longest Poem Card Generation...\")\n    print(f\"Timeline data will be loaded from: {TIMELINE_PATH}\")\n    print(f\"Symbolic genome data will be loaded from: {SYMBOLIC_GENOME_DATA_PATH}\")\n    print(f\"Output directory set to: {OUTPUT_DIR}\")\n    print(f\"Base image directory set to: {BASE_IMAGE_DIR}\")\n\n    # discover_unique_prompt_values(TIMELINE_PATH) # Uncomment to run discovery\n\n    # --- Load Fonts ---\n    fonts = {\n        'id_poem': get_font(\"Menlo\", \"Courier New\", 18),\n        'prompt_label': get_font(\"Menlo\", \"Courier New\", 14),\n        'prompt_text': get_font(\"Menlo\", \"Courier New\", 12),\n        # 'genome': get_font(\"Menlo\", \"Courier New\", 12) # Original genome font\n        'genome_12': get_font(\"Menlo\", \"Courier New\", 12, is_bold_preferred=False, is_bold_fallback=False), # Menlo Regular 12pt\n        'genome_12_bold': get_font(\"Menlo\", \"Courier New\", 12, is_bold_preferred=True, is_bold_fallback=True) # Menlo Bold 12pt\n    }\n    print(\"Fonts loaded.\")\n\n    # --- Load Timeline Data ---\n    try:\n        with open(TIMELINE_PATH, 'r') as f:\n            timeline_data = json.load(f)\n        print(f\"Successfully loaded {len(timeline_data)} entries from timeline.\")\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}. Exiting.\")\n        return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from timeline file {TIMELINE_PATH}. Exiting.\")\n        return\n\n    # --- Load Genome Data ---\n    genome_map = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    if not genome_map:\n        print(\"Warning: Genome map is empty or failed to load. Genome barcodes may not render correctly.\")\n\n    # --- Calculate frame counts for each poem ---\n    poem_frame_counts = calculate_frame_counts(timeline_data)\n    if not poem_frame_counts:\n        print(\"No poems found or frame counts could not be calculated. Exiting.\")\n        return\n\n    # --- Identify the longest poem ---\n    longest_poem_title = max(poem_frame_counts, key=poem_frame_counts.get)\n    num_frames_for_longest_poem = poem_frame_counts[longest_poem_title]\n    print(f\"Identified longest poem: '{longest_poem_title}' with {num_frames_for_longest_poem} frames.\")\n\n    # --- Filter timeline data for the longest poem and sort by frameNumber ---\n    longest_poem_entries = [\n        entry for entry in timeline_data if entry.get('poem') == longest_poem_title\n    ]\n    longest_poem_entries.sort(key=lambda x: x.get('frameNumber', 0)) # Sort by frameNumber\n    \n    print(f\"Processing {len(longest_poem_entries)} entries for poem '{longest_poem_title}'.\")\n\n    # --- Process Timeline Entries for the Longest Poem and Render Cards ---\n    card_output_paths = [] # Initialize list to store card paths\n    processed_ids = set()\n    for i, entry in enumerate(longest_poem_entries):\n        entry_id = entry.get('id', f\"entry_{i}\")\n        # if entry_id in processed_ids:\n        #     continue # Skip if already processed (e.g. if script is run multiple times on same data subset)\n        \n        # Add frameNumber to entry if not present, based on its occurrence within its poem\n        # This logic needs to be more sophisticated if we only process a subset or specific poems\n        # For now, we assume frameNumber is either present or can be sequentially assigned if we were to track poem occurrences.\n        # The current `clapper61prime` relies on `frameNumber` from the JSON if present, or defaults to 0/1.\n        # For this script, we'll use the 'frameNumber' as provided or default.\n        # If we were to generate frame numbers: We'd need to count occurrences of each (poem, content_line) pair if content matters,\n        # or just (poem) if each image for a poem is a new frame.\n        # The `calculate_frame_counts` gives total frames per poem, not per specific line within poem.\n        # Let's assume `frameNumber` in the JSON is the 1-based index for the specific image within the sequence for that poem's line.\n\n        frame_num_str = f\"{entry.get('frameNumber', i+1):05d}\" # Use 5 digits for sorting many frames\n        output_filename = f\"card_{slugify(longest_poem_title)}_{frame_num_str}_{entry.get('id', 'unknown')}.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        \n        # print(f\"Processing entry {i+1}/{len(longest_poem_entries)} for poem '{longest_poem_title}': ID {entry.get('id', 'N/A')}\")\n        render_card(entry, output_path, fonts, genome_map)\n        card_output_paths.append(output_path) # Append path for video generation\n        processed_ids.add(entry_id) # Still useful if we want to count unique entries processed for the poem\n\n        # if i >= 100: # Limiter for testing\n        #     print(\"Reached processing limit for testing.\")\n        #     break\n\n    print(f\"\\nProcessed {len(processed_ids)} unique entries.\")\n    print(f\"Total cards attempted: {len(timeline_data)} (if no limiter was active).\")\n    print(f\"Output directory: {OUTPUT_DIR}\")\n\n    # Render a demo card for layout checking\n    # render_demo_card(fonts)\n    # render_extreme_test_case(fonts, genome_map)\n\n    # --- Generate Video ---\n    if card_output_paths:\n        video_target_duration_seconds = 2 * 60 + 11  # 2 minutes 11 seconds\n        num_actual_cards = len(card_output_paths)\n        \n        if num_actual_cards == 0:\n            print(\"No cards generated for the longest poem, skipping video creation.\")\n        else:\n            duration_per_frame = video_target_duration_seconds / num_actual_cards\n            print(f\"Target video duration: {video_target_duration_seconds}s. Each of {num_actual_cards} frames will be shown for {duration_per_frame:.3f}s.\")\n\n            video_filename = f\"{slugify(longest_poem_title)}_video_{video_target_duration_seconds}s.mp4\"\n            video_output_path = os.path.join(OUTPUT_DIR, video_filename)\n\n            try:\n                print(f\"Attempting to create video from {num_actual_cards} images...\")\n                clips = []\n                for card_path in card_output_paths:\n                    if os.path.exists(card_path):\n                        clips.append(mp.ImageClip(card_path).set_duration(duration_per_frame))\n                    else:\n                        print(f\"Warning: Card image not found at {card_path}, skipping for video.\")\n                \n                if not clips:\n                    print(\"No valid image clips found. Video not created.\")\n                else:\n                    final_clip = mp.concatenate_videoclips(clips, method=\"compose\")\n                    # Consider adding audio, specifying codec, bitrate, etc. if needed\n                    final_clip.write_videofile(video_output_path, fps=24, codec='libx264', audio_codec='aac') \n                    print(f\"Video successfully generated: {video_output_path}\")\n            except Exception as e:\n                print(f\"Error generating video: {e}\")\n                print(\"Please ensure 'moviepy' and its dependencies (like ffmpeg) are correctly installed.\")\n                print(\"You might need to run: pip install moviepy\")\n    else:\n        print(\"No card paths collected, so no video will be created.\")\n\n    print(\"CLAPPER62SINGLESHOT card generation complete.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "UPDATED-TIMELINE.json",
      "symbolic_genome_data.json",
      "MISSING_IMAGE_PATH.png",
      "MR/some_image.png",
      "_demo_card.png",
      "]}.png",
      ")}.png",
      "{slugify(longest_poem_title)}_video_{video_target_duration_seconds}s.mp4",
      "\nCLAPPER62SINGLESHOT - Generates images for the longest poem for video creation.\n- Uses UPDATED-TIMELINE.json from the HONEYBADGER directory.\n- Identifies the longest poem by number of entries (frames).\n- Renders all cards for that specific poem.\n- Outputs images to HONEYBADGER/LONGEST_POEM_OUTPUT/\n- Based on clapper61prime.py.\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES - Mapped from discovered data to glyphs.md symbols\nSYNTAGMA_GLYPHS = {\n    ",
      ") # Correctly points to HONEYBADGER/UPDATED-TIMELINE.json\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), ",
      " images located in TIGER/MR/\n    # If the original path in JSON is like ",
      ") is likely sufficient given current context\n        # For now, we assume if it starts with MR/, it",
      ")\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        text_x = (BASE_WIDTH - text_width) / 2\n        text_y = (IMAGE_DISPLAY_HEIGHT - text_height) / 2\n        draw_placeholder.text((text_x, text_y), placeholder_text, font=placeholder_font, fill=WHITE, align=",
      "N/A",
      "N/A",
      "t loaded\n        char_width_approx = 7 # Arbitrary fallback based on 12pt Menlo approx\n    \n    if char_width_approx == 0: char_width_approx = 7 # Ensure not zero to avoid division error\n    max_visible_chars = int(available_width_for_genome / char_width_approx)\n\n    for i, line_config in enumerate(genome_lines_config):\n        y_pos = GENOME_SECTION_Y_ON_IMG + (i * genome_line_height)\n        full_genome_str = line_config[",
      "\n        \n        # Calculate start and end indices for visible part of the genome string\n        # Highlighted char should be roughly 25% into the visible window\n        highlight_offset_in_window = max_visible_chars // 4\n        \n        # Ensure frame_number is within bounds of the actual genome string (non-space characters)\n        # content_char_index = get_non_space_char_index(entry.get(",
      ")[0]\n        \n        # Optional: Fill remaining space with a subtle line or dots for alignment\n        # remaining_width = BASE_WIDTH - current_x_for_genome - genome_padding\n        # if remaining_width > 10:\n        #     draw.line([(current_x_for_genome + 2, y_pos + genome_line_height // 2),\n        #                (BASE_WIDTH - genome_padding -2, y_pos + genome_line_height // 2)], \n        #               fill=(220,220,220), width=1) # Light gray line\n\n    # Save the image\n    try:\n        card.save(output_path)\n        # print(f",
      ",./<>?`~ emojis \ud83d\ude0a\ud83d\ude02\ud83d\udc4d\ud83c\udf89",
      ",./<>?`~ \ud83d\ude0a\ud83d\ude02\ud83d\udc4d\ud83c\udf89 and newlines\\nand tabs\\t.",
      ") # Replace spaces/hyphens with single hyphen\n    return text if text else ",
      ")\n        # if entry_id in processed_ids:\n        #     continue # Skip if already processed (e.g. if script is run multiple times on same data subset)\n        \n        # Add frameNumber to entry if not present, based on its occurrence within its poem\n        # This logic needs to be more sophisticated if we only process a subset or specific poems\n        # For now, we assume frameNumber is either present or can be sequentially assigned if we were to track poem occurrences.\n        # The current `clapper61prime` relies on `frameNumber` from the JSON if present, or defaults to 0/1.\n        # For this script, we",
      "Processing entry {i+1}/{len(longest_poem_entries)} for poem ",
      "N/A",
      ")\n        else:\n            duration_per_frame = video_target_duration_seconds / num_actual_cards\n            print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER62SINGLESHOT - Generates images for the longest poem for video creation.\n- Uses UPDATED-TIMELINE.json from the HONEYBADGER directory.\n- Identifies the longest poem by number of entries (frames).\n- Renders all cards for that specific poem.\n- Outputs images to HONEYBADGER/LONGEST_POEM_OUTPUT/\n- Based on clapper61prime.py."
  },
  {
    "path": "ELEPHANT/TRUNK/61primeplus.py",
    "size": 26420,
    "lines": 522,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER61PRIMEPLUS - Processes UPDATED-TIMELINE.json for 'Magic ride' poem only.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER61PRIME.\n- Outputs cards for 'Magic ride' to ELEPHANT/TUSK/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\nimport unicodedata # Added for slugify\n\n# --- slugify function ---\ndef slugify(value, allow_unicode=False):\n    \"\"\"\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n    dashes to single dashes. Remove characters that aren't alphanumerics,\n    underscores, or hyphens. Convert to lowercase. Also strip leading and\n    trailing whitespace, dashes, and underscores.\n    \"\"\"\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n\n# --- get_flexible_glyph function ---\ndef get_flexible_glyph(data_string, glyph_dict, default_glyph=\"?\"):\n    \"\"\"Gets a glyph from a dictionary, trying exact match first, then flexible match (ignoring suffix in parentheses).\"\"\"\n    if not data_string:\n        return default_glyph\n\n    # 1. Try exact match first\n    glyph = glyph_dict.get(data_string)\n    if glyph:\n        return glyph.strip()\n\n    # 2. Try flexible match (strip suffix and whitespace)\n    #    e.g., \"Autonomous Syntagma\" from data vs \"Autonomous Syntagma (AS)\" in dict\n    data_string_base = re.sub(r'\\s*\\(.*\\)$', '', data_string).strip()\n    \n    for key, val in glyph_dict.items():\n        key_base = re.sub(r'\\s*\\(.*\\)$', '', key).strip()\n        if key_base.lower() == data_string_base.lower():\n            return val.strip()\n            \n    return default_glyph\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60\nHEADER_HEIGHT = 200\nIMAGE_DISPLAY_HEIGHT = 576\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230)\n\n# Glyph Definitions\nSYNTAGMA_GLYPHS = {\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (CS)\": \"\u2756\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), \"HONEYBADGER\", \"UPDATED-TIMELINE.json\")\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\")\nOUTPUT_DIR = os.path.join(os.path.dirname(BASE_DIR), \"TUSK\") # Changed output directory\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR))\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n    try:\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                try:\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    relative_path = entry.get('image_path')\n    if not relative_path:\n        print(f\"Warning: 'image_path' missing in entry: {entry.get('id', 'Unknown ID')}\")\n        return None\n\n    if relative_path.startswith(\"MR/\"):\n        adjusted_relative_path = os.path.join(\"TIGER\", relative_path)\n    else:\n        adjusted_relative_path = relative_path\n\n    full_image_path = os.path.join(BASE_IMAGE_DIR, adjusted_relative_path)\n    \n    if os.path.exists(full_image_path):\n        return os.path.normpath(full_image_path)\n    else:\n        # Fallback if TIGER/MR path not found, try original MR/ path directly under BASE_IMAGE_DIR\n        if relative_path.startswith(\"MR/\"):\n            original_mr_path = os.path.join(BASE_IMAGE_DIR, relative_path)\n            if os.path.exists(original_mr_path):\n                return os.path.normpath(original_mr_path)\n        \n        # Broader fallback search using entry ID (adapted from original clapper61prime)\n        entry_id_val = entry.get('id', '')\n        if entry_id_val:\n            search_dirs = [BASE_IMAGE_DIR] # General search in base image dir\n            if relative_path.startswith(\"MR/\"):\n                 # If it was an MR path, also specifically check TIGER/MR and TIGER folders\n                search_dirs.insert(0, os.path.join(BASE_IMAGE_DIR, 'TIGER', 'MR'))\n                search_dirs.insert(1, os.path.join(BASE_IMAGE_DIR, 'TIGER'))\n\n            for search_dir_base in search_dirs:\n                if os.path.exists(search_dir_base):\n                    for root, _, files in os.walk(search_dir_base):\n                        for file_name in files:\n                            if file_name.startswith(entry_id_val + '__') and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                                return os.path.normpath(os.path.join(root, file_name))\n                            # Check for exact filename match from image_path as well\n                            if os.path.basename(relative_path) == file_name:\n                                return os.path.normpath(os.path.join(root, file_name))\n        \n        print(f\"Error: Image not found for entry {entry.get('id', 'Unknown ID')}. Path attempted: {full_image_path}\")\n        return None\n\ndef load_genome_data_from_json(file_path):\n    genome_map_dict = {}\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems_data_list = json.load(f)\n        if not isinstance(poems_data_list, list):\n            print(f\"Error: Genome data in {file_path} is not a list. Found {type(poems_data_list)}.\")\n            return {}\n        for item in poems_data_list:\n            if isinstance(item, dict) and 'title' in item:\n                poem_name = item['title'].strip().lower()\n                genome_map_dict[poem_name] = {\n                    's_line': item.get('s_line', ''),\n                    'i_line': item.get('i_line', ''),\n                    'c_line': item.get('c_line', '')\n                }\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}.\")\n    return genome_map_dict\n\ndef get_non_space_char_index(text, original_char_pos):\n    count = -1\n    if original_char_pos >= len(text):\n        return -1\n    for i in range(original_char_pos + 1):\n        if text[i] != ' ':\n            count += 1\n    return count\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10\n    value_start_x = 10 + narrow_label_area_width\n    current_y_offset = 4\n    genome_section_y_on_img = current_y_offset\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70)\n    genome_content_x1_on_img = 10\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n    i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A') if genome_data_entry else 'I_LINE_N/A'\n    s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A') if genome_data_entry else 'S_LINE_N/A'\n    c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A') if genome_data_entry else 'C_LINE_N/A'\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n    frame_position = entry.get('frameNumber', 0)\n    highlight_index = int(frame_position) - 1 if isinstance(frame_position, (int, float)) and frame_position > 0 else -1\n    \n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    if len(genome_lines_to_render) * chosen_genome_line_height > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        if len(genome_lines_to_render) * chosen_genome_line_height > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    if genome_y_start < genome_section_y_on_img + 4: genome_y_start = genome_section_y_on_img + 4\n\n    max_render_width = BASE_WIDTH - 10 # Allow a small margin on the right\n    avg_char_width = chosen_genome_font.getlength(\"X\") if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(\"X\")[2] - chosen_genome_font.getbbox(\"X\")[0]\n    if avg_char_width <= 0: avg_char_width = 1\n    placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR) if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[2] - chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[0]\n    if placeholder_char_width <= 0: placeholder_char_width = 1\n\n    for original_line_text in genome_lines_to_render:\n        current_x = genome_content_x1_on_img\n        chars_that_fit_total_approx = int(max_render_width // avg_char_width)\n        if chars_that_fit_total_approx <= 0: chars_that_fit_total_approx = 1 # Avoid division by zero or negative\n        target_highlight_offset_in_view = chars_that_fit_total_approx // 4\n\n        display_start_index = 0\n        scrolled_from_left = False\n\n        if highlight_index >= 0: # Ensure we have a valid highlight_index\n            if len(original_line_text) <= chars_that_fit_total_approx:\n                # Line fits entirely, no scrolling needed\n                display_start_index = 0\n                scrolled_from_left = False\n            else:\n                # Line is longer than available space, scrolling is needed\n                scrolled_from_left = True\n                # Default start: try to position highlight_index at target_highlight_offset_in_view\n                tentative_display_start_index = max(0, highlight_index - target_highlight_offset_in_view)\n                \n                # Adjust if scrolling near the end of the line\n                # Ensure the window doesn't go past the end of the string\n                if tentative_display_start_index + chars_that_fit_total_approx > len(original_line_text):\n                    display_start_index = max(0, len(original_line_text) - chars_that_fit_total_approx)\n                else:\n                    display_start_index = tentative_display_start_index\n        else: # No valid highlight_index, just show from the beginning\n            display_start_index = 0\n            scrolled_from_left = False\n            \n        # Prefix display has been removed to maximize space for genome characters.\n        # The scrolled_from_left flag is still determined above to correctly set display_start_index.\n        for idx_in_original in range(display_start_index, len(original_line_text)):\n            char_to_draw = original_line_text[idx_in_original]\n            char_width = chosen_genome_font.getlength(char_to_draw) if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(char_to_draw)[2] - chosen_genome_font.getbbox(char_to_draw)[0]\n            if char_width <=0: char_width = avg_char_width # Fallback if char_width is bad\n            \n            # Check if the current character fits\n            if current_x + char_width > max_render_width:\n                break # Stop drawing this line if the character doesn't fit\n            char_color = AMBER\n            if char_to_draw != ' ':\n                current_char_overall_non_space_idx = get_non_space_char_index(original_line_text, idx_in_original)\n                if current_char_overall_non_space_idx == highlight_index:\n                    char_color = WHITE\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n        drawn_width = current_x - genome_content_x1_on_img\n        remaining_line_width = max_render_width - drawn_width\n        if remaining_line_width > 0 and placeholder_char_width > 0:\n            num_padding_chars = int(remaining_line_width // placeholder_char_width)\n            if num_padding_chars > 0:\n                draw.text((current_x, genome_y_start), PLACEHOLDER_CHAR * num_padding_chars, font=chosen_genome_font, fill=PLACEHOLDER_COLOR)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 4\n    header_content_y = current_y_offset + 4\n    top_row_internal_height = 30\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position_val = entry.get('frameNumber', 0)\n    frame_total_val = entry.get('totalFrames', 0)\n    frame_text = f\"FRAME: ({frame_position_val}/{frame_total_val})\"\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name[:25]}\"\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    current_y_offset += top_row_internal_height\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8\n    initial_content_row_y = current_y_offset\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = get_flexible_glyph(image_type_str_raw, IMAGE_TYPE_GLYPHS)\n    line_content = entry.get('content', '---')\n    glyph_column_x = 10\n    draw.text((glyph_column_x, initial_content_row_y), image_glyph, font=chosen_genome_font, fill=WHITE)\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_text_width_bbox = text_font.getbbox(image_type_str_raw)\n    image_text_width = image_text_width_bbox[2] - image_text_width_bbox[0]\n    line_field_x_start = max(int(BASE_WIDTH * 0.35), min(value_start_x + image_text_width + 30, int(BASE_WIDTH * 0.50)))\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width_bbox = header_font.getbbox(line_prefix)\n    line_prefix_width = line_prefix_width_bbox[2] - line_prefix_width_bbox[0]\n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    ONE_TEXT_LINE_HEIGHT = (text_font.getbbox(\"Ay\")[3] - text_font.getbbox(\"Ay\")[1] + 4) if hasattr(text_font, 'getbbox') else 16\n    current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT) if line_content.strip() and line_content.strip() != '---' else initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    current_y_offset += 4\n    divider_start_x = glyph_column_x - 4 if glyph_column_x > 4 else 6\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = get_flexible_glyph(syntagma_type_str_raw, SYNTAGMA_GLYPHS)\n    draw.text((glyph_column_x, current_y_offset), syntagma_glyph, font=chosen_genome_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = get_flexible_glyph(cineosis_func_str_raw, CINEOSIS_FUNCTION_GLYPHS)\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=chosen_genome_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n    img.save(output_path)\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name:\n            poem_frames[poem_name] = poem_frames.get(poem_name, 0) + 1\n    processed_data = []\n    current_poem_entry_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        new_entry = entry.copy()\n        if poem_name:\n            current_poem_entry_count[poem_name] = current_poem_entry_count.get(poem_name, 0) + 1\n            new_entry['frameNumber'] = entry.get('frameNumber', current_poem_entry_count[poem_name]) # Use existing or assign\n            new_entry['totalFrames'] = entry.get('totalFrames', poem_frames.get(poem_name, 0))\n        else:\n            new_entry['frameNumber'] = entry.get('frameNumber', 0)\n            new_entry['totalFrames'] = entry.get('totalFrames', 0)\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER61PRIMEPLUS for 'Magic ride' poem...\")\n    print(f\"Output directory: {OUTPUT_DIR}\")\n    genome_map = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    if not genome_map:\n        print(f\"Warning: No genome data loaded from {SYMBOLIC_GENOME_DATA_PATH}.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16),\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12),\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data_raw = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    # Ensure frameNumber and totalFrames are consistently populated\n    processed_timeline_data = calculate_frame_counts(timeline_data_raw)\n\n    poem_to_process = \"Magic ride\"\n    magic_ride_entries = []\n    for entry in processed_timeline_data:\n        if entry.get('poem', '').strip().lower() == poem_to_process.lower():\n            magic_ride_entries.append(entry)\n    \n    # Sort entries by original frameNumber if available, or by order in JSON as fallback\n    # Assuming 'frameNumber' from JSON is the definitive one for sorting if it exists and is numeric\n    def get_sort_key(e):\n        fn = e.get('frameNumber')\n        if isinstance(fn, (int, float)):\n            return fn\n        if isinstance(fn, str) and fn.isdigit():\n            return int(fn)\n        return float('inf') # Put entries with bad/missing frameNumber last\n\n    magic_ride_entries.sort(key=get_sort_key)\n\n    if not magic_ride_entries:\n        print(f\"No entries found for poem: {poem_to_process}\")\n        return\n\n    print(f\"Found {len(magic_ride_entries)} entries for poem '{poem_to_process}'. Rendering cards...\")\n    \n    cards_rendered_count = 0\n    for i, entry_data in enumerate(magic_ride_entries):\n        entry_id = entry_data.get('id', f'magic-ride_unknown-id_{i+1}')\n        frame_num_str = str(entry_data.get('frameNumber', i + 1)).zfill(3)\n        poem_slug = slugify(poem_to_process)\n        output_filename = f\"{poem_slug}_frame_{frame_num_str}.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        \n        print(f\"Rendering card for: {entry_id} (Frame: {frame_num_str}) -> {output_filename}\")\n        try:\n            render_card(entry_data, output_path, fonts, genome_map)\n            cards_rendered_count += 1\n        except Exception as e:\n            print(f\"  ERROR rendering card for {entry_id}: {e}\")\n\n    print(f\"\\nFinished rendering. Total cards for '{poem_to_process}': {cards_rendered_count}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "UPDATED-TIMELINE.json",
      "symbolic_genome_data.json",
      "{poem_slug}_frame_{frame_num_str}.png",
      " to ELEPHANT/TUSK/\n",
      "\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if ",
      ", relative_path)\n    else:\n        adjusted_relative_path = relative_path\n\n    full_image_path = os.path.join(BASE_IMAGE_DIR, adjusted_relative_path)\n    \n    if os.path.exists(full_image_path):\n        return os.path.normpath(full_image_path)\n    else:\n        # Fallback if TIGER/MR path not found, try original MR/ path directly under BASE_IMAGE_DIR\n        if relative_path.startswith(",
      "):\n                 # If it was an MR path, also specifically check TIGER/MR and TIGER folders\n                search_dirs.insert(0, os.path.join(BASE_IMAGE_DIR, ",
      "I_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    if genome_y_start < genome_section_y_on_img + 4: genome_y_start = genome_section_y_on_img + 4\n\n    max_render_width = BASE_WIDTH - 10 # Allow a small margin on the right\n    avg_char_width = chosen_genome_font.getlength(",
      ") else chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[2] - chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[0]\n    if placeholder_char_width <= 0: placeholder_char_width = 1\n\n    for original_line_text in genome_lines_to_render:\n        current_x = genome_content_x1_on_img\n        chars_that_fit_total_approx = int(max_render_width // avg_char_width)\n        if chars_that_fit_total_approx <= 0: chars_that_fit_total_approx = 1 # Avoid division by zero or negative\n        target_highlight_offset_in_view = chars_that_fit_total_approx // 4\n\n        display_start_index = 0\n        scrolled_from_left = False\n\n        if highlight_index >= 0: # Ensure we have a valid highlight_index\n            if len(original_line_text) <= chars_that_fit_total_approx:\n                # Line fits entirely, no scrolling needed\n                display_start_index = 0\n                scrolled_from_left = False\n            else:\n                # Line is longer than available space, scrolling is needed\n                scrolled_from_left = True\n                # Default start: try to position highlight_index at target_highlight_offset_in_view\n                tentative_display_start_index = max(0, highlight_index - target_highlight_offset_in_view)\n                \n                # Adjust if scrolling near the end of the line\n                # Ensure the window doesn",
      ":\n                current_char_overall_non_space_idx = get_non_space_char_index(original_line_text, idx_in_original)\n                if current_char_overall_non_space_idx == highlight_index:\n                    char_color = WHITE\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n        drawn_width = current_x - genome_content_x1_on_img\n        remaining_line_width = max_render_width - drawn_width\n        if remaining_line_width > 0 and placeholder_char_width > 0:\n            num_padding_chars = int(remaining_line_width // placeholder_char_width)\n            if num_padding_chars > 0:\n                draw.text((current_x, genome_y_start), PLACEHOLDER_CHAR * num_padding_chars, font=chosen_genome_font, fill=PLACEHOLDER_COLOR)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 4\n    header_content_y = current_y_offset + 4\n    top_row_internal_height = 30\n    id_label_text = ",
      "FRAME: ({frame_position_val}/{frame_total_val})",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ") # Put entries with bad/missing frameNumber last\n\n    magic_ride_entries.sort(key=get_sort_key)\n\n    if not magic_ride_entries:\n        print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random",
      "unicodedata"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER61PRIMEPLUS - Processes UPDATED-TIMELINE.json for 'Magic ride' poem only.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER61PRIME.\n- Outputs cards for 'Magic ride' to ELEPHANT/TUSK/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper_menlo_genome.py",
    "size": 25552,
    "lines": 486,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER_MENLO_GENOME - Clapperboard with Dynamic Menlo-Styled Genome Display\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n    \n    genome_lines_to_render = []\n    if genome_data_entry:\n        genome_lines_to_render.append(f\"{genome_data_entry.get('s_line', 'S_LINE_N/A')} S\")\n        genome_lines_to_render.append(f\"{genome_data_entry.get('i_line', 'I_LINE_N/A')} I\")\n        genome_lines_to_render.append(f\"{genome_data_entry.get('c_line', 'C_LINE_N/A')} C\")\n    else:\n        poem_display_title = entry.get('poem', 'Unknown Poem')[:35] # Truncate for display if long\n        genome_lines_to_render = [\n            f\"Genome not found for:\",\n            f\"'{poem_display_title}'\",\n            f\"(ID: {entry.get('id', 'N/A')})\"\n        ]\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2 # +2 for leading\n    \n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1 # +1 for leading\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8: # 8 for padding (4 top, 4 bottom)\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn't draw above top padding\n        genome_y_start = current_y_offset + 4\n\n    for line_text in genome_lines_to_render:\n        line_x = 10 # Left align genome text\n        \n        # Truncate line if too wide for the card\n        try:\n            line_width = chosen_genome_font.getlength(line_text)\n        except AttributeError: # Fallback for older Pillow versions\n            line_bbox = chosen_genome_font.getbbox(line_text)\n            line_width = line_bbox[2] - line_bbox[0]\n\n        if line_x + line_width > BASE_WIDTH - 10: # 10px right padding\n            available_width = BASE_WIDTH - 10 - line_x\n            # Estimate characters to keep (rough approximation)\n            avg_char_width_bbox = chosen_genome_font.getbbox(\"A\") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for \"...\"\n                line_text = line_text[:chars_to_keep] + \"...\"\n            else: # Should not happen with valid fonts\n                line_text = line_text[:10] + \"...\" # Fallback truncation\n\n        draw.text((line_x, genome_y_start), line_text, font=chosen_genome_font, fill=AMBER) # Use AMBER color\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, IMAGE TYPE, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    image_text = f\"IMAGE: {image_glyph} {image_type_str}\"\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), image_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. POEM AND LINE CONTENT ROW\n    poem_line_content_y = current_y_offset\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    draw.text((line_start_x, poem_line_content_y), line_prefix, font=header_font, fill=WHITE)\n    \n    line_content_start_x = line_start_x + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_start_x - 10\n    \n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_start_x, poem_line_content_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER_MENLO_GENOME card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"DejaVu Sans Mono\", \"Courier New\", 16, is_bold_preferred=True, is_bold_fallback=True),\n        'text':   get_font(\"DejaVu Sans Mono\", \"Courier New\", 16),\n        'text_bold': get_font(\"DejaVu Sans Mono\", \"Courier New\", 16, is_bold_preferred=True, is_bold_fallback=True),\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Use Menlo for genome\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Use Menlo for genome (tiny)\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_filename = f\"{entry_id}_clapper_menlo_genome.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {OUTPUT_DIR}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper_menlo_genome.png",
      "\nCLAPPER_MENLO_GENOME - Clapperboard with Dynamic Menlo-Styled Genome Display\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "S_LINE_N/A",
      "I_LINE_N/A",
      "C_LINE_N/A",
      "N/A",
      ")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn",
      ") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for ",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = ",
      ":\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER_MENLO_GENOME - Clapperboard with Dynamic Menlo-Styled Genome Display\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper45.py",
    "size": 27082,
    "lines": 509,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER45 - 'Image:' label, LINE content on same line, full S/I/C in header (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    # max_genome_line_chars = 36 # Removed: Show full genome lines\n    \n    genome_lines_to_render = []\n    if genome_data_entry:\n        s_line_data = genome_data_entry.get('s_line', 'S_LINE_N/A') # Full line\n        i_line_data = genome_data_entry.get('i_line', 'I_LINE_N/A') # Full line\n        c_line_data = genome_data_entry.get('c_line', 'C_LINE_N/A') # Full line\n        genome_lines_to_render.append(s_line_data)\n        genome_lines_to_render.append(i_line_data)\n        genome_lines_to_render.append(c_line_data)\n    else:\n        # poem_display_title = entry.get('poem', 'Unknown Poem')[:35]\n        genome_lines_to_render = [\n            'S_LINE_N/A',\n            'I_LINE_N/A',\n            'C_LINE_N/A'\n        ]\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2 # +2 for leading\n    \n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1 # +1 for leading\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8: # 8 for padding (4 top, 4 bottom)\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn't draw above top padding\n        genome_y_start = current_y_offset + 4\n\n    for line_text in genome_lines_to_render:\n        line_x = 10 # Left align genome text\n        \n        # Truncate line if too wide for the card\n        try:\n            line_width = chosen_genome_font.getlength(line_text)\n        except AttributeError: # Fallback for older Pillow versions\n            line_bbox = chosen_genome_font.getbbox(line_text)\n            line_width = line_bbox[2] - line_bbox[0]\n\n        if line_x + line_width > BASE_WIDTH - 10: # 10px right padding\n            available_width = BASE_WIDTH - 10 - line_x\n            # Estimate characters to keep (rough approximation)\n            avg_char_width_bbox = chosen_genome_font.getbbox(\"A\") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for \"...\"\n                line_text = line_text[:chars_to_keep] + \"...\"\n            else: # Should not happen with valid fonts\n                line_text = line_text[:10] + \"...\" # Fallback truncation\n\n        draw.text((line_x, genome_y_start), line_text, font=chosen_genome_font, fill=AMBER) # Use AMBER color\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Label + Value, single line)\n    image_label_text = f\"{image_glyph} Image:\" # Changed label\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    image_label_width = header_font.getbbox(image_label_text)[2]\n    \n    image_value_x_start = 10 + image_label_width + 5 # 5px padding\n    # Draw actual image_type_str next to label, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((image_value_x_start, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2]\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    line_field_x_start = image_value_x_start + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER45 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper45_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper45.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper45.png",
      " label, LINE content on same line, full S/I/C in header (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "S_LINE_N/A",
      "I_LINE_N/A",
      "C_LINE_N/A",
      "S_LINE_N/A",
      "I_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn",
      ") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER45 - 'Image:' label, LINE content on same line, full S/I/C in header (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper61.py",
    "size": 35522,
    "lines": 690,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER61 - Refined glyph mappings based on data discovery. Generating cards. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES - Mapped from discovered data to glyphs.md symbols\nSYNTAGMA_GLYPHS = {\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (CS)\": \"\u2756\", # Data uses \"CS\", glyphs.md uses \"XS\" for \u2756\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n    # Explicitly add other known syntagma types from glyphs.md for completeness, even if not in current data sample\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    # Image types found in syntagma data (e.g., \"Action-Image\") will correctly get '?'\n}  \n\n# IMAGE TYPES - Mapped from discovered data to glyphs.md symbols\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n    # Any other image types from data not listed here will get '?'\n}\n\n# CINEOSIS FUNCTIONS - Mapped from discovered data to glyphs.md symbols\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n    # Any other cineosis functions from data not listed here will get '?'\n}  \n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper61_output\") # Default output for clapper56TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef discover_unique_prompt_values(timeline_data_path):\n    print(f\"\\nDiscovering unique prompt values from: {timeline_data_path}\")\n    try:\n        with open(timeline_data_path, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline data file not found at {timeline_data_path}\")\n        return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {timeline_data_path}\")\n        return\n\n    unique_image_types = set()\n    unique_syntagma_types = set()\n    unique_cineosis_functions = set()\n\n    for entry in data:\n        image_type = entry.get('imageType')\n        if image_type is not None:\n            unique_image_types.add(image_type)\n        \n        syntagma_type = entry.get('syntagmaType')\n        if syntagma_type is not None:\n            unique_syntagma_types.add(syntagma_type)\n\n        cineosis_function = entry.get('cineosisFunction')\n        if cineosis_function is not None:\n            unique_cineosis_functions.add(cineosis_function)\n\n    print(\"\\n--- Unique Image Types ---\")\n    for item in sorted(list(unique_image_types)):\n        print(item) # Simplified print\n    \n    print(\"\\n--- Unique Syntagma Types ---\")\n    for item in sorted(list(unique_syntagma_types)):\n        print(item) # Simplified print\n\n    print(\"\\n--- Unique Cineosis Functions ---\")\n    for item in sorted(list(unique_cineosis_functions)):\n        print(item) # Simplified print\n    print(\"\\nDiscovery complete.\")\n\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and constants for genome section\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70) # Dim gray for placeholder characters\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Removed ASCII Grid Pattern drawing\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for original_line_text in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n\n        original_length = len(original_line_text)\n        # Calculate width of original text\n        width_of_original_line = 0\n        for i in range(original_length):\n            try:\n                width_of_original_line += chosen_genome_font.getlength(original_line_text[i])\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(original_line_text[i])\n                width_of_original_line += bbox[2] - bbox[0]\n\n        remaining_width = max_render_width - width_of_original_line\n        num_padding_chars = 0\n        padding_string = \"\"\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ' ')\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ' ': # It's an actual glyph from the original line\n                    current_glyph_num_in_line += 1 # Increment for actual glyphs\n                    if current_glyph_num_in_line == highlight_index:\n                        char_color = WHITE # Highlight this glyph\n                    else:\n                        char_color = AMBER # Regular genome glyph color\n                else:\n                    # It's a space from the original line\n                    char_color = AMBER # Spaces are part of the genome line, colored AMBER\n            else:\n                # It's a placeholder padding character\n                char_color = PLACEHOLDER_COLOR\n            \n            # Calculate width of the character to be drawn\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            # Truncation and ellipsis logic\n            if current_x + char_width > max_render_width: # If this char doesn't fit\n                if current_x + ellipsis_width <= max_render_width: # Check if ellipsis fits\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part of the line\n                        ellipsis_fill = AMBER # Default for original part\n                        \n                        # Determine the 0-based index of the first glyph that is part of the truncation\n                        first_truncated_glyph_idx_val = current_glyph_num_in_line\n                        if char_to_draw == ' ': # If the char causing truncation is a space\n                           first_truncated_glyph_idx_val +=1 # then the actual first glyph truncated is the next one\n                        \n                        if highlight_index != -1 and highlight_index >= first_truncated_glyph_idx_val and highlight_index < total_glyphs_in_original:\n                            ellipsis_fill = WHITE # Highlighted glyph is in the truncated part\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line (whether ellipsis was drawn or not)\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str_raw, \"?\").strip() # Default to ?\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    glyph_column_x = 10 # Define glyph_column_x early, it's a fixed position\n    draw.text((glyph_column_x, initial_content_row_y), image_glyph, font=chosen_genome_font, fill=WHITE) # Draw glyph with genome_font\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR) # Draw text\n    image_text_width = text_font.getbbox(image_type_str_raw)[2] # Width of the image type text itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str_raw + padding\n    line_field_x_start = value_start_x + image_text_width + 30 # 30px padding after image text\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    # The glyph_column_x is where glyphs are drawn (e.g., 10px).\n    # The value_start_x is dynamically calculated based on max_glyph_width.\n    # The divider should visually separate the 'Image/Line' row from the 'Prompt' section.\n    # Let's make the divider start just left of the glyph column.\n    divider_start_x = glyph_column_x - 4 if glyph_column_x > 4 else 6 \n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at glyph_column_x (e.g. 10), Values at value_start_x (dynamic)\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str_raw, \"?\").strip() # Default to ?\n    draw.text((glyph_column_x, current_y_offset), syntagma_glyph, font=chosen_genome_font, fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str_raw, \"?\").strip() # Default to ?\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=chosen_genome_font, fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    longest_entry_path = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json\" # Corrected path\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(output_dir_name, \"_EXTREME_TEST_CARD_clapper61.png\") # Use passed output_dir_name\n    render_card(longest_entry, output_path, fonts, genome_map) # Pass genome_map\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER61 card generation with refined glyph mappings...\")\n    # discover_unique_prompt_values(TIMELINE_PATH) # Discovery was done, commenting out\n    # print(\"\\nData discovery run. Card generation is bypassed in clapper61.py.\")\n    # return # Re-enabling card generation\n\n    # Original main content, now active again:\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    output_dir_name = \"cards_clapper61_output\"\n    os.makedirs(output_dir_name, exist_ok=True) # Ensure output directory exists\n\n    # render_demo_card(fonts, genome_map, output_dir_name) # Would also need output_dir_name if uncommented\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    if not processed_timeline_data:\n        print(\"No timeline data to process for random poem entries.\")\n        return\n\n    poems_to_entries = {}\n    for entry in processed_timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            if poem_title not in poems_to_entries:\n                poems_to_entries[poem_title] = []\n            poems_to_entries[poem_title].append(entry)\n\n    if not poems_to_entries:\n        print(\"No poems found in timeline data to select random entries from.\")\n        return\n\n    print(f\"\\nRendering one random entry for each of the {len(poems_to_entries)} poems found...\")\n    count = 0\n    for poem_title, entries_for_poem in poems_to_entries.items():\n        if entries_for_poem:\n            selected_entry = random.choice(entries_for_poem)\n            entry_id = selected_entry.get('id', f'random_poem_entry_{count+1}')\n            output_filename = f\"{entry_id}_clapper61.png\"\n            output_path = os.path.join(output_dir_name, output_filename)\n            \n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_title}'...\")\n            render_card(selected_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n            count += 1\n        else:\n            print(f\"No entries found for poem '{poem_title}' to select a random one.\")\n\n    print(f\"\\nFinished rendering {count} random cards per poem.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json",
      "_EXTREME_TEST_CARD_clapper61.png",
      "{entry_id}_clapper61.png",
      "\nCLAPPER61 - Refined glyph mappings based on data discovery. Generating cards. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES - Mapped from discovered data to glyphs.md symbols\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ",
      ")\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    # The glyph_column_x is where glyphs are drawn (e.g., 10px).\n    # The value_start_x is dynamically calculated based on max_glyph_width.\n    # The divider should visually separate the ",
      ").strip() # Default to ?\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=chosen_genome_font, fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER61 - Refined glyph mappings based on data discovery. Generating cards. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper55.py",
    "size": 31251,
    "lines": 602,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER55 - Use ASCII for Syntagma glyphs, investigate highlighting. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \" [AS] \", \n    \"Parallel Syntagma (PS)\":     \" [PS] \", \n    \"Alternating Syntagma (LS)\":  \" [LS] \", \n    \"--- Syntagma\": \" [---] \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper55_output\") # Default output for clapper55TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and constants for genome section\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70) # Dim gray for placeholder characters\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Removed ASCII Grid Pattern drawing\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for original_line_text in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n\n        original_length = len(original_line_text)\n        # Calculate width of original text\n        width_of_original_line = 0\n        for i in range(original_length):\n            try:\n                width_of_original_line += chosen_genome_font.getlength(original_line_text[i])\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(original_line_text[i])\n                width_of_original_line += bbox[2] - bbox[0]\n\n        remaining_width = max_render_width - width_of_original_line\n        num_padding_chars = 0\n        padding_string = \"\"\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            char_color = PLACEHOLDER_COLOR # Default for padding characters\n            is_original_char = char_idx < original_length\n\n            if is_original_char:\n                char_color = AMBER\n                if char_idx == highlight_index:\n                    char_color = WHITE # Highlight original character\n            \n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                if current_x + ellipsis_width <= max_render_width:\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part\n                        ellipsis_fill = AMBER\n                        if highlight_index != -1 and highlight_index >= char_idx and highlight_index < original_length:\n                            ellipsis_fill = WHITE # Highlighted char is part of ellipsis\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    image_label_text = f\"{image_glyph}\" # Only glyph\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    \n    # Draw actual image_type_str (value) at value_start_x, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((value_start_x, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2] # Width of the image type value itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str + padding\n    line_field_x_start = value_start_x + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    longest_entry_path = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json\" # Corrected path\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(output_dir_name, \"_EXTREME_TEST_CARD_clapper55.png\") # Use passed output_dir_name\n    render_card(longest_entry, output_path, fonts, genome_map) # Pass genome_map\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER55 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    output_dir_name = \"cards_clapper55_output\"\n    os.makedirs(output_dir_name, exist_ok=True) # Ensure output directory exists\n\n    # render_demo_card(fonts, genome_map, output_dir_name) # Would also need output_dir_name if uncommented\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    if not processed_timeline_data:\n        print(\"No timeline data to process for random poem entries.\")\n        return\n\n    poems_to_entries = {}\n    for entry in processed_timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            if poem_title not in poems_to_entries:\n                poems_to_entries[poem_title] = []\n            poems_to_entries[poem_title].append(entry)\n\n    if not poems_to_entries:\n        print(\"No poems found in timeline data to select random entries from.\")\n        return\n\n    print(f\"\\nRendering one random entry for each of the {len(poems_to_entries)} poems found...\")\n    count = 0\n    for poem_title, entries_for_poem in poems_to_entries.items():\n        if entries_for_poem:\n            selected_entry = random.choice(entries_for_poem)\n            entry_id = selected_entry.get('id', f'random_poem_entry_{count+1}')\n            output_filename = f\"{entry_id}_clapper55.png\"\n            output_path = os.path.join(output_dir_name, output_filename)\n            \n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_title}'...\")\n            render_card(selected_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n            count += 1\n        else:\n            print(f\"No entries found for poem '{poem_title}' to select a random one.\")\n\n    print(f\"\\nFinished rendering {count} random cards per poem.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json",
      "_EXTREME_TEST_CARD_clapper55.png",
      "{entry_id}_clapper55.png",
      "\nCLAPPER55 - Use ASCII for Syntagma glyphs, investigate highlighting. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            char_color = PLACEHOLDER_COLOR # Default for padding characters\n            is_original_char = char_idx < original_length\n\n            if is_original_char:\n                char_color = AMBER\n                if char_idx == highlight_index:\n                    char_color = WHITE # Highlight original character\n            \n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                if current_x + ellipsis_width <= max_render_width:\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part\n                        ellipsis_fill = AMBER\n                        if highlight_index != -1 and highlight_index >= char_idx and highlight_index < original_length:\n                            ellipsis_fill = WHITE # Highlighted char is part of ellipsis\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get(",
      " # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER55 - Use ASCII for Syntagma glyphs, investigate highlighting. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper41.py",
    "size": 25694,
    "lines": 488,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER41 - Clapperboard with Dynamic Menlo-Styled Genome Display (Menlo Regular for all text)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n    \n    genome_lines_to_render = []\n    if genome_data_entry:\n        genome_lines_to_render.append(f\"{genome_data_entry.get('s_line', 'S_LINE_N/A')} S\")\n        genome_lines_to_render.append(f\"{genome_data_entry.get('i_line', 'I_LINE_N/A')} I\")\n        genome_lines_to_render.append(f\"{genome_data_entry.get('c_line', 'C_LINE_N/A')} C\")\n    else:\n        poem_display_title = entry.get('poem', 'Unknown Poem')[:35] # Truncate for display if long\n        genome_lines_to_render = [\n            f\"Genome not found for:\",\n            f\"'{poem_display_title}'\",\n            f\"(ID: {entry.get('id', 'N/A')})\"\n        ]\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2 # +2 for leading\n    \n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1 # +1 for leading\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8: # 8 for padding (4 top, 4 bottom)\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn't draw above top padding\n        genome_y_start = current_y_offset + 4\n\n    for line_text in genome_lines_to_render:\n        line_x = 10 # Left align genome text\n        \n        # Truncate line if too wide for the card\n        try:\n            line_width = chosen_genome_font.getlength(line_text)\n        except AttributeError: # Fallback for older Pillow versions\n            line_bbox = chosen_genome_font.getbbox(line_text)\n            line_width = line_bbox[2] - line_bbox[0]\n\n        if line_x + line_width > BASE_WIDTH - 10: # 10px right padding\n            available_width = BASE_WIDTH - 10 - line_x\n            # Estimate characters to keep (rough approximation)\n            avg_char_width_bbox = chosen_genome_font.getbbox(\"A\") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for \"...\"\n                line_text = line_text[:chars_to_keep] + \"...\"\n            else: # Should not happen with valid fonts\n                line_text = line_text[:10] + \"...\" # Fallback truncation\n\n        draw.text((line_x, genome_y_start), line_text, font=chosen_genome_font, fill=AMBER) # Use AMBER color\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, IMAGE TYPE, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    image_text = f\"IMAGE: {image_glyph} {image_type_str}\"\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), image_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. POEM AND LINE CONTENT ROW\n    poem_line_content_y = current_y_offset\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    draw.text((line_start_x, poem_line_content_y), line_prefix, font=header_font, fill=WHITE)\n    \n    line_content_start_x = line_start_x + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_start_x - 10\n    \n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_start_x, poem_line_content_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER41 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper41_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper41.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper41.png",
      "\nCLAPPER41 - Clapperboard with Dynamic Menlo-Styled Genome Display (Menlo Regular for all text)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "S_LINE_N/A",
      "I_LINE_N/A",
      "C_LINE_N/A",
      "N/A",
      ")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn",
      ") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for ",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = ",
      ":\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER41 - Clapperboard with Dynamic Menlo-Styled Genome Display (Menlo Regular for all text)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper51.py",
    "size": 29220,
    "lines": 558,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER51 - Genome report: Denser dot grid, light box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and grid spacing for genome section enhancements\n    GRID_PATTERN_COLOR = (70, 70, 70)  # Slightly lighter Dark gray\n    LIGHT_BOX_COLOR = (100, 100, 100) # Light gray\n    GRID_SPACING = 4 # Denser grid\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Draw ASCII Grid Pattern (BEFORE text rendering)\n    for gx in range(genome_content_x1_on_img, genome_content_x2_on_img, GRID_SPACING):\n        for gy in range(genome_content_y1_on_img, genome_content_y2_on_img, GRID_SPACING):\n            draw.point([(gx, gy)], fill=GRID_PATTERN_COLOR)\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for line_text_full in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n        \n        for char_idx, char_to_draw in enumerate(line_text_full):\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                # Check if ellipsis itself can fit\n                if current_x + ellipsis_width <= max_render_width:\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=AMBER)\n                break # Stop drawing this line\n\n            char_color = AMBER\n            if char_idx == highlight_index:\n                char_color = WHITE # Highlight if current character's index matches frame_position\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Draw Light Box (AFTER text rendering, frames the genome content area)\n    box_x1 = genome_content_x1_on_img - 2\n    box_y1 = genome_content_y1_on_img - 2\n    box_x2 = genome_content_x2_on_img + 2\n    box_y2 = genome_content_y2_on_img + 2\n    \n    draw.rectangle(\n        [(box_x1, box_y1), (box_x2, box_y2)],\n        outline=LIGHT_BOX_COLOR,\n        width=1\n    )\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    image_label_text = f\"{image_glyph}\" # Only glyph\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    \n    # Draw actual image_type_str (value) at value_start_x, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((value_start_x, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2] # Width of the image type value itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str + padding\n    line_field_x_start = value_start_x + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER51 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper51_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper51.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper51.png",
      "\nCLAPPER51 - Genome report: Denser dot grid, light box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get(",
      " # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER51 - Genome report: Denser dot grid, light box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper40.py",
    "size": 25599,
    "lines": 488,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER40 - Clapperboard with Dynamic Menlo-Styled Genome Display (Menlo for all text)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n    \n    genome_lines_to_render = []\n    if genome_data_entry:\n        genome_lines_to_render.append(f\"{genome_data_entry.get('s_line', 'S_LINE_N/A')} S\")\n        genome_lines_to_render.append(f\"{genome_data_entry.get('i_line', 'I_LINE_N/A')} I\")\n        genome_lines_to_render.append(f\"{genome_data_entry.get('c_line', 'C_LINE_N/A')} C\")\n    else:\n        poem_display_title = entry.get('poem', 'Unknown Poem')[:35] # Truncate for display if long\n        genome_lines_to_render = [\n            f\"Genome not found for:\",\n            f\"'{poem_display_title}'\",\n            f\"(ID: {entry.get('id', 'N/A')})\"\n        ]\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2 # +2 for leading\n    \n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1 # +1 for leading\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8: # 8 for padding (4 top, 4 bottom)\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn't draw above top padding\n        genome_y_start = current_y_offset + 4\n\n    for line_text in genome_lines_to_render:\n        line_x = 10 # Left align genome text\n        \n        # Truncate line if too wide for the card\n        try:\n            line_width = chosen_genome_font.getlength(line_text)\n        except AttributeError: # Fallback for older Pillow versions\n            line_bbox = chosen_genome_font.getbbox(line_text)\n            line_width = line_bbox[2] - line_bbox[0]\n\n        if line_x + line_width > BASE_WIDTH - 10: # 10px right padding\n            available_width = BASE_WIDTH - 10 - line_x\n            # Estimate characters to keep (rough approximation)\n            avg_char_width_bbox = chosen_genome_font.getbbox(\"A\") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for \"...\"\n                line_text = line_text[:chars_to_keep] + \"...\"\n            else: # Should not happen with valid fonts\n                line_text = line_text[:10] + \"...\" # Fallback truncation\n\n        draw.text((line_x, genome_y_start), line_text, font=chosen_genome_font, fill=AMBER) # Use AMBER color\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, IMAGE TYPE, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    image_text = f\"IMAGE: {image_glyph} {image_type_str}\"\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), image_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. POEM AND LINE CONTENT ROW\n    poem_line_content_y = current_y_offset\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    draw.text((line_start_x, poem_line_content_y), line_prefix, font=header_font, fill=WHITE)\n    \n    line_content_start_x = line_start_x + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_start_x - 10\n    \n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_start_x, poem_line_content_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER40 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=True, is_bold_fallback=True),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16),\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=True, is_bold_fallback=True),\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper40_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper40.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper40.png",
      "\nCLAPPER40 - Clapperboard with Dynamic Menlo-Styled Genome Display (Menlo for all text)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "S_LINE_N/A",
      "I_LINE_N/A",
      "C_LINE_N/A",
      "N/A",
      ")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn",
      ") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for ",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = ",
      ":\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER40 - Clapperboard with Dynamic Menlo-Styled Genome Display (Menlo for all text)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper50.py",
    "size": 29221,
    "lines": 558,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER50 - Genome report: light box, ASCII grid background. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and grid spacing for genome section enhancements\n    GRID_PATTERN_COLOR = (50, 50, 50)  # Dark gray\n    LIGHT_BOX_COLOR = (100, 100, 100) # Light gray\n    GRID_SPACING = 7 # Pixels for grid density\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Draw ASCII Grid Pattern (BEFORE text rendering)\n    for gx in range(genome_content_x1_on_img, genome_content_x2_on_img, GRID_SPACING):\n        for gy in range(genome_content_y1_on_img, genome_content_y2_on_img, GRID_SPACING):\n            draw.point([(gx, gy)], fill=GRID_PATTERN_COLOR)\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for line_text_full in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n        \n        for char_idx, char_to_draw in enumerate(line_text_full):\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                # Check if ellipsis itself can fit\n                if current_x + ellipsis_width <= max_render_width:\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=AMBER)\n                break # Stop drawing this line\n\n            char_color = AMBER\n            if char_idx == highlight_index:\n                char_color = WHITE # Highlight if current character's index matches frame_position\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Draw Light Box (AFTER text rendering, frames the genome content area)\n    box_x1 = genome_content_x1_on_img - 2\n    box_y1 = genome_content_y1_on_img - 2\n    box_x2 = genome_content_x2_on_img + 2\n    box_y2 = genome_content_y2_on_img + 2\n    \n    draw.rectangle(\n        [(box_x1, box_y1), (box_x2, box_y2)],\n        outline=LIGHT_BOX_COLOR,\n        width=1\n    )\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    image_label_text = f\"{image_glyph}\" # Only glyph\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    \n    # Draw actual image_type_str (value) at value_start_x, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((value_start_x, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2] # Width of the image type value itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str + padding\n    line_field_x_start = value_start_x + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER50 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper50_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper50.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper50.png",
      "\nCLAPPER50 - Genome report: light box, ASCII grid background. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get(",
      " # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER50 - Genome report: light box, ASCII grid background. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper44.py",
    "size": 26488,
    "lines": 499,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER44 - Full untruncated S/I/C lines in header, new Image Type styling (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    # max_genome_line_chars = 36 # Removed: Show full genome lines\n    \n    genome_lines_to_render = []\n    if genome_data_entry:\n        s_line_data = genome_data_entry.get('s_line', 'S_LINE_N/A') # Full line\n        i_line_data = genome_data_entry.get('i_line', 'I_LINE_N/A') # Full line\n        c_line_data = genome_data_entry.get('c_line', 'C_LINE_N/A') # Full line\n        genome_lines_to_render.append(s_line_data)\n        genome_lines_to_render.append(i_line_data)\n        genome_lines_to_render.append(c_line_data)\n    else:\n        # poem_display_title = entry.get('poem', 'Unknown Poem')[:35]\n        genome_lines_to_render = [\n            'S_LINE_N/A',\n            'I_LINE_N/A',\n            'C_LINE_N/A'\n        ]\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2 # +2 for leading\n    \n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1 # +1 for leading\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8: # 8 for padding (4 top, 4 bottom)\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn't draw above top padding\n        genome_y_start = current_y_offset + 4\n\n    for line_text in genome_lines_to_render:\n        line_x = 10 # Left align genome text\n        \n        # Truncate line if too wide for the card\n        try:\n            line_width = chosen_genome_font.getlength(line_text)\n        except AttributeError: # Fallback for older Pillow versions\n            line_bbox = chosen_genome_font.getbbox(line_text)\n            line_width = line_bbox[2] - line_bbox[0]\n\n        if line_x + line_width > BASE_WIDTH - 10: # 10px right padding\n            available_width = BASE_WIDTH - 10 - line_x\n            # Estimate characters to keep (rough approximation)\n            avg_char_width_bbox = chosen_genome_font.getbbox(\"A\") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for \"...\"\n                line_text = line_text[:chars_to_keep] + \"...\"\n            else: # Should not happen with valid fonts\n                line_text = line_text[:10] + \"...\" # Fallback truncation\n\n        draw.text((line_x, genome_y_start), line_text, font=chosen_genome_font, fill=AMBER) # Use AMBER color\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE TYPE AND LINE CONTENT ROW\n    content_row_y = current_y_offset # Renamed from poem_line_content_y\n    image_type_str = entry.get('imageType', '---') # Moved from section 2\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # New Image Type rendering (styled like prompt section items)\n    image_type_label_text = f\"{image_glyph} Image Type:\"\n    draw.text((10, content_row_y), image_type_label_text, font=header_font, fill=WHITE)\n    image_type_label_width = header_font.getbbox(image_type_label_text)[2]\n    prompt_max_width = BASE_WIDTH - 20 # Use same max_width as prompt section\n    content_row_y = draw_colored_text(draw, image_type_str, 10 + image_type_label_width + 5, content_row_y, IMAGE_TYPE_COLOR, text_font, prompt_max_width - (image_type_label_width + 5), text_font)\n    # content_row_y is now updated to be below the rendered image_type_str\n\n    # LINE content rendering, starting below the Image Type\n    line_start_x = 10 # Fixed X for \"LINE:\" prefix\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    draw.text((line_start_x, content_row_y), line_prefix, font=header_font, fill=WHITE)\n    \n    line_content_start_x = line_start_x + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_start_x - 10 # 10px right padding\n    \n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_start_x, content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER44 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper44_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper44.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper44.png",
      "\nCLAPPER44 - Full untruncated S/I/C lines in header, new Image Type styling (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "S_LINE_N/A",
      "I_LINE_N/A",
      "C_LINE_N/A",
      "S_LINE_N/A",
      "I_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn",
      ") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER44 - Full untruncated S/I/C lines in header, new Image Type styling (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper60.py",
    "size": 33251,
    "lines": 639,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER60 - Use single symbolic glyphs (like genome barcode) for prompt section. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES from glyphs.md\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2756\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n}  \n\n# IMAGE TYPES from glyphs.md\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n}\n\n# CINEOSIS FUNCTIONS from glyphs.md\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n}  \n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper60_output\") # Default output for clapper56TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and constants for genome section\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70) # Dim gray for placeholder characters\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Removed ASCII Grid Pattern drawing\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for original_line_text in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n\n        original_length = len(original_line_text)\n        # Calculate width of original text\n        width_of_original_line = 0\n        for i in range(original_length):\n            try:\n                width_of_original_line += chosen_genome_font.getlength(original_line_text[i])\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(original_line_text[i])\n                width_of_original_line += bbox[2] - bbox[0]\n\n        remaining_width = max_render_width - width_of_original_line\n        num_padding_chars = 0\n        padding_string = \"\"\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ' ')\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ' ': # It's an actual glyph from the original line\n                    current_glyph_num_in_line += 1 # Increment for actual glyphs\n                    if current_glyph_num_in_line == highlight_index:\n                        char_color = WHITE # Highlight this glyph\n                    else:\n                        char_color = AMBER # Regular genome glyph color\n                else:\n                    # It's a space from the original line\n                    char_color = AMBER # Spaces are part of the genome line, colored AMBER\n            else:\n                # It's a placeholder padding character\n                char_color = PLACEHOLDER_COLOR\n            \n            # Calculate width of the character to be drawn\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            # Truncation and ellipsis logic\n            if current_x + char_width > max_render_width: # If this char doesn't fit\n                if current_x + ellipsis_width <= max_render_width: # Check if ellipsis fits\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part of the line\n                        ellipsis_fill = AMBER # Default for original part\n                        \n                        # Determine the 0-based index of the first glyph that is part of the truncation\n                        first_truncated_glyph_idx_val = current_glyph_num_in_line\n                        if char_to_draw == ' ': # If the char causing truncation is a space\n                           first_truncated_glyph_idx_val +=1 # then the actual first glyph truncated is the next one\n                        \n                        if highlight_index != -1 and highlight_index >= first_truncated_glyph_idx_val and highlight_index < total_glyphs_in_original:\n                            ellipsis_fill = WHITE # Highlighted glyph is in the truncated part\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line (whether ellipsis was drawn or not)\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str_raw, \"?\").strip() # Default to ?\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    glyph_column_x = 10 # Define glyph_column_x early, it's a fixed position\n    draw.text((glyph_column_x, initial_content_row_y), image_glyph, font=chosen_genome_font, fill=WHITE) # Draw glyph with genome_font\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR) # Draw text\n    image_text_width = text_font.getbbox(image_type_str_raw)[2] # Width of the image type text itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str_raw + padding\n    line_field_x_start = value_start_x + image_text_width + 30 # 30px padding after image text\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    # The glyph_column_x is where glyphs are drawn (e.g., 10px).\n    # The value_start_x is dynamically calculated based on max_glyph_width.\n    # The divider should visually separate the 'Image/Line' row from the 'Prompt' section.\n    # Let's make the divider start just left of the glyph column.\n    divider_start_x = glyph_column_x - 4 if glyph_column_x > 4 else 6 \n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at glyph_column_x (e.g. 10), Values at value_start_x (dynamic)\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str_raw, \"?\").strip() # Default to ?\n    draw.text((glyph_column_x, current_y_offset), syntagma_glyph, font=chosen_genome_font, fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str_raw, \"?\").strip() # Default to ?\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=chosen_genome_font, fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    longest_entry_path = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json\" # Corrected path\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(output_dir_name, \"_EXTREME_TEST_CARD_clapper60.png\") # Use passed output_dir_name\n    render_card(longest_entry, output_path, fonts, genome_map) # Pass genome_map\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER60 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    output_dir_name = \"cards_clapper60_output\"\n    os.makedirs(output_dir_name, exist_ok=True) # Ensure output directory exists\n\n    # render_demo_card(fonts, genome_map, output_dir_name) # Would also need output_dir_name if uncommented\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    if not processed_timeline_data:\n        print(\"No timeline data to process for random poem entries.\")\n        return\n\n    poems_to_entries = {}\n    for entry in processed_timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            if poem_title not in poems_to_entries:\n                poems_to_entries[poem_title] = []\n            poems_to_entries[poem_title].append(entry)\n\n    if not poems_to_entries:\n        print(\"No poems found in timeline data to select random entries from.\")\n        return\n\n    print(f\"\\nRendering one random entry for each of the {len(poems_to_entries)} poems found...\")\n    count = 0\n    for poem_title, entries_for_poem in poems_to_entries.items():\n        if entries_for_poem:\n            selected_entry = random.choice(entries_for_poem)\n            entry_id = selected_entry.get('id', f'random_poem_entry_{count+1}')\n            output_filename = f\"{entry_id}_clapper60.png\"\n            output_path = os.path.join(output_dir_name, output_filename)\n            \n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_title}'...\")\n            render_card(selected_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n            count += 1\n        else:\n            print(f\"No entries found for poem '{poem_title}' to select a random one.\")\n\n    print(f\"\\nFinished rendering {count} random cards per poem.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json",
      "_EXTREME_TEST_CARD_clapper60.png",
      "{entry_id}_clapper60.png",
      "\nCLAPPER60 - Use single symbolic glyphs (like genome barcode) for prompt section. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES from glyphs.md\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ",
      ")\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    # The glyph_column_x is where glyphs are drawn (e.g., 10px).\n    # The value_start_x is dynamically calculated based on max_glyph_width.\n    # The divider should visually separate the ",
      ").strip() # Default to ?\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=chosen_genome_font, fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER60 - Use single symbolic glyphs (like genome barcode) for prompt section. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper54.py",
    "size": 31343,
    "lines": 601,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER54 - Genome lines dynamically padded with placeholders to fill width, no box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and constants for genome section\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70) # Dim gray for placeholder characters\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Removed ASCII Grid Pattern drawing\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for original_line_text in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n\n        original_length = len(original_line_text)\n        # Calculate width of original text\n        width_of_original_line = 0\n        for i in range(original_length):\n            try:\n                width_of_original_line += chosen_genome_font.getlength(original_line_text[i])\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(original_line_text[i])\n                width_of_original_line += bbox[2] - bbox[0]\n\n        remaining_width = max_render_width - width_of_original_line\n        num_padding_chars = 0\n        padding_string = \"\"\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            char_color = PLACEHOLDER_COLOR # Default for padding characters\n            is_original_char = char_idx < original_length\n\n            if is_original_char:\n                char_color = AMBER\n                if char_idx == highlight_index:\n                    char_color = WHITE # Highlight original character\n            \n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                if current_x + ellipsis_width <= max_render_width:\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part\n                        ellipsis_fill = AMBER\n                        if highlight_index != -1 and highlight_index >= char_idx and highlight_index < original_length:\n                            ellipsis_fill = WHITE # Highlighted char is part of ellipsis\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    image_label_text = f\"{image_glyph}\" # Only glyph\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    \n    # Draw actual image_type_str (value) at value_start_x, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((value_start_x, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2] # Width of the image type value itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str + padding\n    line_field_x_start = value_start_x + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    longest_entry_path = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json\" # Corrected path\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(output_dir_name, \"_EXTREME_TEST_CARD_clapper54.png\") # Use passed output_dir_name\n    render_card(longest_entry, output_path, fonts, genome_map) # Pass genome_map\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER54 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    output_dir_name = \"cards_clapper54_output\"\n    os.makedirs(output_dir_name, exist_ok=True) # Ensure output directory exists\n\n    # render_demo_card(fonts, genome_map, output_dir_name) # Would also need output_dir_name if uncommented\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    if not processed_timeline_data:\n        print(\"No timeline data to process for random poem entries.\")\n        return\n\n    poems_to_entries = {}\n    for entry in processed_timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            if poem_title not in poems_to_entries:\n                poems_to_entries[poem_title] = []\n            poems_to_entries[poem_title].append(entry)\n\n    if not poems_to_entries:\n        print(\"No poems found in timeline data to select random entries from.\")\n        return\n\n    print(f\"\\nRendering one random entry for each of the {len(poems_to_entries)} poems found...\")\n    count = 0\n    for poem_title, entries_for_poem in poems_to_entries.items():\n        if entries_for_poem:\n            selected_entry = random.choice(entries_for_poem)\n            entry_id = selected_entry.get('id', f'random_poem_entry_{count+1}')\n            output_filename = f\"{entry_id}_clapper54.png\"\n            output_path = os.path.join(output_dir_name, output_filename)\n            \n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_title}'...\")\n            render_card(selected_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n            count += 1\n        else:\n            print(f\"No entries found for poem '{poem_title}' to select a random one.\")\n\n    print(f\"\\nFinished rendering {count} random cards per poem.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json",
      "_EXTREME_TEST_CARD_clapper54.png",
      "{entry_id}_clapper54.png",
      "\nCLAPPER54 - Genome lines dynamically padded with placeholders to fill width, no box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            char_color = PLACEHOLDER_COLOR # Default for padding characters\n            is_original_char = char_idx < original_length\n\n            if is_original_char:\n                char_color = AMBER\n                if char_idx == highlight_index:\n                    char_color = WHITE # Highlight original character\n            \n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                if current_x + ellipsis_width <= max_render_width:\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part\n                        ellipsis_fill = AMBER\n                        if highlight_index != -1 and highlight_index >= char_idx and highlight_index < original_length:\n                            ellipsis_fill = WHITE # Highlighted char is part of ellipsis\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get(",
      " # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER54 - Genome lines dynamically padded with placeholders to fill width, no box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper43.py",
    "size": 26230,
    "lines": 493,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER43 - Clapperboard with POEM and IMAGE fields swapped (Menlo Regular, new S/I/C style)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    max_genome_line_chars = 36 # Max characters for the S/I/C glyph sequence itself\n    \n    genome_lines_to_render = []\n    if genome_data_entry:\n        s_line_data = genome_data_entry.get('s_line', 'S_LINE_N/A')[:max_genome_line_chars]\n        i_line_data = genome_data_entry.get('i_line', 'I_LINE_N/A')[:max_genome_line_chars]\n        c_line_data = genome_data_entry.get('c_line', 'C_LINE_N/A')[:max_genome_line_chars]\n        genome_lines_to_render.append(f'\u259e Syntagma: \"{s_line_data}\"')\n        genome_lines_to_render.append(f'\u263c Image: \"{i_line_data}\"')\n        genome_lines_to_render.append(f'\u25ff Cineosis: \"{c_line_data}\"')\n    else:\n        poem_display_title = entry.get('poem', 'Unknown Poem')[:35] # Truncate for display if long\n        genome_lines_to_render = [\n            '\u259e Syntagma: \"GENOME_N/A\"',\n            '\u263c Image: \"GENOME_N/A\"',\n            '\u25ff Cineosis: \"GENOME_N/A\"',\n            f\"(for poem: '{poem_display_title}')\"\n        ]\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2 # +2 for leading\n    \n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1 # +1 for leading\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8: # 8 for padding (4 top, 4 bottom)\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn't draw above top padding\n        genome_y_start = current_y_offset + 4\n\n    for line_text in genome_lines_to_render:\n        line_x = 10 # Left align genome text\n        \n        # Truncate line if too wide for the card\n        try:\n            line_width = chosen_genome_font.getlength(line_text)\n        except AttributeError: # Fallback for older Pillow versions\n            line_bbox = chosen_genome_font.getbbox(line_text)\n            line_width = line_bbox[2] - line_bbox[0]\n\n        if line_x + line_width > BASE_WIDTH - 10: # 10px right padding\n            available_width = BASE_WIDTH - 10 - line_x\n            # Estimate characters to keep (rough approximation)\n            avg_char_width_bbox = chosen_genome_font.getbbox(\"A\") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for \"...\"\n                line_text = line_text[:chars_to_keep] + \"...\"\n            else: # Should not happen with valid fonts\n                line_text = line_text[:10] + \"...\" # Fallback truncation\n\n        draw.text((line_x, genome_y_start), line_text, font=chosen_genome_font, fill=AMBER) # Use AMBER color\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE TYPE AND LINE CONTENT ROW\n    content_row_y = current_y_offset # Renamed from poem_line_content_y\n    image_type_str = entry.get('imageType', '---') # Moved from section 2\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \") # Moved from section 2\n    image_text = f\"IMAGE: {image_glyph} {image_type_str}\" # Moved from section 2\n    line_content = entry.get('content', '---')\n    image_text_width = header_font.getbbox(image_text)[2]\n    draw.text((10, content_row_y), image_text, font=header_font, fill=WHITE) # Was poem_text\n\n    line_start_x = min(image_text_width + 40, BASE_WIDTH // 2) # Was poem_width\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    draw.text((line_start_x, content_row_y), line_prefix, font=header_font, fill=WHITE)\n    \n    line_content_start_x = line_start_x + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_start_x - 10\n    \n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_start_x, content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER43 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper43_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper43.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper43.png",
      "\nCLAPPER43 - Clapperboard with POEM and IMAGE fields swapped (Menlo Regular, new S/I/C style)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      ").strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    max_genome_line_chars = 36 # Max characters for the S/I/C glyph sequence itself\n    \n    genome_lines_to_render = []\n    if genome_data_entry:\n        s_line_data = genome_data_entry.get(",
      "S_LINE_N/A",
      "I_LINE_N/A",
      "C_LINE_N/A",
      "GENOME_N/A",
      "GENOME_N/A",
      "GENOME_N/A",
      ")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn",
      ") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for ",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    image_text_width = header_font.getbbox(image_text)[2]\n    draw.text((10, content_row_y), image_text, font=header_font, fill=WHITE) # Was poem_text\n\n    line_start_x = min(image_text_width + 40, BASE_WIDTH // 2) # Was poem_width\n    line_prefix = ",
      ":\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER43 - Clapperboard with POEM and IMAGE fields swapped (Menlo Regular, new S/I/C style)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper53.py",
    "size": 29855,
    "lines": 572,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER53 - Genome lines padded with placeholders to 101 chars, light box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and constants for genome section\n    LIGHT_BOX_COLOR = (100, 100, 100) # Light gray for the border\n    MAX_GENOME_LINE_LENGTH = 101\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70) # Dim gray for placeholder characters\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Removed ASCII Grid Pattern drawing\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for original_line_text in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n\n        original_length = len(original_line_text)\n        num_padding_chars = MAX_GENOME_LINE_LENGTH - original_length\n        padding_string = \"\"\n        if num_padding_chars > 0:\n            padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            char_color = PLACEHOLDER_COLOR # Default for padding characters\n            is_original_char = char_idx < original_length\n\n            if is_original_char:\n                char_color = AMBER\n                if char_idx == highlight_index:\n                    char_color = WHITE # Highlight original character\n            \n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                if current_x + ellipsis_width <= max_render_width:\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part\n                        ellipsis_fill = AMBER\n                        if highlight_index != -1 and highlight_index >= char_idx and highlight_index < original_length:\n                            ellipsis_fill = WHITE # Highlighted char is part of ellipsis\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Draw Light Box (AFTER text rendering, frames the genome content area)\n    box_x1 = genome_content_x1_on_img - 2\n    box_y1 = genome_content_y1_on_img - 2\n    box_x2 = genome_content_x2_on_img + 2\n    box_y2 = genome_content_y2_on_img + 2\n    \n    draw.rectangle(\n        [(box_x1, box_y1), (box_x2, box_y2)],\n        outline=LIGHT_BOX_COLOR,\n        width=1\n    )\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    image_label_text = f\"{image_glyph}\" # Only glyph\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    \n    # Draw actual image_type_str (value) at value_start_x, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((value_start_x, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2] # Width of the image type value itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str + padding\n    line_field_x_start = value_start_x + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER53 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper53_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper53.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper53.png",
      "\nCLAPPER53 - Genome lines padded with placeholders to 101 chars, light box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get(",
      " # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER53 - Genome lines padded with placeholders to 101 chars, light box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper47.py",
    "size": 27232,
    "lines": 516,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER47 - Genome: I,S,C order; triplet at current frame_position in WHITE (playhead). (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4:\n        genome_y_start = current_y_offset + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for line_text_full in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = 10  # Reset X for each line\n        \n        for char_idx, char_to_draw in enumerate(line_text_full):\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                # Check if ellipsis itself can fit\n                if current_x + ellipsis_width <= max_render_width:\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=AMBER)\n                break # Stop drawing this line\n\n            char_color = AMBER\n            if char_idx == highlight_index:\n                char_color = WHITE # Highlight if current character's index matches frame_position\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Label + Value, single line)\n    image_label_text = f\"{image_glyph} Image:\" # Changed label\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    image_label_width = header_font.getbbox(image_label_text)[2]\n    \n    image_value_x_start = 10 + image_label_width + 5 # 5px padding\n    # Draw actual image_type_str next to label, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((image_value_x_start, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2]\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    line_field_x_start = image_value_x_start + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER47 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper47_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper47.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper47.png",
      "\nCLAPPER47 - Genome: I,S,C order; triplet at current frame_position in WHITE (playhead). (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4:\n        genome_y_start = current_y_offset + 4\n\n    ellipsis = ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER47 - Genome: I,S,C order; triplet at current frame_position in WHITE (playhead). (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper63.py",
    "size": 29500,
    "lines": 595,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER63 - Using UPDATED-TIMELINE.json, genome scrolling. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes UPDATED-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper63_output/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES - Mapped from discovered data to glyphs.md symbols\nSYNTAGMA_GLYPHS = {\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (CS)\": \"\u2756\", # Data uses \"CS\", glyphs.md uses \"XS\" for \u2756\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n    # Explicitly add other known syntagma types from glyphs.md for completeness, even if not in current data sample\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    # Image types found in syntagma data (e.g., \"Action-Image\") will correctly get '?'\n}  \n\n# IMAGE TYPES - Mapped from discovered data to glyphs.md symbols\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n    # Any other image types from data not listed here will get '?'\n}\n\n# CINEOSIS FUNCTIONS - Mapped from discovered data to glyphs.md symbols\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n    # Any other cineosis functions from data not listed here will get '?'\n}  \n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), \"HONEYBADGER\", \"UPDATED-TIMELINE.json\") # Points to ../HONEYBADGER/ dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper63_output\")\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef discover_unique_prompt_values(timeline_data_path):\n    print(f\"\\nDiscovering unique prompt values from: {timeline_data_path}\")\n    try:\n        with open(timeline_data_path, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline data file not found at {timeline_data_path}\")\n        return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {timeline_data_path}\")\n        return\n\n    unique_image_types = set()\n    unique_syntagma_types = set()\n    unique_cineosis_functions = set()\n\n    for entry in data:\n        image_type = entry.get('imageType')\n        if image_type is not None:\n            unique_image_types.add(image_type)\n        \n        syntagma_type = entry.get('syntagmaType')\n        if syntagma_type is not None:\n            unique_syntagma_types.add(syntagma_type)\n\n        cineosis_function = entry.get('cineosisFunction')\n        if cineosis_function is not None:\n            unique_cineosis_functions.add(cineosis_function)\n\n    print(\"\\n--- Unique Image Types ---\")\n    for item in sorted(list(unique_image_types)):\n        print(item) # Simplified print\n    \n    print(\"\\n--- Unique Syntagma Types ---\")\n    for item in sorted(list(unique_syntagma_types)):\n        print(item) # Simplified print\n\n    print(\"\\n--- Unique Cineosis Functions ---\")\n    for item in sorted(list(unique_cineosis_functions)):\n        print(item) # Simplified print\n    print(\"\\nDiscovery complete.\")\n\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            current_line_text = [word]\n            current_width = word_width\n            y += line_height\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    \n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n    \n    return y + line_height # Return y for the next line start\n\ndef get_image_path(entry):\n    shot_id = entry.get('shotID')\n    if not shot_id:\n        return None\n\n    # Construct the path based on the structure: RESURRECTING ATLANTIS/SHOTS/SHOT_ID_PREFIX/SHOT_ID/SHOT_ID_FRAME.EXT\n    # Example: RESURRECTING ATLANTIS/SHOTS/AT001-AT100/AT034/AT034_00001.png\n    shot_id_prefix_num = int(shot_id[2:5]) // 100 * 100 +1 # e.g. AT034 -> 1, AT115 -> 101\n    shot_id_prefix = f\"{shot_id[:2]}{shot_id_prefix_num:03d}-{shot_id[:2]}{(shot_id_prefix_num+99):03d}\"\n    # Correctly use BASE_IMAGE_DIR which points to 'resurrecting atlantis'\n    image_folder = os.path.join(BASE_IMAGE_DIR, \"SHOTS\", shot_id_prefix, shot_id)\n\n    frame_number_str = entry.get(\"frameNumber\", \"0\")\n    frame_number_int = int(frame_number_str) if frame_number_str.isdigit() else 0\n    frame_filename = f\"{shot_id}_{frame_number_int:05d}.png\" # Assuming .png, adjust if needed\n    \n    image_path = os.path.join(image_folder, frame_filename)\n    return image_path\n\n# Loads poem genome data from a JSON file.\ndef load_genome_data_from_json(file_path):\n    try:\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n        \n        if isinstance(data, list):\n            # Attempt to convert list of poem objects to a dict keyed by poemName\n            genome_map_dict = {}\n            for item in data:\n                if isinstance(item, dict) and 'title' in item:\n                    poem_name = item['title']\n                    # Store the S, I, C lines, defaulting to empty strings if not present\n                    genome_map_dict[poem_name] = {\n                        'S': item.get('s_line', ''),\n                        'I': item.get('i_line', ''),\n                        'C': item.get('c_line', '')\n                    }\n                else:\n                    print(f\"Warning: Item in genome data list is not a valid poem object: {item}\")\n            return genome_map_dict\n        elif isinstance(data, dict):\n            # If it's already a dict, assume it's in the correct format\n            return data\n        else:\n            print(f\"Error: Genome data in {file_path} is not a list or a dictionary.\")\n            return {}\n            \n    except FileNotFoundError:\n        print(f\"Error: Genome data file not found at {file_path}\")\n        return {}\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}\")\n        return {}\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), color=BLACK)\n    draw = ImageDraw.Draw(img)\n\n    # Fonts\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    # genome_font_small = fonts['genome_small'] # Menlo Regular 12\n    # genome_font_tiny = fonts['genome_tiny'] # Menlo Regular 10\n\n    # --- 1. GENOME REPORT SECTION (Top) ---\n    GENOME_SECTION_Y_ON_IMG = 10 # Y position for the genome section on the image\n    genome_char_render_start_x = 40 # X position for the first genome character\n    genome_label_x = 10 # X position for S:, I:, C: labels\n\n    # Retrieve poem name from entry to match with genome_map key\n    poem_name_from_entry = entry.get('poemName', 'Unknown Poem')\n    genome_lines_for_poem = genome_map.get(poem_name_from_entry, {})\n\n    s_line_data_full = genome_lines_for_poem.get('S', '')\n    i_line_data_full = genome_lines_for_poem.get('I', '')\n    c_line_data_full = genome_lines_for_poem.get('C', '')\n    \n    # Calculate actual character height for line spacing (using genome_small font)\n    ay_bbox_genome12 = fonts['genome_small'].getbbox(\"Ay\")\n    actual_char_height = (ay_bbox_genome12[3] - ay_bbox_genome12[1]) + 2 # Height of one line of text\n\n    # --- Genome Scrolling Logic ---\n    font_for_char_width_calc = fonts['genome_small'] # Base font for width calculations\n    avg_char_width_bbox = font_for_char_width_calc.getbbox(\"X\")\n    avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n    if avg_char_width <= 0: avg_char_width = 7 # Fallback width (Menlo Regular 12 'X' is ~7px)\n\n    genome_display_area_width = BASE_WIDTH - genome_char_render_start_x - 10 # 10px right margin\n    num_visible_chars = max(1, int(genome_display_area_width / avg_char_width)) # Ensure at least 1 char is visible\n\n    frame_number_str = entry.get(\"frameNumber\", \"0\")\n    valid_frame = frame_number_str.isdigit() and int(frame_number_str) > 0\n    frame_number_int = int(frame_number_str) if valid_frame else 0\n    \n    highlight_idx_in_full_genome = frame_number_int - 1 if valid_frame else -1 # 0-indexed\n\n    display_start_idx = 0\n    highlight_idx_to_use_in_slice = -1 # Default to no highlight if not applicable\n\n    if highlight_idx_in_full_genome != -1 and highlight_idx_in_full_genome < len(s_line_data_full): # Ensure highlight is within bounds\n        # Try to center the highlighted char, or place it ~25% from left if possible\n        preferred_highlight_pos_in_window = int(num_visible_chars * 0.25)\n        \n        display_start_idx = highlight_idx_in_full_genome - preferred_highlight_pos_in_window\n        display_start_idx = max(0, display_start_idx) # Don't scroll before the beginning\n        \n        # Ensure display_start_idx doesn't go too far, such that the window exceeds genome length\n        if display_start_idx + num_visible_chars > len(s_line_data_full):\n            display_start_idx = len(s_line_data_full) - num_visible_chars\n            display_start_idx = max(0, display_start_idx) # Again, ensure it's not negative if genome is very short\n\n        highlight_idx_to_use_in_slice = highlight_idx_in_full_genome - display_start_idx\n    else:\n        # No valid highlight or highlight out of bounds, just show from the beginning\n        display_start_idx = 0\n        # highlight_idx_to_use_in_slice remains -1\n\n    # Slice genome lines accordingly\n    genome_s_to_render = s_line_data_full[display_start_idx : display_start_idx + num_visible_chars]\n    genome_i_to_render = i_line_data_full[display_start_idx : display_start_idx + num_visible_chars]\n    genome_c_to_render = c_line_data_full[display_start_idx : display_start_idx + num_visible_chars]\n\n    all_sliced_lines = [genome_s_to_render, genome_i_to_render, genome_c_to_render]\n    genome_labels = [\"S:\", \"I:\", \"C:\"]\n    current_line_y = GENOME_SECTION_Y_ON_IMG\n\n    # Draw S, I, C lines with labels and glyphs, highlighting the selected glyph\n    for line_idx, sliced_line_text in enumerate(all_sliced_lines):\n        draw.text((genome_label_x, current_line_y), genome_labels[line_idx], font=fonts['genome_12_bold'], fill=WHITE)\n        current_char_x = genome_char_render_start_x\n        for char_idx_in_slice, char_to_draw in enumerate(sliced_line_text):\n            is_highlighted = (char_idx_in_slice == highlight_idx_to_use_in_slice)\n            active_font = fonts['genome_12_bold'] if is_highlighted else fonts['genome_small']\n            color = AMBER if is_highlighted else WHITE # Highlight color Amber, else White\n            \n            # Get width of the current character with its specific font to advance X correctly\n            char_bbox = active_font.getbbox(char_to_draw)\n            char_width = char_bbox[2] - char_bbox[0]\n            \n            draw.text((current_char_x, current_line_y), char_to_draw, font=active_font, fill=color)\n            current_char_x += char_width # Advance by actual width of drawn char\n        current_line_y += actual_char_height\n    # --- END GENOME REPORT SECTION ---\n\n    # --- 2. HEADER SECTION (ID, POEM, TIME, FRAME) ---\n    current_y_offset = GENOME_REPORT_HEIGHT # Start Y for header, below genome section\n    header_content_y = current_y_offset + 8 # Add some padding from top of header section\n    top_row_internal_height = 25 # Estimated height of this row of text\n\n    # Extract and format header data\n    shot_id = entry.get('shotID', 'Unknown ID')\n    poem_text = entry.get('poemName', 'Unknown Poem')\n    time_text = entry.get('time', '00:00:00,000')\n    frame_text = f\"F: {entry.get('frameNumber', 'N/A')}\"\n\n    draw.text((10, header_content_y), shot_id, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str_raw, \"?\").strip() # Default to ?\n    line_content = entry.get('content', '---')\n\n    # Calculate max_glyph_width for dynamic layout of prompt section\n    # Consider all glyphs from all three dictionaries for robust max width calculation\n    # Using fonts['genome_small'] as it's now used for prompt glyphs\n    all_possible_glyphs = list(IMAGE_TYPE_GLYPHS.values()) + \\\n                          list(SYNTAGMA_GLYPHS.values()) + \\\n                          list(CINEOSIS_FUNCTION_GLYPHS.values()) + [\"?\"] # Include fallback\n    max_glyph_width = 0\n    for g in all_possible_glyphs:\n        glyph_bbox = fonts['genome_small'].getbbox(g.strip())\n        glyph_width = glyph_bbox[2] - glyph_bbox[0]\n        if glyph_width > max_glyph_width:\n            max_glyph_width = glyph_width\n    \n    gap_after_glyph = 15 # pixels\n    glyph_column_x = 10 # Define glyph_column_x early, it's a fixed position\n    value_start_x = glyph_column_x + max_glyph_width + gap_after_glyph\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    draw.text((glyph_column_x, initial_content_row_y), image_glyph, font=fonts['genome_small'], fill=WHITE) # Draw glyph with genome_font\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR) # Draw text\n    image_text_width = text_font.getbbox(image_type_str_raw)[2] # Width of the image type text itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str_raw + padding\n    line_field_x_start = value_start_x + image_text_width + 30 # 30px padding after image text\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    # The glyph_column_x is where glyphs are drawn (e.g., 10px).\n    # The value_start_x is dynamically calculated based on max_glyph_width.\n    # The divider should visually separate the 'Image/Line' row from the 'Prompt' section.\n    # Let's make the divider start just left of the glyph column.\n    divider_start_x = glyph_column_x - 4 if glyph_column_x > 4 else 6 \n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at glyph_column_x (e.g. 10), Values at value_start_x (dynamic)\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str_raw, \"?\").strip() # Default to ?\n    draw.text((glyph_column_x, current_y_offset), syntagma_glyph, font=fonts['genome_small'], fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str_raw, \"?\").strip() # Default to ?\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=fonts['genome_small'], fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get('ekphrasis', '')\n    if ekphrasis_str: # Only draw if there's content\n        current_y_offset = draw_colored_text(draw, \"Ekphrasis: \" + ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get('style', '')\n    if style_str: # Only draw if there's content\n        current_y_offset = draw_colored_text(draw, \"Style: \" + style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # --- 5. IMAGE (Scaled and Centered) ---\n    image_path = get_image_path(entry)\n    if image_path and os.path.exists(image_path):\n        try:\n            source_img = Image.open(image_path).convert(\"RGB\")\n            # Scale image to fit IMAGE_DISPLAY_HEIGHT while maintaining aspect ratio\n            img_aspect_ratio = source_img.width / source_img.height\n            scaled_height = IMAGE_DISPLAY_HEIGHT\n            scaled_width = int(scaled_height * img_aspect_ratio)\n            if scaled_width > BASE_WIDTH:\n                scaled_width = BASE_WIDTH\n                scaled_height = int(scaled_width / img_aspect_ratio)\n            \n            source_img_resized = source_img.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)\n            \n            # Calculate position to center the image\n            paste_x = (BASE_WIDTH - scaled_width) // 2\n            paste_y = HEADER_HEIGHT + GENOME_REPORT_HEIGHT + (IMAGE_DISPLAY_HEIGHT - scaled_height) // 2\n            \n            img.paste(source_img_resized, (paste_x, paste_y))\n        except Exception as e:\n            print(f\"Error loading or processing image {image_path}: {e}\")\n            draw.text((10, HEADER_HEIGHT + GENOME_REPORT_HEIGHT + 10), f\"Error loading image: {os.path.basename(image_path)}\", font=text_font, fill=AMBER)\n    else:\n        draw.text((10, HEADER_HEIGHT + GENOME_REPORT_HEIGHT + 10), \"Image not found\", font=text_font, fill=LIGHT_GRAY)\n\n    # Save the final image\n    img.save(output_path)\n\n# For testing with a specific entry known for long genome or high frame number\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    # This entry should have a long genome and a high frame number to test scrolling\n    longest_entry = {\n        \"shotID\": \"AT034\",\n        \"poemName\": \"Resurrecting Atlantis\", # Ensure this poem exists in genome_map\n        \"frameNumber\": \"150\", # High frame number to force scrolling\n        \"time\": \"00:00:06,250\",\n        \"imageType\": \"Crystal-Image\",\n        \"syntagmaType\": \"Crystal Syntagma (CS)\",\n        \"cineosisFunction\": \"Temporal Reflection Loop\",\n        \"content\": \"The scroll unfurls, revealing ancient glyphs that shimmer with stored time.\"\n    }\n    output_path = os.path.join(output_dir_name, f\"_EXTREME_TEST_CARD_clapper63.png\")\n    print(f\"Rendered extreme test card: {output_path}\\n\")\n    render_card(longest_entry, output_path, fonts, genome_map) # Pass genome_map\n\ndef calculate_frame_counts(timeline_data):\n    frame_counts = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poemName')\n        if poem_name:\n            frame_counts[poem_name] = frame_counts.get(poem_name, 0) + 1\n    return frame_counts\n\ndef main():\n    print(\"Starting CLAPPER63 card generation (UPDATED-TIMELINE.json)...\")\n\n    # Load fonts\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 18),\n        'text': get_font(\"Menlo\", \"Courier New\", 14),\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 14, is_bold_preferred=True, is_bold_fallback=True),\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # For S,I,C lines (regular)\n        'genome_tiny': get_font(\"Menlo\", \"Courier New\", 10), # If ever needed again\n        'genome_12_bold': get_font(\"Menlo\", \"Courier New\", 12, is_bold_preferred=True, is_bold_fallback=True) # For highlighted genome char\n    }\n\n    # Load symbolic genome data\n    genome_map = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    if not genome_map:\n        print(\"Genome data could not be loaded. Genome report will be empty.\")\n\n    # Load timeline data\n    try:\n        with open(TIMELINE_PATH, 'r') as f:\n            timeline_data_local = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH} (Expected: /Users/gaia/resurrecting atlantis/HONEYBADGER/UPDATED-TIMELINE.json)\")\n        return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\")\n        return\n\n    global timeline_data\n    timeline_data = timeline_data_local\n\n    # Create output directory based on the OUTPUT_DIR constant\n    output_dir_name = OUTPUT_DIR \n    os.makedirs(output_dir_name, exist_ok=True)\n\n    # Render the extreme test case card first\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    # Group timeline entries by poem\n    entries_by_poem = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem') # Corrected key from 'poemName' to 'poem'\n        if poem_name:\n            if poem_name not in entries_by_poem:\n                entries_by_poem[poem_name] = []\n            entries_by_poem[poem_name].append(entry)\n\n    # Render one random entry for each poem\n    print(f\"Rendering one random entry for each of the {len(entries_by_poem)} poems found...\")\n    for poem_name, entries_in_poem in entries_by_poem.items():\n        if entries_in_poem:\n            random_entry = random.choice(entries_in_poem)\n            entry_id = random_entry.get('id', 'UnknownID') # Corrected key from 'shotID' to 'id'\n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_name}'...\")\n            output_path = os.path.join(output_dir_name, f\"{entry_id}_clapper63.png\")\n            render_card(random_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "UPDATED-TIMELINE.json",
      "symbolic_genome_data.json",
      "{shot_id}_{frame_number_int:05d}.png",
      "_EXTREME_TEST_CARD_clapper63.png",
      "{entry_id}_clapper63.png",
      "\nCLAPPER63 - Using UPDATED-TIMELINE.json, genome scrolling. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes UPDATED-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper63_output/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES - Mapped from discovered data to glyphs.md symbols\nSYNTAGMA_GLYPHS = {\n    ",
      ") # Points to ../HONEYBADGER/ dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), ",
      ")\n    if not shot_id:\n        return None\n\n    # Construct the path based on the structure: RESURRECTING ATLANTIS/SHOTS/SHOT_ID_PREFIX/SHOT_ID/SHOT_ID_FRAME.EXT\n    # Example: RESURRECTING ATLANTIS/SHOTS/AT001-AT100/AT034/AT034_00001.png\n    shot_id_prefix_num = int(shot_id[2:5]) // 100 * 100 +1 # e.g. AT034 -> 1, AT115 -> 101\n    shot_id_prefix = f",
      " is ~7px)\n\n    genome_display_area_width = BASE_WIDTH - genome_char_render_start_x - 10 # 10px right margin\n    num_visible_chars = max(1, int(genome_display_area_width / avg_char_width)) # Ensure at least 1 char is visible\n\n    frame_number_str = entry.get(",
      "N/A",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    # The glyph_column_x is where glyphs are drawn (e.g., 10px).\n    # The value_start_x is dynamically calculated based on max_glyph_width.\n    # The divider should visually separate the ",
      "], fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get(",
      " + ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get(",
      ")\n            # Scale image to fit IMAGE_DISPLAY_HEIGHT while maintaining aspect ratio\n            img_aspect_ratio = source_img.width / source_img.height\n            scaled_height = IMAGE_DISPLAY_HEIGHT\n            scaled_width = int(scaled_height * img_aspect_ratio)\n            if scaled_width > BASE_WIDTH:\n                scaled_width = BASE_WIDTH\n                scaled_height = int(scaled_width / img_aspect_ratio)\n            \n            source_img_resized = source_img.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)\n            \n            # Calculate position to center the image\n            paste_x = (BASE_WIDTH - scaled_width) // 2\n            paste_y = HEADER_HEIGHT + GENOME_REPORT_HEIGHT + (IMAGE_DISPLAY_HEIGHT - scaled_height) // 2\n            \n            img.paste(source_img_resized, (paste_x, paste_y))\n        except Exception as e:\n            print(f",
      "Error: Timeline file not found at {TIMELINE_PATH} (Expected: /Users/gaia/resurrecting atlantis/HONEYBADGER/UPDATED-TIMELINE.json)"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER63 - Using UPDATED-TIMELINE.json, genome scrolling. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes UPDATED-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper63_output/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper57.py",
    "size": 33259,
    "lines": 638,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER57 - Fix prompt section glyph rendering by combining glyph and text. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII Standardized\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \" [AS]  \",\n    \"Parallel Syntagma (PS)\":     \" [PS]  \",\n    \"Alternating Syntagma (LS)\":  \" [LS]  \",\n    \"Chronological Syntagma (CS)\":\" [CS]  \",\n    \"Descriptive Syntagma (DS)\":  \" [DS]  \",\n    \"Flashback Syntagma (FS)\":    \" [FS]  \",\n    \"Thematic Montage (TM)\":      \" [TM]  \", # Added from older versions\n    \"Crystal Syntagma (XS)\":      \" [XS]  \", # Added from older versions\n    \"--- Syntagma\":               \" [---] \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\":         \" [Act] \",\n    \"Affection-Image\":      \" [Aff] \",\n    \"Crystal-Image\":        \" [Cry] \",\n    \"Descriptive Image\":    \" [Des] \",\n    \"Opsign\":               \" [Ops] \",\n    \"Perception-Image\":     \" [Per] \",\n    \"Recollection-Image\":   \" [Rec] \",\n    \"Sonsign\":              \" [Son] \",\n    \"Thematic Montage\":     \" [TMg] \", # Note: Key matches a Syntagma type, ensure context handles this\n    \"--- Image Type\":       \" [---] \"  # Changed key for clarity\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\":         \" [AEc] \",\n    \"Causal Motion Trigger\":        \" [CMT] \",\n    \"Emotion Relay\":                \" [EmR] \",\n    \"Event Pause Invocation\":       \" [EPI] \",\n    \"Memory Storage Retrieval\":     \" [MSR] \",\n    \"Mood Environment Stabilizer\":  \" [MES] \",\n    \"Narrative Modifier\":           \" [NaM] \",\n    \"Subjective Frame Recalibration\":\" [SFR] \",\n    \"Temporal Reflection Loop\":     \" [TRL] \",\n    \"--- Cineosis Function\":      \" [---] \"  # Changed key for clarity\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper57_output\") # Default output for clapper56TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and constants for genome section\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70) # Dim gray for placeholder characters\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Removed ASCII Grid Pattern drawing\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for original_line_text in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n\n        original_length = len(original_line_text)\n        # Calculate width of original text\n        width_of_original_line = 0\n        for i in range(original_length):\n            try:\n                width_of_original_line += chosen_genome_font.getlength(original_line_text[i])\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(original_line_text[i])\n                width_of_original_line += bbox[2] - bbox[0]\n\n        remaining_width = max_render_width - width_of_original_line\n        num_padding_chars = 0\n        padding_string = \"\"\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ' ')\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ' ': # It's an actual glyph from the original line\n                    current_glyph_num_in_line += 1 # Increment for actual glyphs\n                    if current_glyph_num_in_line == highlight_index:\n                        char_color = WHITE # Highlight this glyph\n                    else:\n                        char_color = AMBER # Regular genome glyph color\n                else:\n                    # It's a space from the original line\n                    char_color = AMBER # Spaces are part of the genome line, colored AMBER\n            else:\n                # It's a placeholder padding character\n                char_color = PLACEHOLDER_COLOR\n            \n            # Calculate width of the character to be drawn\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            # Truncation and ellipsis logic\n            if current_x + char_width > max_render_width: # If this char doesn't fit\n                if current_x + ellipsis_width <= max_render_width: # Check if ellipsis fits\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part of the line\n                        ellipsis_fill = AMBER # Default for original part\n                        \n                        # Determine the 0-based index of the first glyph that is part of the truncation\n                        first_truncated_glyph_idx_val = current_glyph_num_in_line\n                        if char_to_draw == ' ': # If the char causing truncation is a space\n                           first_truncated_glyph_idx_val +=1 # then the actual first glyph truncated is the next one\n                        \n                        if highlight_index != -1 and highlight_index >= first_truncated_glyph_idx_val and highlight_index < total_glyphs_in_original:\n                            ellipsis_fill = WHITE # Highlighted glyph is in the truncated part\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line (whether ellipsis was drawn or not)\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str_raw, \" \").strip()\n    image_display_text = f\"{image_glyph} {image_type_str_raw}\"\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Combined glyph + text, single line, starting at x=10)\n    # This part does not wrap. It's drawn directly.\n    draw.text((10, initial_content_row_y), image_display_text, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_display_text)[2] # Width of the combined glyph + image type value\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is 10 (start of image_display_text) + width of image_display_text + padding\n    line_field_x_start = 10 + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str_raw, \" \").strip()\n    syntagma_display_text = f\"{syntagma_glyph} {syntagma_type_str_raw}\"\n    # The x-coordinate for drawing will be 10 (left_padding)\n    current_y_offset = draw_colored_text(draw, syntagma_display_text, 10, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str_raw, \" \").strip()\n    cineosis_display_text = f\"{cineosis_glyph} {cineosis_func_str_raw}\"\n    # The x-coordinate for drawing will be 10 (left_padding)\n    current_y_offset = draw_colored_text(draw, cineosis_display_text, 10, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at 10)\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, 10, current_y_offset, EKPHRASIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Style (No label/glyph at x=10, content starts at 10)\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, 10, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    longest_entry_path = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json\" # Corrected path\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(output_dir_name, \"_EXTREME_TEST_CARD_clapper57.png\") # Use passed output_dir_name\n    render_card(longest_entry, output_path, fonts, genome_map) # Pass genome_map\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER57 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    output_dir_name = \"cards_clapper57_output\"\n    os.makedirs(output_dir_name, exist_ok=True) # Ensure output directory exists\n\n    # render_demo_card(fonts, genome_map, output_dir_name) # Would also need output_dir_name if uncommented\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    if not processed_timeline_data:\n        print(\"No timeline data to process for random poem entries.\")\n        return\n\n    poems_to_entries = {}\n    for entry in processed_timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            if poem_title not in poems_to_entries:\n                poems_to_entries[poem_title] = []\n            poems_to_entries[poem_title].append(entry)\n\n    if not poems_to_entries:\n        print(\"No poems found in timeline data to select random entries from.\")\n        return\n\n    print(f\"\\nRendering one random entry for each of the {len(poems_to_entries)} poems found...\")\n    count = 0\n    for poem_title, entries_for_poem in poems_to_entries.items():\n        if entries_for_poem:\n            selected_entry = random.choice(entries_for_poem)\n            entry_id = selected_entry.get('id', f'random_poem_entry_{count+1}')\n            output_filename = f\"{entry_id}_clapper57.png\"\n            output_path = os.path.join(output_dir_name, output_filename)\n            \n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_title}'...\")\n            render_card(selected_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n            count += 1\n        else:\n            print(f\"No entries found for poem '{poem_title}' to select a random one.\")\n\n    print(f\"\\nFinished rendering {count} random cards per poem.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json",
      "_EXTREME_TEST_CARD_clapper57.png",
      "{entry_id}_clapper57.png",
      "\nCLAPPER57 - Fix prompt section glyph rendering by combining glyph and text. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII Standardized\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ",
      ")\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str_raw = entry.get(",
      "\n    # The x-coordinate for drawing will be 10 (left_padding)\n    current_y_offset = draw_colored_text(draw, cineosis_display_text, 10, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at 10)\n    ekphrasis_str = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, 10, current_y_offset, EKPHRASIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Style (No label/glyph at x=10, content starts at 10)\n    style_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER57 - Fix prompt section glyph rendering by combining glyph and text. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper46.py",
    "size": 27542,
    "lines": 522,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER46 - Genome: I,S,C order; current triplet in WHITE. 'Image:' label, LINE on same line (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine target glyphs for highlighting based on current entry\n    current_image_type = entry.get('imageType')\n    current_syntagma_type = entry.get('syntagmaType')\n    current_cineosis_function = entry.get('cineosisFunction')\n\n    target_i_glyph = IMAGE_TYPE_GLYPHS.get(current_image_type) if current_image_type else None\n    target_s_glyph = SYNTAGMA_GLYPHS.get(current_syntagma_type) if current_syntagma_type else None\n    target_c_glyph = CINEOSIS_FUNCTION_GLYPHS.get(current_cineosis_function) if current_cineosis_function else None\n\n    # Ordered list of target glyphs corresponding to genome_lines_to_render\n    target_glyphs_ordered = [target_i_glyph, target_s_glyph, target_c_glyph]\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4:\n        genome_y_start = current_y_offset + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for idx, line_text_full in enumerate(genome_lines_to_render):\n        current_x = 10  # Reset X for each line\n        target_glyph_for_this_line = target_glyphs_ordered[idx]\n        \n        for char_to_draw in line_text_full:\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                if current_x + ellipsis_width <= max_render_width:\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=AMBER)\n                break \n\n            char_color = AMBER\n            if target_glyph_for_this_line is not None and char_to_draw == target_glyph_for_this_line:\n                char_color = WHITE\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Label + Value, single line)\n    image_label_text = f\"{image_glyph} Image:\" # Changed label\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    image_label_width = header_font.getbbox(image_label_text)[2]\n    \n    image_value_x_start = 10 + image_label_width + 5 # 5px padding\n    # Draw actual image_type_str next to label, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((image_value_x_start, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2]\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    line_field_x_start = image_value_x_start + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER46 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper46_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper46.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper46.png",
      " label, LINE on same line (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4:\n        genome_y_start = current_y_offset + 4\n\n    ellipsis = ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER46 - Genome: I,S,C order; current triplet in WHITE. 'Image:' label, LINE on same line (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper62.py",
    "size": 34420,
    "lines": 670,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER62 - Implemented horizontal scrolling for genome barcode. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES - Mapped from discovered data to glyphs.md symbols\nSYNTAGMA_GLYPHS = {\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (CS)\": \"\u2756\", # Data uses \"CS\", glyphs.md uses \"XS\" for \u2756\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n    # Explicitly add other known syntagma types from glyphs.md for completeness, even if not in current data sample\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    # Image types found in syntagma data (e.g., \"Action-Image\") will correctly get '?'\n}  \n\n# IMAGE TYPES - Mapped from discovered data to glyphs.md symbols\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n    # Any other image types from data not listed here will get '?'\n}\n\n# CINEOSIS FUNCTIONS - Mapped from discovered data to glyphs.md symbols\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n    # Any other cineosis functions from data not listed here will get '?'\n}  \n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper62_output\") # Default output for clapper56TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef discover_unique_prompt_values(timeline_data_path):\n    print(f\"\\nDiscovering unique prompt values from: {timeline_data_path}\")\n    try:\n        with open(timeline_data_path, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline data file not found at {timeline_data_path}\")\n        return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {timeline_data_path}\")\n        return\n\n    unique_image_types = set()\n    unique_syntagma_types = set()\n    unique_cineosis_functions = set()\n\n    for entry in data:\n        image_type = entry.get('imageType')\n        if image_type is not None:\n            unique_image_types.add(image_type)\n        \n        syntagma_type = entry.get('syntagmaType')\n        if syntagma_type is not None:\n            unique_syntagma_types.add(syntagma_type)\n\n        cineosis_function = entry.get('cineosisFunction')\n        if cineosis_function is not None:\n            unique_cineosis_functions.add(cineosis_function)\n\n    print(\"\\n--- Unique Image Types ---\")\n    for item in sorted(list(unique_image_types)):\n        print(item) # Simplified print\n    \n    print(\"\\n--- Unique Syntagma Types ---\")\n    for item in sorted(list(unique_syntagma_types)):\n        print(item) # Simplified print\n\n    print(\"\\n--- Unique Cineosis Functions ---\")\n    for item in sorted(list(unique_cineosis_functions)):\n        print(item) # Simplified print\n    print(\"\\nDiscovery complete.\")\n\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and constants for genome section\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70) # Dim gray for placeholder characters\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Removed ASCII Grid Pattern drawing\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    # --- Genome Report Section --- (Replaced with scrolling logic)\n    # Position and dimensions\n    GENOME_SECTION_Y_ON_IMG = 4 # Explicitly defined, was current_y_offset (4) at this point\n    GENOME_CONTENT_X1_ON_IMG = 10 # Explicitly defined\n    genome_section_draw_y = GENOME_SECTION_Y_ON_IMG + 4 # Add a small top padding within the section\n    genome_label_x = GENOME_CONTENT_X1_ON_IMG\n    # Calculate actual character height for line spacing (using genome_small font)\n    ay_bbox_genome12 = fonts['genome_small'].getbbox(\"Ay\")\n    actual_char_height = (ay_bbox_genome12[3] - ay_bbox_genome12[1]) + 2 # Height of one line of text\n\n    # Get full genome lines (these should be available from earlier in render_card)\n    # s_line_data_full, i_line_data_full, c_line_data_full are used directly\n    full_genome_len = len(s_line_data_full) # Assuming all lines have the same length\n\n    # Labels for S, I, C lines\n    genome_labels = [\"S:\", \"I:\", \"C:\"]\n    max_label_width = 0\n    for label_text in genome_labels:\n        bbox = fonts['genome_12_bold'].getbbox(label_text)\n        max_label_width = max(max_label_width, bbox[2] - bbox[0])\n    genome_char_render_start_x = genome_label_x + max_label_width + 10 # X-coordinate for start of genome characters\n\n    # --- Genome Scrolling Logic ---\n    font_for_char_width_calc = fonts['genome_small'] # Base font for width calculations\n    # Get width of a representative character like 'X'\n    avg_char_width_bbox = font_for_char_width_calc.getbbox(\"X\")\n    avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n    if avg_char_width <= 0: avg_char_width = 7 # Fallback width (Menlo Regular 12 'X' is ~7px)\n\n    genome_display_area_width = BASE_WIDTH - genome_char_render_start_x - 10 # 10px right margin\n    num_visible_chars = max(1, int(genome_display_area_width / avg_char_width)) # Ensure at least 1 char is visible\n\n    frame_number_str = entry.get(\"frameNumber\", \"0\")\n    frame_number_int = 0\n    try:\n        frame_number_int = int(frame_number_str)\n    except ValueError:\n        pass # Keep 0 if not a valid int\n\n    highlight_idx_in_full_genome = -1 # Default to no highlight if frame_number is invalid\n    if 0 < frame_number_int <= full_genome_len:\n        highlight_idx_in_full_genome = frame_number_int - 1 # Convert to 0-indexed\n\n    display_start_idx = 0\n    highlight_idx_to_use_in_slice = highlight_idx_in_full_genome # Default if no scrolling / full genome fits\n\n    if full_genome_len > num_visible_chars:\n        # Scrolling is needed\n        preferred_highlight_offset_in_window = num_visible_chars // 4 # Aim to place highlight ~25% into the window\n        \n        display_start_idx = highlight_idx_in_full_genome - preferred_highlight_offset_in_window\n        \n        # Boundary checks for display_start_idx\n        display_start_idx = max(0, display_start_idx) # Don't start before the beginning of the genome\n        # Ensure display_start_idx doesn't go so far that the end of the genome is before the end of the window\n        display_start_idx = min(display_start_idx, full_genome_len - num_visible_chars)\n        \n        highlight_idx_to_use_in_slice = highlight_idx_in_full_genome - display_start_idx\n    else:\n        # No scrolling needed, the entire genome fits\n        num_visible_chars = full_genome_len # Adjust num_visible_chars to actual full length\n        # highlight_idx_to_use_in_slice is already highlight_idx_in_full_genome\n\n    # Slice the genome lines to get the visible part\n    genome_s_to_render = s_line_data_full[display_start_idx : display_start_idx + num_visible_chars]\n    genome_i_to_render = i_line_data_full[display_start_idx : display_start_idx + num_visible_chars]\n    genome_c_to_render = c_line_data_full[display_start_idx : display_start_idx + num_visible_chars]\n\n    all_sliced_lines = [genome_s_to_render, genome_i_to_render, genome_c_to_render]\n    # --- End Genome Scrolling Logic ---\n\n    # Drawing the S, I, C lines\n    for line_idx, sliced_line_text in enumerate(all_sliced_lines):\n        current_line_y = genome_section_draw_y + (line_idx * actual_char_height)\n        \n        # Draw label (S:, I:, C:)\n        draw.text((genome_label_x, current_line_y), genome_labels[line_idx], font=fonts['genome_12_bold'], fill=WHITE)\n        \n        current_char_x = genome_char_render_start_x\n        for char_idx_in_slice, char_to_draw in enumerate(sliced_line_text):\n            is_highlighted_char = (char_idx_in_slice == highlight_idx_to_use_in_slice) and (highlight_idx_in_full_genome != -1)\n            \n            active_font = fonts['genome_12_bold'] if is_highlighted_char else fonts['genome_small']\n            active_color = HIGHLIGHT_GENOME_COLOR if is_highlighted_char else WHITE\n            \n            if char_to_draw: # Ensure char is not empty\n                draw.text((current_char_x, current_line_y), char_to_draw, font=active_font, fill=active_color)\n                \n                try:\n                    char_actual_width = active_font.getlength(char_to_draw)\n                except AttributeError: # Fallback for older Pillow versions\n                    char_bbox = active_font.getbbox(char_to_draw)\n                    char_actual_width = char_bbox[2] - char_bbox[0]\n                \n                if char_actual_width <= 0: char_actual_width = avg_char_width # Fallback for zero-width chars\n                current_char_x += char_actual_width\n    # --- End Clean Genome Report Section ---\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str_raw, \"?\").strip() # Default to ?\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    glyph_column_x = 10 # Define glyph_column_x early, it's a fixed position\n    draw.text((glyph_column_x, initial_content_row_y), image_glyph, font=fonts['genome_small'], fill=WHITE) # Draw glyph with genome_font\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR) # Draw text\n    image_text_width = text_font.getbbox(image_type_str_raw)[2] # Width of the image type text itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str_raw + padding\n    line_field_x_start = value_start_x + image_text_width + 30 # 30px padding after image text\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    # The glyph_column_x is where glyphs are drawn (e.g., 10px).\n    # The value_start_x is dynamically calculated based on max_glyph_width.\n    # The divider should visually separate the 'Image/Line' row from the 'Prompt' section.\n    # Let's make the divider start just left of the glyph column.\n    divider_start_x = glyph_column_x - 4 if glyph_column_x > 4 else 6 \n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at glyph_column_x (e.g. 10), Values at value_start_x (dynamic)\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str_raw, \"?\").strip() # Default to ?\n    draw.text((glyph_column_x, current_y_offset), syntagma_glyph, font=fonts['genome_small'], fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str_raw, \"?\").strip() # Default to ?\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=fonts['genome_small'], fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    longest_entry_path = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json\" # Corrected path\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(output_dir_name, \"_EXTREME_TEST_CARD_clapper62.png\") # Use passed output_dir_name\n    render_card(longest_entry, output_path, fonts, genome_map) # Pass genome_map\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER62 card generation with genome scrolling...\")\n    # discover_unique_prompt_values(TIMELINE_PATH) # Discovery was done, commenting out\n    # print(\"\\nData discovery run. Card generation is bypassed in clapper61.py.\")\n    # return # Re-enabling card generation\n\n    # Original main content, now active again:\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10),  # Already Menlo regular\n        'genome_12_bold': get_font(\"Menlo\", \"Courier New\", 12, is_bold_preferred=True)\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    output_dir_name = \"cards_clapper62_output\"\n    os.makedirs(output_dir_name, exist_ok=True) # Ensure output directory exists\n\n    # render_demo_card(fonts, genome_map, output_dir_name) # Would also need output_dir_name if uncommented\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    if not processed_timeline_data:\n        print(\"No timeline data to process for random poem entries.\")\n        return\n\n    poems_to_entries = {}\n    for entry in processed_timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            if poem_title not in poems_to_entries:\n                poems_to_entries[poem_title] = []\n            poems_to_entries[poem_title].append(entry)\n\n    if not poems_to_entries:\n        print(\"No poems found in timeline data to select random entries from.\")\n        return\n\n    print(f\"\\nRendering one random entry for each of the {len(poems_to_entries)} poems found...\")\n    count = 0\n    for poem_title, entries_for_poem in poems_to_entries.items():\n        if entries_for_poem:\n            selected_entry = random.choice(entries_for_poem)\n            entry_id = selected_entry.get('id', f'random_poem_entry_{count+1}')\n            output_filename = f\"{entry_id}_clapper62.png\"\n            output_path = os.path.join(output_dir_name, output_filename)\n            \n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_title}'...\")\n            render_card(selected_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n            count += 1\n        else:\n            print(f\"No entries found for poem '{poem_title}' to select a random one.\")\n\n    print(f\"\\nFinished rendering {count} random cards per poem.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json",
      "_EXTREME_TEST_CARD_clapper62.png",
      "{entry_id}_clapper62.png",
      "\nCLAPPER62 - Implemented horizontal scrolling for genome barcode. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES - Mapped from discovered data to glyphs.md symbols\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      " is ~7px)\n\n    genome_display_area_width = BASE_WIDTH - genome_char_render_start_x - 10 # 10px right margin\n    num_visible_chars = max(1, int(genome_display_area_width / avg_char_width)) # Ensure at least 1 char is visible\n\n    frame_number_str = entry.get(",
      ")\n    frame_number_int = 0\n    try:\n        frame_number_int = int(frame_number_str)\n    except ValueError:\n        pass # Keep 0 if not a valid int\n\n    highlight_idx_in_full_genome = -1 # Default to no highlight if frame_number is invalid\n    if 0 < frame_number_int <= full_genome_len:\n        highlight_idx_in_full_genome = frame_number_int - 1 # Convert to 0-indexed\n\n    display_start_idx = 0\n    highlight_idx_to_use_in_slice = highlight_idx_in_full_genome # Default if no scrolling / full genome fits\n\n    if full_genome_len > num_visible_chars:\n        # Scrolling is needed\n        preferred_highlight_offset_in_window = num_visible_chars // 4 # Aim to place highlight ~25% into the window\n        \n        display_start_idx = highlight_idx_in_full_genome - preferred_highlight_offset_in_window\n        \n        # Boundary checks for display_start_idx\n        display_start_idx = max(0, display_start_idx) # Don",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    # The glyph_column_x is where glyphs are drawn (e.g., 10px).\n    # The value_start_x is dynamically calculated based on max_glyph_width.\n    # The divider should visually separate the ",
      "], fill=WHITE) # Draw glyph with genome_font\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER62 - Implemented horizontal scrolling for genome barcode. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper56.py",
    "size": 33495,
    "lines": 641,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER56 - Standardize all prompt section glyphs to ASCII. Verify highlighting. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII Standardized\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \" [AS]  \",\n    \"Parallel Syntagma (PS)\":     \" [PS]  \",\n    \"Alternating Syntagma (LS)\":  \" [LS]  \",\n    \"Chronological Syntagma (CS)\":\" [CS]  \",\n    \"Descriptive Syntagma (DS)\":  \" [DS]  \",\n    \"Flashback Syntagma (FS)\":    \" [FS]  \",\n    \"Thematic Montage (TM)\":      \" [TM]  \", # Added from older versions\n    \"Crystal Syntagma (XS)\":      \" [XS]  \", # Added from older versions\n    \"--- Syntagma\":               \" [---] \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\":         \" [Act] \",\n    \"Affection-Image\":      \" [Aff] \",\n    \"Crystal-Image\":        \" [Cry] \",\n    \"Descriptive Image\":    \" [Des] \",\n    \"Opsign\":               \" [Ops] \",\n    \"Perception-Image\":     \" [Per] \",\n    \"Recollection-Image\":   \" [Rec] \",\n    \"Sonsign\":              \" [Son] \",\n    \"Thematic Montage\":     \" [TMg] \", # Note: Key matches a Syntagma type, ensure context handles this\n    \"--- Image Type\":       \" [---] \"  # Changed key for clarity\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\":         \" [AEc] \",\n    \"Causal Motion Trigger\":        \" [CMT] \",\n    \"Emotion Relay\":                \" [EmR] \",\n    \"Event Pause Invocation\":       \" [EPI] \",\n    \"Memory Storage Retrieval\":     \" [MSR] \",\n    \"Mood Environment Stabilizer\":  \" [MES] \",\n    \"Narrative Modifier\":           \" [NaM] \",\n    \"Subjective Frame Recalibration\":\" [SFR] \",\n    \"Temporal Reflection Loop\":     \" [TRL] \",\n    \"--- Cineosis Function\":      \" [---] \"  # Changed key for clarity\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper56_output\") # Default output for clapper56TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and constants for genome section\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70) # Dim gray for placeholder characters\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Removed ASCII Grid Pattern drawing\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for original_line_text in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n\n        original_length = len(original_line_text)\n        # Calculate width of original text\n        width_of_original_line = 0\n        for i in range(original_length):\n            try:\n                width_of_original_line += chosen_genome_font.getlength(original_line_text[i])\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(original_line_text[i])\n                width_of_original_line += bbox[2] - bbox[0]\n\n        remaining_width = max_render_width - width_of_original_line\n        num_padding_chars = 0\n        padding_string = \"\"\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ' ')\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ' ': # It's an actual glyph from the original line\n                    current_glyph_num_in_line += 1 # Increment for actual glyphs\n                    if current_glyph_num_in_line == highlight_index:\n                        char_color = WHITE # Highlight this glyph\n                    else:\n                        char_color = AMBER # Regular genome glyph color\n                else:\n                    # It's a space from the original line\n                    char_color = AMBER # Spaces are part of the genome line, colored AMBER\n            else:\n                # It's a placeholder padding character\n                char_color = PLACEHOLDER_COLOR\n            \n            # Calculate width of the character to be drawn\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            # Truncation and ellipsis logic\n            if current_x + char_width > max_render_width: # If this char doesn't fit\n                if current_x + ellipsis_width <= max_render_width: # Check if ellipsis fits\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part of the line\n                        ellipsis_fill = AMBER # Default for original part\n                        \n                        # Determine the 0-based index of the first glyph that is part of the truncation\n                        first_truncated_glyph_idx_val = current_glyph_num_in_line\n                        if char_to_draw == ' ': # If the char causing truncation is a space\n                           first_truncated_glyph_idx_val +=1 # then the actual first glyph truncated is the next one\n                        \n                        if highlight_index != -1 and highlight_index >= first_truncated_glyph_idx_val and highlight_index < total_glyphs_in_original:\n                            ellipsis_fill = WHITE # Highlighted glyph is in the truncated part\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line (whether ellipsis was drawn or not)\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    image_label_text = f\"{image_glyph}\" # Only glyph\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    \n    # Draw actual image_type_str (value) at value_start_x, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((value_start_x, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2] # Width of the image type value itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str + padding\n    line_field_x_start = value_start_x + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    longest_entry_path = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json\" # Corrected path\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(output_dir_name, \"_EXTREME_TEST_CARD_clapper56.png\") # Use passed output_dir_name\n    render_card(longest_entry, output_path, fonts, genome_map) # Pass genome_map\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER56 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    output_dir_name = \"cards_clapper56_output\"\n    os.makedirs(output_dir_name, exist_ok=True) # Ensure output directory exists\n\n    # render_demo_card(fonts, genome_map, output_dir_name) # Would also need output_dir_name if uncommented\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    if not processed_timeline_data:\n        print(\"No timeline data to process for random poem entries.\")\n        return\n\n    poems_to_entries = {}\n    for entry in processed_timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            if poem_title not in poems_to_entries:\n                poems_to_entries[poem_title] = []\n            poems_to_entries[poem_title].append(entry)\n\n    if not poems_to_entries:\n        print(\"No poems found in timeline data to select random entries from.\")\n        return\n\n    print(f\"\\nRendering one random entry for each of the {len(poems_to_entries)} poems found...\")\n    count = 0\n    for poem_title, entries_for_poem in poems_to_entries.items():\n        if entries_for_poem:\n            selected_entry = random.choice(entries_for_poem)\n            entry_id = selected_entry.get('id', f'random_poem_entry_{count+1}')\n            output_filename = f\"{entry_id}_clapper56.png\"\n            output_path = os.path.join(output_dir_name, output_filename)\n            \n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_title}'...\")\n            render_card(selected_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n            count += 1\n        else:\n            print(f\"No entries found for poem '{poem_title}' to select a random one.\")\n\n    print(f\"\\nFinished rendering {count} random cards per poem.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json",
      "_EXTREME_TEST_CARD_clapper56.png",
      "{entry_id}_clapper56.png",
      "\nCLAPPER56 - Standardize all prompt section glyphs to ASCII. Verify highlighting. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII Standardized\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ",
      ")\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get(",
      " # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER56 - Standardize all prompt section glyphs to ASCII. Verify highlighting. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper42.py",
    "size": 26028,
    "lines": 493,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER42 - Clapperboard with new S/I/C line styling (Menlo Regular for all text)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    max_genome_line_chars = 36 # Max characters for the S/I/C glyph sequence itself\n    \n    genome_lines_to_render = []\n    if genome_data_entry:\n        s_line_data = genome_data_entry.get('s_line', 'S_LINE_N/A')[:max_genome_line_chars]\n        i_line_data = genome_data_entry.get('i_line', 'I_LINE_N/A')[:max_genome_line_chars]\n        c_line_data = genome_data_entry.get('c_line', 'C_LINE_N/A')[:max_genome_line_chars]\n        genome_lines_to_render.append(f'\u259e Syntagma: \"{s_line_data}\"')\n        genome_lines_to_render.append(f'\u263c Image: \"{i_line_data}\"')\n        genome_lines_to_render.append(f'\u25ff Cineosis: \"{c_line_data}\"')\n    else:\n        poem_display_title = entry.get('poem', 'Unknown Poem')[:35] # Truncate for display if long\n        genome_lines_to_render = [\n            '\u259e Syntagma: \"GENOME_N/A\"',\n            '\u263c Image: \"GENOME_N/A\"',\n            '\u25ff Cineosis: \"GENOME_N/A\"',\n            f\"(for poem: '{poem_display_title}')\"\n        ]\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2 # +2 for leading\n    \n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1 # +1 for leading\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8: # 8 for padding (4 top, 4 bottom)\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn't draw above top padding\n        genome_y_start = current_y_offset + 4\n\n    for line_text in genome_lines_to_render:\n        line_x = 10 # Left align genome text\n        \n        # Truncate line if too wide for the card\n        try:\n            line_width = chosen_genome_font.getlength(line_text)\n        except AttributeError: # Fallback for older Pillow versions\n            line_bbox = chosen_genome_font.getbbox(line_text)\n            line_width = line_bbox[2] - line_bbox[0]\n\n        if line_x + line_width > BASE_WIDTH - 10: # 10px right padding\n            available_width = BASE_WIDTH - 10 - line_x\n            # Estimate characters to keep (rough approximation)\n            avg_char_width_bbox = chosen_genome_font.getbbox(\"A\") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for \"...\"\n                line_text = line_text[:chars_to_keep] + \"...\"\n            else: # Should not happen with valid fonts\n                line_text = line_text[:10] + \"...\" # Fallback truncation\n\n        draw.text((line_x, genome_y_start), line_text, font=chosen_genome_font, fill=AMBER) # Use AMBER color\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, IMAGE TYPE, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    image_text = f\"IMAGE: {image_glyph} {image_type_str}\"\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), image_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. POEM AND LINE CONTENT ROW\n    poem_line_content_y = current_y_offset\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    draw.text((line_start_x, poem_line_content_y), line_prefix, font=header_font, fill=WHITE)\n    \n    line_content_start_x = line_start_x + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_start_x - 10\n    \n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_start_x, poem_line_content_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER42 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper42_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper42.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper42.png",
      "\nCLAPPER42 - Clapperboard with new S/I/C line styling (Menlo Regular for all text)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      ").strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    max_genome_line_chars = 36 # Max characters for the S/I/C glyph sequence itself\n    \n    genome_lines_to_render = []\n    if genome_data_entry:\n        s_line_data = genome_data_entry.get(",
      "S_LINE_N/A",
      "I_LINE_N/A",
      "C_LINE_N/A",
      "GENOME_N/A",
      "GENOME_N/A",
      "GENOME_N/A",
      ")\n\n    # Vertical centering for the block of genome lines\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4: # Ensure it doesn",
      ") # Use bbox of a char for average width\n            avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n            if avg_char_width > 0:\n                chars_to_keep = max(0, int(available_width / avg_char_width) - 3) # -3 for ",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = ",
      ":\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER42 - Clapperboard with new S/I/C line styling (Menlo Regular for all text)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper61prime.py",
    "size": 32609,
    "lines": 688,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER61PRIME - Refined glyph mappings, using UPDATED-TIMELINE.json.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER61, processes UPDATED-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper61prime_output/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES - Mapped from discovered data to glyphs.md symbols\nSYNTAGMA_GLYPHS = {\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (CS)\": \"\u2756\", # Data uses \"CS\", glyphs.md uses \"XS\" for \u2756\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n    # Explicitly add other known syntagma types from glyphs.md for completeness, even if not in current data sample\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    # Image types found in syntagma data (e.g., \"Action-Image\") will correctly get '?'\n}  \n\n# IMAGE TYPES - Mapped from discovered data to glyphs.md symbols\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n    # Any other image types from data not listed here will get '?'\n}\n\n# CINEOSIS FUNCTIONS - Mapped from discovered data to glyphs.md symbols\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n    # Any other cineosis functions from data not listed here will get '?'\n}  \n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n# TIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nTIMELINE_PATH = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), \"HONEYBADGER\", \"UPDATED-TIMELINE.json\") # Points to HONEYBADGER dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper61prime_output\")\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef discover_unique_prompt_values(timeline_data_path):\n    print(f\"\\nDiscovering unique prompt values from: {timeline_data_path}\")\n    try:\n        with open(timeline_data_path, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline data file not found at {timeline_data_path}\")\n        return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {timeline_data_path}\")\n        return\n\n    unique_image_types = set()\n    unique_syntagma_types = set()\n    unique_cineosis_functions = set()\n\n    for entry in data:\n        image_type = entry.get('imageType')\n        if image_type is not None:\n            unique_image_types.add(image_type)\n        \n        syntagma_type = entry.get('syntagmaType')\n        if syntagma_type is not None:\n            unique_syntagma_types.add(syntagma_type)\n\n        cineosis_function = entry.get('cineosisFunction')\n        if cineosis_function is not None:\n            unique_cineosis_functions.add(cineosis_function)\n\n    print(\"\\n--- Unique Image Types ---\")\n    for item in sorted(list(unique_image_types)):\n        print(item) # Simplified print\n    \n    print(\"\\n--- Unique Syntagma Types ---\")\n    for item in sorted(list(unique_syntagma_types)):\n        print(item) # Simplified print\n\n    print(\"\\n--- Unique Cineosis Functions ---\")\n    for item in sorted(list(unique_cineosis_functions)):\n        print(item) # Simplified print\n    print(\"\\nDiscovery complete.\")\n\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems_data_list = [] \n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems_data_list = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems_data_list:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    \n    genome_map_dict = {}\n    if isinstance(poems_data_list, list):\n        for item in poems_data_list:\n            if isinstance(item, dict) and 'title' in item:\n                poem_name = item['title'].strip().lower()\n                genome_map_dict[poem_name] = {\n                    's_line': item.get('s_line', ''),\n                    'i_line': item.get('i_line', ''),\n                    'c_line': item.get('c_line', '')\n                }\n    elif isinstance(poems_data_list, dict):\n        print(f\"Warning: Genome data at {file_path} was already a dictionary. Using as is.\")\n        genome_map_dict = {k.lower(): v for k, v in poems_data_list.items()} # Ensure keys are lowercase\n    else:\n        print(f\"Error: Genome data in {file_path} is not a list or a dictionary.\")\n\n    return genome_map_dict\n\n\ndef get_non_space_char_index(text, original_char_pos):\n    \"\"\"Calculates the 0-based index of text[original_char_pos] among non-space characters up to that point.\"\"\"\n    count = -1\n    if original_char_pos >= len(text):\n        # This can happen if original_char_pos is for a character that doesn't exist (e.g. highlight_index out of bounds)\n        # or if we are querying for a space character that should not have a non-space index.\n        # However, highlight_index should always point to a valid non-space char if data is correct.\n        # For safety, return a high value or handle appropriately if text[original_char_pos] is space.\n        return -1 # Indicates not a valid non-space char or out of bounds for this purpose\n\n    for i in range(original_char_pos + 1):\n        if text[i] != ' ':\n            count += 1\n    return count\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10\n    value_start_x = 10 + narrow_label_area_width\n\n    current_y_offset = 4\n    genome_section_y_on_img = current_y_offset\n\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70)\n\n    genome_content_x1_on_img = 10\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    frame_position = entry.get('frameNumber', 0)\n    highlight_index = -1\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1\n\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10\n    avg_char_width = 0\n    try:\n        avg_char_width = chosen_genome_font.getlength(\"X\")\n    except AttributeError:\n        avg_char_width_bbox = chosen_genome_font.getbbox(\"X\")\n        avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n    if avg_char_width <= 0: avg_char_width = 1 # Prevent division by zero\n\n    ellipsis_char_width = 0\n    try:\n        ellipsis_char_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_char_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    if ellipsis_char_width <= 0: ellipsis_char_width = avg_char_width * 3 # Fallback\n\n    placeholder_char_width = 0\n    try:\n        placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n    except AttributeError:\n        ph_bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n        placeholder_char_width = ph_bbox[2] - ph_bbox[0]\n    if placeholder_char_width <= 0: placeholder_char_width = 1 # Fallback\n\n    for original_line_text in genome_lines_to_render:\n        current_x = genome_content_x1_on_img\n        display_start_index = 0\n        scrolled_from_left = False\n\n        # Determine if scrolling from left is needed\n        chars_that_fit_total_approx = int(max_render_width // avg_char_width)\n        target_highlight_offset_in_view = chars_that_fit_total_approx // 4\n\n        if highlight_index >= target_highlight_offset_in_view and len(original_line_text) > chars_that_fit_total_approx:\n            scrolled_from_left = True\n            display_start_index = max(0, highlight_index - target_highlight_offset_in_view)\n\n        # Draw Prefix (if any)\n        if scrolled_from_left:\n            prefix_text = f\"{display_start_index}+\"\n            prefix_color = AMBER # Or a specific color for indicators\n            try:\n                prefix_width = chosen_genome_font.getlength(prefix_text)\n            except AttributeError:\n                prefix_bbox = chosen_genome_font.getbbox(prefix_text)\n                prefix_width = prefix_bbox[2] - prefix_bbox[0]\n            \n            if current_x + prefix_width <= max_render_width:\n                draw.text((current_x, genome_y_start), prefix_text, font=chosen_genome_font, fill=prefix_color)\n                current_x += prefix_width\n            else:\n                # Not enough space even for the prefix, might draw '...' if possible\n                if current_x + ellipsis_char_width <= max_render_width:\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=AMBER)\n                    current_x += ellipsis_char_width\n                genome_y_start += chosen_genome_line_height\n                continue # Move to next line, this one is too cramped\n        \n        # Draw Genome Segment and Suffix Ellipsis (if needed)\n        genome_chars_drawn_this_line = 0\n        for idx_in_original in range(display_start_index, len(original_line_text)):\n            char_to_draw = original_line_text[idx_in_original]\n            char_width = 0\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            if char_width <=0: char_width = avg_char_width # Fallback\n\n            # Check if there's space for this char + potential ellipsis if it's not the last char\n            need_space_for_suffix_ellipsis = (idx_in_original < len(original_line_text) - 1)\n            required_ellipsis_space = ellipsis_char_width if need_space_for_suffix_ellipsis else 0\n            \n            if current_x + char_width > max_render_width - required_ellipsis_space:\n                # Not enough space for current char and potential suffix ellipsis\n                if current_x + ellipsis_char_width <= max_render_width:\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=AMBER)\n                    current_x += ellipsis_char_width\n                break # Stop drawing this genome line\n\n            # Determine color\n            char_color = AMBER\n            if char_to_draw != ' ':\n                current_char_overall_non_space_idx = get_non_space_char_index(original_line_text, idx_in_original)\n                if current_char_overall_non_space_idx == highlight_index:\n                    char_color = WHITE\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            genome_chars_drawn_this_line +=1\n\n        # Padding with PLACEHOLDER_CHAR\n        drawn_width = current_x - genome_content_x1_on_img\n        remaining_line_width = max_render_width - drawn_width\n        if remaining_line_width > 0 and placeholder_char_width > 0:\n            num_padding_chars = int(remaining_line_width // placeholder_char_width)\n            if num_padding_chars > 0:\n                padding_text = PLACEHOLDER_CHAR * num_padding_chars\n                draw.text((current_x, genome_y_start), padding_text, font=chosen_genome_font, fill=PLACEHOLDER_COLOR)\n\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 4\n\n    header_content_y = current_y_offset + 4\n    top_row_internal_height = 30\n    \n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position_val = entry.get('frameNumber', 0) \n    frame_total_val = entry.get('totalFrames', 0)\n    frame_text = f\"FRAME: ({frame_position_val}/{frame_total_val})\"\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name[:25]}\"\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8\n\n    initial_content_row_y = current_y_offset\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str_raw, \"?\").strip()\n    line_content = entry.get('content', '---')\n\n    glyph_column_x = 10\n    draw.text((glyph_column_x, initial_content_row_y), image_glyph, font=chosen_genome_font, fill=WHITE)\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_text_width_bbox = text_font.getbbox(image_type_str_raw)\n    image_text_width = image_text_width_bbox[2] - image_text_width_bbox[0]\n\n    line_field_x_start = value_start_x + image_text_width + 30\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width_bbox = header_font.getbbox(line_prefix)\n    line_prefix_width = line_prefix_width_bbox[2] - line_prefix_width_bbox[0]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10\n    \n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    try:\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4\n    except AttributeError:\n        ONE_TEXT_LINE_HEIGHT = 16\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    current_y_offset += 4\n    divider_start_x = glyph_column_x - 4 if glyph_column_x > 4 else 6 \n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8\n\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10\n\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str_raw, \"?\").strip()\n    draw.text((glyph_column_x, current_y_offset), syntagma_glyph, font=chosen_genome_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str_raw, \"?\").strip()\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=chosen_genome_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4\n    \n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4)\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frameNumber\": 1, \"totalFrames\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_genome_map = {\"demo poem title\": {\"s_line\": \"SSS\", \"i_line\": \"III\", \"c_line\": \"CCC\"}}\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_clapper61prime.png\")\n    render_card(demo_entry, demo_output_path, fonts, demo_genome_map)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    longest_entry_path = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json\"\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frameNumber\": 999, \"totalFrames\": 999,\n            \"imageType\": \"Descriptive Image\", \n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", \n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", \n            \"cineosisFunction\": \"Mood Environment Stabilizer\", \n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(output_dir_name, \"_EXTREME_TEST_CARD_clapper61prime.png\")\n    render_card(longest_entry, output_path, fonts, genome_map)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name:\n            if poem_name not in poem_frames: \n                poem_frames[poem_name] = 0\n            poem_frames[poem_name] += 1\n\n    processed_data = []\n    current_poem_entry_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        new_entry = entry.copy()\n        if poem_name:\n            if poem_name not in current_poem_entry_count: \n                current_poem_entry_count[poem_name] = 0\n            current_poem_entry_count[poem_name] += 1\n            new_entry['frameNumber'] = entry.get('frameNumber', current_poem_entry_count[poem_name])\n            new_entry['totalFrames'] = entry.get('totalFrames', poem_frames.get(poem_name, 0))\n        else:\n            new_entry['frameNumber'] = entry.get('frameNumber', 0)\n            new_entry['totalFrames'] = entry.get('totalFrames', 0)\n            \n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER61PRIME card generation with UPDATED-TIMELINE.json...\")\n\n    genome_map = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16),\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12),\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data_raw = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data_raw)\n\n    output_dir_name = OUTPUT_DIR\n    os.makedirs(output_dir_name, exist_ok=True)\n\n    # render_demo_card(fonts)\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    if not processed_timeline_data:\n        print(\"No timeline data to process for random poem entries.\")\n        return\n\n    poems_to_entries = {}\n    for entry in processed_timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            if poem_title not in poems_to_entries:\n                poems_to_entries[poem_title] = []\n            poems_to_entries[poem_title].append(entry)\n\n    if not poems_to_entries:\n        print(\"No poems found in timeline data to select random entries from.\")\n        return\n\n    print(f\"\\nRendering one random entry for each of the {len(poems_to_entries)} poems found...\")\n    count = 0\n    for poem_title, entries_for_poem in poems_to_entries.items():\n        if entries_for_poem:\n            selected_entry = random.choice(entries_for_poem)\n            entry_id = selected_entry.get('id', f'random_poem_entry_{count+1}')\n            output_filename = f\"{entry_id}_clapper61prime.png\"\n            output_path = os.path.join(output_dir_name, output_filename)\n            \n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_title}'...\")\n            render_card(selected_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n            count += 1\n        else:\n            print(f\"No entries found for poem '{poem_title}' to select a random one.\")\n\n    print(f\"\\nFinished rendering {count} random cards per poem.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "UPDATED-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_clapper61prime.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json",
      "_EXTREME_TEST_CARD_clapper61prime.png",
      "{entry_id}_clapper61prime.png",
      "\nCLAPPER61PRIME - Refined glyph mappings, using UPDATED-TIMELINE.json.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER61, processes UPDATED-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper61prime_output/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII# SYNTAGMA TYPES - Mapped from discovered data to glyphs.md symbols\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = ",
      ")\n        avg_char_width = avg_char_width_bbox[2] - avg_char_width_bbox[0]\n    if avg_char_width <= 0: avg_char_width = 1 # Prevent division by zero\n\n    ellipsis_char_width = 0\n    try:\n        ellipsis_char_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_char_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    if ellipsis_char_width <= 0: ellipsis_char_width = avg_char_width * 3 # Fallback\n\n    placeholder_char_width = 0\n    try:\n        placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n    except AttributeError:\n        ph_bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n        placeholder_char_width = ph_bbox[2] - ph_bbox[0]\n    if placeholder_char_width <= 0: placeholder_char_width = 1 # Fallback\n\n    for original_line_text in genome_lines_to_render:\n        current_x = genome_content_x1_on_img\n        display_start_index = 0\n        scrolled_from_left = False\n\n        # Determine if scrolling from left is needed\n        chars_that_fit_total_approx = int(max_render_width // avg_char_width)\n        target_highlight_offset_in_view = chars_that_fit_total_approx // 4\n\n        if highlight_index >= target_highlight_offset_in_view and len(original_line_text) > chars_that_fit_total_approx:\n            scrolled_from_left = True\n            display_start_index = max(0, highlight_index - target_highlight_offset_in_view)\n\n        # Draw Prefix (if any)\n        if scrolled_from_left:\n            prefix_text = f",
      ":\n                current_char_overall_non_space_idx = get_non_space_char_index(original_line_text, idx_in_original)\n                if current_char_overall_non_space_idx == highlight_index:\n                    char_color = WHITE\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            genome_chars_drawn_this_line +=1\n\n        # Padding with PLACEHOLDER_CHAR\n        drawn_width = current_x - genome_content_x1_on_img\n        remaining_line_width = max_render_width - drawn_width\n        if remaining_line_width > 0 and placeholder_char_width > 0:\n            num_padding_chars = int(remaining_line_width // placeholder_char_width)\n            if num_padding_chars > 0:\n                padding_text = PLACEHOLDER_CHAR * num_padding_chars\n                draw.text((current_x, genome_y_start), padding_text, font=chosen_genome_font, fill=PLACEHOLDER_COLOR)\n\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 4\n\n    header_content_y = current_y_offset + 4\n    top_row_internal_height = 30\n    \n    id_label_text = ",
      "FRAME: ({frame_position_val}/{frame_total_val})",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER61PRIME - Refined glyph mappings, using UPDATED-TIMELINE.json.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER61, processes UPDATED-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper61prime_output/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper52.py",
    "size": 29718,
    "lines": 565,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER52 - Genome report: Plus (+) grid background, light box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and grid spacing for genome section enhancements\n    GRID_PATTERN_COLOR = (50, 50, 50)  # Dark gray\n    LIGHT_BOX_COLOR = (100, 100, 100) # Light gray\n    GRID_SPACING = 7 # Pixels for grid density\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Draw '+' Grid Pattern (BEFORE text rendering)\n    # Ensure genome_font_tiny is available here, it's defined a bit lower usually.\n    # We'll use it for the '+' character. If it's not defined yet, this could be an issue.\n    # Let's assume fonts['genome_tiny'] is accessible.\n    plus_font = fonts.get('genome_tiny', fonts['text']) # Fallback to text_font if tiny isn't loaded yet\n\n    for gx in range(genome_content_x1_on_img, genome_content_x2_on_img, GRID_SPACING):\n        for gy in range(genome_content_y1_on_img, genome_content_y2_on_img, GRID_SPACING):\n            # To center the '+', we might need to offset it slightly based on its bbox\n            # For simplicity now, just draw at (gx, gy)\n            draw.text((gx, gy), \"+\", font=plus_font, fill=GRID_PATTERN_COLOR)\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for line_text_full in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n        \n        for char_idx, char_to_draw in enumerate(line_text_full):\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                # Check if ellipsis itself can fit\n                if current_x + ellipsis_width <= max_render_width:\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=AMBER)\n                break # Stop drawing this line\n\n            char_color = AMBER\n            if char_idx == highlight_index:\n                char_color = WHITE # Highlight if current character's index matches frame_position\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Draw Light Box (AFTER text rendering, frames the genome content area)\n    box_x1 = genome_content_x1_on_img - 2\n    box_y1 = genome_content_y1_on_img - 2\n    box_x2 = genome_content_x2_on_img + 2\n    box_y2 = genome_content_y2_on_img + 2\n    \n    draw.rectangle(\n        [(box_x1, box_y1), (box_x2, box_y2)],\n        outline=LIGHT_BOX_COLOR,\n        width=1\n    )\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    image_label_text = f\"{image_glyph}\" # Only glyph\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    \n    # Draw actual image_type_str (value) at value_start_x, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((value_start_x, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2] # Width of the image type value itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str + padding\n    line_field_x_start = value_start_x + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER52 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper52_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper52.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper52.png",
      "\nCLAPPER52 - Genome report: Plus (+) grid background, light box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get(",
      " # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER52 - Genome report: Plus (+) grid background, light box. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper49.py",
    "size": 27406,
    "lines": 522,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER49 - Glyph-only labels for I/S/C; no labels for Ekphrasis/Style. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4:\n        genome_y_start = current_y_offset + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for line_text_full in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = 10  # Reset X for each line\n        \n        for char_idx, char_to_draw in enumerate(line_text_full):\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                # Check if ellipsis itself can fit\n                if current_x + ellipsis_width <= max_render_width:\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=AMBER)\n                break # Stop drawing this line\n\n            char_color = AMBER\n            if char_idx == highlight_index:\n                char_color = WHITE # Highlight if current character's index matches frame_position\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    image_label_text = f\"{image_glyph}\" # Only glyph\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    \n    # Draw actual image_type_str (value) at value_start_x, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((value_start_x, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2] # Width of the image type value itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str + padding\n    line_field_x_start = value_start_x + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph}\" # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER49 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper49_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper49.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper49.png",
      "\nCLAPPER49 - Glyph-only labels for I/S/C; no labels for Ekphrasis/Style. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4:\n        genome_y_start = current_y_offset + 4\n\n    ellipsis = ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get(",
      " # Only glyph\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    operative_ekphrasis = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_conditioning = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER49 - Glyph-only labels for I/S/C; no labels for Ekphrasis/Style. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper59.py",
    "size": 33689,
    "lines": 640,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER59 - Dynamically calculate value_start_x for robust prompt layout. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII Standardized\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \" [AS]  \",\n    \"Parallel Syntagma (PS)\":     \" [PS]  \",\n    \"Alternating Syntagma (LS)\":  \" [LS]  \",\n    \"Chronological Syntagma (CS)\":\" [CS]  \",\n    \"Descriptive Syntagma (DS)\":  \" [DS]  \",\n    \"Flashback Syntagma (FS)\":    \" [FS]  \",\n    \"Thematic Montage (TM)\":      \" [TM]  \", # Added from older versions\n    \"Crystal Syntagma (XS)\":      \" [XS]  \", # Added from older versions\n    \"--- Syntagma\":               \" [---] \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\":         \" [Act] \",\n    \"Affection-Image\":      \" [Aff] \",\n    \"Crystal-Image\":        \" [Cry] \",\n    \"Descriptive Image\":    \" [Des] \",\n    \"Opsign\":               \" [Ops] \",\n    \"Perception-Image\":     \" [Per] \",\n    \"Recollection-Image\":   \" [Rec] \",\n    \"Sonsign\":              \" [Son] \",\n    \"Thematic Montage\":     \" [TMg] \", # Note: Key matches a Syntagma type, ensure context handles this\n    \"--- Image Type\":       \" [---] \"  # Changed key for clarity\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\":         \" [AEc] \",\n    \"Causal Motion Trigger\":        \" [CMT] \",\n    \"Emotion Relay\":                \" [EmR] \",\n    \"Event Pause Invocation\":       \" [EPI] \",\n    \"Memory Storage Retrieval\":     \" [MSR] \",\n    \"Mood Environment Stabilizer\":  \" [MES] \",\n    \"Narrative Modifier\":           \" [NaM] \",\n    \"Subjective Frame Recalibration\":\" [SFR] \",\n    \"Temporal Reflection Loop\":     \" [TRL] \",\n    \"--- Cineosis Function\":      \" [---] \"  # Changed key for clarity\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper59_output\") # Default output for clapper56TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and constants for genome section\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70) # Dim gray for placeholder characters\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Removed ASCII Grid Pattern drawing\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for original_line_text in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n\n        original_length = len(original_line_text)\n        # Calculate width of original text\n        width_of_original_line = 0\n        for i in range(original_length):\n            try:\n                width_of_original_line += chosen_genome_font.getlength(original_line_text[i])\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(original_line_text[i])\n                width_of_original_line += bbox[2] - bbox[0]\n\n        remaining_width = max_render_width - width_of_original_line\n        num_padding_chars = 0\n        padding_string = \"\"\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ' ')\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ' ': # It's an actual glyph from the original line\n                    current_glyph_num_in_line += 1 # Increment for actual glyphs\n                    if current_glyph_num_in_line == highlight_index:\n                        char_color = WHITE # Highlight this glyph\n                    else:\n                        char_color = AMBER # Regular genome glyph color\n                else:\n                    # It's a space from the original line\n                    char_color = AMBER # Spaces are part of the genome line, colored AMBER\n            else:\n                # It's a placeholder padding character\n                char_color = PLACEHOLDER_COLOR\n            \n            # Calculate width of the character to be drawn\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            # Truncation and ellipsis logic\n            if current_x + char_width > max_render_width: # If this char doesn't fit\n                if current_x + ellipsis_width <= max_render_width: # Check if ellipsis fits\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part of the line\n                        ellipsis_fill = AMBER # Default for original part\n                        \n                        # Determine the 0-based index of the first glyph that is part of the truncation\n                        first_truncated_glyph_idx_val = current_glyph_num_in_line\n                        if char_to_draw == ' ': # If the char causing truncation is a space\n                           first_truncated_glyph_idx_val +=1 # then the actual first glyph truncated is the next one\n                        \n                        if highlight_index != -1 and highlight_index >= first_truncated_glyph_idx_val and highlight_index < total_glyphs_in_original:\n                            ellipsis_fill = WHITE # Highlighted glyph is in the truncated part\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line (whether ellipsis was drawn or not)\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str_raw, \"[???]\").strip() # Default to [???]\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    draw.text((10, initial_content_row_y), image_glyph, font=header_font, fill=WHITE) # Draw glyph\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR) # Draw text\n    image_text_width = text_font.getbbox(image_type_str_raw)[2] # Width of the image type text itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str_raw + padding\n    line_field_x_start = value_start_x + image_text_width + 30 # 30px padding after image text\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    glyph_column_x = 10 # Define glyph_column_x before its first use\n    # The glyph_column_x is where glyphs are drawn (e.g., 10px).\n    # The value_start_x is dynamically calculated based on max_glyph_width.\n    # The divider should visually separate the 'Image/Line' row from the 'Prompt' section.\n    # Let's make the divider start just left of the glyph column.\n    divider_start_x = glyph_column_x - 4 if glyph_column_x > 4 else 6 \n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at glyph_column_x (e.g. 10), Values at value_start_x (dynamic)\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str_raw, \"[???]\").strip() # Default to [???]\n    draw.text((glyph_column_x, current_y_offset), syntagma_glyph, font=header_font, fill=WHITE) # Draw glyph\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str_raw, \"[???]\").strip() # Default to [???]\n    draw.text((10, current_y_offset), cineosis_glyph, font=header_font, fill=WHITE) # Draw glyph\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    longest_entry_path = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json\" # Corrected path\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(output_dir_name, \"_EXTREME_TEST_CARD_clapper59.png\") # Use passed output_dir_name\n    render_card(longest_entry, output_path, fonts, genome_map) # Pass genome_map\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER59 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    output_dir_name = \"cards_clapper59_output\"\n    os.makedirs(output_dir_name, exist_ok=True) # Ensure output directory exists\n\n    # render_demo_card(fonts, genome_map, output_dir_name) # Would also need output_dir_name if uncommented\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    if not processed_timeline_data:\n        print(\"No timeline data to process for random poem entries.\")\n        return\n\n    poems_to_entries = {}\n    for entry in processed_timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            if poem_title not in poems_to_entries:\n                poems_to_entries[poem_title] = []\n            poems_to_entries[poem_title].append(entry)\n\n    if not poems_to_entries:\n        print(\"No poems found in timeline data to select random entries from.\")\n        return\n\n    print(f\"\\nRendering one random entry for each of the {len(poems_to_entries)} poems found...\")\n    count = 0\n    for poem_title, entries_for_poem in poems_to_entries.items():\n        if entries_for_poem:\n            selected_entry = random.choice(entries_for_poem)\n            entry_id = selected_entry.get('id', f'random_poem_entry_{count+1}')\n            output_filename = f\"{entry_id}_clapper59.png\"\n            output_path = os.path.join(output_dir_name, output_filename)\n            \n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_title}'...\")\n            render_card(selected_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n            count += 1\n        else:\n            print(f\"No entries found for poem '{poem_title}' to select a random one.\")\n\n    print(f\"\\nFinished rendering {count} random cards per poem.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json",
      "_EXTREME_TEST_CARD_clapper59.png",
      "{entry_id}_clapper59.png",
      "\nCLAPPER59 - Dynamically calculate value_start_x for robust prompt layout. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII Standardized\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ",
      ")\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    glyph_column_x = 10 # Define glyph_column_x before its first use\n    # The glyph_column_x is where glyphs are drawn (e.g., 10px).\n    # The value_start_x is dynamically calculated based on max_glyph_width.\n    # The divider should visually separate the ",
      ").strip() # Default to [???]\n    draw.text((10, current_y_offset), cineosis_glyph, font=header_font, fill=WHITE) # Draw glyph\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER59 - Dynamically calculate value_start_x for robust prompt layout. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/assemblerprimeplus.py",
    "size": 3645,
    "lines": 85,
    "source": "#!/usr/bin/env python3\n\"\"\"\nASSEMBLERPRIMEPLUS - Creates a video from image cards.\n- Specifically designed to assemble images generated by 61primeplus.py for the 'Magic ride' poem.\n- Reads images from ELEPHANT/TUSK/\n- Outputs a video file (e.g., magic_ride_video.mp4) to ELEPHANT/TUSK/\n- Target video duration is 2 minutes 11 seconds (131 seconds).\n\"\"\"\n\nimport os\nimport glob\nfrom moviepy.editor import ImageSequenceClip\n\n# --- Configuration ---\n# Assumes this script is in ELEPHANT/TRUNK/\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nELEPHANT_DIR = os.path.dirname(SCRIPT_DIR)\n\nIMAGE_DIR_NAME = \"TUSK\"\nIMAGE_SUBDIR = os.path.join(ELEPHANT_DIR, IMAGE_DIR_NAME)\n\nOUTPUT_VIDEO_FILENAME = \"magic_ride_video.mp4\"\nOUTPUT_VIDEO_PATH = os.path.join(IMAGE_SUBDIR, OUTPUT_VIDEO_FILENAME)\n\nTARGET_TOTAL_DURATION_SECONDS = 131  # 2 minutes 11 seconds\nIMAGE_FILE_PATTERN = \"magic-ride_frame_*.png\"\n\n# --- Main script ---\ndef main():\n    print(f\"Starting video assembly...\")\n    print(f\"Image source directory: {IMAGE_SUBDIR}\")\n    print(f\"Output video path: {OUTPUT_VIDEO_PATH}\")\n    print(f\"Target total duration: {TARGET_TOTAL_DURATION_SECONDS} seconds\")\n\n    image_search_pattern = os.path.join(IMAGE_SUBDIR, IMAGE_FILE_PATTERN)\n    image_files = sorted(glob.glob(image_search_pattern))\n\n    if not image_files:\n        print(f\"Error: No image files found matching pattern '{IMAGE_FILE_PATTERN}' in '{IMAGE_SUBDIR}'.\")\n        print(\"Please ensure 61primeplus.py has run successfully and generated images.\")\n        return\n\n    num_images = len(image_files)\n    print(f\"Found {num_images} image frames.\")\n\n    if num_images == 0:\n        print(\"Error: No images to process. Exiting.\")\n        return\n\n    # Calculate FPS required to achieve the target total duration with the given number of images.\n    # Each frame will be displayed for (TARGET_TOTAL_DURATION_SECONDS / num_images) seconds.\n    # FPS = 1 / duration_per_frame = num_images / TARGET_TOTAL_DURATION_SECONDS.\n    fps = num_images / TARGET_TOTAL_DURATION_SECONDS\n\n    print(f\"Calculated FPS: {fps:.4f} (for a total duration of {TARGET_TOTAL_DURATION_SECONDS}s with {num_images} frames)\")\n    \n    try:\n        print(\"Creating video clip...\")\n        # Create the video clip using the image files and calculated fps\n        clip = ImageSequenceClip(image_files, fps=fps)\n\n        print(f\"Writing video file to {OUTPUT_VIDEO_PATH}...\")\n        # Write the video file to disk\n        # Common codecs: 'libx264' (for H.264, good quality/compression), 'mpeg4'\n        # preset options: ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow\n        # threads for parallel processing (can speed up encoding)\n        clip.write_videofile(OUTPUT_VIDEO_PATH, \n                             codec='libx264', \n                             fps=fps, \n                             preset='medium', \n                             threads=4, \n                             logger='bar') # Use 'bar' for progress bar, or None for less verbose output\n        \n        print(f\"\\nVideo '{OUTPUT_VIDEO_FILENAME}' created successfully in '{IMAGE_SUBDIR}'.\")\n        # moviepy might slightly adjust duration due to fps quantization, check actual duration.\n        print(f\"Actual video duration reported by moviepy: {clip.duration:.2f} seconds.\")\n\n    except Exception as e:\n        print(f\"An error occurred during video creation: {e}\")\n        print(\"Please ensure 'moviepy' is installed (e.g., 'pip install moviepy') and that FFmpeg is available on your system.\")\n        print(\"FFmpeg is a dependency for moviepy to read/write video files.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "magic_ride_video.mp4",
      "magic-ride_frame_*.png",
      " poem.\n- Reads images from ELEPHANT/TUSK/\n- Outputs a video file (e.g., magic_ride_video.mp4) to ELEPHANT/TUSK/\n- Target video duration is 2 minutes 11 seconds (131 seconds).\n",
      "\n\nimport os\nimport glob\nfrom moviepy.editor import ImageSequenceClip\n\n# --- Configuration ---\n# Assumes this script is in ELEPHANT/TRUNK/\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nELEPHANT_DIR = os.path.dirname(SCRIPT_DIR)\n\nIMAGE_DIR_NAME = ",
      ")\n        return\n\n    # Calculate FPS required to achieve the target total duration with the given number of images.\n    # Each frame will be displayed for (TARGET_TOTAL_DURATION_SECONDS / num_images) seconds.\n    # FPS = 1 / duration_per_frame = num_images / TARGET_TOTAL_DURATION_SECONDS.\n    fps = num_images / TARGET_TOTAL_DURATION_SECONDS\n\n    print(f",
      " (for H.264, good quality/compression), ",
      "FFmpeg is a dependency for moviepy to read/write video files."
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "glob",
      "moviepy.editor"
    ],
    "generates": [],
    "reads": [],
    "docstring": "ASSEMBLERPRIMEPLUS - Creates a video from image cards.\n- Specifically designed to assemble images generated by 61primeplus.py for the 'Magic ride' poem.\n- Reads images from ELEPHANT/TUSK/\n- Outputs a video file (e.g., magic_ride_video.mp4) to ELEPHANT/TUSK/\n- Target video duration is 2 minutes 11 seconds (131 seconds)."
  },
  {
    "path": "ELEPHANT/TRUNK/clapper48.py",
    "size": 27536,
    "lines": 526,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER48 - Narrow left column for value alignment. Shortened divider. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper_menlo_genome\") # Output within TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4:\n        genome_y_start = current_y_offset + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for line_text_full in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = 10  # Reset X for each line\n        \n        for char_idx, char_to_draw in enumerate(line_text_full):\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            if current_x + char_width > max_render_width:\n                # Check if ellipsis itself can fit\n                if current_x + ellipsis_width <= max_render_width:\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=AMBER)\n                break # Stop drawing this line\n\n            char_color = AMBER\n            if char_idx == highlight_index:\n                char_color = WHITE # Highlight if current character's index matches frame_position\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Label at x=10, Value at value_start_x, single line)\n    image_label_text = f\"{image_glyph} Image:\"\n    draw.text((10, initial_content_row_y), image_label_text, font=header_font, fill=WHITE)\n    \n    # Draw actual image_type_str (value) at value_start_x, in IMAGE_TYPE_COLOR, using text_font (not bold)\n    # This part does not wrap.\n    draw.text((value_start_x, initial_content_row_y), image_type_str, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_value_width = text_font.getbbox(image_type_str)[2] # Width of the image type value itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str + padding\n    line_field_x_start = value_start_x + image_value_width + 30 # 30px padding after image value\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis\n    ekphrasis_label_text = f\"\ud83c\udfa8 Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # Style\n    style_label_text = f\"\u2728 Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER48 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    # render_demo_card(fonts) # Commented out for single image test\n    # render_extreme_test_case(fonts) # Commented out for single image test\n\n    count = 0\n    # Process only the first entry for this experiment\n    if processed_timeline_data:\n        entry = processed_timeline_data[0] # Take the first entry\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_dir_name = \"cards_clapper48_output\"\n        os.makedirs(output_dir_name, exist_ok=True)\n        output_filename = f\"{entry_id}_clapper48.png\"\n        output_path = os.path.join(output_dir_name, output_filename)\n        render_card(entry, output_path, fonts, genome_map)\n        count += 1\n    \n    if count > 0:\n        print(f\"First card rendered successfully in {output_dir_name}\")\n    else:\n        print(\"No entries in timeline to render or timeline empty.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_clapper48.png",
      "\nCLAPPER48 - Narrow left column for value alignment. Shortened divider. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    if genome_y_start < current_y_offset + 4:\n        genome_y_start = current_y_offset + 4\n\n    ellipsis = ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER48 - Narrow left column for value alignment. Shortened divider. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/TRUNK/clapper58.py",
    "size": 33314,
    "lines": 635,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER58 - Restore two-column layout for prompt glyphs/text. Robust glyph fallback. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII Standardized\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \" [AS]  \",\n    \"Parallel Syntagma (PS)\":     \" [PS]  \",\n    \"Alternating Syntagma (LS)\":  \" [LS]  \",\n    \"Chronological Syntagma (CS)\":\" [CS]  \",\n    \"Descriptive Syntagma (DS)\":  \" [DS]  \",\n    \"Flashback Syntagma (FS)\":    \" [FS]  \",\n    \"Thematic Montage (TM)\":      \" [TM]  \", # Added from older versions\n    \"Crystal Syntagma (XS)\":      \" [XS]  \", # Added from older versions\n    \"--- Syntagma\":               \" [---] \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\":         \" [Act] \",\n    \"Affection-Image\":      \" [Aff] \",\n    \"Crystal-Image\":        \" [Cry] \",\n    \"Descriptive Image\":    \" [Des] \",\n    \"Opsign\":               \" [Ops] \",\n    \"Perception-Image\":     \" [Per] \",\n    \"Recollection-Image\":   \" [Rec] \",\n    \"Sonsign\":              \" [Son] \",\n    \"Thematic Montage\":     \" [TMg] \", # Note: Key matches a Syntagma type, ensure context handles this\n    \"--- Image Type\":       \" [---] \"  # Changed key for clarity\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\":         \" [AEc] \",\n    \"Causal Motion Trigger\":        \" [CMT] \",\n    \"Emotion Relay\":                \" [EmR] \",\n    \"Event Pause Invocation\":       \" [EPI] \",\n    \"Memory Storage Retrieval\":     \" [MSR] \",\n    \"Mood Environment Stabilizer\":  \" [MES] \",\n    \"Narrative Modifier\":           \" [NaM] \",\n    \"Subjective Frame Recalibration\":\" [SFR] \",\n    \"Temporal Reflection Loop\":     \" [TRL] \",\n    \"--- Cineosis Function\":      \" [---] \"  # Changed key for clarity\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Points to ELEPHANT dir\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # Points to ELEPHANT dir\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper58_output\") # Default output for clapper56TRUNK\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Should still correctly point to 'resurrecting atlantis'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        # Attempt 1: Preferred font, direct name\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            # Attempt 2: Preferred font, with .ttf\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                # Attempt 3: Preferred font, with .otf\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                # All attempts for preferred font failed\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                \n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                \n                try:\n                    # Attempt 4: Fallback font, direct name\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        # Attempt 5: Fallback font, with .ttf\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            # Attempt 6: Fallback font, with .otf\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            # All attempts for fallback font also failed\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n                    except IOError:\n                        print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # Define narrow column width for aligning values\n    # Based on width of 'ID' (no colon) + small padding\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10 # Width of 'ID' + 10px padding\n    value_start_x = 10 + narrow_label_area_width\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_section_y_on_img = current_y_offset # current_y_offset is 4 here, top of genome area on img\n\n    # Define colors and constants for genome section\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70) # Dim gray for placeholder characters\n\n    # Define genome content display area bounds on the img canvas\n    genome_content_x1_on_img = 10\n    genome_content_y1_on_img = genome_section_y_on_img\n    # Text uses max_render_width = BASE_WIDTH - 10. So text ends at 10 + (BASE_WIDTH - 10) = BASE_WIDTH.\n    genome_content_x2_on_img = BASE_WIDTH \n    genome_content_y2_on_img = genome_section_y_on_img + GENOME_REPORT_HEIGHT\n\n    # Removed ASCII Grid Pattern drawing\n\n    # genome_section_y_on_img now holds the starting Y for this section (which is 4)\n    # current_y_offset will be updated at the END of this section.\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n\n    i_line_data_full = 'I_LINE_N/A'\n    s_line_data_full = 'S_LINE_N/A'\n    c_line_data_full = 'C_LINE_N/A'\n\n    if genome_data_entry:\n        i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A')\n        s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A')\n        c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A')\n\n    # New order: I, S, C\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n\n    # Determine highlight index from frame_position (1-indexed in data)\n    frame_position = entry.get('frame_position', 0) # 0 if not found or invalid\n    highlight_index = -1 # Default to no highlight\n    if isinstance(frame_position, (int, float)) and frame_position > 0:\n        highlight_index = int(frame_position) - 1 # Convert to 0-indexed\n\n    # Calculate text height and choose font size\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    \n    total_text_height_for_small_font = len(genome_lines_to_render) * chosen_genome_line_height\n    if total_text_height_for_small_font > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        total_text_height_for_tiny_font = len(genome_lines_to_render) * chosen_genome_line_height\n        if total_text_height_for_tiny_font > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall. May be clipped.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn't go above a minimal padding from top of section\n    if genome_y_start < genome_section_y_on_img + 4: \n        genome_y_start = genome_section_y_on_img + 4\n\n    ellipsis = \"...\"\n    try:\n        ellipsis_width = chosen_genome_font.getlength(ellipsis)\n    except AttributeError:\n        ellipsis_bbox = chosen_genome_font.getbbox(ellipsis)\n        ellipsis_width = ellipsis_bbox[2] - ellipsis_bbox[0]\n    max_render_width = BASE_WIDTH - 10 # 10px right padding\n\n    for original_line_text in genome_lines_to_render: # Iterate I, S, C lines\n        current_x = genome_content_x1_on_img  # Reset X for each line (starts at 10)\n\n        original_length = len(original_line_text)\n        # Calculate width of original text\n        width_of_original_line = 0\n        for i in range(original_length):\n            try:\n                width_of_original_line += chosen_genome_font.getlength(original_line_text[i])\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(original_line_text[i])\n                width_of_original_line += bbox[2] - bbox[0]\n\n        remaining_width = max_render_width - width_of_original_line\n        num_padding_chars = 0\n        padding_string = \"\"\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ' ')\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ' ': # It's an actual glyph from the original line\n                    current_glyph_num_in_line += 1 # Increment for actual glyphs\n                    if current_glyph_num_in_line == highlight_index:\n                        char_color = WHITE # Highlight this glyph\n                    else:\n                        char_color = AMBER # Regular genome glyph color\n                else:\n                    # It's a space from the original line\n                    char_color = AMBER # Spaces are part of the genome line, colored AMBER\n            else:\n                # It's a placeholder padding character\n                char_color = PLACEHOLDER_COLOR\n            \n            # Calculate width of the character to be drawn\n            try:\n                char_width = chosen_genome_font.getlength(char_to_draw)\n            except AttributeError:\n                char_bbox = chosen_genome_font.getbbox(char_to_draw)\n                char_width = char_bbox[2] - char_bbox[0]\n            \n            # Truncation and ellipsis logic\n            if current_x + char_width > max_render_width: # If this char doesn't fit\n                if current_x + ellipsis_width <= max_render_width: # Check if ellipsis fits\n                    ellipsis_fill = PLACEHOLDER_COLOR # Default for ellipsis on padding\n                    if is_original_char: # Truncating original part of the line\n                        ellipsis_fill = AMBER # Default for original part\n                        \n                        # Determine the 0-based index of the first glyph that is part of the truncation\n                        first_truncated_glyph_idx_val = current_glyph_num_in_line\n                        if char_to_draw == ' ': # If the char causing truncation is a space\n                           first_truncated_glyph_idx_val +=1 # then the actual first glyph truncated is the next one\n                        \n                        if highlight_index != -1 and highlight_index >= first_truncated_glyph_idx_val and highlight_index < total_glyphs_in_original:\n                            ellipsis_fill = WHITE # Highlighted glyph is in the truncated part\n                    draw.text((current_x, genome_y_start), ellipsis, font=chosen_genome_font, fill=ellipsis_fill)\n                break # Stop drawing this line (whether ellipsis was drawn or not)\n\n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n            \n        genome_y_start += chosen_genome_line_height\n    \n    # Light box removed\n\n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, POEM, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n    \n    # ID Field - Label at x=10, Value at value_start_x\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    poem_name = entry.get('poem', '---') # Moved from section 3\n    poem_text = f\"POEM: {poem_name[:25]}\" # Truncate poem name for header\n\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE) # Was image_text\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. IMAGE AND LINE CONTENT ROW\n    initial_content_row_y = current_y_offset # Y-coordinate for this entire row\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str_raw, \"[???]\").strip() # Default to [???]\n    line_content = entry.get('content', '---')\n\n    # Part 1: Image field (Glyph at x=10, Value at value_start_x, single line)\n    draw.text((10, initial_content_row_y), image_glyph, font=header_font, fill=WHITE) # Draw glyph\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR) # Draw text\n    image_text_width = text_font.getbbox(image_type_str_raw)[2] # Width of the image type text itself\n\n    # Part 2: LINE field (Label + Value, value can wrap), on the same initial_content_row_y\n    # X position for LINE: is value_start_x + width of image_type_str_raw + padding\n    line_field_x_start = value_start_x + image_text_width + 30 # 30px padding after image text\n    # Ensure LINE field doesn't start too far left if image_type_str is short, or too far right if it's long.\n    # Let's cap its starting position to a range, e.g., between 35% and 50% of card width.\n    min_line_x = int(BASE_WIDTH * 0.35)\n    max_line_x = int(BASE_WIDTH * 0.50)\n    line_field_x_start = max(min_line_x, min(line_field_x_start, max_line_x))\n\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10 # 10px right padding\n    \n    # Draw the actual line_content using draw_colored_text, starting at initial_content_row_y\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str_raw, \"[???]\").strip() # Default to [???]\n    draw.text((10, current_y_offset), syntagma_glyph, font=header_font, fill=WHITE) # Draw glyph\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Cineosis\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str_raw, \"[???]\").strip() # Default to [???]\n    draw.text((10, current_y_offset), cineosis_glyph, font=header_font, fill=WHITE) # Draw glyph\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts, genome_map, output_dir_name):\n    longest_entry_path = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json\" # Corrected path\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(output_dir_name, \"_EXTREME_TEST_CARD_clapper58.png\") # Use passed output_dir_name\n    render_card(longest_entry, output_path, fonts, genome_map) # Pass genome_map\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER58 card generation...\")\n\n    # Load all genome data from JSON into a dictionary for easy lookup\n    all_poems_data = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    genome_map = {poem.get('title', '').strip().lower(): poem for poem in all_poems_data if poem.get('title')}\n    if not genome_map:\n        print(f\"Warning: No genome data loaded or processed from {SYMBOLIC_GENOME_DATA_PATH}. Genome section on cards may be empty.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16), # Default is already non-bold\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False), # Make text_bold also non-bold Menlo\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12), # Already Menlo regular\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)  # Already Menlo regular\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    output_dir_name = \"cards_clapper58_output\"\n    os.makedirs(output_dir_name, exist_ok=True) # Ensure output directory exists\n\n    # render_demo_card(fonts, genome_map, output_dir_name) # Would also need output_dir_name if uncommented\n    render_extreme_test_case(fonts, genome_map, output_dir_name)\n\n    if not processed_timeline_data:\n        print(\"No timeline data to process for random poem entries.\")\n        return\n\n    poems_to_entries = {}\n    for entry in processed_timeline_data:\n        poem_title = entry.get('poem')\n        if poem_title:\n            if poem_title not in poems_to_entries:\n                poems_to_entries[poem_title] = []\n            poems_to_entries[poem_title].append(entry)\n\n    if not poems_to_entries:\n        print(\"No poems found in timeline data to select random entries from.\")\n        return\n\n    print(f\"\\nRendering one random entry for each of the {len(poems_to_entries)} poems found...\")\n    count = 0\n    for poem_title, entries_for_poem in poems_to_entries.items():\n        if entries_for_poem:\n            selected_entry = random.choice(entries_for_poem)\n            entry_id = selected_entry.get('id', f'random_poem_entry_{count+1}')\n            output_filename = f\"{entry_id}_clapper58.png\"\n            output_path = os.path.join(output_dir_name, output_filename)\n            \n            print(f\"Rendering card for random entry '{entry_id}' from poem '{poem_title}'...\")\n            render_card(selected_entry, output_path, fonts, genome_map)\n            print(f\"  -> Saved to {output_path}\")\n            count += 1\n        else:\n            print(f\"No entries found for poem '{poem_title}' to select a random one.\")\n\n    print(f\"\\nFinished rendering {count} random cards per poem.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "symbolic_genome_data.json",
      "_DEMO_CARD_genome_exp.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json",
      "_EXTREME_TEST_CARD_clapper58.png",
      "{entry_id}_clapper58.png",
      "\nCLAPPER58 - Restore two-column layout for prompt glyphs/text. Robust glyph fallback. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/\n",
      "\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230) # Pale Blue / Light Cyan-ish\n\n# Glyph Definitions - ASCII Standardized\nSYNTAGMA_GLYPHS = {\n    ",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    # Vertically center text within its allocated GENOME_REPORT_HEIGHT area, which starts at genome_section_y_on_img\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    # Ensure it doesn",
      "\n\n        if remaining_width > 0:\n            try:\n                placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR)\n            except AttributeError:\n                bbox = chosen_genome_font.getbbox(PLACEHOLDER_CHAR)\n                placeholder_char_width = bbox[2] - bbox[0]\n            \n            if placeholder_char_width > 0:\n                num_padding_chars = int(remaining_width // placeholder_char_width)\n                if num_padding_chars > 0:\n                    padding_string = PLACEHOLDER_CHAR * num_padding_chars\n        \n        display_line_text = original_line_text + padding_string\n\n        current_glyph_num_in_line = -1 # Initialize glyph counter for the current S/I/C line\n        # Pre-calculate total glyphs in the original line to help with ellipsis logic\n        total_glyphs_in_original = sum(1 for gc_in_orig in original_line_text if gc_in_orig != ",
      ")\n\n        for char_idx, char_to_draw in enumerate(display_line_text):\n            is_original_char = char_idx < original_length # original_length is len of S/I/C string from JSON (includes spaces)\n\n            # Determine char_color based on highlighting logic\n            if is_original_char:\n                if char_to_draw != ",
      "FRAME: ({frame_position}/{frame_total})",
      ":\n        # If line content is empty, current_y_offset is one line below initial_content_row_y\n        current_y_offset = initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below initial_content_row_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Image/Line and before Prompt - starts at value_start_x\n    current_y_offset += 4 # Padding before the line\n    divider_start_x = value_start_x - 4 # Start just left of the value column\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    # Labels at x=10, Values at value_start_x\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10 # Max width for the value text, with 10px right padding\n\n    # Syntagma\n    syntagma_type_str_raw = entry.get(",
      ").strip() # Default to [???]\n    draw.text((10, current_y_offset), cineosis_glyph, font=header_font, fill=WHITE) # Draw glyph\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Ekphrasis (No label/glyph at x=10, content starts at value_start_x)\n    ekphrasis_str = entry.get(",
      ")\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font, prompt_value_max_width, text_font)\n\n    # Style (No label/glyph at x=10, content starts at value_start_x)\n    style_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER58 - Restore two-column layout for prompt glyphs/text. Robust glyph fallback. (Menlo Regular)\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER_GENOME_EXP, processes COMPLETE-TIMELINE.json.\n- Outputs to TRUNK/cards_clapper_menlo_genome/"
  },
  {
    "path": "ELEPHANT/LEG/scripts/leg_generator_v1.py",
    "size": 27302,
    "lines": 536,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER61PRIMEPLUS - Processes UPDATED-TIMELINE.json for 'Magic ride' poem only.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER61PRIME.\n- Outputs cards for 'Magic ride' to ELEPHANT/TUSK/\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\nimport unicodedata # Added for slugify\n\n# --- slugify function ---\ndef slugify(value, allow_unicode=False):\n    \"\"\"\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n    dashes to single dashes. Remove characters that aren't alphanumerics,\n    underscores, or hyphens. Convert to lowercase. Also strip leading and\n    trailing whitespace, dashes, and underscores.\n    \"\"\"\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n\n# --- get_flexible_glyph function ---\ndef get_flexible_glyph(data_string, glyph_dict, default_glyph=\"?\"):\n    \"\"\"Gets a glyph from a dictionary, trying exact match first, then flexible match (ignoring suffix in parentheses).\"\"\"\n    if not data_string:\n        return default_glyph\n\n    # 1. Try exact match first\n    glyph = glyph_dict.get(data_string)\n    if glyph:\n        return glyph.strip()\n\n    # 2. Try flexible match (strip suffix and whitespace)\n    #    e.g., \"Autonomous Syntagma\" from data vs \"Autonomous Syntagma (AS)\" in dict\n    data_string_base = re.sub(r'\\s*\\(.*\\)$', '', data_string).strip()\n    \n    for key, val in glyph_dict.items():\n        key_base = re.sub(r'\\s*\\(.*\\)$', '', key).strip()\n        if key_base.lower() == data_string_base.lower():\n            return val.strip()\n            \n    return default_glyph\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60\nHEADER_HEIGHT = 200\nIMAGE_DISPLAY_HEIGHT = 576\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230)\n\n# Glyph Definitions\nSYNTAGMA_GLYPHS = {\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (CS)\": \"\u2756\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(BASE_DIR))), \"HONEYBADGER\", \"UPDATED-TIMELINE.json\")\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(BASE_DIR), \"symbolic_genome_data.json\") # This assumes symbolic_genome_data.json is in ELEPHANT/LEG/\nOUTPUT_DIR = os.path.join(os.path.dirname(BASE_DIR), \"output_media\")\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(BASE_DIR))) # Adjust if TIGER is relative to 'resurrecting atlantis'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n    try:\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                try:\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    relative_path = entry.get('image_path')\n    if not relative_path:\n        print(f\"Warning: 'image_path' missing in entry: {entry.get('id', 'Unknown ID')}\")\n        return None\n\n    if relative_path.startswith(\"MR/\"):\n        adjusted_relative_path = os.path.join(\"TIGER\", relative_path)\n    else:\n        adjusted_relative_path = relative_path\n\n    full_image_path = os.path.join(BASE_IMAGE_DIR, adjusted_relative_path)\n    \n    if os.path.exists(full_image_path):\n        return os.path.normpath(full_image_path)\n    else:\n        # Fallback if TIGER/MR path not found, try original MR/ path directly under BASE_IMAGE_DIR\n        if relative_path.startswith(\"MR/\"):\n            original_mr_path = os.path.join(BASE_IMAGE_DIR, relative_path)\n            if os.path.exists(original_mr_path):\n                return os.path.normpath(original_mr_path)\n        \n        # Broader fallback search using entry ID (adapted from original clapper61prime)\n        entry_id_val = entry.get('id', '')\n        if entry_id_val:\n            search_dirs = [BASE_IMAGE_DIR] # General search in base image dir\n            if relative_path.startswith(\"MR/\"):\n                 # If it was an MR path, also specifically check TIGER/MR and TIGER folders\n                search_dirs.insert(0, os.path.join(BASE_IMAGE_DIR, 'TIGER', 'MR'))\n                search_dirs.insert(1, os.path.join(BASE_IMAGE_DIR, 'TIGER'))\n\n            for search_dir_base in search_dirs:\n                if os.path.exists(search_dir_base):\n                    for root, _, files in os.walk(search_dir_base):\n                        for file_name in files:\n                            if file_name.startswith(entry_id_val + '__') and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                                return os.path.normpath(os.path.join(root, file_name))\n                            # Check for exact filename match from image_path as well\n                            if os.path.basename(relative_path) == file_name:\n                                return os.path.normpath(os.path.join(root, file_name))\n        \n        print(f\"Error: Image not found for entry {entry.get('id', 'Unknown ID')}. Path attempted: {full_image_path}\")\n        return None\n\ndef load_genome_data_from_json(file_path):\n    genome_map_dict = {}\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems_data_list = json.load(f)\n        if not isinstance(poems_data_list, list):\n            print(f\"Error: Genome data in {file_path} is not a list. Found {type(poems_data_list)}.\")\n            return {}\n        for item in poems_data_list:\n            if isinstance(item, dict) and 'title' in item:\n                poem_name = item['title'].strip().lower()\n                genome_map_dict[poem_name] = {\n                    's_line': item.get('s_line', ''),\n                    'i_line': item.get('i_line', ''),\n                    'c_line': item.get('c_line', '')\n                }\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}.\")\n    return genome_map_dict\n\ndef get_non_space_char_index(text, original_char_pos):\n    count = -1\n    if original_char_pos >= len(text):\n        return -1\n    for i in range(original_char_pos + 1):\n        if text[i] != ' ':\n            count += 1\n    return count\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10\n    value_start_x = 10 + narrow_label_area_width\n    current_y_offset = 4\n    genome_section_y_on_img = current_y_offset\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70)\n    genome_content_x1_on_img = 10\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n    i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A') if genome_data_entry else 'I_LINE_N/A'\n    s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A') if genome_data_entry else 'S_LINE_N/A'\n    c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A') if genome_data_entry else 'C_LINE_N/A'\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n    frame_position = entry.get('frameNumber', 0)\n    highlight_index = int(frame_position) - 1 if isinstance(frame_position, (int, float)) and frame_position > 0 else -1\n    \n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    if len(genome_lines_to_render) * chosen_genome_line_height > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        if len(genome_lines_to_render) * chosen_genome_line_height > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    if genome_y_start < genome_section_y_on_img + 4: genome_y_start = genome_section_y_on_img + 4\n\n    max_render_width = BASE_WIDTH - 10 # Allow a small margin on the right\n    avg_char_width = chosen_genome_font.getlength(\"X\") if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(\"X\")[2] - chosen_genome_font.getbbox(\"X\")[0]\n    if avg_char_width <= 0: avg_char_width = 1\n    placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR) if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[2] - chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[0]\n    if placeholder_char_width <= 0: placeholder_char_width = 1\n\n    for line_idx, original_line_text in enumerate(genome_lines_to_render):\n        current_x = genome_content_x1_on_img\n        chars_that_fit_total_approx = int(max_render_width // avg_char_width)\n        if chars_that_fit_total_approx <= 0: chars_that_fit_total_approx = 1 # Avoid division by zero or negative\n        target_highlight_offset_in_view = chars_that_fit_total_approx // 4\n\n        display_start_index = 0\n        scrolled_from_left = False\n\n        if highlight_index >= 0: # Ensure we have a valid highlight_index\n            if len(original_line_text) <= chars_that_fit_total_approx:\n                # Line fits entirely, no scrolling needed\n                display_start_index = 0\n                scrolled_from_left = False\n            else:\n                # Line is longer than available space, scrolling is needed\n                scrolled_from_left = True\n                # Default start: try to position highlight_index at target_highlight_offset_in_view\n                tentative_display_start_index = max(0, highlight_index - target_highlight_offset_in_view)\n                \n                # Adjust if scrolling near the end of the line\n                # Ensure the window doesn't go past the end of the string\n                if tentative_display_start_index + chars_that_fit_total_approx > len(original_line_text):\n                    display_start_index = max(0, len(original_line_text) - chars_that_fit_total_approx)\n                else:\n                    display_start_index = tentative_display_start_index\n        else: # No valid highlight_index, just show from the beginning\n            display_start_index = 0\n            scrolled_from_left = False\n            \n        # Prefix display has been removed to maximize space for genome characters.\n        # The scrolled_from_left flag is still determined above to correctly set display_start_index.\n        for idx_in_original in range(display_start_index, len(original_line_text)):\n            char_to_draw = original_line_text[idx_in_original]\n            char_width = chosen_genome_font.getlength(char_to_draw) if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(char_to_draw)[2] - chosen_genome_font.getbbox(char_to_draw)[0]\n            if char_width <=0: char_width = avg_char_width # Fallback if char_width is bad\n            \n            # Check if the current character fits\n            if current_x + char_width > max_render_width:\n                break # Stop drawing this line if the character doesn't fit\n\n            # Determine default color based on line index\n            if line_idx == 0: # I-line (corresponds to Image Type)\n                default_char_color = IMAGE_TYPE_COLOR\n            elif line_idx == 1: # S-line (corresponds to Syntagma Type)\n                default_char_color = SYNTAGMA_COLOR\n            elif line_idx == 2: # C-line (corresponds to Cineosis Function)\n                default_char_color = CINEOSIS_COLOR\n            else: # Fallback, though should not happen with 3 lines\n                default_char_color = LIGHT_GRAY # Fallback to a neutral color\n\n            char_color = default_char_color\n            \n            # Highlighted character is white, overriding line color\n            if char_to_draw != ' ': # Only check for highlight if not a space\n                current_char_overall_non_space_idx = get_non_space_char_index(original_line_text, idx_in_original)\n                if current_char_overall_non_space_idx == highlight_index:\n                    char_color = AMBER\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n        drawn_width = current_x - genome_content_x1_on_img\n        remaining_line_width = max_render_width - drawn_width\n        if remaining_line_width > 0 and placeholder_char_width > 0:\n            num_padding_chars = int(remaining_line_width // placeholder_char_width)\n            if num_padding_chars > 0:\n                draw.text((current_x, genome_y_start), PLACEHOLDER_CHAR * num_padding_chars, font=chosen_genome_font, fill=PLACEHOLDER_COLOR)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 4\n    header_content_y = current_y_offset + 4\n    top_row_internal_height = 30\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position_val = entry.get('frameNumber', 0)\n    frame_total_val = entry.get('totalFrames', 0)\n    frame_text = f\"FRAME: ({frame_position_val}/{frame_total_val})\"\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name[:25]}\"\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    current_y_offset += top_row_internal_height\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8\n    initial_content_row_y = current_y_offset\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = get_flexible_glyph(image_type_str_raw, IMAGE_TYPE_GLYPHS)\n    line_content = entry.get('content', '---')\n    glyph_column_x = 10\n    draw.text((glyph_column_x, initial_content_row_y), image_glyph, font=chosen_genome_font, fill=AMBER)\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_text_width_bbox = text_font.getbbox(image_type_str_raw)\n    image_text_width = image_text_width_bbox[2] - image_text_width_bbox[0]\n    line_field_x_start = max(int(BASE_WIDTH * 0.35), min(value_start_x + image_text_width + 30, int(BASE_WIDTH * 0.50)))\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width_bbox = header_font.getbbox(line_prefix)\n    line_prefix_width = line_prefix_width_bbox[2] - line_prefix_width_bbox[0]\n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    ONE_TEXT_LINE_HEIGHT = (text_font.getbbox(\"Ay\")[3] - text_font.getbbox(\"Ay\")[1] + 4) if hasattr(text_font, 'getbbox') else 16\n    current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT) if line_content.strip() and line_content.strip() != '---' else initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    current_y_offset += 4\n    divider_start_x = glyph_column_x - 4 if glyph_column_x > 4 else 6\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = get_flexible_glyph(syntagma_type_str_raw, SYNTAGMA_GLYPHS)\n    draw.text((glyph_column_x, current_y_offset), syntagma_glyph, font=chosen_genome_font, fill=AMBER)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = get_flexible_glyph(cineosis_func_str_raw, CINEOSIS_FUNCTION_GLYPHS)\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=chosen_genome_font, fill=AMBER)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n    img.save(output_path)\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name:\n            poem_frames[poem_name] = poem_frames.get(poem_name, 0) + 1\n    processed_data = []\n    current_poem_entry_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        new_entry = entry.copy()\n        if poem_name:\n            current_poem_entry_count[poem_name] = current_poem_entry_count.get(poem_name, 0) + 1\n            new_entry['frameNumber'] = entry.get('frameNumber', current_poem_entry_count[poem_name]) # Use existing or assign\n            new_entry['totalFrames'] = entry.get('totalFrames', poem_frames.get(poem_name, 0))\n        else:\n            new_entry['frameNumber'] = entry.get('frameNumber', 0)\n            new_entry['totalFrames'] = entry.get('totalFrames', 0)\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER61PRIMEPLUS for 'Magic ride' poem...\")\n    print(f\"Output directory: {OUTPUT_DIR}\")\n    genome_map = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    if not genome_map:\n        print(f\"Warning: No genome data loaded from {SYMBOLIC_GENOME_DATA_PATH}.\")\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16),\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12),\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data_raw = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    # Ensure frameNumber and totalFrames are consistently populated\n    processed_timeline_data = calculate_frame_counts(timeline_data_raw)\n\n    poem_to_process = \"Magic ride\"\n    magic_ride_entries = []\n    for entry in processed_timeline_data:\n        if entry.get('poem', '').strip().lower() == poem_to_process.lower():\n            magic_ride_entries.append(entry)\n    \n    # Sort entries by original frameNumber if available, or by order in JSON as fallback\n    # Assuming 'frameNumber' from JSON is the definitive one for sorting if it exists and is numeric\n    def get_sort_key(e):\n        fn = e.get('frameNumber')\n        if isinstance(fn, (int, float)):\n            return fn\n        if isinstance(fn, str) and fn.isdigit():\n            return int(fn)\n        return float('inf') # Put entries with bad/missing frameNumber last\n\n    magic_ride_entries.sort(key=get_sort_key)\n\n    if not magic_ride_entries:\n        print(f\"No entries found for poem: {poem_to_process}\")\n        return\n\n    print(f\"Found {len(magic_ride_entries)} entries for poem '{poem_to_process}'. Rendering cards...\")\n    \n    cards_rendered_count = 0\n    for i, entry_data in enumerate(magic_ride_entries):\n        entry_id = entry_data.get('id', f'magic-ride_unknown-id_{i+1}')\n        frame_num_str = str(entry_data.get('frameNumber', i + 1)).zfill(3)\n        poem_slug = slugify(poem_to_process)\n        output_filename = f\"{poem_slug}_frame_{frame_num_str}.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        \n        print(f\"Rendering card for: {entry_id} (Frame: {frame_num_str}) -> {output_filename}\")\n        try:\n            render_card(entry_data, output_path, fonts, genome_map)\n            cards_rendered_count += 1\n        except Exception as e:\n            print(f\"  ERROR rendering card for {entry_id}: {e}\")\n\n    print(f\"\\nFinished rendering. Total cards for '{poem_to_process}': {cards_rendered_count}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "UPDATED-TIMELINE.json",
      "symbolic_genome_data.json",
      "{poem_slug}_frame_{frame_num_str}.png",
      " to ELEPHANT/TUSK/\n",
      "\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if ",
      ") # This assumes symbolic_genome_data.json is in ELEPHANT/LEG/\nOUTPUT_DIR = os.path.join(os.path.dirname(BASE_DIR), ",
      ", relative_path)\n    else:\n        adjusted_relative_path = relative_path\n\n    full_image_path = os.path.join(BASE_IMAGE_DIR, adjusted_relative_path)\n    \n    if os.path.exists(full_image_path):\n        return os.path.normpath(full_image_path)\n    else:\n        # Fallback if TIGER/MR path not found, try original MR/ path directly under BASE_IMAGE_DIR\n        if relative_path.startswith(",
      "):\n                 # If it was an MR path, also specifically check TIGER/MR and TIGER folders\n                search_dirs.insert(0, os.path.join(BASE_IMAGE_DIR, ",
      "I_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    if genome_y_start < genome_section_y_on_img + 4: genome_y_start = genome_section_y_on_img + 4\n\n    max_render_width = BASE_WIDTH - 10 # Allow a small margin on the right\n    avg_char_width = chosen_genome_font.getlength(",
      ") else chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[2] - chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[0]\n    if placeholder_char_width <= 0: placeholder_char_width = 1\n\n    for line_idx, original_line_text in enumerate(genome_lines_to_render):\n        current_x = genome_content_x1_on_img\n        chars_that_fit_total_approx = int(max_render_width // avg_char_width)\n        if chars_that_fit_total_approx <= 0: chars_that_fit_total_approx = 1 # Avoid division by zero or negative\n        target_highlight_offset_in_view = chars_that_fit_total_approx // 4\n\n        display_start_index = 0\n        scrolled_from_left = False\n\n        if highlight_index >= 0: # Ensure we have a valid highlight_index\n            if len(original_line_text) <= chars_that_fit_total_approx:\n                # Line fits entirely, no scrolling needed\n                display_start_index = 0\n                scrolled_from_left = False\n            else:\n                # Line is longer than available space, scrolling is needed\n                scrolled_from_left = True\n                # Default start: try to position highlight_index at target_highlight_offset_in_view\n                tentative_display_start_index = max(0, highlight_index - target_highlight_offset_in_view)\n                \n                # Adjust if scrolling near the end of the line\n                # Ensure the window doesn",
      ": # Only check for highlight if not a space\n                current_char_overall_non_space_idx = get_non_space_char_index(original_line_text, idx_in_original)\n                if current_char_overall_non_space_idx == highlight_index:\n                    char_color = AMBER\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n        drawn_width = current_x - genome_content_x1_on_img\n        remaining_line_width = max_render_width - drawn_width\n        if remaining_line_width > 0 and placeholder_char_width > 0:\n            num_padding_chars = int(remaining_line_width // placeholder_char_width)\n            if num_padding_chars > 0:\n                draw.text((current_x, genome_y_start), PLACEHOLDER_CHAR * num_padding_chars, font=chosen_genome_font, fill=PLACEHOLDER_COLOR)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 4\n    header_content_y = current_y_offset + 4\n    top_row_internal_height = 30\n    id_label_text = ",
      "FRAME: ({frame_position_val}/{frame_total_val})",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ") # Put entries with bad/missing frameNumber last\n\n    magic_ride_entries.sort(key=get_sort_key)\n\n    if not magic_ride_entries:\n        print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random",
      "unicodedata"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER61PRIMEPLUS - Processes UPDATED-TIMELINE.json for 'Magic ride' poem only.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Based on CLAPPER61PRIME.\n- Outputs cards for 'Magic ride' to ELEPHANT/TUSK/"
  },
  {
    "path": "ELEPHANT/LEG/scripts/leg_assembler_v1.py",
    "size": 3703,
    "lines": 85,
    "source": "#!/usr/bin/env python3\n\"\"\"\nLEG_ASSEMBLER_V1 - Creates a video from image cards for the LEG workflow.\n- Specifically designed to assemble images generated by leg_generator_v1.py for the 'Magic ride' poem.\n- Reads images from ELEPHANT/LEG/output_media/\n- Outputs a video file (e.g., leg_magic_ride_v1.mp4) to ELEPHANT/LEG/output_media/\n- Target video duration is 2 minutes 11 seconds (131 seconds).\n\"\"\"\n\nimport os\nimport glob\nfrom moviepy.editor import ImageSequenceClip\n\n# --- Configuration ---\n# Assumes this script is in ELEPHANT/TRUNK/\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nELEPHANT_DIR = os.path.dirname(SCRIPT_DIR)\n\nIMAGE_DIR_NAME = \"output_media\"\nIMAGE_SUBDIR = os.path.join(ELEPHANT_DIR, IMAGE_DIR_NAME)\n\nOUTPUT_VIDEO_FILENAME = \"leg_magic_ride_v1.mp4\"\nOUTPUT_VIDEO_PATH = os.path.join(IMAGE_SUBDIR, OUTPUT_VIDEO_FILENAME)\n\nTARGET_TOTAL_DURATION_SECONDS = 131  # 2 minutes 11 seconds\nIMAGE_FILE_PATTERN = \"magic-ride_frame_*.png\"\n\n# --- Main script ---\ndef main():\n    print(f\"Starting video assembly...\")\n    print(f\"Image source directory: {IMAGE_SUBDIR}\")\n    print(f\"Output video path: {OUTPUT_VIDEO_PATH}\")\n    print(f\"Target total duration: {TARGET_TOTAL_DURATION_SECONDS} seconds\")\n\n    image_search_pattern = os.path.join(IMAGE_SUBDIR, IMAGE_FILE_PATTERN)\n    image_files = sorted(glob.glob(image_search_pattern))\n\n    if not image_files:\n        print(f\"Error: No image files found matching pattern '{IMAGE_FILE_PATTERN}' in '{IMAGE_SUBDIR}'.\")\n        print(\"Please ensure 61primeplus.py has run successfully and generated images.\")\n        return\n\n    num_images = len(image_files)\n    print(f\"Found {num_images} image frames.\")\n\n    if num_images == 0:\n        print(\"Error: No images to process. Exiting.\")\n        return\n\n    # Calculate FPS required to achieve the target total duration with the given number of images.\n    # Each frame will be displayed for (TARGET_TOTAL_DURATION_SECONDS / num_images) seconds.\n    # FPS = 1 / duration_per_frame = num_images / TARGET_TOTAL_DURATION_SECONDS.\n    fps = num_images / TARGET_TOTAL_DURATION_SECONDS\n\n    print(f\"Calculated FPS: {fps:.4f} (for a total duration of {TARGET_TOTAL_DURATION_SECONDS}s with {num_images} frames)\")\n    \n    try:\n        print(\"Creating video clip...\")\n        # Create the video clip using the image files and calculated fps\n        clip = ImageSequenceClip(image_files, fps=fps)\n\n        print(f\"Writing video file to {OUTPUT_VIDEO_PATH}...\")\n        # Write the video file to disk\n        # Common codecs: 'libx264' (for H.264, good quality/compression), 'mpeg4'\n        # preset options: ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow\n        # threads for parallel processing (can speed up encoding)\n        clip.write_videofile(OUTPUT_VIDEO_PATH, \n                             codec='libx264', \n                             fps=fps, \n                             preset='medium', \n                             threads=4, \n                             logger='bar') # Use 'bar' for progress bar, or None for less verbose output\n        \n        print(f\"\\nVideo '{OUTPUT_VIDEO_FILENAME}' created successfully in '{IMAGE_SUBDIR}'.\")\n        # moviepy might slightly adjust duration due to fps quantization, check actual duration.\n        print(f\"Actual video duration reported by moviepy: {clip.duration:.2f} seconds.\")\n\n    except Exception as e:\n        print(f\"An error occurred during video creation: {e}\")\n        print(\"Please ensure 'moviepy' is installed (e.g., 'pip install moviepy') and that FFmpeg is available on your system.\")\n        print(\"FFmpeg is a dependency for moviepy to read/write video files.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "leg_magic_ride_v1.mp4",
      "magic-ride_frame_*.png",
      " poem.\n- Reads images from ELEPHANT/LEG/output_media/\n- Outputs a video file (e.g., leg_magic_ride_v1.mp4) to ELEPHANT/LEG/output_media/\n- Target video duration is 2 minutes 11 seconds (131 seconds).\n",
      "\n\nimport os\nimport glob\nfrom moviepy.editor import ImageSequenceClip\n\n# --- Configuration ---\n# Assumes this script is in ELEPHANT/TRUNK/\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nELEPHANT_DIR = os.path.dirname(SCRIPT_DIR)\n\nIMAGE_DIR_NAME = ",
      ")\n        return\n\n    # Calculate FPS required to achieve the target total duration with the given number of images.\n    # Each frame will be displayed for (TARGET_TOTAL_DURATION_SECONDS / num_images) seconds.\n    # FPS = 1 / duration_per_frame = num_images / TARGET_TOTAL_DURATION_SECONDS.\n    fps = num_images / TARGET_TOTAL_DURATION_SECONDS\n\n    print(f",
      " (for H.264, good quality/compression), ",
      "FFmpeg is a dependency for moviepy to read/write video files."
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "glob",
      "moviepy.editor"
    ],
    "generates": [],
    "reads": [],
    "docstring": "LEG_ASSEMBLER_V1 - Creates a video from image cards for the LEG workflow.\n- Specifically designed to assemble images generated by leg_generator_v1.py for the 'Magic ride' poem.\n- Reads images from ELEPHANT/LEG/output_media/\n- Outputs a video file (e.g., leg_magic_ride_v1.mp4) to ELEPHANT/LEG/output_media/\n- Target video duration is 2 minutes 11 seconds (131 seconds)."
  },
  {
    "path": "HONEYBADGER/HIVE/render_elegant_cards_genome_experiment.py",
    "size": 8972,
    "lines": 246,
    "source": "#!/usr/bin/env python3\n\"\"\"\nRender Genome Data Cards - Elegant Spatial Design\n\nThis script renders genome data for poems, displaying title and S, I, C lines.\nand proper visual hierarchy, inspired by the provided examples.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Base dimensions and aspect ratios\nBASE_WIDTH = 1200\nASPECT_16_9 = 16/9\nASPECT_3_2 = 3/2\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (30, 30, 30)\nBLUE_BORDER = (0, 0, 255)\nGOLD = (212, 175, 55)\nCYAN = (0, 183, 235)\nSOFT_GREEN = (144, 238, 144)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nGENOME_DATA_PATH = os.path.join(SCRIPT_DIR, 'symbolic_genome_data.json') # Genome data is in HIVE (SCRIPT_DIR)\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_genome_experiment_output')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\" \n\n# Define glyphs for different types\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_genome_data(genome_filepath):\n    \"\"\"Load genome data from JSON file.\"\"\"\n    try:\n        with open(genome_filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading genome data: {e}\")\n        return []\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef draw_horizontal_divider(draw, y_position, width, color=WHITE):\n    \"\"\"Draw a horizontal divider line with proper padding.\"\"\"\n    draw.line([(0, y_position), (width, y_position)], fill=color, width=1)\n\ndef render_card(poem_data, output_path):\n    \"\"\"Render a genome data card with elegant spatial design.\"\"\"\n    # Create base image with blue border\n    img = Image.new('RGB', (BASE_WIDTH+8, TOTAL_HEIGHT+8), BLUE_BORDER)\n    canvas = Image.new('RGB', (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    id_font = find_font(HEADER_FONT_PATH, 22)\n    header_font = find_font(HEADER_FONT_PATH, 20)\n    label_font = find_font(HEADER_FONT_PATH, 18)\n    body_font = find_font(BODY_FONT_PATH, 18)\n    small_font = find_font(BODY_FONT_PATH, 16)\n    \n    # Fill header area black, image area light gray\n    draw.rectangle([(0, 0), (BASE_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Draw dividing line between header and image\n    draw.line([(0, HEADER_HEIGHT-1), (BASE_WIDTH, HEADER_HEIGHT-1)], fill=WHITE, width=2)\n    \n    # Placeholder text for image area\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n              \"16:9 Image Area\", font=header_font, fill=BLACK, anchor=\"mm\")\n    \n    # Get poem data\n    poem_title = poem_data.get('title', 'Untitled Poem')\n    s_line = poem_data.get('s_line', '')\n    i_line = poem_data.get('i_line', '')\n    c_line = poem_data.get('c_line', '')\n    timeline_entries_count = poem_data.get('timeline_entries_count', 0)\n\n    padding = 20\n    current_y = padding\n    line_height_large = 30\n    line_height_medium = 25\n    line_height_small = 22\n    label_width = 40 # For S:, I:, C:\n\n    # ---- ROW 1: Poem Title ----\n    title_font = find_font(HEADER_FONT_PATH, 28) # Larger font for title\n    # Truncate title if too long, or wrap\n    max_title_len = 70\n    display_title = poem_title if len(poem_title) <= max_title_len else poem_title[:max_title_len-3] + \"...\"\n    draw.text((padding, current_y), display_title, font=title_font, fill=GOLD)\n    current_y += line_height_large + 10\n    draw_horizontal_divider(draw, current_y, BASE_WIDTH)\n    current_y += 10\n\n    # ---- ROW 2: Timeline Entries Count ----\n    count_text = f\"Timeline Segments: {timeline_entries_count}\"\n    draw.text((padding, current_y), count_text, font=header_font, fill=WHITE)\n    current_y += line_height_medium + 5\n    draw_horizontal_divider(draw, current_y, BASE_WIDTH)\n    current_y += 10\n\n    # ---- ROW 3: S-Line (Syntagma) ----\n    genome_label_font = find_font(HEADER_FONT_PATH, 18)\n    genome_font = find_font(BODY_FONT_PATH, 18) # Courier New for genome lines\n\n    draw.text((padding, current_y), \"S:\", font=genome_label_font, fill=CYAN)\n    draw.text((padding + label_width, current_y), s_line, font=genome_font, fill=WHITE)\n    current_y += line_height_small + 5\n    # No divider after S, I, C lines to make them a block\n\n    # ---- ROW 4: I-Line (Image) ----\n    draw.text((padding, current_y), \"I:\", font=genome_label_font, fill=CYAN)\n    draw.text((padding + label_width, current_y), i_line, font=genome_font, fill=WHITE)\n    current_y += line_height_small + 5\n\n    # ---- ROW 5: C-Line (Cineosis) ----\n    draw.text((padding, current_y), \"C:\", font=genome_label_font, fill=CYAN)\n    draw.text((padding + label_width, current_y), c_line, font=genome_font, fill=WHITE)\n    current_y += line_height_small + 10\n    \n    # Final divider if space allows and if needed for visual separation before image area\n    if current_y < HEADER_HEIGHT - padding:\n        draw_horizontal_divider(draw, current_y, BASE_WIDTH)\n\n    \n    # Paste canvas onto bordered image\n    img.paste(canvas, (4, 4))\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_genome_cards(genome_data_list, output_dir, limit=None):\n    \"\"\"Render genome data for each poem to a card.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    poems_to_render = genome_data_list[:limit] if limit else genome_data_list\n    \n    for i, poem_data in enumerate(poems_to_render):\n        poem_title = poem_data.get('title', f'untitled_poem_{i}')\n        # Sanitize title for filename\n        safe_title = \"\".join(c if c.isalnum() or c in (' ', '_') else '_' for c in poem_title).rstrip()\n        safe_title = safe_title.replace(' ', '_')\n        if not safe_title: safe_title = f'untitled_poem_{i}'\n        output_filename = f\"{safe_title}_genome_card.png\"\n        output_path = os.path.join(output_dir, output_filename)\n        render_card(poem_data, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(poems_to_render)} genome cards\")\n\ndef main():\n    \"\"\"Main function to render genome data cards.\"\"\"\n    print(f\"Loading genome data from: {GENOME_DATA_PATH}\")\n    try:\n        genome_data = load_genome_data(GENOME_DATA_PATH)\n        \n        if not genome_data:\n            print(\"No genome data found. Exiting.\")\n            return\n        \n        print(f\"Found {len(genome_data)} poems in genome data\")\n        \n        print(f\"Rendering all genome cards to: {OUTPUT_DIR}\")\n        # Render all genome cards (or a limit for testing, e.g., 5)\n        render_all_genome_cards(genome_data, OUTPUT_DIR, limit=None) # Set limit=5 for a quick test\n        \n        print(f\"Completed rendering genome cards to: {OUTPUT_DIR}\")\n        print(f\"Check the output directory: {OUTPUT_DIR}\")\n    except Exception as e:\n        print(f\"ERROR: An exception occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "symbolic_genome_data.json",
      "{safe_title}_genome_card.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Base dimensions and aspect ratios\nBASE_WIDTH = 1200\nASPECT_16_9 = 16/9\nASPECT_3_2 = 3/2\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (30, 30, 30)\nBLUE_BORDER = (0, 0, 255)\nGOLD = (212, 175, 55)\nCYAN = (0, 183, 235)\nSOFT_GREEN = (144, 238, 144)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nGENOME_DATA_PATH = os.path.join(SCRIPT_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ", (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    id_font = find_font(HEADER_FONT_PATH, 22)\n    header_font = find_font(HEADER_FONT_PATH, 20)\n    label_font = find_font(HEADER_FONT_PATH, 18)\n    body_font = find_font(BODY_FONT_PATH, 18)\n    small_font = find_font(BODY_FONT_PATH, 16)\n    \n    # Fill header area black, image area light gray\n    draw.rectangle([(0, 0), (BASE_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Draw dividing line between header and image\n    draw.line([(0, HEADER_HEIGHT-1), (BASE_WIDTH, HEADER_HEIGHT-1)], fill=WHITE, width=2)\n    \n    # Placeholder text for image area\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n              ",
      "Rendered {i+1}/{len(poems_to_render)} genome cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Render Genome Data Cards - Elegant Spatial Design\n\nThis script renders genome data for poems, displaying title and S, I, C lines.\nand proper visual hierarchy, inspired by the provided examples."
  },
  {
    "path": "HONEYBADGER/HIVE/find_longest_entries.py",
    "size": 5295,
    "lines": 135,
    "source": "#!/usr/bin/env python3\n\"\"\"\nFind the longest entries in the timeline data for testing layout boundaries.\n\"\"\"\n\nimport json\nimport os\nimport sys\n\n# Path to timeline\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef analyze_timeline(timeline):\n    \"\"\"Find the longest entries for each field.\"\"\"\n    longest = {\n        'poem': {'length': 0, 'entry': None},\n        'content': {'length': 0, 'entry': None},\n        'operativeEkphrasis': {'length': 0, 'entry': None},\n        'styleConditioning': {'length': 0, 'entry': None},\n        'fullPrompt': {'length': 0, 'entry': None},\n    }\n    \n    for entry in timeline:\n        # Check poem name\n        poem = entry.get('poem', '')\n        if len(poem) > longest['poem']['length']:\n            longest['poem']['length'] = len(poem)\n            longest['poem']['entry'] = entry\n        \n        # Check line content\n        content = entry.get('content', '')\n        if len(content) > longest['content']['length']:\n            longest['content']['length'] = len(content)\n            longest['content']['entry'] = entry\n            \n        # Check operative ekphrasis\n        ekphrasis = entry.get('operativeEkphrasis', '')\n        if len(ekphrasis) > longest['operativeEkphrasis']['length']:\n            longest['operativeEkphrasis']['length'] = len(ekphrasis)\n            longest['operativeEkphrasis']['entry'] = entry\n            \n        # Check style conditioning\n        style = entry.get('styleConditioning', '')\n        if len(style) > longest['styleConditioning']['length']:\n            longest['styleConditioning']['length'] = len(style)\n            longest['styleConditioning']['entry'] = entry\n        \n        # Calculate full prompt length\n        syntagma = entry.get('syntagmaType', '')\n        cineosis = entry.get('cineosisFunction', '')\n        full_prompt_len = len(syntagma) + len(cineosis) + len(ekphrasis) + len(style)\n        \n        if full_prompt_len > longest['fullPrompt']['length']:\n            longest['fullPrompt']['length'] = full_prompt_len\n            longest['fullPrompt']['entry'] = entry\n    \n    return longest\n\ndef create_test_entry(longest):\n    \"\"\"Create a test entry combining the longest fields.\"\"\"\n    test_entry = {\n        \"id\": \"TEST001\",\n        \"timestamp\": \"00:00:00\",\n        \"poem\": longest['poem']['entry']['poem'],\n        \"content\": longest['content']['entry']['content'],\n        \"syntagmaType\": longest['fullPrompt']['entry'].get('syntagmaType', ''),\n        \"imageType\": longest['fullPrompt']['entry'].get('imageType', ''),\n        \"cineosisFunction\": longest['fullPrompt']['entry'].get('cineosisFunction', ''),\n        \"operativeEkphrasis\": longest['operativeEkphrasis']['entry'].get('operativeEkphrasis', ''),\n        \"styleConditioning\": longest['styleConditioning']['entry'].get('styleConditioning', '')\n    }\n    return test_entry\n\ndef main():\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Analyzing {len(timeline)} timeline entries...\")\n    longest = analyze_timeline(timeline)\n    \n    # Print the findings\n    print(\"\\n--- LONGEST ENTRIES ---\")\n    for field, data in longest.items():\n        print(f\"\\n{field.upper()}: {data['length']} characters\")\n        if field == 'poem':\n            print(f\"Value: {data['entry'].get('poem', '')}\")\n            print(f\"Entry ID: {data['entry'].get('id', '')}\")\n        elif field == 'content':\n            print(f\"Value: {data['entry'].get('content', '')}\")\n            print(f\"Entry ID: {data['entry'].get('id', '')}\")\n        elif field == 'operativeEkphrasis':\n            print(f\"Value: {data['entry'].get('operativeEkphrasis', '')}\")\n            print(f\"Entry ID: {data['entry'].get('id', '')}\")\n        elif field == 'styleConditioning':\n            print(f\"Value: {data['entry'].get('styleConditioning', '')}\")\n            print(f\"Entry ID: {data['entry'].get('id', '')}\")\n        elif field == 'fullPrompt':\n            entry = data['entry']\n            print(f\"Entry ID: {entry.get('id', '')}\")\n            print(f\"Syntagma: {entry.get('syntagmaType', '')}\")\n            print(f\"Cineosis: {entry.get('cineosisFunction', '')}\")\n            print(f\"Ekphrasis: {entry.get('operativeEkphrasis', '')}\")\n            print(f\"Style: {entry.get('styleConditioning', '')}\")\n    \n    # Create and output the test entry\n    test_entry = create_test_entry(longest)\n    print(\"\\n--- TEST ENTRY CREATED ---\")\n    print(json.dumps(test_entry, indent=2))\n    \n    # Save the test entry to a file\n    test_path = os.path.join(SCRIPT_DIR, 'longest_test_entry.json')\n    with open(test_path, 'w') as f:\n        json.dump(test_entry, f, indent=2)\n    print(f\"\\nSaved test entry to: {test_path}\")\n    \n    print(\"\\nUse this test entry with clapper23.py to ensure all content fits properly.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "longest_test_entry.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "sys"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Find the longest entries in the timeline data for testing layout boundaries."
  },
  {
    "path": "HONEYBADGER/HIVE/menlo_genome_pattern_tester.py",
    "size": 7727,
    "lines": 175,
    "source": "from PIL import Image, ImageDraw, ImageFont\nimport json # Added for JSON parsing\nimport os\nimport re # Still used by old code, will be removed if not needed after full refactor, but safe for now\n\n# --- Configuration ---\nJSON_DATA_PATH = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/symbolic_genome_data.json\" # Changed from GENOME_REPORT_PATH\nOUTPUT_IMAGE_NAME = \"menlo_genome_pattern.png\"\nOUTPUT_DIR = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/\"\n\nFONT_NAME_PRIMARY = \"Menlo\"\nFONT_NAME_FALLBACK = \"Courier New\" # Fallback if Menlo is not found\nFONT_SIZE = 14\nTITLE_FONT_SIZE = 18\n\nPADDING = 20\nLINE_SPACING = 5       # Vertical space between S, I, C lines\nSECTION_SPACING = 15   # Vertical space between poem blocks\nTITLE_BOTTOM_MARGIN = 8 # Space below a poem title\n\nBACKGROUND_COLOR = (30, 30, 30)    # Dark gray\nTEXT_COLOR = (220, 220, 220)     # Light gray\nTITLE_COLOR = (100, 180, 255)    # Light blue\nERROR_COLOR = (255, 100, 100)    # Light red\nBOX_BORDER_COLOR = (80, 80, 80)  # For the genome box\n\nMAX_IMAGE_WIDTH = 3000 # To prevent extremely wide images if a line is very long\n\ndef get_font(font_name, fallback_name, size):\n    \"\"\"Attempts to load the primary font, then fallback, then Pillow default.\"\"\"\n    try:\n        return ImageFont.truetype(font_name, size)\n    except IOError:\n        print(f\"Warning: Font '{font_name}' not found.\")\n        try:\n            return ImageFont.truetype(fallback_name, size)\n        except IOError:\n            print(f\"Warning: Fallback font '{fallback_name}' not found. Using Pillow's default font.\")\n            return ImageFont.load_default()\n\ndef load_genome_data_from_json(file_path):\n    \"\"\"Loads poem genome data from a JSON file.\"\"\"\n    poems = []\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n    \n    if not poems:\n        print(f\"Warning: No poem data loaded from {file_path}. Ensure the file is not empty and contains a valid JSON list of poems.\")\n    return poems\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    output_path = os.path.join(OUTPUT_DIR, OUTPUT_IMAGE_NAME)\n\n    font = get_font(FONT_NAME_PRIMARY, FONT_NAME_FALLBACK, FONT_SIZE)\n    title_font = get_font(FONT_NAME_PRIMARY, FONT_NAME_FALLBACK, TITLE_FONT_SIZE)\n\n    poems_data = load_genome_data_from_json(JSON_DATA_PATH)\n    if not poems_data:\n        print(\"Exiting: No data to render.\")\n        return\n\n    # Calculate image dimensions\n    total_height = PADDING * 2\n    max_line_width = 0\n\n    # Create a dummy draw object to measure text\n    dummy_img = Image.new(\"RGB\", (1, 1))\n    draw_context = ImageDraw.Draw(dummy_img)\n\n    for i, poem in enumerate(poems_data):\n        if i > 0:\n            total_height += SECTION_SPACING\n        \n        # Title height and width\n        try:\n            title_bbox = draw_context.textbbox((0,0), poem['title'], font=title_font)\n            title_width = title_bbox[2] - title_bbox[0]\n            title_height = title_bbox[3] - title_bbox[1]\n            max_line_width = max(max_line_width, title_width)\n            total_height += title_height + TITLE_BOTTOM_MARGIN\n        except UnicodeEncodeError:\n            # Handle cases where title might have unrenderable chars, though less likely\n            error_msg_bbox = draw_context.textbbox((0,0), \"[Title Error]\", font=font)\n            max_line_width = max(max_line_width, error_msg_bbox[2] - error_msg_bbox[0])\n            total_height += (error_msg_bbox[3] - error_msg_bbox[1]) + TITLE_BOTTOM_MARGIN\n\n        # Genome lines height and width\n        for line_key in ['s_line', 'i_line', 'c_line']:\n            line_content = poem[line_key]\n            try:\n                line_bbox = draw_context.textbbox((0,0), line_content, font=font)\n                line_width = line_bbox[2] - line_bbox[0]\n                line_height = line_bbox[3] - line_bbox[1]\n            except UnicodeEncodeError:\n                # This can happen if the font doesn't support some glyphs\n                # We'll draw an error message in its place later\n                error_msg = f\"[Unrenderable chars in {line_key}]\"\n                line_bbox = draw_context.textbbox((0,0), error_msg, font=font)\n                line_width = line_bbox[2] - line_bbox[0]\n                line_height = line_bbox[3] - line_bbox[1]\n            \n            max_line_width = max(max_line_width, line_width)\n            total_height += line_height\n            if line_key != 'c_line': # Add spacing for S and I lines\n                total_height += LINE_SPACING\n        total_height += LINE_SPACING # Extra spacing after C line before box bottom\n\n    img_width = min(max_line_width + PADDING * 2, MAX_IMAGE_WIDTH)\n    img_height = total_height\n\n    if img_width <= PADDING * 2 or img_height <= PADDING * 2:\n        print(\"Error: Calculated image dimensions are too small. Check parsing and font metrics.\")\n        return\n\n    image = Image.new(\"RGB\", (int(img_width), int(img_height)), BACKGROUND_COLOR)\n    draw = ImageDraw.Draw(image)\n\n    current_y = PADDING\n\n    for poem in poems_data:\n        # Draw title\n        try:\n            title_bbox = draw.textbbox((PADDING, current_y), poem['title'], font=title_font)\n            draw.text((PADDING, current_y), poem['title'], font=title_font, fill=TITLE_COLOR)\n            current_y += (title_bbox[3] - title_bbox[1]) + TITLE_BOTTOM_MARGIN\n        except UnicodeEncodeError:\n            error_msg = \"[Title Error]\"\n            error_bbox = draw.textbbox((PADDING, current_y), error_msg, font=font)\n            draw.text((PADDING, current_y), error_msg, font=font, fill=ERROR_COLOR)\n            current_y += (error_bbox[3] - error_bbox[1]) + TITLE_BOTTOM_MARGIN\n\n        # Draw genome lines (S, I, C)\n        genome_block_start_y = current_y\n        max_genome_line_width_for_box = 0\n\n        for line_key in ['s_line', 'i_line', 'c_line']:\n            line_content = poem[line_key] + f\" | {line_key[0].upper()}\"\n            try:\n                line_bbox = draw.textbbox((PADDING + 5, current_y), line_content, font=font)\n                draw.text((PADDING + 5, current_y), line_content, font=font, fill=TEXT_COLOR)\n                max_genome_line_width_for_box = max(max_genome_line_width_for_box, line_bbox[2] - line_bbox[0])\n                current_y += (line_bbox[3] - line_bbox[1]) + LINE_SPACING\n            except UnicodeEncodeError:\n                error_msg = f\"[Unrenderable chars in {line_key}]\"\n                error_bbox = draw.textbbox((PADDING + 5, current_y), error_msg, font=font)\n                draw.text((PADDING + 5, current_y), error_msg, font=font, fill=ERROR_COLOR)\n                max_genome_line_width_for_box = max(max_genome_line_width_for_box, error_bbox[2] - error_bbox[0])\n                current_y += (error_bbox[3] - error_bbox[1]) + LINE_SPACING\n        \n        # Draw box around the genome lines\n        box_padding = 5\n        draw.rectangle([\n            PADDING - box_padding,\n            genome_block_start_y - box_padding,\n            PADDING + max_genome_line_width_for_box + 5 + box_padding, # +5 for the initial offset of text\n            current_y - LINE_SPACING + box_padding # Adjust to enclose last line properly\n        ], outline=BOX_BORDER_COLOR, width=1)\n\n        current_y += SECTION_SPACING - LINE_SPACING # Adjust for next section\n\n    try:\n        image.save(output_path)\n        print(f\"Successfully generated image: {output_path}\")\n    except Exception as e:\n        print(f\"Error saving image: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/symbolic_genome_data.json",
      "menlo_genome_pattern.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/symbolic_genome_data.json",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/"
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "json",
      "os",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "HONEYBADGER/HIVE/menlo_opsign_candidates_test.py",
    "size": 3531,
    "lines": 89,
    "source": "from PIL import Image, ImageDraw, ImageFont\nimport os\n\n# Configuration\nGLYPH_NAME = \"Opsign\"\nORIGINAL_GLYPH = \"\\u2A00\"  # \u2a00 N-ARY CIRCLED DOT OPERATOR\nCANDIDATE_GLYPHS = [\n    (\"\\u25CE\", \"U+25CE BULLSEYE\"),\n    (\"\\u25C9\", \"U+25C9 FISHEYE\"),\n    (\"\\u29BF\", \"U+29BF CIRCLED BULLET\"),\n    (\"\\u2299\", \"U+2299 CIRCLED DOT OPERATOR\"),\n    (\"\\u25EF\", \"U+25EF LARGE CIRCLE\"),\n]\n\nFONT_SIZE = 48\nGLYPH_AREA_WIDTH = 80  # Fixed width for each glyph's rendering area\nGLYPH_AREA_HEIGHT = 80 # Fixed height for each glyph's rendering area\nPADDING = 20\nBACKGROUND_COLOR = \"white\"\nTEXT_COLOR = \"black\"\nFONT_PATH_MENLO = \"/System/Library/Fonts/Menlo.ttc\"\nFONT_PATH_COURIER = \"/System/Library/Fonts/Courier New.ttf\" # Fallback\nOUTPUT_FILENAME = \"menlo_opsign_candidates.png\"\n\ndef get_font(font_path, size):\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        return None\n\ndef main():\n    font_menlo = get_font(FONT_PATH_MENLO, FONT_SIZE)\n    font_courier = get_font(FONT_PATH_COURIER, FONT_SIZE)\n    font_default = ImageFont.load_default()\n\n    font = font_menlo\n    if font is None:\n        print(f\"Warning: Menlo font not found at {FONT_PATH_MENLO}. Trying Courier New.\")\n        font = font_courier\n    if font is None:\n        print(f\"Warning: Courier New font not found at {FONT_PATH_COURIER}. Using Pillow default font.\")\n        font = font_default\n\n    all_glyphs_info = [\n        (ORIGINAL_GLYPH, f\"Original {GLYPH_NAME}: {ORIGINAL_GLYPH} (U+{ord(ORIGINAL_GLYPH):04X})\")] + \\\n        [(glyph, f\"Candidate: {glyph} ({desc})\") for glyph, desc in CANDIDATE_GLYPHS]\n\n    img_width = GLYPH_AREA_WIDTH + PADDING * 2 + font.getbbox(\"ABC\")[2] # Approx width for labels\n    img_height = len(all_glyphs_info) * GLYPH_AREA_HEIGHT + PADDING * (len(all_glyphs_info) + 1)\n\n    image = Image.new(\"RGB\", (img_width, img_height), BACKGROUND_COLOR)\n    draw = ImageDraw.Draw(image)\n\n    current_y = PADDING\n\n    for i, (glyph_char, label_text) in enumerate(all_glyphs_info):\n        # Calculate position for the glyph (centered in its area)\n        glyph_bbox = draw.textbbox((0, 0), glyph_char, font=font, anchor=\"lt\")\n        glyph_width = glyph_bbox[2] - glyph_bbox[0]\n        glyph_height = glyph_bbox[3] - glyph_bbox[1]\n        \n        glyph_x = PADDING + (GLYPH_AREA_WIDTH - glyph_width) / 2 - glyph_bbox[0]\n        glyph_y = current_y + (GLYPH_AREA_HEIGHT - glyph_height) / 2 - glyph_bbox[1]\n\n        # Draw the glyph\n        try:\n            draw.text((glyph_x, glyph_y), glyph_char, font=font, fill=TEXT_COLOR, anchor=\"lt\")\n        except UnicodeEncodeError:\n            print(f\"Error: Could not render glyph {glyph_char} ({label_text}) with the selected font.\")\n            draw.text((glyph_x, glyph_y), \"?\", font=font, fill=\"red\", anchor=\"lt\")\n\n        # Calculate position for the label (vertically centered with the glyph area)\n        label_bbox = draw.textbbox((0,0), label_text, font=font, anchor=\"lt\") # Use main font for labels\n        label_height = label_bbox[3] - label_bbox[1]\n        label_x = PADDING + GLYPH_AREA_WIDTH + PADDING\n        label_y = current_y + (GLYPH_AREA_HEIGHT - label_height) / 2 - label_bbox[1]\n        \n        draw.text((label_x, label_y), label_text, font=font, fill=TEXT_COLOR, anchor=\"lt\")\n        \n        current_y += GLYPH_AREA_HEIGHT + PADDING\n\n    # Save the image\n    output_path = os.path.join(os.path.dirname(__file__), OUTPUT_FILENAME)\n    image.save(output_path)\n    print(f\"Successfully generated image: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "menlo_opsign_candidates.png",
      "/System/Library/Fonts/Menlo.ttc",
      "/System/Library/Fonts/Courier New.ttf",
      ")\n        glyph_width = glyph_bbox[2] - glyph_bbox[0]\n        glyph_height = glyph_bbox[3] - glyph_bbox[1]\n        \n        glyph_x = PADDING + (GLYPH_AREA_WIDTH - glyph_width) / 2 - glyph_bbox[0]\n        glyph_y = current_y + (GLYPH_AREA_HEIGHT - glyph_height) / 2 - glyph_bbox[1]\n\n        # Draw the glyph\n        try:\n            draw.text((glyph_x, glyph_y), glyph_char, font=font, fill=TEXT_COLOR, anchor=",
      ") # Use main font for labels\n        label_height = label_bbox[3] - label_bbox[1]\n        label_x = PADDING + GLYPH_AREA_WIDTH + PADDING\n        label_y = current_y + (GLYPH_AREA_HEIGHT - label_height) / 2 - label_bbox[1]\n        \n        draw.text((label_x, label_y), label_text, font=font, fill=TEXT_COLOR, anchor="
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "HONEYBADGER/HIVE/clapper20.py",
    "size": 8830,
    "lines": 248,
    "source": "#!/usr/bin/env python3\n\"\"\"\nClapper20 - Film Style Timeline Visualization\n\nThis script renders timeline entries in a film clapperboard style with high contrast\ntext on dark background, emphasizing technical metadata and visual components.\nInspired by film production slates/clappers.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nBASE_HEIGHT = 240\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_clapper20')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths (system fonts - we'll use these initially)\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\"\n\n# Define glyphs for different types\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card in film clapperboard style.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH + 2*BORDER_WIDTH, BASE_HEIGHT + 2*BORDER_WIDTH), BLUE)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (BASE_WIDTH, BASE_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font = find_font(HEADER_FONT_PATH, 16)\n    body_font = find_font(BODY_FONT_PATH, 14)\n    large_font = find_font(HEADER_FONT_PATH, 18)\n    \n    # Calculate row heights\n    top_row_height = 30\n    second_row_height = 30\n    third_row_height = 40\n    content_row_height = BASE_HEIGHT - (top_row_height + second_row_height + third_row_height)\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = f\"FRM: (---)\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), \"TT: 0 DESCRIPTIVE IMAGE\", font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height), (BASE_WIDTH, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_content = entry.get('content', '---')\n    # Truncate line content if too long\n    if len(line_content) > 60:\n        line_content = line_content[:57] + \"...\"\n    line_text = f\"LINE: \\\"{line_content}\\\"\"\n    \n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    draw.text((500, top_row_height + 8), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height + second_row_height), (BASE_WIDTH, top_row_height + second_row_height)], fill=WHITE, width=1)\n    \n    # THIRD ROW: Column headers - AMBER BAR\n    header_bar_y = top_row_height + second_row_height\n    draw.rectangle([(0, header_bar_y), (BASE_WIDTH, header_bar_y + third_row_height)], fill=AMBER)\n    \n    # Add column headers in the amber bar\n    col_width = BASE_WIDTH // 3\n    draw.text((10, header_bar_y + 5), \"SYNTAGMA\", font=header_font, fill=BLACK)\n    draw.text((col_width, header_bar_y + 5), \"CINEOSIS FUNCTION\", font=header_font, fill=BLACK)\n    draw.text((2*col_width, header_bar_y + 5), \"OPERATIVE EKPHRASIS\", font=header_font, fill=BLACK)\n    \n    # Get the types and prompt components\n    syntagma_type = entry.get('syntagmaType', '---')\n    cineosis_func = entry.get('cineosisFunction', '---')\n    operative_ekphrasis = entry.get('operativeEkphrasis', '')\n    \n    # Abbreviate syntagma type\n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # CONTENT ROW: Display the content with bullet points\n    content_y = header_bar_y + third_row_height + 15\n    \n    syntagma_text = f\"DS\"  # Hardcode DS to match image\n    cineosis_text = f\"\u00b7 Mood Environment Stabilizer\"  # Hardcode to match image\n    ekphrasis_text = f\"\u00b7 A symbolic tableau rendered through chiaroscuro\"  # Hardcode to match image\n    \n    # If we have actual data, use it instead of hardcoded values\n    if syntagma_type != \"---\":\n        syntagma_text = f\"{syntagma_abbrev}\"\n    if cineosis_func != \"---\":\n        cineosis_text = f\"\u00b7 {cineosis_func}\"\n    if operative_ekphrasis:\n        ekphrasis_text = f\"\u00b7 {operative_ekphrasis}\"\n    \n    # Draw the content\n    draw.text((10, content_y), syntagma_text, font=large_font, fill=AMBER)\n    draw.text((40, content_y), cineosis_text, font=large_font, fill=AMBER)\n    draw.text((350, content_y), ekphrasis_text, font=large_font, fill=AMBER)\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (BORDER_WIDTH, BORDER_WIDTH))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_clapper.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    \n    # Render first 10 entries as samples\n    render_all_entries(timeline, OUTPUT_DIR, 10)\n    \n    # Also render a demo card showing the layout\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"SH001\",\n        \"timestamp\": \"00:03:51\",\n        \"poem\": \"Out of Life\",\n        \"content\": \"What we've made-\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"A symbolic tableau rendered through chiaroscuro\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_clapper.png\")\n    render_card(demo_entry, demo_path)\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Total dimensions: {BASE_WIDTH}x{BASE_HEIGHT}px\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_clapper.png",
      "demo_clapper.png",
      "\nClapper20 - Film Style Timeline Visualization\n\nThis script renders timeline entries in a film clapperboard style with high contrast\ntext on dark background, emphasizing technical metadata and visual components.\nInspired by film production slates/clappers.\n",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      "\n    \n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    draw.text((500, top_row_height + 8), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height + second_row_height), (BASE_WIDTH, top_row_height + second_row_height)], fill=WHITE, width=1)\n    \n    # THIRD ROW: Column headers - AMBER BAR\n    header_bar_y = top_row_height + second_row_height\n    draw.rectangle([(0, header_bar_y), (BASE_WIDTH, header_bar_y + third_row_height)], fill=AMBER)\n    \n    # Add column headers in the amber bar\n    col_width = BASE_WIDTH // 3\n    draw.text((10, header_bar_y + 5), ",
      "Rendered {i+1}/{len(entries_to_render)} cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Clapper20 - Film Style Timeline Visualization\n\nThis script renders timeline entries in a film clapperboard style with high contrast\ntext on dark background, emphasizing technical metadata and visual components.\nInspired by film production slates/clappers."
  },
  {
    "path": "HONEYBADGER/HIVE/menlo_emotion_relay_visibility_test.py",
    "size": 5237,
    "lines": 119,
    "source": "from PIL import Image, ImageDraw, ImageFont\nimport os\n\n# Configuration\nGLYPH_NAME = \"Emotion Relay\"\nORIGINAL_GLYPH = \"\\u2661\"  # \u2661 WHITE HEART SUIT\nCANDIDATE_GLYPHS = [\n    (\"\\u2665\", \"U+2665 BLACK HEART SUIT\"),\n    (\"\\u2764\", \"U+2764 HEAVY BLACK HEART\"),\n    (\"\\u2765\", \"U+2765 ROTATED HEAVY BLACK HEART BULLET\"),\n    (\"\\u2763\", \"U+2763 HEAVY HEART EXCLAMATION MARK ORNAMENT\"),\n]\n\nFONT_SIZE = 48\nGLYPH_AREA_WIDTH = 80\nGLYPH_AREA_HEIGHT = 80\nPADDING = 20\nPAGE_BACKGROUND_COLOR = \"gray\"\nWHITE_BG_COLOR = \"white\"\nBLACK_BG_COLOR = \"black\"\nTEXT_COLOR_ON_WHITE = \"black\"\nTEXT_COLOR_ON_BLACK = \"white\"\nFONT_PATH_MENLO = \"/System/Library/Fonts/Menlo.ttc\"\nFONT_PATH_COURIER = \"/System/Library/Fonts/Courier New.ttf\"\nOUTPUT_FILENAME = \"menlo_emotion_relay_visibility.png\"\n\ndef get_font(font_path, size):\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        return None\n\ndef main():\n    font_menlo = get_font(FONT_PATH_MENLO, FONT_SIZE)\n    font_courier = get_font(FONT_PATH_COURIER, FONT_SIZE)\n    font_default = ImageFont.load_default()\n\n    font = font_menlo\n    if font is None:\n        print(f\"Warning: Menlo font not found at {FONT_PATH_MENLO}. Trying Courier New.\")\n        font = font_courier\n    if font is None:\n        print(f\"Warning: Courier New font not found at {FONT_PATH_COURIER}. Using Pillow default font for glyphs.\")\n        font = font_default\n\n    label_font = font\n\n    all_glyphs_info = [\n        (ORIGINAL_GLYPH, f\"Original {GLYPH_NAME}: {ORIGINAL_GLYPH} (U+{ord(ORIGINAL_GLYPH):04X})\")] + \\\n        [(glyph, f\"Candidate: {glyph} ({desc})\") for glyph, desc in CANDIDATE_GLYPHS]\n\n    # Estimate max label width for image sizing\n    max_label_width = 0\n    temp_image_for_measure = Image.new(\"RGB\", (1,1))\n    temp_draw_for_measure = ImageDraw.Draw(temp_image_for_measure)\n    for _, label_text in all_glyphs_info:\n        bbox = temp_draw_for_measure.textbbox((0,0), label_text, font=label_font, anchor='lt')\n        max_label_width = max(max_label_width, bbox[2] - bbox[0])\n    del temp_draw_for_measure, temp_image_for_measure\n\n    img_width = PADDING + GLYPH_AREA_WIDTH + PADDING + GLYPH_AREA_WIDTH + PADDING + max_label_width + PADDING\n    img_height = len(all_glyphs_info) * (GLYPH_AREA_HEIGHT + PADDING) + PADDING\n\n    image = Image.new(\"RGB\", (int(img_width), int(img_height)), PAGE_BACKGROUND_COLOR)\n    draw = ImageDraw.Draw(image)\n\n    current_y = PADDING\n\n    for i, (glyph_char, label_text) in enumerate(all_glyphs_info):\n        # --- Render on White Background ---\n        bg_white_x0 = PADDING\n        bg_white_y0 = current_y\n        bg_white_x1 = bg_white_x0 + GLYPH_AREA_WIDTH\n        bg_white_y1 = bg_white_y0 + GLYPH_AREA_HEIGHT\n        draw.rectangle([bg_white_x0, bg_white_y0, bg_white_x1, bg_white_y1], fill=WHITE_BG_COLOR)\n\n        glyph_bbox_white = draw.textbbox((0, 0), glyph_char, font=font, anchor=\"lt\")\n        glyph_width_white = glyph_bbox_white[2] - glyph_bbox_white[0]\n        glyph_height_white = glyph_bbox_white[3] - glyph_bbox_white[1]\n        glyph_x_white = bg_white_x0 + (GLYPH_AREA_WIDTH - glyph_width_white) / 2 - glyph_bbox_white[0]\n        glyph_y_white = bg_white_y0 + (GLYPH_AREA_HEIGHT - glyph_height_white) / 2 - glyph_bbox_white[1]\n        try:\n            draw.text((glyph_x_white, glyph_y_white), glyph_char, font=font, fill=TEXT_COLOR_ON_WHITE, anchor=\"lt\")\n        except UnicodeEncodeError:\n            draw.text((glyph_x_white, glyph_y_white), \"?\", font=font, fill=\"red\", anchor=\"lt\")\n\n        # --- Render on Black Background ---\n        bg_black_x0 = PADDING + GLYPH_AREA_WIDTH + PADDING\n        bg_black_y0 = current_y\n        bg_black_x1 = bg_black_x0 + GLYPH_AREA_WIDTH\n        bg_black_y1 = bg_black_y0 + GLYPH_AREA_HEIGHT\n        draw.rectangle([bg_black_x0, bg_black_y0, bg_black_x1, bg_black_y1], fill=BLACK_BG_COLOR)\n\n        glyph_bbox_black = draw.textbbox((0, 0), glyph_char, font=font, anchor=\"lt\")\n        glyph_width_black = glyph_bbox_black[2] - glyph_bbox_black[0]\n        glyph_height_black = glyph_bbox_black[3] - glyph_bbox_black[1]\n        glyph_x_black = bg_black_x0 + (GLYPH_AREA_WIDTH - glyph_width_black) / 2 - glyph_bbox_black[0]\n        glyph_y_black = bg_black_y0 + (GLYPH_AREA_HEIGHT - glyph_height_black) / 2 - glyph_bbox_black[1]\n        try:\n            draw.text((glyph_x_black, glyph_y_black), glyph_char, font=font, fill=TEXT_COLOR_ON_BLACK, anchor=\"lt\")\n        except UnicodeEncodeError:\n            draw.text((glyph_x_black, glyph_y_black), \"?\", font=font, fill=\"red\", anchor=\"lt\")\n\n        # --- Draw Label ---\n        label_bbox = draw.textbbox((0,0), label_text, font=label_font, anchor=\"lt\")\n        label_height_val = label_bbox[3] - label_bbox[1]\n        label_x = PADDING + GLYPH_AREA_WIDTH + PADDING + GLYPH_AREA_WIDTH + PADDING\n        label_y = current_y + (GLYPH_AREA_HEIGHT - label_height_val) / 2 - label_bbox[1]\n        draw.text((label_x, label_y), label_text, font=label_font, fill=TEXT_COLOR_ON_WHITE, anchor=\"lt\")\n        \n        current_y += GLYPH_AREA_HEIGHT + PADDING\n\n    output_path = os.path.join(os.path.dirname(__file__), OUTPUT_FILENAME)\n    image.save(output_path)\n    print(f\"Successfully generated image: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "menlo_emotion_relay_visibility.png",
      "/System/Library/Fonts/Menlo.ttc",
      "/System/Library/Fonts/Courier New.ttf",
      ")\n        glyph_width_white = glyph_bbox_white[2] - glyph_bbox_white[0]\n        glyph_height_white = glyph_bbox_white[3] - glyph_bbox_white[1]\n        glyph_x_white = bg_white_x0 + (GLYPH_AREA_WIDTH - glyph_width_white) / 2 - glyph_bbox_white[0]\n        glyph_y_white = bg_white_y0 + (GLYPH_AREA_HEIGHT - glyph_height_white) / 2 - glyph_bbox_white[1]\n        try:\n            draw.text((glyph_x_white, glyph_y_white), glyph_char, font=font, fill=TEXT_COLOR_ON_WHITE, anchor=",
      ")\n        glyph_width_black = glyph_bbox_black[2] - glyph_bbox_black[0]\n        glyph_height_black = glyph_bbox_black[3] - glyph_bbox_black[1]\n        glyph_x_black = bg_black_x0 + (GLYPH_AREA_WIDTH - glyph_width_black) / 2 - glyph_bbox_black[0]\n        glyph_y_black = bg_black_y0 + (GLYPH_AREA_HEIGHT - glyph_height_black) / 2 - glyph_bbox_black[1]\n        try:\n            draw.text((glyph_x_black, glyph_y_black), glyph_char, font=font, fill=TEXT_COLOR_ON_BLACK, anchor=",
      ")\n        label_height_val = label_bbox[3] - label_bbox[1]\n        label_x = PADDING + GLYPH_AREA_WIDTH + PADDING + GLYPH_AREA_WIDTH + PADDING\n        label_y = current_y + (GLYPH_AREA_HEIGHT - label_height_val) / 2 - label_bbox[1]\n        draw.text((label_x, label_y), label_text, font=label_font, fill=TEXT_COLOR_ON_WHITE, anchor="
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "HONEYBADGER/HIVE/render_cards_clapperboard.py",
    "size": 9928,
    "lines": 278,
    "source": "#!/usr/bin/env python3\n\"\"\"\nRender Timeline Data Cards as Clapperboard-style Headers\n\nThis script renders timeline entries with a structured clapperboard-style header\nthat sits above a 16:9 image area, creating an overall 3:2 aspect ratio.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants (based on user requirements)\nASPECT_16_9 = 16/9  # Image area\nASPECT_3_2 = 3/2    # Overall frame\n\n# Calculate dimensions to maintain proper aspect ratios\nBASE_WIDTH = 1200\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height of the 16:9 image area\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)   # Total height for 3:2 aspect ratio\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT   # Height of the header area\n\n# Card dimensions\nCARD_WIDTH = BASE_WIDTH\nCARD_HEIGHT = TOTAL_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nDARK_GRAY = (40, 40, 40)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\nAMBER = (255, 191, 0)\nTEAL = (0, 128, 128)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths (system fonts - we'll use these initially)\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\" \n\n# Define glyphs for different types\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card with a clapperboard-style header above a 16:9 image area.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (CARD_WIDTH+8, CARD_HEIGHT+8), BLUE_BORDER)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (CARD_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font_lg = find_font(HEADER_FONT_PATH, 32)\n    header_font = find_font(HEADER_FONT_PATH, 28)\n    body_font = find_font(BODY_FONT_PATH, 24)\n    symbol_font = find_font(SYMBOL_FONT_PATH, 28)\n    \n    # Fill the header area black\n    draw.rectangle([(0, 0), (CARD_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    \n    # Fill the image area light gray (placeholder)\n    draw.rectangle([(0, HEADER_HEIGHT), (CARD_WIDTH, CARD_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Add text to the image area (placeholder)\n    draw.text((CARD_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=header_font, fill=BLACK, anchor=\"mm\")\n    \n    # Draw horizontal dividing line\n    draw.line([(0, HEADER_HEIGHT), (CARD_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # Layout the header like a clapperboard\n    \n    # Top row - ID and timestamp\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = \"FRM: (---)\"\n    \n    top_y = 20\n    draw.text((40, top_y), id_text, font=header_font_lg, fill=WHITE)\n    draw.text((400, top_y), time_text, font=header_font_lg, fill=WHITE)\n    draw.text((800, top_y), frame_text, font=header_font_lg, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, top_y+40), (CARD_WIDTH, top_y+40)], fill=WHITE, width=1)\n    \n    # Second row - Poem and Line\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_text = f\"LINE: \\\"{entry.get('content', '---')}\\\"\"\n    \n    row2_y = top_y + 50\n    draw.text((40, row2_y), poem_text, font=header_font, fill=WHITE)\n    draw.text((500, row2_y), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, row2_y+40), (CARD_WIDTH, row2_y+40)], fill=WHITE, width=1)\n    \n    # Third row - Syntagma Type, Image Type, and Cineosis Function\n    # This row needs careful alignment and proper spacing\n    syntagma_type = entry.get('syntagmaType', '---')\n    image_type = entry.get('imageType', '---')\n    cineosis_func = entry.get('cineosisFunction', '---')\n    \n    syntagma_glyph = get_glyph(SYNTAGMA_GLYPHS, syntagma_type, \"\u25a1\")\n    image_glyph = get_glyph(IMAGE_GLYPHS, image_type, \"\u25a1\")\n    cineosis_glyph = get_glyph(CINEOSIS_GLYPHS, cineosis_func, \"\u25ef\")\n    \n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # Split the row into clear sections\n    row3_y = row2_y + 50\n    \n    # Section 1: Syntagma Type - First 33% of width\n    draw.text((40, row3_y), f\"SY: {syntagma_glyph} {syntagma_abbrev}\", font=header_font, fill=WHITE)\n    \n    # Section 2: Image Type - Middle 33% of width\n    image_text = f\"IT: {image_glyph} {image_type.split('-')[0].upper()}\"\n    draw.text((CARD_WIDTH//3 + 40, row3_y), image_text, font=header_font, fill=WHITE)\n    \n    # Section 3: Cineosis Function - Last 33% of width\n    cf_text = f\"CF: {cineosis_glyph}\"\n    cf_name = cineosis_func.split(' ')[0].upper()\n    if len(cineosis_func.split(' ')) > 1:\n        cf_name += \" \" + cineosis_func.split(' ')[1].upper()\n    draw.text((2*CARD_WIDTH//3 + 40, row3_y), cf_text + \" \" + cf_name, font=header_font, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, row3_y+40), (CARD_WIDTH, row3_y+40)], fill=WHITE, width=1)\n    \n    # Fourth row - Raw prompt\n    prompt_label = \"RAW PROMPT (MACHINE INPUT):\"\n    \n    row4_y = row3_y + 50\n    draw.rectangle([(0, row4_y-10), (CARD_WIDTH, row4_y+30)], fill=LIGHT_GRAY)\n    draw.text((40, row4_y), prompt_label, font=header_font, fill=BLACK)\n    \n    # Format the prompt (shortened to fit in the remaining header space)\n    prompt_text = f\"{syntagma_abbrev} \u00b7 {cineosis_func} \u00b7 {entry.get('operativeEkphrasis', '---')}\"\n    \n    # Truncate the prompt if it's too long\n    if len(prompt_text) > 90:\n        prompt_text = prompt_text[:87] + \"...\"\n    \n    # Draw the prompt text\n    draw.text((40, row4_y+40), prompt_text, font=body_font, fill=WHITE)\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (4, 4))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_card.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    \n    # Render first 10 entries as samples\n    render_all_entries(timeline, OUTPUT_DIR, 10)\n    \n    # Also render a demo card showing the layout with dimensions\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"DEMO\",\n        \"timestamp\": \"00:00:00\",\n        \"poem\": \"Layout Example\",\n        \"content\": \"3:2 overall with 16:9 image\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"This is a demo card showing the layout structure with proper aspect ratios\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_layout.png\")\n    render_card(demo_entry, demo_path)\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {CARD_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {CARD_WIDTH}x{CARD_HEIGHT}px (3:2)\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_card.png",
      "demo_layout.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants (based on user requirements)\nASPECT_16_9 = 16/9  # Image area\nASPECT_3_2 = 3/2    # Overall frame\n\n# Calculate dimensions to maintain proper aspect ratios\nBASE_WIDTH = 1200\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height of the 16:9 image area\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)   # Total height for 3:2 aspect ratio\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT   # Height of the header area\n\n# Card dimensions\nCARD_WIDTH = BASE_WIDTH\nCARD_HEIGHT = TOTAL_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nDARK_GRAY = (40, 40, 40)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\nAMBER = (255, 191, 0)\nTEAL = (0, 128, 128)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ", (CARD_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font_lg = find_font(HEADER_FONT_PATH, 32)\n    header_font = find_font(HEADER_FONT_PATH, 28)\n    body_font = find_font(BODY_FONT_PATH, 24)\n    symbol_font = find_font(SYMBOL_FONT_PATH, 28)\n    \n    # Fill the header area black\n    draw.rectangle([(0, 0), (CARD_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    \n    # Fill the image area light gray (placeholder)\n    draw.rectangle([(0, HEADER_HEIGHT), (CARD_WIDTH, CARD_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Add text to the image area (placeholder)\n    draw.text((CARD_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), ",
      "\n    draw.text((CARD_WIDTH//3 + 40, row3_y), image_text, font=header_font, fill=WHITE)\n    \n    # Section 3: Cineosis Function - Last 33% of width\n    cf_text = f",
      ")[1].upper()\n    draw.text((2*CARD_WIDTH//3 + 40, row3_y), cf_text + ",
      "Rendered {i+1}/{len(entries_to_render)} cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Render Timeline Data Cards as Clapperboard-style Headers\n\nThis script renders timeline entries with a structured clapperboard-style header\nthat sits above a 16:9 image area, creating an overall 3:2 aspect ratio."
  },
  {
    "path": "HONEYBADGER/HIVE/clapper30.py",
    "size": 17179,
    "lines": 412,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER30 - Film Clapperboard Style Timeline Data Cards with Actual Images\n- Image integration from image_path field in timeline data \n- Poem and Line Content on same line\n- Updated header labels: \"IMAGE\" and \"FRAME\"\n- No amber syntagma bar, with syntagma abbreviation directly in prompt\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\n\n# Constants\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 200  # Increased height for text\nCARD_HEIGHT = HEADER_HEIGHT + 576  # 576 is the height for a 16:9 aspect ratio of 1024 width\nIMAGE_HEIGHT = 576  # 16:9 aspect ratio\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Using the complete timeline with image paths\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper30\")\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Base directory for relative image paths\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Helper function to wrap text\ndef wrap_text(text, font, max_width):\n    \"\"\"Wrap text to fit within a given width.\"\"\"\n    words = text.split()\n    wrapped_lines = []\n    current_line = []\n    \n    for word in words:\n        # Test with current word added\n        test_line = ' '.join(current_line + [word])\n        line_width = font.getbbox(test_line)[2]\n        \n        if line_width <= max_width:\n            current_line.append(word)\n        else:\n            # If the current line has words, complete it\n            if current_line:\n                wrapped_lines.append(' '.join(current_line))\n                current_line = [word]\n            else:\n                # If the word itself is too long, force it on its own line\n                wrapped_lines.append(word)\n                current_line = []\n    \n    # Add the last line if there's anything left\n    if current_line:\n        wrapped_lines.append(' '.join(current_line))\n    \n    return wrapped_lines\n\ndef get_image_path(entry):\n    \"\"\"Get the image path from the entry data or return None.\"\"\"\n    # Check if image_path exists in the entry\n    if 'image_path' in entry and entry['image_path']:\n        # Construct absolute path from relative path in the timeline data\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        # Try TIGER directory as alternative\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    \n    # Fallback: search in TIGER directory by ID\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        # Try to find any file with ID prefix\n        for root, _, files in os.walk(base_dir):\n            for file in files:\n                if file.startswith(entry_id + '__') and file.lower().endswith('.png'):\n                    return os.path.join(root, file)\n    \n    return None\n\ndef render_card(entry, output_path):\n    \"\"\"Render a single data card with the entry information.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH+8, CARD_HEIGHT+8), BLUE_BORDER)\n    \n    # Create inner black image (main card)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    \n    draw = ImageDraw.Draw(img)\n    \n    # Load fonts\n    try:\n        header_font = ImageFont.truetype(\"Courier New Bold.ttf\", 16)  \n        text_font = ImageFont.truetype(\"Courier New.ttf\", 16)\n    except IOError:\n        # Fallback to default font if Courier New is not available\n        header_font = ImageFont.load_default()\n        text_font = ImageFont.load_default()\n        print(\"Warning: Courier New font not found, using default font.\")\n    \n    top_row_height = 30  # Height of the top row\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    \n    # Get frame position information if available\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    \n    # Image type without the frame number\n    image_type = entry.get('imageType', 'Descriptive Image')\n    image_text = f\"IMAGE: {image_type}\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), image_text, font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(4, top_row_height), (BASE_WIDTH+4, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line with proper wrapping for long content\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    \n    # Calculate poem width\n    poem_width = header_font.getbbox(poem_text)[2]\n    \n    # Draw poem name\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # LINE CONTENT: Highlighted with amber to give it emphasis\n    # Calculate start position for line content (after poem with some spacing)\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)  # Either after poem or at half width\n    \n    # Draw the line content with amber highlighting\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    # Draw prefix in white\n    draw.text((line_start_x, top_row_height + 8), line_prefix, font=header_font, fill=WHITE)\n    \n    # Available width for line content\n    available_width = BASE_WIDTH - line_start_x - line_prefix_width - 20\n    \n    # Wrap the line content if needed\n    wrapped_content = wrap_text(line_content, header_font, available_width)\n    \n    for i, line_part in enumerate(wrapped_content):\n        y_pos = top_row_height + 8 + (i * 16)\n        \n        # Add closing quote to last line\n        if i == len(wrapped_content) - 1:\n            line_part += \"\\\"\"\n        \n        # Draw the line content in amber\n        draw.text((line_start_x + line_prefix_width, y_pos), line_part, font=header_font, fill=AMBER)\n    \n    # Calculate actual height used by the line content\n    line_content_actual_height = 20 + max(1, len(wrapped_content)) * 16  # Base + line height\n    \n    # Draw horizontal divider after line content\n    divider_y = top_row_height + line_content_actual_height\n    draw.line([(4, divider_y), (BASE_WIDTH+4, divider_y)], fill=WHITE, width=1)\n    \n    # PROMPT ROW: Continuous prompt text with wrapping\n    # Check if full_prompt exists in the data\n    if 'full_prompt' in entry and entry['full_prompt']:\n        full_prompt = entry['full_prompt']\n    else:\n        # If not, construct it from the parts\n        # Extract syntagma abbreviation from syntagmaType (e.g. \"Descriptive Syntagma (DS)\" -> \"DS\")\n        syntagma_type = entry.get('syntagmaType', '')\n        syntagma_abbr = ''\n        if '(' in syntagma_type and ')' in syntagma_type:\n            syntagma_abbr = syntagma_type.split('(')[1].split(')')[0]\n        else:\n            # Fallback: use first letter of each word\n            syntagma_abbr = ''.join(word[0] for word in syntagma_type.split() if word)\n        \n        # Make sure we have a valid abbreviation\n        if not syntagma_abbr:\n            syntagma_abbr = 'SS'\n            \n        operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n        style_conditioning = entry.get('styleConditioning', '---')\n        cineosis_function = entry.get('cineosisFunction', '')\n        \n        # Construct the full prompt with all available data\n        full_prompt = f\"{syntagma_abbr} \u00b7 {cineosis_function} \u00b7 {operative_ekphrasis} \u00b7 {style_conditioning}\"\n    \n    # Wrap the prompt text to fit the width\n    wrapped_prompt = wrap_text(full_prompt, text_font, BASE_WIDTH - 20)\n    \n    # Draw the prompt text\n    for i, line in enumerate(wrapped_prompt):\n        y_pos = divider_y + 8 + (i * 16)\n        draw.text((10, y_pos), line, font=text_font, fill=AMBER)\n    \n    # Get the actual image file path from the entry\n    image_path = get_image_path(entry)\n    \n    if image_path and os.path.exists(image_path):\n        try:\n            # Open and resize the actual image\n            actual_image = Image.open(image_path)\n            actual_image = actual_image.resize((BASE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)\n            \n            # Paste the actual image into the card\n            img.paste(actual_image, (4, HEADER_HEIGHT + 4))\n        except Exception as e:\n            print(f\"Error loading image for {entry_id}: {e}\")\n            # Fill image area with placeholder if loading fails\n            draw.rectangle([(4, HEADER_HEIGHT + 4), (BASE_WIDTH + 4, CARD_HEIGHT + 4)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n                     f\"Image Error: {os.path.basename(image_path)}\", \n                     font=header_font, fill=BLACK, anchor=\"mm\")\n    else:\n        # Fill image area with placeholder if no matching image is found\n        draw.rectangle([(4, HEADER_HEIGHT + 4), (BASE_WIDTH + 4, CARD_HEIGHT + 4)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n                 \"No Image Found\", \n                 font=header_font, fill=BLACK, anchor=\"mm\")\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n    return output_path\n\ndef render_demo_card():\n    \"\"\"Render a demo card with sample data.\"\"\"\n    demo_entry = {\n        \"id\": \"DEMO\",\n        \"timestamp\": \"00:00:00\",\n        \"poem\": \"Sample Poem Name\",\n        \"content\": \"This is a sample line of content.\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"This is a sample operative ekphrasis description text\",\n        \"styleConditioning\": \"This is style conditioning text that would normally continue with more details about the visual style.\",\n        \"full_prompt\": \"DS \u00b7 Mood Environment Stabilizer \u00b7 This is a sample operative ekphrasis description text \u00b7 This is style conditioning text that would normally continue with more details about the visual style.\",\n        \"imageType\": \"Descriptive Image\",\n        \"image_path\": \"\",\n        \"frame_position\": 1,\n        \"frame_total\": 12\n    }\n    output_path = os.path.join(OUTPUT_DIR, \"demo_clapper.png\")\n    return render_card(demo_entry, output_path)\n\ndef render_extreme_test_case():\n    \"\"\"Render a test card with the longest entries to test layout boundaries.\"\"\"\n    # These are sample longest entries from testing\n    longest_entry = {\n        \"id\": \"EXTREME\",\n        \"timestamp\": \"99:99:99\",\n        \"poem\": \"How to break off an engagement\",\n        \"content\": \"where you go when you leave\u2014 & I fall out of life.\",\n        \"syntagmaType\": \"Subjective Syntagma (SS)\",\n        \"cineosisFunction\": \"Memory Storage Retrieval\",\n        \"operativeEkphrasis\": \"Through a reminiscent lens that distorts memory like warped glass, an intimate letter is folded into origami wings, suspended between white candles that melt too quickly.\",\n        \"styleConditioning\": \"In the evocative style of Taryn Simon's meticulous forensic compositions, with precise documentary lighting that creates a stark visual inventory, rendered with unnerving clarity that transforms emotional artifacts into clinical evidence.\",\n        \"full_prompt\": \"SS \u00b7 Memory Storage Retrieval \u00b7 Through a reminiscent lens that distorts memory like warped glass, an intimate letter is folded into origami wings, suspended between white candles that melt too quickly. \u00b7 In the evocative style of Taryn Simon's meticulous forensic compositions, with precise documentary lighting that creates a stark visual inventory, rendered with unnerving clarity that transforms emotional artifacts into clinical evidence.\",\n        \"imageType\": \"Descriptive Image\",\n        \"image_path\": \"TEST/EXTREME_TEST_3.png\",\n        \"frame_position\": 7,\n        \"frame_total\": 34\n    }\n    output_path = os.path.join(OUTPUT_DIR, \"extreme_test.png\")\n    card_path = render_card(longest_entry, output_path)\n    print(f\"Rendered extreme test case: {card_path}\")\n    return card_path\n\ndef calculate_frame_counts(timeline_data):\n    \"\"\"Calculate the total number of frames per poem and frame position for each entry.\"\"\"\n    # Group by poem name and count entries per poem\n    poem_counts = {}\n    poem_entries = {}\n    \n    for entry in timeline_data:\n        poem_name = entry.get('poem', '')\n        if poem_name not in poem_counts:\n            poem_counts[poem_name] = 0\n            poem_entries[poem_name] = []\n        poem_counts[poem_name] += 1\n        poem_entries[poem_name].append(entry)\n    \n    # Sort entries within each poem by timestamp\n    for poem, entries in poem_entries.items():\n        poem_entries[poem] = sorted(entries, key=lambda x: x.get('timestamp', '00:00:00'))\n    \n    # Create a lookup table for frame position and total count\n    frame_info = {}\n    for poem, entries in poem_entries.items():\n        for i, entry in enumerate(entries):\n            entry_id = entry.get('id', '')\n            if entry_id:\n                frame_info[entry_id] = {\n                    'position': i + 1,  # 1-based position\n                    'total': len(entries)\n                }\n    \n    return frame_info\n\ndef main():\n    \"\"\"Main function to load timeline and render all cards.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    \n    try:\n        with open(TIMELINE_PATH, 'r') as file:\n            timeline_data = json.load(file)\n    except Exception as e:\n        print(f\"Error loading timeline data: {e}\")\n        return\n    \n    print(f\"Found {len(timeline_data)} timeline entries\")\n    print(f\"Header height: {HEADER_HEIGHT}px\")\n    print(f\"Image height (16:9): {IMAGE_HEIGHT}px\")\n    print(f\"Using base image directory: {BASE_IMAGE_DIR}\")\n    \n    # Calculate frame counts for each poem\n    frame_info = calculate_frame_counts(timeline_data)\n    print(f\"Calculated frame positions for {len(frame_info)} unique entries\")\n    \n    # Group entries by poem name\n    poem_entries = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem', 'Unknown')\n        if poem_name not in poem_entries:\n            poem_entries[poem_name] = []\n        poem_entries[poem_name].append(entry)\n    \n    print(f\"Found {len(poem_entries)} unique poems\")\n    \n    # Track image paths found and not found\n    images_found = 0\n    images_not_found = 0\n    rendered_count = 0\n    \n    # Render one card from each poem\n    for poem_name, entries in poem_entries.items():\n        if poem_name == 'Unknown' or not poem_name:\n            continue  # Skip entries without a valid poem name\n            \n        # Select the first entry with a valid image path as a representative\n        selected_entry = None\n        for entry in entries:\n            entry_id = entry.get('id', '')\n            if entry_id and get_image_path(entry):\n                selected_entry = entry\n                break\n        \n        # If no entry with image found, use the first entry\n        if not selected_entry and entries:\n            selected_entry = entries[0]\n        \n        if not selected_entry:\n            continue\n        \n        # Add frame count information to the entry\n        entry_id = selected_entry.get('id', 'unknown')\n        if entry_id in frame_info:\n            selected_entry['frame_position'] = frame_info[entry_id]['position']\n            selected_entry['frame_total'] = frame_info[entry_id]['total']\n        \n        # Render representative card for this poem\n        output_filename = f\"{poem_name.replace(' ', '_')}_card.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        \n        image_path = get_image_path(selected_entry)\n        if image_path:\n            images_found += 1\n            print(f\"Found image for poem '{poem_name}' (ID: {entry_id}): {os.path.basename(image_path)}\")\n        else:\n            images_not_found += 1\n            print(f\"No image found for poem '{poem_name}' (ID: {entry_id})\")\n        \n        render_card(selected_entry, output_path)\n        rendered_count += 1\n    \n    print(f\"Rendered {rendered_count} cards (one from each poem)\")\n    print(f\"Images found: {images_found}, not found: {images_not_found}\")\n    \n    print(\"Rendering demo layout card\")\n    render_demo_card()\n    \n    print(\"Rendering extreme test case with longest entries\")\n    render_extreme_test_case()\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {BASE_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {BASE_WIDTH}x{CARD_HEIGHT}px\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "demo_clapper.png",
      "TEST/EXTREME_TEST_3.png",
      "extreme_test.png",
      ")}_card.png",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    \n    # Calculate poem width\n    poem_width = header_font.getbbox(poem_text)[2]\n    \n    # Draw poem name\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # LINE CONTENT: Highlighted with amber to give it emphasis\n    # Calculate start position for line content (after poem with some spacing)\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)  # Either after poem or at half width\n    \n    # Draw the line content with amber highlighting\n    line_prefix = ",
      ")\n            # Fill image area with placeholder if loading fails\n            draw.rectangle([(4, HEADER_HEIGHT + 4), (BASE_WIDTH + 4, CARD_HEIGHT + 4)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n                     f",
      ")\n    else:\n        # Fill image area with placeholder if no matching image is found\n        draw.rectangle([(4, HEADER_HEIGHT + 4), (BASE_WIDTH + 4, CARD_HEIGHT + 4)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n                 ",
      "TEST/EXTREME_TEST_3.png"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER30 - Film Clapperboard Style Timeline Data Cards with Actual Images\n- Image integration from image_path field in timeline data \n- Poem and Line Content on same line\n- Updated header labels: \"IMAGE\" and \"FRAME\"\n- No amber syntagma bar, with syntagma abbreviation directly in prompt"
  },
  {
    "path": "HONEYBADGER/HIVE/clapper24.py",
    "size": 12715,
    "lines": 330,
    "source": "#!/usr/bin/env python3\n\"\"\"\nClapper24 - Film Style Timeline Visualization with Optimized Height\n\nThis script renders timeline entries in a film clapperboard style with a continuous\nprompt layout and optimized header height (200px) to ensure all content fits properly,\neven with the longest entries in the dataset.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions - optimized based on testing\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 200  # Optimized for longest entries\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (50, 50, 50)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_clapper24')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\"\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\", \n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef wrap_text(text, font, max_width):\n    \"\"\"Wrap text to fit within a given width.\"\"\"\n    if not text:\n        return []\n        \n    words = text.split()\n    lines = []\n    current_line = []\n    \n    for word in words:\n        # Try adding the word to the current line\n        test_line = \" \".join(current_line + [word])\n        text_width = font.getbbox(test_line)[2]\n        \n        if text_width <= max_width:\n            current_line.append(word)\n        else:\n            # Start a new line\n            lines.append(\" \".join(current_line))\n            current_line = [word]\n    \n    # Add the last line\n    if current_line:\n        lines.append(\" \".join(current_line))\n    \n    return lines\n\ndef build_full_prompt(entry):\n    \"\"\"Build a single continuous prompt from entry components.\"\"\"\n    syntagma_type = entry.get('syntagmaType', '')\n    cineosis_func = entry.get('cineosisFunction', '')\n    operative_ekphrasis = entry.get('operativeEkphrasis', '')\n    style_conditioning = entry.get('styleConditioning', '')\n    \n    # Remove any redundant \"(XX)\" from syntagma type\n    syntagma_cleaned = syntagma_type.split('(')[0].strip() if '(' in syntagma_type else syntagma_type\n    \n    # Build the continuous prompt\n    prompt_parts = []\n    if syntagma_cleaned:\n        prompt_parts.append(syntagma_cleaned)\n    if cineosis_func:\n        prompt_parts.append(cineosis_func)\n    if operative_ekphrasis:\n        prompt_parts.append(operative_ekphrasis)\n    \n    # Join the main prompt components\n    main_prompt = \". \".join(prompt_parts)\n    if main_prompt and not main_prompt.endswith('.'):\n        main_prompt += '.'\n    \n    # Add style conditioning as a separate component but still part of the unified prompt\n    if style_conditioning:\n        prompt = f\"{main_prompt} Style: {style_conditioning}\"\n    else:\n        prompt = main_prompt\n        \n    return prompt\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card in film clapperboard style with continuous prompt text.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH + 2*BORDER_WIDTH, TOTAL_HEIGHT + 2*BORDER_WIDTH), BLUE)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font = find_font(HEADER_FONT_PATH, 16)\n    body_font = find_font(BODY_FONT_PATH, 14)\n    label_font = find_font(HEADER_FONT_PATH, 14)\n    prompt_font = find_font(BODY_FONT_PATH, 16)\n    \n    # Calculate row heights for the header section\n    top_row_height = 30\n    second_row_height = 30\n    info_bar_height = 25\n    prompt_section_height = HEADER_HEIGHT - (top_row_height + second_row_height + info_bar_height)\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = f\"FRM: (---)\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), \"TT: 0 DESCRIPTIVE IMAGE\", font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height), (BASE_WIDTH, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line with proper wrapping for long content\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    line_text = f\"LINE: \\\"{line_content}\\\"\"\n    \n    # Handle long poem name and line content\n    poem_width = header_font.getbbox(poem_text)[2]\n    line_width = header_font.getbbox(line_text)[2]\n    \n    # Check if we need to wrap either element\n    if poem_width > 450 or line_width > 450 or (poem_width + line_width > BASE_WIDTH - 40):\n        # Need to stack them vertically\n        draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n        \n        # For very long line content, wrap it\n        if line_width > BASE_WIDTH - 20:\n            # Split into a prefix and the content for wrapping\n            line_prefix = \"LINE: \\\"\"\n            draw.text((10, top_row_height + 24), line_prefix, font=header_font, fill=WHITE)\n            \n            # Wrap the actual line content\n            wrapped_content = wrap_text(line_content, header_font, BASE_WIDTH - 80)\n            for i, line_part in enumerate(wrapped_content):\n                y_pos = top_row_height + 24 + (i * 16)\n                if i == len(wrapped_content) - 1:\n                    # Add closing quote to last line\n                    line_part += \"\\\"\"\n                draw.text((10 + header_font.getbbox(line_prefix)[2], y_pos), \n                         line_part, font=header_font, fill=WHITE)\n                \n            # Adjust second row height if needed for multiple line parts\n            if len(wrapped_content) > 1:\n                second_row_height = 30 + (len(wrapped_content) - 1) * 16\n        else:\n            # Line fits on one row\n            draw.text((10, top_row_height + 24), line_text, font=header_font, fill=WHITE)\n    else:\n        # Both fit horizontally\n        draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n        draw.text((500, top_row_height + 8), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height + second_row_height), (BASE_WIDTH, top_row_height + second_row_height)], fill=WHITE, width=1)\n    \n    # INFO BAR: AMBER BAR\n    info_bar_y = top_row_height + second_row_height\n    draw.rectangle([(0, info_bar_y), (BASE_WIDTH, info_bar_y + info_bar_height)], fill=AMBER)\n    \n    # Add title in the amber bar\n    draw.text((10, info_bar_y + 4), \"SYNTAGMA\", font=label_font, fill=BLACK)\n    \n    # Get abbreviated syntagma for the leading prefix\n    syntagma_type = entry.get('syntagmaType', '')\n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # PROMPT SECTION: Unified continuous prompt layout\n    prompt_y = info_bar_y + info_bar_height + 10\n    \n    # Default values for demonstration/example\n    prefix = \"DS\"\n    prompt_text = \"Mood Environment Stabilizer. A symbolic tableau rendered through chiaroscuro.\"\n    \n    # Use actual data if available\n    if entry:\n        prefix = syntagma_abbrev if syntagma_abbrev != \"??\" else \"DS\"\n        # Build the full continuous prompt from the entry components\n        prompt_text = build_full_prompt(entry)\n    \n    # Draw the prefix (syntagma abbreviation)\n    draw.text((10, prompt_y), prefix, font=header_font, fill=AMBER)\n    \n    # Draw the main prompt with wrapping - prefix width is about 30px\n    text_lines = wrap_text(prompt_text, prompt_font, BASE_WIDTH - 40)\n    for i, line in enumerate(text_lines):\n        draw.text((40, prompt_y + i * 22), line, font=prompt_font, fill=AMBER)\n    \n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # 16:9 Image area - Fill with dark gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=DARK_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=img_placeholder_font, fill=LIGHT_GRAY, anchor=\"mm\")\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (BORDER_WIDTH, BORDER_WIDTH))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_clapper.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    print(f\"Header height: {HEADER_HEIGHT}px (optimized for longest entries)\")\n    print(f\"Image height (16:9): {IMAGE_HEIGHT}px\")\n    \n    # Render first 10 entries as samples\n    render_all_entries(timeline, OUTPUT_DIR, 10)\n    \n    # Also render a demo card showing the layout\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"SH001\",\n        \"timestamp\": \"00:03:51\",\n        \"poem\": \"Out of Life\",\n        \"content\": \"What we've made-\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"A symbolic tableau rendered through chiaroscuro\",\n        \"styleConditioning\": \"High contrast black and white photography with deep shadows and bright highlights. Dramatic lighting with silhouettes and volumetric light effects.\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_clapper.png\")\n    render_card(demo_entry, demo_path)\n    \n    # Also render the extreme test case\n    print(\"Rendering extreme test case with longest entries\")\n    try:\n        test_entry_path = os.path.join(SCRIPT_DIR, 'longest_test_entry.json')\n        with open(test_entry_path, 'r') as f:\n            test_entry = json.load(f)\n            test_path = os.path.join(OUTPUT_DIR, \"extreme_test.png\")\n            render_card(test_entry, test_path)\n            print(f\"Rendered extreme test case: {test_path}\")\n    except Exception as e:\n        print(f\"Could not render extreme test case: {e}\")\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {BASE_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {BASE_WIDTH}x{TOTAL_HEIGHT}px\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_clapper.png",
      "demo_clapper.png",
      "longest_test_entry.json",
      "extreme_test.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions - optimized based on testing\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 200  # Optimized for longest entries\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (50, 50, 50)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ")\n    \n    # PROMPT SECTION: Unified continuous prompt layout\n    prompt_y = info_bar_y + info_bar_height + 10\n    \n    # Default values for demonstration/example\n    prefix = ",
      "\n        # Build the full continuous prompt from the entry components\n        prompt_text = build_full_prompt(entry)\n    \n    # Draw the prefix (syntagma abbreviation)\n    draw.text((10, prompt_y), prefix, font=header_font, fill=AMBER)\n    \n    # Draw the main prompt with wrapping - prefix width is about 30px\n    text_lines = wrap_text(prompt_text, prompt_font, BASE_WIDTH - 40)\n    for i, line in enumerate(text_lines):\n        draw.text((40, prompt_y + i * 22), line, font=prompt_font, fill=AMBER)\n    \n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # 16:9 Image area - Fill with dark gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=DARK_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), ",
      "Rendered {i+1}/{len(entries_to_render)} cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Clapper24 - Film Style Timeline Visualization with Optimized Height\n\nThis script renders timeline entries in a film clapperboard style with a continuous\nprompt layout and optimized header height (200px) to ensure all content fits properly,\neven with the longest entries in the dataset."
  },
  {
    "path": "HONEYBADGER/HIVE/clapper25.py",
    "size": 11848,
    "lines": 320,
    "source": "#!/usr/bin/env python3\n\"\"\"\nClapper25 - Film Style Timeline Visualization with Removed Amber Bar\n\nThis script renders timeline entries in a film clapperboard style with the amber bar removed,\ngiving more space for the line content. The syntagma abbreviation is now directly\nincorporated with the prompt text.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 200  # Optimized for longest entries\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (50, 50, 50)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_clapper25')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\"\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\", \n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef wrap_text(text, font, max_width):\n    \"\"\"Wrap text to fit within a given width.\"\"\"\n    if not text:\n        return []\n        \n    words = text.split()\n    lines = []\n    current_line = []\n    \n    for word in words:\n        # Try adding the word to the current line\n        test_line = \" \".join(current_line + [word])\n        text_width = font.getbbox(test_line)[2]\n        \n        if text_width <= max_width:\n            current_line.append(word)\n        else:\n            # Start a new line\n            lines.append(\" \".join(current_line))\n            current_line = [word]\n    \n    # Add the last line\n    if current_line:\n        lines.append(\" \".join(current_line))\n    \n    return lines\n\ndef build_full_prompt(entry):\n    \"\"\"Build a single continuous prompt from entry components.\"\"\"\n    syntagma_type = entry.get('syntagmaType', '')\n    cineosis_func = entry.get('cineosisFunction', '')\n    operative_ekphrasis = entry.get('operativeEkphrasis', '')\n    style_conditioning = entry.get('styleConditioning', '')\n    \n    # Remove any redundant \"(XX)\" from syntagma type\n    syntagma_cleaned = syntagma_type.split('(')[0].strip() if '(' in syntagma_type else syntagma_type\n    \n    # Build the continuous prompt\n    prompt_parts = []\n    if syntagma_cleaned:\n        prompt_parts.append(syntagma_cleaned)\n    if cineosis_func:\n        prompt_parts.append(cineosis_func)\n    if operative_ekphrasis:\n        prompt_parts.append(operative_ekphrasis)\n    \n    # Join the main prompt components\n    main_prompt = \". \".join(prompt_parts)\n    if main_prompt and not main_prompt.endswith('.'):\n        main_prompt += '.'\n    \n    # Add style conditioning as a separate component but still part of the unified prompt\n    if style_conditioning:\n        prompt = f\"{main_prompt} Style: {style_conditioning}\"\n    else:\n        prompt = main_prompt\n        \n    return prompt\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card in film clapperboard style with continuous prompt text.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH + 2*BORDER_WIDTH, TOTAL_HEIGHT + 2*BORDER_WIDTH), BLUE)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font = find_font(HEADER_FONT_PATH, 16)\n    body_font = find_font(BODY_FONT_PATH, 14)\n    prompt_font = find_font(BODY_FONT_PATH, 16)\n    \n    # Calculate row heights for the header section\n    top_row_height = 30\n    line_content_height = 50  # More space for line content\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = f\"FRAME: (---)\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), \"IMAGE: 0 DESCRIPTIVE IMAGE\", font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height), (BASE_WIDTH, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line with proper wrapping for long content\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    \n    # Calculate poem width\n    poem_width = header_font.getbbox(poem_text)[2]\n    \n    # Draw poem name\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # LINE CONTENT: Highlighted with amber to give it emphasis\n    # Calculate start position for line content (after poem with some spacing)\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)  # Either after poem or at half width\n    \n    # Draw the line content with amber highlighting\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    # Draw prefix in white\n    draw.text((line_start_x, top_row_height + 8), line_prefix, font=header_font, fill=WHITE)\n    \n    # Available width for line content\n    available_width = BASE_WIDTH - line_start_x - line_prefix_width - 20\n    \n    # Wrap the line content if needed\n    wrapped_content = wrap_text(line_content, header_font, available_width)\n    \n    for i, line_part in enumerate(wrapped_content):\n        y_pos = top_row_height + 8 + (i * 16)\n        \n        # Add closing quote to last line\n        if i == len(wrapped_content) - 1:\n            line_part += \"\\\"\"\n        \n        # Draw the line content in amber\n        draw.text((line_start_x + line_prefix_width, y_pos), line_part, font=header_font, fill=AMBER)\n    \n    # Calculate actual height used by the line content\n    line_content_actual_height = 20 + max(1, len(wrapped_content)) * 16  # Base + line height\n    \n    # Draw horizontal divider after line content\n    divider_y = top_row_height + line_content_actual_height\n    draw.line([(0, divider_y), (BASE_WIDTH, divider_y)], fill=WHITE, width=1)\n    \n    # PROMPT SECTION directly below line content\n    prompt_y = divider_y + 10\n    \n    # Get abbreviated syntagma for the leading prefix\n    syntagma_type = entry.get('syntagmaType', '')\n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # Default values for demonstration/example\n    prefix = \"DS\"\n    prompt_text = \"Mood Environment Stabilizer. A symbolic tableau rendered through chiaroscuro.\"\n    \n    # Use actual data if available\n    if entry:\n        prefix = syntagma_abbrev if syntagma_abbrev != \"??\" else \"DS\"\n        # Build the full continuous prompt from the entry components\n        prompt_text = build_full_prompt(entry)\n    \n    # Draw the prefix (syntagma abbreviation)\n    draw.text((10, prompt_y), prefix, font=header_font, fill=WHITE)\n    \n    # Draw the main prompt with wrapping - prefix width is about 30px\n    text_lines = wrap_text(prompt_text, prompt_font, BASE_WIDTH - 40)\n    for i, line in enumerate(text_lines):\n        draw.text((40, prompt_y + i * 22), line, font=prompt_font, fill=AMBER)\n    \n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # 16:9 Image area - Fill with dark gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=DARK_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=img_placeholder_font, fill=LIGHT_GRAY, anchor=\"mm\")\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (BORDER_WIDTH, BORDER_WIDTH))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_clapper.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    print(f\"Header height: {HEADER_HEIGHT}px\")\n    print(f\"Image height (16:9): {IMAGE_HEIGHT}px\")\n    \n    # Render first 10 entries as samples\n    render_all_entries(timeline, OUTPUT_DIR, 10)\n    \n    # Also render a demo card showing the layout\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"SH001\",\n        \"timestamp\": \"00:03:51\",\n        \"poem\": \"Out of Life\",\n        \"content\": \"What we've made-\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"A symbolic tableau rendered through chiaroscuro\",\n        \"styleConditioning\": \"High contrast black and white photography with deep shadows and bright highlights. Dramatic lighting with silhouettes and volumetric light effects.\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_clapper.png\")\n    render_card(demo_entry, demo_path)\n    \n    # Also render the extreme test case\n    print(\"Rendering extreme test case with longest entries\")\n    try:\n        test_entry_path = os.path.join(SCRIPT_DIR, 'longest_test_entry.json')\n        with open(test_entry_path, 'r') as f:\n            test_entry = json.load(f)\n            test_path = os.path.join(OUTPUT_DIR, \"extreme_test.png\")\n            render_card(test_entry, test_path)\n            print(f\"Rendered extreme test case: {test_path}\")\n    except Exception as e:\n        print(f\"Could not render extreme test case: {e}\")\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {BASE_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {BASE_WIDTH}x{TOTAL_HEIGHT}px\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_clapper.png",
      "demo_clapper.png",
      "longest_test_entry.json",
      "extreme_test.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 200  # Optimized for longest entries\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (50, 50, 50)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ")\n    \n    # Calculate poem width\n    poem_width = header_font.getbbox(poem_text)[2]\n    \n    # Draw poem name\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # LINE CONTENT: Highlighted with amber to give it emphasis\n    # Calculate start position for line content (after poem with some spacing)\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)  # Either after poem or at half width\n    \n    # Draw the line content with amber highlighting\n    line_prefix = ",
      ")\n    \n    # Default values for demonstration/example\n    prefix = ",
      "\n        # Build the full continuous prompt from the entry components\n        prompt_text = build_full_prompt(entry)\n    \n    # Draw the prefix (syntagma abbreviation)\n    draw.text((10, prompt_y), prefix, font=header_font, fill=WHITE)\n    \n    # Draw the main prompt with wrapping - prefix width is about 30px\n    text_lines = wrap_text(prompt_text, prompt_font, BASE_WIDTH - 40)\n    for i, line in enumerate(text_lines):\n        draw.text((40, prompt_y + i * 22), line, font=prompt_font, fill=AMBER)\n    \n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # 16:9 Image area - Fill with dark gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=DARK_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), ",
      "Rendered {i+1}/{len(entries_to_render)} cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Clapper25 - Film Style Timeline Visualization with Removed Amber Bar\n\nThis script renders timeline entries in a film clapperboard style with the amber bar removed,\ngiving more space for the line content. The syntagma abbreviation is now directly\nincorporated with the prompt text."
  },
  {
    "path": "HONEYBADGER/HIVE/clapper21.py",
    "size": 9929,
    "lines": 269,
    "source": "#!/usr/bin/env python3\n\"\"\"\nClapper21 - Film Style Timeline Visualization with 16:9 Image Area\n\nThis script renders timeline entries in a film clapperboard style with high contrast\ntext on dark background, emphasizing technical metadata and visual components.\nIncludes a 16:9 image area for visual content representation.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 160  # Height for metadata section\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_clapper21')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths (system fonts - we'll use these initially)\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\"\n\n# Define glyphs for different types\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card in film clapperboard style with 16:9 image area.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH + 2*BORDER_WIDTH, TOTAL_HEIGHT + 2*BORDER_WIDTH), BLUE)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font = find_font(HEADER_FONT_PATH, 16)\n    body_font = find_font(BODY_FONT_PATH, 14)\n    large_font = find_font(HEADER_FONT_PATH, 18)\n    \n    # Calculate row heights for the header section\n    top_row_height = 30\n    second_row_height = 30\n    third_row_height = 40\n    content_row_height = HEADER_HEIGHT - (top_row_height + second_row_height + third_row_height)\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = f\"FRM: (---)\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), \"TT: 0 DESCRIPTIVE IMAGE\", font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height), (BASE_WIDTH, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_content = entry.get('content', '---')\n    # Truncate line content if too long\n    if len(line_content) > 60:\n        line_content = line_content[:57] + \"...\"\n    line_text = f\"LINE: \\\"{line_content}\\\"\"\n    \n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    draw.text((500, top_row_height + 8), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height + second_row_height), (BASE_WIDTH, top_row_height + second_row_height)], fill=WHITE, width=1)\n    \n    # THIRD ROW: Column headers - AMBER BAR\n    header_bar_y = top_row_height + second_row_height\n    draw.rectangle([(0, header_bar_y), (BASE_WIDTH, header_bar_y + third_row_height)], fill=AMBER)\n    \n    # Add column headers in the amber bar\n    col_width = BASE_WIDTH // 3\n    draw.text((10, header_bar_y + 5), \"SYNTAGMA\", font=header_font, fill=BLACK)\n    draw.text((col_width, header_bar_y + 5), \"CINEOSIS FUNCTION\", font=header_font, fill=BLACK)\n    draw.text((2*col_width, header_bar_y + 5), \"OPERATIVE EKPHRASIS\", font=header_font, fill=BLACK)\n    \n    # Get the types and prompt components\n    syntagma_type = entry.get('syntagmaType', '---')\n    cineosis_func = entry.get('cineosisFunction', '---')\n    operative_ekphrasis = entry.get('operativeEkphrasis', '')\n    \n    # Abbreviate syntagma type\n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # CONTENT ROW: Display the content with bullet points\n    content_y = header_bar_y + third_row_height + 15\n    \n    syntagma_text = f\"DS\"  # Hardcode DS to match image\n    cineosis_text = f\"\u00b7 Mood Environment Stabilizer\"  # Hardcode to match image\n    ekphrasis_text = f\"\u00b7 A symbolic tableau rendered through chiaroscuro\"  # Hardcode to match image\n    \n    # If we have actual data, use it instead of hardcoded values\n    if syntagma_type != \"---\":\n        syntagma_text = f\"{syntagma_abbrev}\"\n    if cineosis_func != \"---\":\n        cineosis_text = f\"\u00b7 {cineosis_func}\"\n    if operative_ekphrasis:\n        ekphrasis_text = f\"\u00b7 {operative_ekphrasis}\"\n    \n    # Draw the content\n    draw.text((10, content_y), syntagma_text, font=large_font, fill=AMBER)\n    draw.text((40, content_y), cineosis_text, font=large_font, fill=AMBER)\n    draw.text((350, content_y), ekphrasis_text, font=large_font, fill=AMBER)\n\n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # 16:9 Image area - Fill with light gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=img_placeholder_font, fill=BLACK, anchor=\"mm\")\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (BORDER_WIDTH, BORDER_WIDTH))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_clapper.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    print(f\"Header height: {HEADER_HEIGHT}px\")\n    print(f\"Image height (16:9): {IMAGE_HEIGHT}px\")\n    \n    # Render first 10 entries as samples\n    render_all_entries(timeline, OUTPUT_DIR, 10)\n    \n    # Also render a demo card showing the layout\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"SH001\",\n        \"timestamp\": \"00:03:51\",\n        \"poem\": \"Out of Life\",\n        \"content\": \"What we've made-\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"A symbolic tableau rendered through chiaroscuro\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_clapper.png\")\n    render_card(demo_entry, demo_path)\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {BASE_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {BASE_WIDTH}x{TOTAL_HEIGHT}px\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_clapper.png",
      "demo_clapper.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 160  # Height for metadata section\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      "\n    \n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    draw.text((500, top_row_height + 8), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height + second_row_height), (BASE_WIDTH, top_row_height + second_row_height)], fill=WHITE, width=1)\n    \n    # THIRD ROW: Column headers - AMBER BAR\n    header_bar_y = top_row_height + second_row_height\n    draw.rectangle([(0, header_bar_y), (BASE_WIDTH, header_bar_y + third_row_height)], fill=AMBER)\n    \n    # Add column headers in the amber bar\n    col_width = BASE_WIDTH // 3\n    draw.text((10, header_bar_y + 5), ",
      "\n    \n    # Draw the content\n    draw.text((10, content_y), syntagma_text, font=large_font, fill=AMBER)\n    draw.text((40, content_y), cineosis_text, font=large_font, fill=AMBER)\n    draw.text((350, content_y), ekphrasis_text, font=large_font, fill=AMBER)\n\n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # 16:9 Image area - Fill with light gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), ",
      "Rendered {i+1}/{len(entries_to_render)} cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Clapper21 - Film Style Timeline Visualization with 16:9 Image Area\n\nThis script renders timeline entries in a film clapperboard style with high contrast\ntext on dark background, emphasizing technical metadata and visual components.\nIncludes a 16:9 image area for visual content representation."
  },
  {
    "path": "HONEYBADGER/HIVE/render_cards_compact.py",
    "size": 10870,
    "lines": 289,
    "source": "#!/usr/bin/env python3\n\"\"\"\nRender Timeline Data Cards as Clapperboard-style Headers\n\nThis script renders timeline entries with a compact clapperboard-style header\nthat sits above a 16:9 image area, creating an overall 3:2 aspect ratio.\nAll text fits entirely within the black header area.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\nASPECT_3_2 = 3/2    # Overall frame\n\n# Calculate dimensions to maintain proper aspect ratios\nBASE_WIDTH = 1200\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height of the 16:9 image area\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)   # Total height for 3:2 aspect ratio\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT   # Height of the header area\n\n# Card dimensions\nCARD_WIDTH = BASE_WIDTH\nCARD_HEIGHT = TOTAL_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nDARK_GRAY = (40, 40, 40)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\nAMBER = (255, 191, 0)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_compact')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths (system fonts - we'll use these initially)\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\" \n\n# Define glyphs for different types\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card with a clapperboard-style header above a 16:9 image area.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (CARD_WIDTH+8, CARD_HEIGHT+8), BLUE_BORDER)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (CARD_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts - much smaller sizes to fit in the header\n    header_font = find_font(HEADER_FONT_PATH, 18)  # Reduced from 32/28\n    body_font = find_font(BODY_FONT_PATH, 14)      # Reduced from 24\n    small_font = find_font(BODY_FONT_PATH, 12)     # New smaller font\n    symbol_font = find_font(SYMBOL_FONT_PATH, 16)  # Reduced from 28\n    \n    # Fill the header area black\n    draw.rectangle([(0, 0), (CARD_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    \n    # Fill the image area light gray (placeholder)\n    draw.rectangle([(0, HEADER_HEIGHT), (CARD_WIDTH, CARD_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Add text to the image area (placeholder)\n    draw.text((CARD_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=header_font, fill=BLACK, anchor=\"mm\")\n    \n    # Draw horizontal dividing line between header and image\n    draw.line([(0, HEADER_HEIGHT-1), (CARD_WIDTH, HEADER_HEIGHT-1)], fill=WHITE, width=2)\n    \n    # Layout header as a compact clapperboard - everything must fit in HEADER_HEIGHT\n    \n    # Calculate row heights - everything must be proportional to fit in header\n    row_height = HEADER_HEIGHT / 4  # 4 rows in header\n    padding = 4  # reduced padding\n    \n    # Top row - ID and timestamp\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = f\"FRM: (---)\"\n    \n    row1_y = padding\n    draw.text((padding*2, row1_y), id_text, font=header_font, fill=WHITE)\n    draw.text((CARD_WIDTH//3, row1_y), time_text, font=header_font, fill=WHITE)\n    draw.text((2*CARD_WIDTH//3, row1_y), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, row_height), (CARD_WIDTH, row_height)], fill=WHITE, width=1)\n    \n    # Second row - Poem and Line\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_content = entry.get('content', '---')\n    # Truncate line content if too long\n    if len(line_content) > 60:\n        line_content = line_content[:57] + \"...\"\n    line_text = f\"LINE: \\\"{line_content}\\\"\"\n    \n    row2_y = row_height + padding\n    draw.text((padding*2, row2_y), poem_text, font=header_font, fill=WHITE)\n    draw.text((CARD_WIDTH//2, row2_y), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, 2*row_height), (CARD_WIDTH, 2*row_height)], fill=WHITE, width=1)\n    \n    # Third row - Syntagma, Image Type, and Cineosis Function with equal spacing\n    row3_y = 2*row_height + padding\n    \n    # Get the types and glyphs\n    syntagma_type = entry.get('syntagmaType', '---')\n    image_type = entry.get('imageType', '---')\n    cineosis_func = entry.get('cineosisFunction', '---')\n    \n    syntagma_glyph = get_glyph(SYNTAGMA_GLYPHS, syntagma_type, \"\u25a1\")\n    image_glyph = get_glyph(IMAGE_GLYPHS, image_type, \"\u25a1\")\n    cineosis_glyph = get_glyph(CINEOSIS_GLYPHS, cineosis_func, \"\u25ef\")\n    \n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # Equal spacing for the three elements\n    col_width = CARD_WIDTH // 3\n    \n    # Syntagma\n    draw.text((padding*2, row3_y), f\"SY: {syntagma_glyph} {syntagma_abbrev}\", font=header_font, fill=WHITE)\n    \n    # Image Type - abbreviated to first word\n    img_label = image_type.split('-')[0].upper() if '-' in image_type else image_type.upper()\n    draw.text((col_width, row3_y), f\"IT: {image_glyph} {img_label}\", font=header_font, fill=WHITE)\n    \n    # Cineosis - abbreviated\n    cf_label = \" \".join([word[:3].upper() for word in cineosis_func.split(' ')[:2]])\n    draw.text((2*col_width, row3_y), f\"CF: {cineosis_glyph} {cf_label}\", font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, 3*row_height), (CARD_WIDTH, 3*row_height)], fill=WHITE, width=1)\n    \n    # Fourth row - Raw prompt (abbreviated to fit)\n    row4_y = 3*row_height + padding\n    \n    # Display prompt label and content directly, without background rectangle\n    prompt_label = \"PROMPT:\"\n    draw.text((padding*2, row4_y), prompt_label, font=header_font, fill=WHITE)\n    \n    # Raw prompt content - use full available space\n    # Include more of the operative ekphrasis since we have more vertical space now\n    prompt_text = f\"{syntagma_abbrev} \u00b7 {cineosis_func} \u00b7 {entry.get('operativeEkphrasis', '')}\"\n    \n    # Wrap text to fit available width\n    wrapped_lines = textwrap.wrap(prompt_text, width=95)  # Wider wrapping for more text\n    \n    # Display wrapped prompt text with proper spacing\n    prompt_start_x = padding*2 + 80  # Offset from left to align after the label\n    for i, line in enumerate(wrapped_lines[:2]):  # Show up to 2 lines of wrapped text\n        current_y = row4_y + (i * 18)  # Space between lines\n        if current_y < HEADER_HEIGHT - 5:  # Stay within header bounds\n            draw.text((prompt_start_x, current_y), line, font=body_font, fill=WHITE)\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (4, 4))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_card.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    \n    # Render first 10 entries as samples\n    render_all_entries(timeline, OUTPUT_DIR, 10)\n    \n    # Also render a demo card showing the layout with dimensions\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"DEMO\",\n        \"timestamp\": \"00:00:00\",\n        \"poem\": \"Layout Example\",\n        \"content\": \"3:2 overall with 16:9 image\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"This is a demo card showing the layout structure with proper aspect ratios\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_layout.png\")\n    render_card(demo_entry, demo_path)\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {CARD_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {CARD_WIDTH}x{CARD_HEIGHT}px (3:2)\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_card.png",
      "demo_layout.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\nASPECT_3_2 = 3/2    # Overall frame\n\n# Calculate dimensions to maintain proper aspect ratios\nBASE_WIDTH = 1200\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height of the 16:9 image area\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)   # Total height for 3:2 aspect ratio\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT   # Height of the header area\n\n# Card dimensions\nCARD_WIDTH = BASE_WIDTH\nCARD_HEIGHT = TOTAL_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nDARK_GRAY = (40, 40, 40)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\nAMBER = (255, 191, 0)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ", (CARD_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts - much smaller sizes to fit in the header\n    header_font = find_font(HEADER_FONT_PATH, 18)  # Reduced from 32/28\n    body_font = find_font(BODY_FONT_PATH, 14)      # Reduced from 24\n    small_font = find_font(BODY_FONT_PATH, 12)     # New smaller font\n    symbol_font = find_font(SYMBOL_FONT_PATH, 16)  # Reduced from 28\n    \n    # Fill the header area black\n    draw.rectangle([(0, 0), (CARD_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    \n    # Fill the image area light gray (placeholder)\n    draw.rectangle([(0, HEADER_HEIGHT), (CARD_WIDTH, CARD_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Add text to the image area (placeholder)\n    draw.text((CARD_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), ",
      ")\n    \n    # Draw horizontal dividing line between header and image\n    draw.line([(0, HEADER_HEIGHT-1), (CARD_WIDTH, HEADER_HEIGHT-1)], fill=WHITE, width=2)\n    \n    # Layout header as a compact clapperboard - everything must fit in HEADER_HEIGHT\n    \n    # Calculate row heights - everything must be proportional to fit in header\n    row_height = HEADER_HEIGHT / 4  # 4 rows in header\n    padding = 4  # reduced padding\n    \n    # Top row - ID and timestamp\n    id_text = f",
      "\n    \n    row1_y = padding\n    draw.text((padding*2, row1_y), id_text, font=header_font, fill=WHITE)\n    draw.text((CARD_WIDTH//3, row1_y), time_text, font=header_font, fill=WHITE)\n    draw.text((2*CARD_WIDTH//3, row1_y), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, row_height), (CARD_WIDTH, row_height)], fill=WHITE, width=1)\n    \n    # Second row - Poem and Line\n    poem_text = f",
      "\n    \n    row2_y = row_height + padding\n    draw.text((padding*2, row2_y), poem_text, font=header_font, fill=WHITE)\n    draw.text((CARD_WIDTH//2, row2_y), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, 2*row_height), (CARD_WIDTH, 2*row_height)], fill=WHITE, width=1)\n    \n    # Third row - Syntagma, Image Type, and Cineosis Function with equal spacing\n    row3_y = 2*row_height + padding\n    \n    # Get the types and glyphs\n    syntagma_type = entry.get(",
      ")\n    \n    # Equal spacing for the three elements\n    col_width = CARD_WIDTH // 3\n    \n    # Syntagma\n    draw.text((padding*2, row3_y), f",
      "Rendered {i+1}/{len(entries_to_render)} cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Render Timeline Data Cards as Clapperboard-style Headers\n\nThis script renders timeline entries with a compact clapperboard-style header\nthat sits above a 16:9 image area, creating an overall 3:2 aspect ratio.\nAll text fits entirely within the black header area."
  },
  {
    "path": "HONEYBADGER/HIVE/default_font_glyph_test.py",
    "size": 6171,
    "lines": 141,
    "source": "#!/usr/bin/env python3\n\"\"\"\nDefault Font Glyph Tester - Compares Courier New with Pillow's default font.\n\"\"\"\n\nimport os\nimport re\nfrom PIL import Image, ImageDraw, ImageFont\n\n# --- Configuration ---\nOUTPUT_IMAGE_NAME = \"default_font_glyph_comparison.png\"\nIMAGE_WIDTH = 800\nLINE_HEIGHT = 20       # For font name labels\nGLYPH_LINE_HEIGHT = 18 # For rendering glyph lines\nGLYPH_FONT_SIZE = 12\nLABEL_FONT_SIZE = 14\nTOP_MARGIN = 20\nLEFT_MARGIN = 20\nSECTION_SPACING = 25 # Space between each font test section\n\nFONTS_TO_TEST_INFO = [\n    {\"name\": \"Courier New\", \"type\": \"system\"},\n    {\"name\": \"Pillow Default\", \"type\": \"pillow_default\"}\n]\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nGRAY = (150, 150, 150)\nRED = (255, 100, 100)\n\n# --- Genome Snippet Loading Logic (from clapper_genome_exp.py) ---\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nGENOME_REPORT_PATH = os.path.join(os.path.dirname(BASE_DIR), \"NOTATION\", \"SYMBOLIC_GENOME_REPORT.md\")\nPRELOADED_GENOME_LINES = []\n\ndef load_fixed_genome_report_snippet():\n    global PRELOADED_GENOME_LINES\n    try:\n        with open(GENOME_REPORT_PATH, 'r', encoding='utf-8') as f:\n            content = f.read()\n        match = re.search(r\"\\u250c[\\u2500\\u252c]+\\u2510\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 S\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 I\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 C\\s*\\n\\s*\\u2514\", content, re.MULTILINE)\n        if match:\n            PRELOADED_GENOME_LINES = [\n                match.group(1).strip() + \" S\", \n                match.group(2).strip() + \" I\", \n                match.group(3).strip() + \" C\"\n            ]\n        else:\n            PRELOADED_GENOME_LINES = [\"Genome report snippet not found in MD file.\", \"\", \"\"]\n            print(f\"Warning: Could not find genome block in {GENOME_REPORT_PATH}\")\n    except FileNotFoundError:\n        PRELOADED_GENOME_LINES = [f\"{os.path.basename(GENOME_REPORT_PATH)} not found.\", \"\", \"\"]\n        print(f\"Error: {GENOME_REPORT_PATH} not found.\")\n    except Exception as e:\n        PRELOADED_GENOME_LINES = [f\"Error loading report: {e}\", \"\", \"\"]\n        print(f\"Error loading genome report: {e}\")\n\ndef get_system_font(font_name, size):\n    \"\"\"Attempts to load a system font by name.\"\"\"\n    try:\n        return ImageFont.truetype(font_name + \".ttf\", size)\n    except IOError:\n        try:\n            return ImageFont.truetype(font_name + \".otf\", size)\n        except IOError:\n            # Try with common bold variations if plain not found (less likely for Courier New)\n            try: return ImageFont.truetype(font_name + \" Bold.ttf\", size)\n            except IOError: \n                try: return ImageFont.truetype(font_name + \" Bold.otf\", size)\n                except IOError: return None\n\ndef main():\n    load_fixed_genome_report_snippet()\n\n    if not PRELOADED_GENOME_LINES or \"not found\" in PRELOADED_GENOME_LINES[0]:\n        print(\"Could not load genome lines for testing. Aborting.\")\n        return\n\n    num_glyph_lines = len(PRELOADED_GENOME_LINES)\n    estimated_section_height = LINE_HEIGHT + (num_glyph_lines * GLYPH_LINE_HEIGHT) + SECTION_SPACING\n    image_height = TOP_MARGIN * 2 + len(FONTS_TO_TEST_INFO) * estimated_section_height\n\n    img = Image.new('RGB', (IMAGE_WIDTH, image_height), WHITE)\n    draw = ImageDraw.Draw(img)\n    \n    try: default_label_font = ImageFont.load_default(size=LABEL_FONT_SIZE) \n    except TypeError: default_label_font = ImageFont.load_default()\n\n    current_y = TOP_MARGIN\n\n    for font_info in FONTS_TO_TEST_INFO:\n        font_display_name = font_info[\"name\"]\n        font_type = font_info[\"type\"]\n\n        draw.text((LEFT_MARGIN, current_y), f\"Font: {font_display_name}\", font=default_label_font, fill=BLACK)\n        current_y += LINE_HEIGHT\n\n        glyph_font = None\n        if font_type == \"system\":\n            glyph_font = get_system_font(font_info[\"name\"], GLYPH_FONT_SIZE)\n        elif font_type == \"pillow_default\":\n            try: glyph_font = ImageFont.load_default(size=GLYPH_FONT_SIZE)\n            except TypeError: glyph_font = ImageFont.load_default() # Older Pillow\n\n        if glyph_font:\n            for line_idx, line in enumerate(PRELOADED_GENOME_LINES):\n                try:\n                    draw.text((LEFT_MARGIN + 10, current_y), line, font=glyph_font, fill=BLACK)\n                except UnicodeEncodeError:\n                    error_message = \"Error: Cannot render (UnicodeEncodeError)\"\n                    # Use default_label_font for the error message as glyph_font might be the problematic one\n                    draw.text((LEFT_MARGIN + 10, current_y), error_message, font=default_label_font, fill=RED)\n                    # If it's the first line of glyphs for this font, print a console warning too\n                    if line_idx == 0 and font_type == \"pillow_default\":\n                        print(f\"Note: Pillow default font failed to render glyphs due to UnicodeEncodeError. This is expected.\")\n                except Exception as e:\n                    # Catch any other potential drawing errors with this font\n                    error_message = f\"Error: {type(e).__name__}\"\n                    draw.text((LEFT_MARGIN + 10, current_y), error_message, font=default_label_font, fill=RED)\n                    if line_idx == 0 and font_type == \"pillow_default\":\n                        print(f\"Note: Pillow default font encountered an error: {e}\")\n                current_y += GLYPH_LINE_HEIGHT\n        else:\n            draw.text((LEFT_MARGIN + 10, current_y), f\"{font_display_name} not found on system.\", font=default_label_font, fill=RED)\n            current_y += GLYPH_LINE_HEIGHT * num_glyph_lines # Keep spacing consistent\n        \n        current_y += SECTION_SPACING\n        if font_info != FONTS_TO_TEST_INFO[-1]: # Don't draw line after last section\n            draw.line([(LEFT_MARGIN, current_y - SECTION_SPACING//2), (IMAGE_WIDTH - LEFT_MARGIN, current_y - SECTION_SPACING//2)], fill=GRAY, width=1)\n\n    output_path = os.path.join(BASE_DIR, OUTPUT_IMAGE_NAME)\n    try:\n        img.save(output_path)\n        print(f\"Default font glyph comparison sheet saved to: {output_path}\")\n    except Exception as e:\n        print(f\"Error saving image: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "default_font_glyph_comparison.png",
      "t draw line after last section\n            draw.line([(LEFT_MARGIN, current_y - SECTION_SPACING//2), (IMAGE_WIDTH - LEFT_MARGIN, current_y - SECTION_SPACING//2)], fill=GRAY, width=1)\n\n    output_path = os.path.join(BASE_DIR, OUTPUT_IMAGE_NAME)\n    try:\n        img.save(output_path)\n        print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "PIL"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Default Font Glyph Tester - Compares Courier New with Pillow's default font."
  },
  {
    "path": "HONEYBADGER/HIVE/menlo_event_pause_candidates_test.py",
    "size": 5534,
    "lines": 124,
    "source": "from PIL import Image, ImageDraw, ImageFont\nimport os\n\n# --- Configuration ---\nOUTPUT_IMAGE_NAME = \"menlo_event_pause_candidates.png\"\nOUTPUT_DIR = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/\"\n\nFONT_NAME_PRIMARY = \"Menlo\"\nFONT_NAME_FALLBACK = \"Courier New\"\nFONT_SIZE = 24\nLABEL_FONT_SIZE = 12\n\nPADDING = 20\nGLYPH_SPACING = 15 # Space between glyph and its label\nLINE_HEIGHT = 40\n\nBACKGROUND_COLOR = (240, 240, 240)  # Light gray\nTEXT_COLOR = (20, 20, 20)        # Dark gray\nLABEL_COLOR = (100, 100, 100)    # Medium gray\nERROR_COLOR = (255, 0, 0)        # Red\n\nORIGINAL_GLYPH = {'char': '\\u2016', 'label': '\u2016 DOUBLE VERTICAL LINE (U+2016) - Original'}\nCANDIDATE_GLYPHS = [\n    {'char': '\\u275A', 'label': '\u275a HEAVY VERTICAL BAR (U+275A)'},\n    {'char': '\\u2758', 'label': '\u2758 LIGHT VERTICAL BAR (U+2758)'},\n    {'char': '\\u2395', 'label': '\u2395 APL FUNCTIONAL SYMBOL QUAD (U+2395)'},\n    {'char': '\\u233F', 'label': '\u233f APL FUNCTIONAL SYMBOL SLASH BAR (U+233F)'},\n    {'char': '\\u2999', 'label': '\u2999 DOTTED FENCE (U+2999)'},\n    {'char': '\\u29D7', 'label': '\u29d7 BLACK HOURGLASS (U+29D7)'}\n]\n\nALL_GLYPHS_TO_TEST = [ORIGINAL_GLYPH] + CANDIDATE_GLYPHS\n\ndef get_font(font_name, fallback_name, size):\n    try:\n        return ImageFont.truetype(font_name, size)\n    except IOError:\n        print(f\"Warning: Font '{font_name}' not found.\")\n        try:\n            return ImageFont.truetype(fallback_name, size)\n        except IOError:\n            print(f\"Warning: Fallback font '{fallback_name}' not found. Using Pillow's default font.\")\n            return ImageFont.load_default()\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    output_path = os.path.join(OUTPUT_DIR, OUTPUT_IMAGE_NAME)\n\n    glyph_font = get_font(FONT_NAME_PRIMARY, FONT_NAME_FALLBACK, FONT_SIZE)\n    label_font = get_font(FONT_NAME_PRIMARY, FONT_NAME_FALLBACK, LABEL_FONT_SIZE)\n\n    # Calculate image dimensions\n    num_glyphs = len(ALL_GLYPHS_TO_TEST)\n    img_height = PADDING * 2 + num_glyphs * LINE_HEIGHT\n    max_label_width = 0\n    # All candidates are now single characters, so FONT_SIZE * 1.5 or * 2 should be enough for glyph area\n    fixed_glyph_area_width = int(FONT_SIZE * 1.5) \n\n    dummy_img = Image.new(\"RGB\", (1, 1))\n    draw_context = ImageDraw.Draw(dummy_img)\n    for item in ALL_GLYPHS_TO_TEST:\n        try:\n            bbox = draw_context.textbbox((0, 0), item['label'], font=label_font, anchor=\"lt\")\n            max_label_width = max(max_label_width, bbox[2] - bbox[0])\n        except UnicodeEncodeError:\n            pass # Should not happen for ASCII labels\n\n    img_width = PADDING * 2 + fixed_glyph_area_width + GLYPH_SPACING + max_label_width\n\n    image = Image.new(\"RGB\", (int(img_width), int(img_height)), BACKGROUND_COLOR)\n    draw = ImageDraw.Draw(image)\n\n    current_y = PADDING  # Top of the current line's allocated slot\n\n    for item in ALL_GLYPHS_TO_TEST:\n        glyph_char = item['char']\n        glyph_label = item['label']\n\n        # --- Draw Glyph ---\n        try:\n            glyph_bbox_origin = draw.textbbox((0, 0), glyph_char, font=glyph_font, anchor=\"lt\")\n            glyph_actual_width = glyph_bbox_origin[2] - glyph_bbox_origin[0]\n            glyph_actual_height = glyph_bbox_origin[3] - glyph_bbox_origin[1]\n\n            draw_glyph_top_y = current_y + (LINE_HEIGHT - glyph_actual_height) / 2\n            # Center the glyph within its allocated fixed_glyph_area_width\n            draw_glyph_left_x = PADDING + (fixed_glyph_area_width - glyph_actual_width) / 2\n\n            draw.text((draw_glyph_left_x, draw_glyph_top_y), glyph_char, font=glyph_font, fill=TEXT_COLOR, anchor=\"lt\")\n        except UnicodeEncodeError:\n            error_indicator = \"[!]\"\n            error_bbox_origin = draw.textbbox((0,0), error_indicator, font=glyph_font, anchor=\"lt\")\n            error_width = error_bbox_origin[2] - error_bbox_origin[0]\n            error_height = error_bbox_origin[3] - error_bbox_origin[1]\n            draw_error_top_y = current_y + (LINE_HEIGHT - error_height) / 2\n            draw_error_left_x = PADDING + (fixed_glyph_area_width - error_width) / 2\n            draw.text((draw_error_left_x, draw_error_top_y), error_indicator, font=glyph_font, fill=ERROR_COLOR, anchor=\"lt\")\n            print(f\"Menlo (or fallback) could not render: '{glyph_char}' ({glyph_label})\")\n        except Exception as e:\n            print(f\"Other error rendering '{glyph_char}': {e}\")\n            draw.text((PADDING, current_y + (LINE_HEIGHT - FONT_SIZE)/2), \"[E]\", font=glyph_font, fill=ERROR_COLOR, anchor=\"lt\")\n\n        # --- Draw Label ---\n        label_x_start = PADDING + fixed_glyph_area_width + GLYPH_SPACING\n        try:\n            label_bbox_origin = draw.textbbox((0, 0), glyph_label, font=label_font, anchor=\"lt\")\n            label_actual_height = label_bbox_origin[3] - label_bbox_origin[1]\n            draw_label_top_y = current_y + (LINE_HEIGHT - label_actual_height) / 2\n            draw.text((label_x_start, draw_label_top_y), glyph_label, font=label_font, fill=LABEL_COLOR, anchor=\"lt\")\n        except UnicodeEncodeError:\n            draw_label_error_top_y = current_y + (LINE_HEIGHT - LABEL_FONT_SIZE) / 2\n            draw.text((label_x_start, draw_label_error_top_y), \"[Label Error]\", font=label_font, fill=ERROR_COLOR, anchor=\"lt\")\n\n        current_y += LINE_HEIGHT\n\n    try:\n        image.save(output_path)\n        print(f\"Successfully generated glyph candidate test image: {output_path}\")\n    except Exception as e:\n        print(f\"Error saving image: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "menlo_event_pause_candidates.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/",
      ")\n            glyph_actual_width = glyph_bbox_origin[2] - glyph_bbox_origin[0]\n            glyph_actual_height = glyph_bbox_origin[3] - glyph_bbox_origin[1]\n\n            draw_glyph_top_y = current_y + (LINE_HEIGHT - glyph_actual_height) / 2\n            # Center the glyph within its allocated fixed_glyph_area_width\n            draw_glyph_left_x = PADDING + (fixed_glyph_area_width - glyph_actual_width) / 2\n\n            draw.text((draw_glyph_left_x, draw_glyph_top_y), glyph_char, font=glyph_font, fill=TEXT_COLOR, anchor=",
      ")\n            error_width = error_bbox_origin[2] - error_bbox_origin[0]\n            error_height = error_bbox_origin[3] - error_bbox_origin[1]\n            draw_error_top_y = current_y + (LINE_HEIGHT - error_height) / 2\n            draw_error_left_x = PADDING + (fixed_glyph_area_width - error_width) / 2\n            draw.text((draw_error_left_x, draw_error_top_y), error_indicator, font=glyph_font, fill=ERROR_COLOR, anchor=",
      ")\n            draw.text((PADDING, current_y + (LINE_HEIGHT - FONT_SIZE)/2), ",
      ")\n            label_actual_height = label_bbox_origin[3] - label_bbox_origin[1]\n            draw_label_top_y = current_y + (LINE_HEIGHT - label_actual_height) / 2\n            draw.text((label_x_start, draw_label_top_y), glyph_label, font=label_font, fill=LABEL_COLOR, anchor=",
      ")\n        except UnicodeEncodeError:\n            draw_label_error_top_y = current_y + (LINE_HEIGHT - LABEL_FONT_SIZE) / 2\n            draw.text((label_x_start, draw_label_error_top_y), "
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "HONEYBADGER/HIVE/clapper31.py",
    "size": 23451,
    "lines": 546,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER31 - Film Clapperboard Style Timeline Data Cards with Actual Images and Enhanced Graphics\n- Image integration from image_path field in timeline data \n- Poem and Line Content on same line\n- Updated header labels: \"IMAGE\" and \"FRAME\"\n- Glyph rendering based on syntagma/image type\n- Multi-colored prompt components: syntagma, cineosis function, operative ekphrasis, and style conditioning\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\n\n# Constants\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 200  # Increased height for text\nCARD_HEIGHT = HEADER_HEIGHT + 576  # 576 is the height for a 16:9 aspect ratio of 1024 width\nIMAGE_HEIGHT = 576  # 16:9 aspect ratio\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\n# Colors for different prompt components - improved contrast for readability\nSYNTAGMA_COLOR = (100, 200, 255)  # Brighter blue\nCINEOSIS_COLOR = (230, 150, 255)  # Lighter purple\nEKPHRASIS_COLOR = AMBER           # Same as LINE color (amber)\nSTYLE_COLOR = (150, 255, 180)     # Lighter green\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Using the complete timeline with image paths\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper31\")\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Base directory for relative image paths\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Helper function to wrap text\ndef wrap_text(text, font, max_width):\n    \"\"\"Wrap text to fit within a given width.\"\"\"\n    words = text.split()\n    wrapped_lines = []\n    current_line = []\n    \n    for word in words:\n        # Test with current word added\n        test_line = ' '.join(current_line + [word])\n        line_width = font.getbbox(test_line)[2]\n        \n        if line_width <= max_width:\n            current_line.append(word)\n        else:\n            # If the current line has words, complete it\n            if current_line:\n                wrapped_lines.append(' '.join(current_line))\n                current_line = [word]\n            else:\n                # If the word itself is too long, force it on its own line\n                wrapped_lines.append(word)\n                current_line = []\n    \n    # Add the last line if there's anything left\n    if current_line:\n        wrapped_lines.append(' '.join(current_line))\n    \n    return wrapped_lines\n\ndef get_image_path(entry):\n    \"\"\"Get the image path from the entry data or return None.\"\"\"\n    # Check if image_path exists in the entry\n    if 'image_path' in entry and entry['image_path']:\n        # Construct absolute path from relative path in the timeline data\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        # Try TIGER directory as alternative\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    \n    # Fallback: search in TIGER directory by ID\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        # Try to find any file with ID prefix\n        for root, _, files in os.walk(base_dir):\n            for file in files:\n                if file.startswith(entry_id + '__') and file.lower().endswith('.png'):\n                    return os.path.join(root, file)\n    \n    return None\n\ndef draw_regular_polygon(draw, center_x, center_y, radius, sides, color, width=1):\n    \"\"\"Helper function to draw a regular polygon since PIL doesn't have this built-in\"\"\"\n    # Calculate the points of the regular polygon\n    import math\n    points = []\n    for i in range(sides):\n        angle = 2 * math.pi * i / sides - math.pi / 2  # Start from top\n        x = center_x + radius * math.cos(angle)\n        y = center_y + radius * math.sin(angle)\n        points.append((x, y))\n    \n    # Draw the polygon\n    draw.polygon(points, outline=color, width=width)\n\ndef draw_glyph(draw, entry, x, y, size=40):\n    \"\"\"Draw a glyph based on syntagma/image type at the specified position.\"\"\"\n    syntagma_type = entry.get('syntagmaType', '').upper()\n    image_type = entry.get('imageType', '').upper()\n    \n    # Use a combination of first characters or abbreviation to determine glyph\n    glyph_type = ''\n    if '(' in syntagma_type and ')' in syntagma_type:\n        # Extract abbreviation from syntagma type (e.g., \"Descriptive Syntagma (DS)\" -> \"DS\")\n        glyph_type = syntagma_type.split('(')[1].split(')')[0]\n    else:\n        # Try to get from image type or first chars of syntagma\n        type_text = image_type or syntagma_type\n        glyph_type = ''.join([word[0] for word in type_text.split() if word])\n    \n    # Default to SS if we couldn't determine a type\n    if not glyph_type:\n        glyph_type = 'SS'\n    \n    center_x = x + size // 2\n    center_y = y + size // 2\n    radius = size // 2\n        \n    # Map of glyph types to shapes\n    shapes = {\n        'DS': lambda: draw.ellipse([x, y, x+size, y+size], outline=AMBER, width=2),          # Circle\n        'PS': lambda: draw.polygon([(x+size//2, y), (x+size, y+size), (x, y+size)], outline=AMBER, width=2),  # Triangle\n        'CS': lambda: draw.rectangle([x, y, x+size, y+size], outline=AMBER, width=2),         # Square\n        'SS': lambda: draw.polygon([(x+size//2, y), (x+size, y+size//2), (x+size//2, y+size), (x, y+size//2)], outline=AMBER, width=2),  # Diamond\n        'AF': lambda: draw.ellipse([x, y, x+size, y+size], outline=AMBER, width=2),          # Circle with dot\n        'PI': lambda: draw_regular_polygon(draw, center_x, center_y, radius, 6, AMBER, 2),  # Hexagon\n        'RI': lambda: draw_regular_polygon(draw, center_x, center_y, radius, 5, AMBER, 2),  # Pentagon\n    }\n    \n    # Draw the shape based on the glyph type\n    draw_shape = shapes.get(glyph_type, shapes['SS'])\n    draw_shape()\n    \n    # Draw the glyph text inside\n    font = ImageFont.truetype(\"Courier New Bold.ttf\", size//2) if os.path.exists(\"Courier New Bold.ttf\") else ImageFont.load_default()\n    text_width = font.getbbox(glyph_type)[2]\n    text_x = x + (size - text_width) // 2\n    text_y = y + (size - font.getbbox(glyph_type)[3]) // 2\n    draw.text((text_x, text_y), glyph_type, font=font, fill=AMBER)\n\ndef render_card(entry, output_path):\n    \"\"\"Render a single data card with the entry information.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH+8, CARD_HEIGHT+8), BLUE_BORDER)\n    \n    # Create inner black image (main card)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    \n    draw = ImageDraw.Draw(img)\n    \n    # Add glyph representation in proper position (top-right corner)\n    draw_glyph(draw, entry, BASE_WIDTH - 60, 15)\n    \n    # Load fonts\n    try:\n        header_font = ImageFont.truetype(\"Courier New Bold.ttf\", 16)  \n        text_font = ImageFont.truetype(\"Courier New.ttf\", 16)\n    except IOError:\n        # Fallback to default font if Courier New is not available\n        header_font = ImageFont.load_default()\n        text_font = ImageFont.load_default()\n        print(\"Warning: Courier New font not found, using default font.\")\n    \n    top_row_height = 30  # Height of the top row\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    \n    # Get frame position information if available\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    \n    # Image type without the frame number\n    image_type = entry.get('imageType', 'Descriptive Image')\n    image_text = f\"IMAGE: {image_type}\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), image_text, font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(4, top_row_height), (BASE_WIDTH+4, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line with proper wrapping for long content\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    \n    # Calculate poem width\n    poem_width = header_font.getbbox(poem_text)[2]\n    \n    # Draw poem name\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # LINE CONTENT: Highlighted with amber to give it emphasis\n    # Calculate start position for line content (after poem with some spacing)\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)  # Either after poem or at half width\n    \n    # Draw the line content with amber highlighting\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    # Draw prefix in white\n    draw.text((line_start_x, top_row_height + 8), line_prefix, font=header_font, fill=WHITE)\n    \n    # Available width for line content\n    available_width = BASE_WIDTH - line_start_x - line_prefix_width - 20\n    \n    # Wrap the line content if needed\n    wrapped_content = wrap_text(line_content, header_font, available_width)\n    \n    for i, line_part in enumerate(wrapped_content):\n        y_pos = top_row_height + 8 + (i * 16)\n        \n        # Add closing quote to last line\n        if i == len(wrapped_content) - 1:\n            line_part += \"\\\"\"\n        \n        # Draw the line content in amber\n        draw.text((line_start_x + line_prefix_width, y_pos), line_part, font=header_font, fill=AMBER)\n    \n    # Calculate actual height used by the line content\n    line_content_actual_height = 20 + max(1, len(wrapped_content)) * 16  # Base + line height\n    \n    # Draw horizontal divider after line content\n    divider_y = top_row_height + line_content_actual_height\n    draw.line([(4, divider_y), (BASE_WIDTH+4, divider_y)], fill=WHITE, width=1)\n    \n    # PROMPT ROW: Colored components but still continuous prompt text with wrapping\n    \n    # Extract or construct components\n    if 'full_prompt' in entry and entry['full_prompt']:\n        # Parse parts from the full_prompt using the middle dot separator\n        parts = entry['full_prompt'].split(' \u00b7 ')\n        if len(parts) >= 4:\n            syntagma_abbr = parts[0]\n            cineosis_function = parts[1]\n            operative_ekphrasis = parts[2]\n            style_conditioning = parts[3]\n        else:\n            # Fallback if parts aren't properly separated\n            syntagma_abbr = entry.get('syntagmaType', 'SS')\n            cineosis_function = entry.get('cineosisFunction', '')\n            operative_ekphrasis = entry.get('operativeEkphrasis', '')\n            style_conditioning = entry.get('styleConditioning', '')\n    else:\n        # Extract syntagma abbreviation from syntagmaType\n        syntagma_type = entry.get('syntagmaType', '')\n        if '(' in syntagma_type and ')' in syntagma_type:\n            syntagma_abbr = syntagma_type.split('(')[1].split(')')[0]\n        else:\n            # Fallback: use first letter of each word\n            syntagma_abbr = ''.join(word[0] for word in syntagma_type.split() if word)\n        \n        # Make sure we have a valid abbreviation\n        if not syntagma_abbr:\n            syntagma_abbr = 'SS'\n            \n        cineosis_function = entry.get('cineosisFunction', '')\n        operative_ekphrasis = entry.get('operativeEkphrasis', '')\n        style_conditioning = entry.get('styleConditioning', '')\n    \n    # Create the colored components prompt with improved layout\n    prompt_y = divider_y + 8\n    left_margin = 10\n    indent = 20  # Indent after syntagma code\n    max_width = BASE_WIDTH // 2 - 20  # Limit width to approximately half the card width\n    \n    # Try to load a bold font for operative ekphrasis\n    try:\n        bold_font = ImageFont.truetype(\"Courier New Bold.ttf\", 16)\n    except IOError:\n        bold_font = text_font  # Fallback to regular font if bold not available\n    \n    # Helper function to draw colored text and return the new position\n    def draw_colored_text(text, x, y, color, font, max_width=None):\n        if not text:\n            return y\n            \n        # Always wrap text to the max width\n        words = text.split()\n        current_line = []\n        current_width = 0\n        start_x = x\n        start_y = y\n            \n        for word in words:\n            word_width = font.getbbox(word)[2]\n            space_width = font.getbbox(' ')[2] if current_line else 0\n            \n            # If adding this word would exceed max width, draw the current line and start a new one\n            if current_line and current_width + word_width + space_width > max_width:\n                line_text = ' '.join(current_line)\n                draw.text((start_x, y), line_text, font=font, fill=color)\n                y += 16  # Move to next line\n                current_line = [word]\n                current_width = word_width\n            else:\n                current_line.append(word)\n                current_width += word_width + space_width\n        \n        # Draw any remaining text\n        if current_line:\n            line_text = ' '.join(current_line)\n            draw.text((start_x, y), line_text, font=font, fill=color)\n            y += 16  # Add space after this component\n            \n        return y\n    \n    # First row: Syntagma abbreviation + Cineosis function\n    draw.text((left_margin, prompt_y), syntagma_abbr + ' \u00b7', font=text_font, fill=SYNTAGMA_COLOR)\n    \n    # Calculate the position after syntagma code\n    x_after_syntagma = left_margin + text_font.getbbox(syntagma_abbr + ' \u00b7')[2] + indent\n    \n    # Draw cineosis function after the indent\n    draw.text((x_after_syntagma, prompt_y), cineosis_function, font=text_font, fill=CINEOSIS_COLOR)\n    prompt_y += 16  # Move to next line\n    \n    # Second row: Operative ekphrasis (in AMBER and bold)\n    prompt_y = draw_colored_text(operative_ekphrasis, left_margin, prompt_y, EKPHRASIS_COLOR, bold_font, max_width)\n    \n    # Third row: Style conditioning (in easier to read green)\n    prompt_y = draw_colored_text(style_conditioning, left_margin, prompt_y, STYLE_COLOR, text_font, max_width)\n    \n    # Get the actual image file path from the entry\n    image_path = get_image_path(entry)\n    \n    if image_path and os.path.exists(image_path):\n        try:\n            # Open and resize the actual image\n            actual_image = Image.open(image_path)\n            actual_image = actual_image.resize((BASE_WIDTH, IMAGE_HEIGHT), Image.LANCZOS)\n            \n            # Paste the actual image into the card\n            img.paste(actual_image, (4, HEADER_HEIGHT + 4))\n        except Exception as e:\n            print(f\"Error loading image for {entry_id}: {e}\")\n            # Fill image area with placeholder if loading fails\n            draw.rectangle([(4, HEADER_HEIGHT + 4), (BASE_WIDTH + 4, CARD_HEIGHT + 4)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n                     f\"Image Error: {os.path.basename(image_path)}\", \n                     font=header_font, fill=BLACK, anchor=\"mm\")\n    else:\n        # Fill image area with placeholder if no matching image is found\n        draw.rectangle([(4, HEADER_HEIGHT + 4), (BASE_WIDTH + 4, CARD_HEIGHT + 4)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n                 \"No Image Found\", \n                 font=header_font, fill=BLACK, anchor=\"mm\")\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n    return output_path\n\ndef render_demo_card():\n    \"\"\"Render a demo card with sample data.\"\"\"\n    demo_entry = {\n        \"id\": \"DEMO\",\n        \"timestamp\": \"00:00:00\",\n        \"poem\": \"Sample Poem Name\",\n        \"content\": \"This is a sample line of content.\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"This is a sample operative ekphrasis description text\",\n        \"styleConditioning\": \"This is style conditioning text that would normally continue with more details about the visual style.\",\n        \"full_prompt\": \"DS \u00b7 Mood Environment Stabilizer \u00b7 This is a sample operative ekphrasis description text \u00b7 This is style conditioning text that would normally continue with more details about the visual style.\",\n        \"imageType\": \"Descriptive Image\",\n        \"image_path\": \"\",\n        \"frame_position\": 1,\n        \"frame_total\": 12\n    }\n    output_path = os.path.join(OUTPUT_DIR, \"demo_clapper.png\")\n    return render_card(demo_entry, output_path)\n\ndef render_extreme_test_case():\n    \"\"\"Render a test card with the longest entries to test layout boundaries.\"\"\"\n    # These are sample longest entries from testing\n    longest_entry = {\n        \"id\": \"EXTREME\",\n        \"timestamp\": \"99:99:99\",\n        \"poem\": \"How to break off an engagement\",\n        \"content\": \"where you go when you leave\u2014 & I fall out of life.\",\n        \"syntagmaType\": \"Subjective Syntagma (SS)\",\n        \"cineosisFunction\": \"Memory Storage Retrieval\",\n        \"operativeEkphrasis\": \"Through a reminiscent lens that distorts memory like warped glass, an intimate letter is folded into origami wings, suspended between white candles that melt too quickly.\",\n        \"styleConditioning\": \"In the evocative style of Taryn Simon's meticulous forensic compositions, with precise documentary lighting that creates a stark visual inventory, rendered with unnerving clarity that transforms emotional artifacts into clinical evidence.\",\n        \"full_prompt\": \"SS \u00b7 Memory Storage Retrieval \u00b7 Through a reminiscent lens that distorts memory like warped glass, an intimate letter is folded into origami wings, suspended between white candles that melt too quickly. \u00b7 In the evocative style of Taryn Simon's meticulous forensic compositions, with precise documentary lighting that creates a stark visual inventory, rendered with unnerving clarity that transforms emotional artifacts into clinical evidence.\",\n        \"imageType\": \"Descriptive Image\",\n        \"image_path\": \"TEST/EXTREME_TEST_3.png\",\n        \"frame_position\": 7,\n        \"frame_total\": 34\n    }\n    output_path = os.path.join(OUTPUT_DIR, \"extreme_test.png\")\n    card_path = render_card(longest_entry, output_path)\n    print(f\"Rendered extreme test case: {card_path}\")\n    return card_path\n\ndef calculate_frame_counts(timeline_data):\n    \"\"\"Calculate the total number of frames per poem and frame position for each entry.\"\"\"\n    # Group by poem name and count entries per poem\n    poem_counts = {}\n    poem_entries = {}\n    \n    for entry in timeline_data:\n        poem_name = entry.get('poem', '')\n        if poem_name not in poem_counts:\n            poem_counts[poem_name] = 0\n            poem_entries[poem_name] = []\n        poem_counts[poem_name] += 1\n        poem_entries[poem_name].append(entry)\n    \n    # Sort entries within each poem by timestamp\n    for poem, entries in poem_entries.items():\n        poem_entries[poem] = sorted(entries, key=lambda x: x.get('timestamp', '00:00:00'))\n    \n    # Create a lookup table for frame position and total count\n    frame_info = {}\n    for poem, entries in poem_entries.items():\n        for i, entry in enumerate(entries):\n            entry_id = entry.get('id', '')\n            if entry_id:\n                frame_info[entry_id] = {\n                    'position': i + 1,  # 1-based position\n                    'total': len(entries)\n                }\n    \n    return frame_info\n\ndef main():\n    \"\"\"Main function to load timeline and render all cards.\"\"\"\n    print(\"CLAPPER31 - Film Clapperboard Style Timeline Cards with Glyphs and Color\")\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    \n    try:\n        with open(TIMELINE_PATH, 'r') as file:\n            timeline_data = json.load(file)\n    except Exception as e:\n        print(f\"Error loading timeline data: {e}\")\n        return\n    \n    print(f\"Found {len(timeline_data)} timeline entries\")\n    print(f\"Header height: {HEADER_HEIGHT}px\")\n    print(f\"Image height (16:9): {IMAGE_HEIGHT}px\")\n    print(f\"Using base image directory: {BASE_IMAGE_DIR}\")\n    \n    # Calculate frame counts for each poem\n    frame_info = calculate_frame_counts(timeline_data)\n    print(f\"Calculated frame positions for {len(frame_info)} unique entries\")\n    \n    # Group entries by poem name\n    poem_entries = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem', 'Unknown')\n        if poem_name not in poem_entries:\n            poem_entries[poem_name] = []\n        poem_entries[poem_name].append(entry)\n    \n    print(f\"Found {len(poem_entries)} unique poems\")\n    \n    # Track image paths found and not found\n    images_found = 0\n    images_not_found = 0\n    rendered_count = 0\n    \n    # Render one card from each poem\n    for poem_name, entries in poem_entries.items():\n        if poem_name == 'Unknown' or not poem_name:\n            continue  # Skip entries without a valid poem name\n            \n        # Select the first entry with a valid image path as a representative\n        selected_entry = None\n        for entry in entries:\n            entry_id = entry.get('id', '')\n            if entry_id and get_image_path(entry):\n                selected_entry = entry\n                break\n        \n        # If no entry with image found, use the first entry\n        if not selected_entry and entries:\n            selected_entry = entries[0]\n        \n        if not selected_entry:\n            continue\n        \n        # Add frame count information to the entry\n        entry_id = selected_entry.get('id', 'unknown')\n        if entry_id in frame_info:\n            selected_entry['frame_position'] = frame_info[entry_id]['position']\n            selected_entry['frame_total'] = frame_info[entry_id]['total']\n        \n        # Render representative card for this poem\n        output_filename = f\"{poem_name.replace(' ', '_')}_card.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        \n        image_path = get_image_path(selected_entry)\n        if image_path:\n            images_found += 1\n            print(f\"Found image for poem '{poem_name}' (ID: {entry_id}): {os.path.basename(image_path)}\")\n        else:\n            images_not_found += 1\n            print(f\"No image found for poem '{poem_name}' (ID: {entry_id})\")\n        \n        render_card(selected_entry, output_path)\n        rendered_count += 1\n    \n    print(f\"Rendered {rendered_count} cards (one from each poem)\")\n    print(f\"Images found: {images_found}, not found: {images_not_found}\")\n    \n    print(\"Rendering demo layout card\")\n    render_demo_card()\n    \n    print(\"Rendering extreme test case with longest entries\")\n    render_extreme_test_case()\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {BASE_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {BASE_WIDTH}x{CARD_HEIGHT}px\")\n    print(\"- Added features: Glyphs and multi-colored prompt components\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "demo_clapper.png",
      "TEST/EXTREME_TEST_3.png",
      "extreme_test.png",
      ")}_card.png",
      "\n- Glyph rendering based on syntagma/image type\n- Multi-colored prompt components: syntagma, cineosis function, operative ekphrasis, and style conditioning\n",
      "\n    # Calculate the points of the regular polygon\n    import math\n    points = []\n    for i in range(sides):\n        angle = 2 * math.pi * i / sides - math.pi / 2  # Start from top\n        x = center_x + radius * math.cos(angle)\n        y = center_y + radius * math.sin(angle)\n        points.append((x, y))\n    \n    # Draw the polygon\n    draw.polygon(points, outline=color, width=width)\n\ndef draw_glyph(draw, entry, x, y, size=40):\n    ",
      "Draw a glyph based on syntagma/image type at the specified position.",
      "\n    \n    center_x = x + size // 2\n    center_y = y + size // 2\n    radius = size // 2\n        \n    # Map of glyph types to shapes\n    shapes = {\n        ",
      ": lambda: draw.polygon([(x+size//2, y), (x+size, y+size), (x, y+size)], outline=AMBER, width=2),  # Triangle\n        ",
      ": lambda: draw.polygon([(x+size//2, y), (x+size, y+size//2), (x+size//2, y+size), (x, y+size//2)], outline=AMBER, width=2),  # Diamond\n        ",
      ", size//2) if os.path.exists(",
      ") else ImageFont.load_default()\n    text_width = font.getbbox(glyph_type)[2]\n    text_x = x + (size - text_width) // 2\n    text_y = y + (size - font.getbbox(glyph_type)[3]) // 2\n    draw.text((text_x, text_y), glyph_type, font=font, fill=AMBER)\n\ndef render_card(entry, output_path):\n    ",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    \n    # Calculate poem width\n    poem_width = header_font.getbbox(poem_text)[2]\n    \n    # Draw poem name\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # LINE CONTENT: Highlighted with amber to give it emphasis\n    # Calculate start position for line content (after poem with some spacing)\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)  # Either after poem or at half width\n    \n    # Draw the line content with amber highlighting\n    line_prefix = ",
      ")\n    \n    # Create the colored components prompt with improved layout\n    prompt_y = divider_y + 8\n    left_margin = 10\n    indent = 20  # Indent after syntagma code\n    max_width = BASE_WIDTH // 2 - 20  # Limit width to approximately half the card width\n    \n    # Try to load a bold font for operative ekphrasis\n    try:\n        bold_font = ImageFont.truetype(",
      ")\n            # Fill image area with placeholder if loading fails\n            draw.rectangle([(4, HEADER_HEIGHT + 4), (BASE_WIDTH + 4, CARD_HEIGHT + 4)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n                     f",
      ")\n    else:\n        # Fill image area with placeholder if no matching image is found\n        draw.rectangle([(4, HEADER_HEIGHT + 4), (BASE_WIDTH + 4, CARD_HEIGHT + 4)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n                 ",
      "TEST/EXTREME_TEST_3.png"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER31 - Film Clapperboard Style Timeline Data Cards with Actual Images and Enhanced Graphics\n- Image integration from image_path field in timeline data \n- Poem and Line Content on same line\n- Updated header labels: \"IMAGE\" and \"FRAME\"\n- Glyph rendering based on syntagma/image type\n- Multi-colored prompt components: syntagma, cineosis function, operative ekphrasis, and style conditioning"
  },
  {
    "path": "HONEYBADGER/HIVE/render_cards_prompt_focus.py",
    "size": 10898,
    "lines": 281,
    "source": "#!/usr/bin/env python3\n\"\"\"\nRender Timeline Data Cards with Prompt Focus\n\nThis script renders timeline entries with emphasis on the prompt text,\ngiving it more space and better visibility while maintaining the aspect ratios.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\nASPECT_3_2 = 3/2    # Overall frame\n\n# Calculate dimensions to maintain proper aspect ratios\nBASE_WIDTH = 1200\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height of the 16:9 image area\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)   # Total height for 3:2 aspect ratio\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT   # Height of the header area\n\n# Card dimensions\nCARD_WIDTH = BASE_WIDTH\nCARD_HEIGHT = TOTAL_HEIGHT\n\n# Redistribute header space - give prompt section 50% of the header height\nMETADATA_SECTION_HEIGHT = int(HEADER_HEIGHT * 0.5)  # Top 3 rows combined\nPROMPT_SECTION_HEIGHT = HEADER_HEIGHT - METADATA_SECTION_HEIGHT  # Bottom section for prompt\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nDARK_GRAY = (40, 40, 40)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)  # Darker amber for better contrast with white text\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_prompt_focus')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths (system fonts - we'll use these initially)\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\" \n\n# Define glyphs for different types\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card with emphasis on the prompt text.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (CARD_WIDTH+8, CARD_HEIGHT+8), BLUE_BORDER)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (CARD_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts - more compact for metadata, larger for prompt\n    tiny_font = find_font(BODY_FONT_PATH, 12)  # For compact metadata\n    small_font = find_font(HEADER_FONT_PATH, 14)  # For section headers\n    metadata_font = find_font(HEADER_FONT_PATH, 16)  # For main metadata\n    prompt_label_font = find_font(HEADER_FONT_PATH, 18)  # For prompt label\n    prompt_font = find_font(BODY_FONT_PATH, 18)  # Larger for prompt content\n    symbol_font = find_font(SYMBOL_FONT_PATH, 16)  # For glyphs\n    \n    # Fill the header area black\n    draw.rectangle([(0, 0), (CARD_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    \n    # Fill the image area light gray (placeholder)\n    draw.rectangle([(0, HEADER_HEIGHT), (CARD_WIDTH, CARD_HEIGHT)], fill=LIGHT_GRAY)\n    draw.text((CARD_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=metadata_font, fill=BLACK, anchor=\"mm\")\n    \n    # Draw horizontal dividing line between header and image\n    draw.line([(0, HEADER_HEIGHT-1), (CARD_WIDTH, HEADER_HEIGHT-1)], fill=WHITE, width=2)\n    \n    # Calculate compact metadata section heights - 3 rows in top half\n    row_height = METADATA_SECTION_HEIGHT // 3\n    padding = 3  # minimal padding\n    \n    # ROW 1: ID, timestamp, frame number - very compact\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = f\"FRM: (---)\"\n    \n    row1_y = padding\n    draw.text((padding*2, row1_y), id_text, font=metadata_font, fill=WHITE)\n    draw.text((CARD_WIDTH//3, row1_y), time_text, font=metadata_font, fill=WHITE)\n    draw.text((2*CARD_WIDTH//3, row1_y), frame_text, font=metadata_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, row_height), (CARD_WIDTH, row_height)], fill=WHITE, width=1)\n    \n    # ROW 2: Poem and Line - compact one line\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_content = entry.get('content', '---')\n    # Truncate line content if too long\n    if len(line_content) > 60:\n        line_content = line_content[:57] + \"...\"\n    line_text = f\"LINE: \\\"{line_content}\\\"\"\n    \n    row2_y = row_height + padding\n    draw.text((padding*2, row2_y), poem_text, font=metadata_font, fill=WHITE)\n    draw.text((CARD_WIDTH//2, row2_y), line_text, font=metadata_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, 2*row_height), (CARD_WIDTH, 2*row_height)], fill=WHITE, width=1)\n    \n    # Get the types and prompt components\n    syntagma_type = entry.get('syntagmaType', '---')\n    image_type = entry.get('imageType', '---')\n    cineosis_func = entry.get('cineosisFunction', '---')\n    operative_ekphrasis = entry.get('operativeEkphrasis', '')\n    style_conditioning = entry.get('styleConditioning', '')\n    \n    syntagma_glyph = get_glyph(SYNTAGMA_GLYPHS, syntagma_type, \"\u25a1\")\n    image_glyph = get_glyph(IMAGE_GLYPHS, image_type, \"\u25a1\")\n    cineosis_glyph = get_glyph(CINEOSIS_GLYPHS, cineosis_func, \"\u25ef\")\n    \n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # Draw horizontal divider after the metadata sections\n    draw.line([(0, 2*row_height), (CARD_WIDTH, 2*row_height)], fill=WHITE, width=1)\n    \n    # AMBER HEADER BAR WITH COLUMN LABELS\n    header_bar_y = 2*row_height\n    header_bar_height = 22\n    \n    # Draw amber header bar\n    draw.rectangle([(0, header_bar_y), (CARD_WIDTH, header_bar_y + header_bar_height)], fill=AMBER)\n    \n    # Add column headers in the amber bar\n    col_width = CARD_WIDTH // 3\n    draw.text((padding*2, header_bar_y + padding-1), \"SYNTAGMA\", font=small_font, fill=BLACK)\n    draw.text((col_width, header_bar_y + padding-1), \"CINEOSIS FUNCTION\", font=small_font, fill=BLACK)\n    draw.text((2*col_width, header_bar_y + padding-1), \"OPERATIVE EKPHRASIS\", font=small_font, fill=BLACK)\n    \n    # PROMPT CONTENT ROW\n    prompt_y = header_bar_y + header_bar_height + padding\n    \n    # Display the prompt content with DS and cineosis function\n    draw.text((padding*2, prompt_y), f\"{syntagma_abbrev}\", font=prompt_font, fill=WHITE)\n    draw.text((col_width, prompt_y), f\"{cineosis_func}\", font=prompt_font, fill=WHITE)\n    draw.text((2*col_width, prompt_y), f\"{operative_ekphrasis}\", font=prompt_font, fill=WHITE)\n    \n    # Draw thin divider line between prompt content and image area\n    draw.line([(0, HEADER_HEIGHT - 2), (CARD_WIDTH, HEADER_HEIGHT - 2)], fill=WHITE, width=1)\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (4, 4))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_card.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    print(f\"Metadata section height: {METADATA_SECTION_HEIGHT}px\")\n    print(f\"Prompt section height: {PROMPT_SECTION_HEIGHT}px\")\n    \n    # Render first 10 entries as samples\n    render_all_entries(timeline, OUTPUT_DIR, 10)\n    \n    # Also render a demo card showing the layout with dimensions\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"DEMO\",\n        \"timestamp\": \"00:00:00\",\n        \"poem\": \"Layout Example\",\n        \"content\": \"3:2 overall with 16:9 image\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"This is a demo card showing the layout structure with proper aspect ratios. The prompt section has been enlarged to improve readability and visibility since it contains the most important information.\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_layout.png\")\n    render_card(demo_entry, demo_path)\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px (total)\")\n    print(f\"- Image area: {CARD_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {CARD_WIDTH}x{CARD_HEIGHT}px (3:2)\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_card.png",
      "demo_layout.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\nASPECT_3_2 = 3/2    # Overall frame\n\n# Calculate dimensions to maintain proper aspect ratios\nBASE_WIDTH = 1200\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height of the 16:9 image area\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)   # Total height for 3:2 aspect ratio\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT   # Height of the header area\n\n# Card dimensions\nCARD_WIDTH = BASE_WIDTH\nCARD_HEIGHT = TOTAL_HEIGHT\n\n# Redistribute header space - give prompt section 50% of the header height\nMETADATA_SECTION_HEIGHT = int(HEADER_HEIGHT * 0.5)  # Top 3 rows combined\nPROMPT_SECTION_HEIGHT = HEADER_HEIGHT - METADATA_SECTION_HEIGHT  # Bottom section for prompt\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nDARK_GRAY = (40, 40, 40)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)  # Darker amber for better contrast with white text\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ", (CARD_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts - more compact for metadata, larger for prompt\n    tiny_font = find_font(BODY_FONT_PATH, 12)  # For compact metadata\n    small_font = find_font(HEADER_FONT_PATH, 14)  # For section headers\n    metadata_font = find_font(HEADER_FONT_PATH, 16)  # For main metadata\n    prompt_label_font = find_font(HEADER_FONT_PATH, 18)  # For prompt label\n    prompt_font = find_font(BODY_FONT_PATH, 18)  # Larger for prompt content\n    symbol_font = find_font(SYMBOL_FONT_PATH, 16)  # For glyphs\n    \n    # Fill the header area black\n    draw.rectangle([(0, 0), (CARD_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    \n    # Fill the image area light gray (placeholder)\n    draw.rectangle([(0, HEADER_HEIGHT), (CARD_WIDTH, CARD_HEIGHT)], fill=LIGHT_GRAY)\n    draw.text((CARD_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), ",
      ")\n    \n    # Draw horizontal dividing line between header and image\n    draw.line([(0, HEADER_HEIGHT-1), (CARD_WIDTH, HEADER_HEIGHT-1)], fill=WHITE, width=2)\n    \n    # Calculate compact metadata section heights - 3 rows in top half\n    row_height = METADATA_SECTION_HEIGHT // 3\n    padding = 3  # minimal padding\n    \n    # ROW 1: ID, timestamp, frame number - very compact\n    id_text = f",
      "\n    \n    row1_y = padding\n    draw.text((padding*2, row1_y), id_text, font=metadata_font, fill=WHITE)\n    draw.text((CARD_WIDTH//3, row1_y), time_text, font=metadata_font, fill=WHITE)\n    draw.text((2*CARD_WIDTH//3, row1_y), frame_text, font=metadata_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, row_height), (CARD_WIDTH, row_height)], fill=WHITE, width=1)\n    \n    # ROW 2: Poem and Line - compact one line\n    poem_text = f",
      "\n    \n    row2_y = row_height + padding\n    draw.text((padding*2, row2_y), poem_text, font=metadata_font, fill=WHITE)\n    draw.text((CARD_WIDTH//2, row2_y), line_text, font=metadata_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, 2*row_height), (CARD_WIDTH, 2*row_height)], fill=WHITE, width=1)\n    \n    # Get the types and prompt components\n    syntagma_type = entry.get(",
      ")\n    \n    # Draw horizontal divider after the metadata sections\n    draw.line([(0, 2*row_height), (CARD_WIDTH, 2*row_height)], fill=WHITE, width=1)\n    \n    # AMBER HEADER BAR WITH COLUMN LABELS\n    header_bar_y = 2*row_height\n    header_bar_height = 22\n    \n    # Draw amber header bar\n    draw.rectangle([(0, header_bar_y), (CARD_WIDTH, header_bar_y + header_bar_height)], fill=AMBER)\n    \n    # Add column headers in the amber bar\n    col_width = CARD_WIDTH // 3\n    draw.text((padding*2, header_bar_y + padding-1), ",
      "Rendered {i+1}/{len(entries_to_render)} cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Render Timeline Data Cards with Prompt Focus\n\nThis script renders timeline entries with emphasis on the prompt text,\ngiving it more space and better visibility while maintaining the aspect ratios."
  },
  {
    "path": "HONEYBADGER/HIVE/clapper_genome_exp.py",
    "size": 22505,
    "lines": 430,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER_GENOME_EXP - Experimental Clapperboard with Genome Report Display\n- Adds a top section to display content from SYMBOLIC_GENOME_REPORT.md.\n- Initially displays a fixed portion of the genome report (first poem's genome).\n- Based on CLAPPER33 (glyphs, color-coded prompts).\n- Processes a limited batch of cards for faster testing.\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60 # Height for the new genome report section\nHEADER_HEIGHT = 200  # Height for existing header text area (ID, POEM, PROMPT)\nIMAGE_DISPLAY_HEIGHT = 576  # Height of the 16:9 image itself\n\n# Adjusted CARD_HEIGHT to include the new genome report section\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\n\n# Glyph Definitions (copied from clapper33)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\", \"Chronological Syntagma (CS)\": \"\\u2592\", \n    \"Crystal Syntagma (XS)\": \"\\u2662\", \"Descriptive Syntagma (DS)\": \"\\u259e\",\n    \"Flashback Syntagma (FS)\": \"\\u2599\", \"Thematic Montage (TM)\": \"\\u2588\", \"---\": \" \"\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\", \"Affection-Image\": \"\\u2639\", \"Crystal-Image\": \"\\u2b27\",\n    \"Descriptive Image\": \"\\u263c\", \"Opsign\": \"\\u2a00\", \"Perception-Image\": \"\\u2691\",\n    \"Recollection-Image\": \"\\u2302\", \"Sonsign\": \"\\u266c\", \"Thematic Montage\": \"\\u2263\", \"---\": \" \"\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\", \"Causal Motion Trigger\": \"\\u2794\", \"Emotion Relay\": \"\\u2661\",\n    \"Event Pause Invocation\": \"\\u2016\", \"Memory Storage Retrieval\": \"\\u21bb\", \n    \"Mood Environment Stabilizer\": \"\\u25ff\", \"Narrative Modifier\": \"\\u2726\",\n    \"Subjective Frame Recalibration\": \"\\u223f\", \"Temporal Reflection Loop\": \"\\u2318\", \"---\": \" \"\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_genome_exp\")\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR))\nGENOME_REPORT_PATH = os.path.join(os.path.dirname(BASE_DIR), \"NOTATION\", \"SYMBOLIC_GENOME_REPORT.md\")\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Global variable to store the pre-loaded genome report lines\nPRELOADED_GENOME_LINES = []\n\ndef load_fixed_genome_report_snippet():\n    \"\"\"Loads the first poem's genome block from the report file.\"\"\"\n    global PRELOADED_GENOME_LINES\n    try:\n        with open(GENOME_REPORT_PATH, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        # Regex to find the first genome block (S, I, C lines)\n        # This looks for the start marker, captures the three lines, and stops at the end marker.\n        match = re.search(r\"\\u250c[\\u2500\\u252c]+\\u2510\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 S\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 I\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 C\\s*\\n\\s*\\u2514\", content, re.MULTILINE)\n        if match:\n            PRELOADED_GENOME_LINES = [\n                match.group(1).strip() + \" S\", \n                match.group(2).strip() + \" I\", \n                match.group(3).strip() + \" C\"\n            ]\n        else:\n            PRELOADED_GENOME_LINES = [\"Genome report snippet not found.\", \"Please check GENOME_REPORT_PATH and regex.\", \"\"]\n            print(f\"Warning: Could not find genome block in {GENOME_REPORT_PATH}\")\n            \n    except FileNotFoundError:\n        PRELOADED_GENOME_LINES = [\"SYMBOLIC_GENOME_REPORT.md not found.\", \"\", \"\"]\n        print(f\"Error: {GENOME_REPORT_PATH} not found.\")\n    except Exception as e:\n        PRELOADED_GENOME_LINES = [f\"Error loading genome report: {e}\", \"\", \"\"]\n        print(f\"Error loading genome report: {e}\")\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        # Attempt to find a bold version. OS/font specific. Common: \"FontName Bold.ttf\"\n        font_name_to_try = preferred_name + \" Bold\"\n\n    try:\n        return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n    except IOError:\n        try:\n            return ImageFont.truetype(font_name_to_try + \".otf\", size)\n        except IOError:\n            if preferred_name != fallback_name : # Avoid redundant warnings if preferred is already fallback\n                print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n            \n            fallback_font_name_to_try = fallback_name\n            if is_bold_fallback:\n                 fallback_font_name_to_try = fallback_name + \" Bold\"\n            try:\n                return ImageFont.truetype(fallback_font_name_to_try + \".ttf\", size)\n            except IOError:\n                try:\n                    return ImageFont.truetype(fallback_font_name_to_try + \".otf\", size)\n                except IOError:\n                    print(f\"Warning: Fallback font '{fallback_font_name_to_try}' also not found. Using default font for size {size}.\")\n                    # For Pillow 10.0.0+, size can be passed to load_default. Otherwise, it's ignored.\n                    try:\n                        return ImageFont.load_default(size=size)\n                    except TypeError: # Older Pillow version might not support size argument\n                        return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4 # A bit of leading\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file_name in files:\n                if file_name.startswith(entry_id + '__') and file_name.lower().endswith('.png'):\n                    return os.path.join(root, file_name)\n    return None\n\ndef render_card(entry, output_path, fonts):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    # 1. GENOME REPORT SECTION (Topmost)\n    current_y_offset = 4 # Start after top border\n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    genome_line_height_small = genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1] + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    genome_line_height_tiny = genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1] + 1\n\n    total_genome_text_height_small = len(PRELOADED_GENOME_LINES) * genome_line_height_small\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = genome_line_height_small\n\n    if total_genome_text_height_small > GENOME_REPORT_HEIGHT - 8: # 8 for padding\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = genome_line_height_tiny\n        total_genome_text_height_tiny = len(PRELOADED_GENOME_LINES) * genome_line_height_tiny\n        if total_genome_text_height_tiny > GENOME_REPORT_HEIGHT -8:\n            print(f\"Warning: Genome text still too tall for {entry.get('id')} even with tiny font.\")\n\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - (len(PRELOADED_GENOME_LINES) * chosen_genome_line_height)) // 2 # Center vertically\n    for line in PRELOADED_GENOME_LINES:\n        # Attempt to center text, or left align if too wide\n        line_width = chosen_genome_font.getbbox(line)[2]\n        line_x = 10\n        if line_width < BASE_WIDTH - 20:\n            line_x = (BASE_WIDTH - line_width) // 2\n        draw.text((line_x, genome_y_start), line, font=chosen_genome_font, fill=WHITE)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, IMAGE TYPE, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row's content before next divider\n\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    image_text = f\"IMAGE: {image_glyph} {image_type_str}\"\n\n    draw.text((10, header_content_y), id_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), image_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    \n    current_y_offset += top_row_internal_height # Move down past this row\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 8 # Padding\n\n    # 3. POEM AND LINE CONTENT ROW\n    poem_line_content_y = current_y_offset\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    draw.text((line_start_x, poem_line_content_y), line_prefix, font=header_font, fill=WHITE)\n    \n    line_content_start_x = line_start_x + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_start_x - 10\n    \n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_start_x, poem_line_content_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    \n    # Determine current_y based on where line content finished or default spacing\n    # Calculate standard line height based on the text_font for consistent spacing\n    try:\n        # Get bounding box for a character to estimate line height\n        # Using 'Ay' to get ascent and descent. bbox[3] is y_max, bbox[1] is y_min.\n        line_height_bbox = text_font.getbbox(\"Ay\") \n        ONE_TEXT_LINE_HEIGHT = line_height_bbox[3] - line_height_bbox[1] + 4 # Add a little leading\n    except AttributeError: # Fallback if getbbox is not available or font is minimal\n        ONE_TEXT_LINE_HEIGHT = 16 # Default if font metrics are tricky\n\n    if line_content.strip() == '---' or line_content.strip() == '':\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y_offset), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y_offset = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y_offset), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y_offset = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y_offset, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y_offset), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    style_label_text = \"Style:\"\n    draw.text((10, current_y_offset), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y_offset, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # 5. IMAGE DISPLAY AREA\n    # Ensure the prompt section does not exceed HEADER_HEIGHT (relative to its start)\n    # The image_placement_y is fixed at GENOME_REPORT_HEIGHT + HEADER_HEIGHT from the top of the card (inner black area)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4 # +4 for border\n    \n    # Check if prompt content overflowed (current_y_offset is from top of card, excluding border)\n    prompt_section_end_y = current_y_offset - (GENOME_REPORT_HEIGHT + 4) # Relative to start of header section\n    if prompt_section_end_y > HEADER_HEIGHT:\n         print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({prompt_section_end_y}px) and may overlap image. Max header content height is {HEADER_HEIGHT}px.\")\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card(fonts):\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\", \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\", \"image_path\": None\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_genome_exp.png\")\n    render_card(demo_entry, demo_output_path, fonts)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case(fonts):\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Descriptive Image\", # Default to one with a glyph\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Autonomous Syntagma (AS)\", # Default\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista...\", # Truncated for brevity in code\n            \"cineosisFunction\": \"Mood Environment Stabilizer\", # Default\n            \"styleConditioning\": \"Rendered in Unreal Engine 5...\", \"image_path\": None }\n    else:\n        with open(longest_entry_path, 'r', encoding='utf-8') as f:\n            longest_entry = json.load(f)\n    \n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS: longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS: longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS: longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_genome_exp.png\")\n    render_card(longest_entry, output_path, fonts)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames: poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count: current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER_GENOME_EXP card generation...\")\n    load_fixed_genome_report_snippet() # Load the genome snippet once\n\n    fonts = {\n        'header': get_font(\"DejaVu Sans Mono\", \"Courier New\", 16, is_bold_preferred=True, is_bold_fallback=True),\n        'text':   get_font(\"DejaVu Sans Mono\", \"Courier New\", 16),\n        'text_bold': get_font(\"DejaVu Sans Mono\", \"Courier New\", 16, is_bold_preferred=True, is_bold_fallback=True),\n        'genome_small': get_font(\"DejaVu Sans Mono\", \"Courier New\", 12),\n        'genome_tiny':  get_font(\"DejaVu Sans Mono\", \"Courier New\", 10)\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    render_demo_card(fonts)\n    render_extreme_test_case(fonts)\n\n    count = 0\n    # Process only the first 10 entries for this experiment\n    for entry in processed_timeline_data[:10]:\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_filename = f\"{entry_id}_genome_exp.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        render_card(entry, output_path, fonts)\n        count += 1\n    \n    print(f\"First {count} cards (plus demo/extreme) rendered successfully in {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "_DEMO_CARD_genome_exp.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_genome_exp.png",
      "{entry_id}_genome_exp.png",
      ")\n\n# --- Font loading and helper functions (copied and adapted from clapper33) ---\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        # Attempt to find a bold version. OS/font specific. Common: ",
      ")\n\n    genome_y_start = current_y_offset + (GENOME_REPORT_HEIGHT - (len(PRELOADED_GENOME_LINES) * chosen_genome_line_height)) // 2 # Center vertically\n    for line in PRELOADED_GENOME_LINES:\n        # Attempt to center text, or left align if too wide\n        line_width = chosen_genome_font.getbbox(line)[2]\n        line_x = 10\n        if line_width < BASE_WIDTH - 20:\n            line_x = (BASE_WIDTH - line_width) // 2\n        draw.text((line_x, genome_y_start), line, font=chosen_genome_font, fill=WHITE)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT # Move down past genome section\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1) # Divider\n    current_y_offset += 4 # Padding after divider\n\n    # 2. STANDARD HEADER ROW (ID, IMAGE TYPE, TIME, FRAME)\n    header_content_y = current_y_offset + 4 # Relative to start of this section\n    top_row_internal_height = 30 # Max height for this row",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, poem_line_content_y), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = ",
      ":\n        # If line content is empty, current_y_offset should be one line below poem_line_content_y\n        current_y_offset = poem_line_content_y + ONE_TEXT_LINE_HEIGHT\n    else:\n        # y_after_line is the y-coordinate for the next line after rendering line_content.\n        # Ensure current_y_offset is at least one line below poem_line_content_y or y_after_line if line_content wrapped.\n        current_y_offset = max(y_after_line, poem_line_content_y + ONE_TEXT_LINE_HEIGHT)\n\n    # Add a divider line after Poem/Line and before Prompt\n    current_y_offset += 4 # Padding before the line\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8 # Padding after the line, before prompt starts\n\n    # 4. PROMPT SECTION (Syntagma, Cineosis, Ekphrasis, Style)\n    prompt_max_width = BASE_WIDTH - 20\n\n    syntagma_type_str = entry.get(",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      "First {count} cards (plus demo/extreme) rendered successfully in {OUTPUT_DIR}"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER_GENOME_EXP - Experimental Clapperboard with Genome Report Display\n- Adds a top section to display content from SYMBOLIC_GENOME_REPORT.md.\n- Initially displays a fixed portion of the genome report (first poem's genome).\n- Based on CLAPPER33 (glyphs, color-coded prompts).\n- Processes a limited batch of cards for faster testing."
  },
  {
    "path": "HONEYBADGER/HIVE/clapper23_test.py",
    "size": 10934,
    "lines": 306,
    "source": "#!/usr/bin/env python3\n\"\"\"\nClapper23 Test - Testing with the longest entries to ensure proper fit\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 165  # May need adjustment based on testing\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (50, 50, 50)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nTEST_ENTRY_PATH = os.path.join(SCRIPT_DIR, 'longest_test_entry.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_test')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\"\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\", \n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_test_entry(filepath):\n    \"\"\"Load the test entry from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading test entry: {e}\")\n        return {}\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef wrap_text(text, font, max_width):\n    \"\"\"Wrap text to fit within a given width.\"\"\"\n    if not text:\n        return []\n        \n    words = text.split()\n    lines = []\n    current_line = []\n    \n    for word in words:\n        # Try adding the word to the current line\n        test_line = \" \".join(current_line + [word])\n        text_width = font.getbbox(test_line)[2]\n        \n        if text_width <= max_width:\n            current_line.append(word)\n        else:\n            # Start a new line\n            lines.append(\" \".join(current_line))\n            current_line = [word]\n    \n    # Add the last line\n    if current_line:\n        lines.append(\" \".join(current_line))\n    \n    return lines\n\ndef build_full_prompt(entry):\n    \"\"\"Build a single continuous prompt from entry components.\"\"\"\n    syntagma_type = entry.get('syntagmaType', '')\n    cineosis_func = entry.get('cineosisFunction', '')\n    operative_ekphrasis = entry.get('operativeEkphrasis', '')\n    style_conditioning = entry.get('styleConditioning', '')\n    \n    # Remove any redundant \"(XX)\" from syntagma type\n    syntagma_cleaned = syntagma_type.split('(')[0].strip() if '(' in syntagma_type else syntagma_type\n    \n    # Build the continuous prompt\n    prompt_parts = []\n    if syntagma_cleaned:\n        prompt_parts.append(syntagma_cleaned)\n    if cineosis_func:\n        prompt_parts.append(cineosis_func)\n    if operative_ekphrasis:\n        prompt_parts.append(operative_ekphrasis)\n    \n    # Join the main prompt components\n    main_prompt = \". \".join(prompt_parts)\n    if main_prompt and not main_prompt.endswith('.'):\n        main_prompt += '.'\n    \n    # Add style conditioning as a separate component but still part of the unified prompt\n    if style_conditioning:\n        prompt = f\"{main_prompt} Style: {style_conditioning}\"\n    else:\n        prompt = main_prompt\n        \n    return prompt\n\ndef render_test_card(entry, output_path, header_height=None):\n    \"\"\"Render a test data card to check if content fits.\"\"\"\n    if header_height:\n        global HEADER_HEIGHT\n        HEADER_HEIGHT = header_height\n        global TOTAL_HEIGHT\n        TOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\n        \n    print(f\"Testing with header height: {HEADER_HEIGHT}px\")\n    \n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH + 2*BORDER_WIDTH, TOTAL_HEIGHT + 2*BORDER_WIDTH), BLUE)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font = find_font(HEADER_FONT_PATH, 16)\n    body_font = find_font(BODY_FONT_PATH, 14)\n    label_font = find_font(HEADER_FONT_PATH, 14)\n    prompt_font = find_font(BODY_FONT_PATH, 16)\n    \n    # Calculate row heights for the header section\n    top_row_height = 30\n    second_row_height = 30\n    info_bar_height = 25\n    prompt_section_height = HEADER_HEIGHT - (top_row_height + second_row_height + info_bar_height)\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = f\"FRM: (---)\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), \"TT: 0 DESCRIPTIVE IMAGE\", font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height), (BASE_WIDTH, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_content = entry.get('content', '---')\n    line_text = f\"LINE: \\\"{line_content}\\\"\"\n    \n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # Handle long poem name - wrap if needed\n    poem_width = header_font.getbbox(poem_text)[2]\n    if poem_width > 480:\n        line_x = 10  # Start the line on a new row\n        line_y = top_row_height + 8 + 20\n        draw.text((line_x, line_y), line_text, font=header_font, fill=WHITE)\n    else:\n        line_x = 500\n        line_y = top_row_height + 8\n        draw.text((line_x, line_y), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height + second_row_height), (BASE_WIDTH, top_row_height + second_row_height)], fill=WHITE, width=1)\n    \n    # INFO BAR: AMBER BAR\n    info_bar_y = top_row_height + second_row_height\n    draw.rectangle([(0, info_bar_y), (BASE_WIDTH, info_bar_y + info_bar_height)], fill=AMBER)\n    \n    # Add title in the amber bar\n    draw.text((10, info_bar_y + 4), \"SYNTAGMA\", font=label_font, fill=BLACK)\n    \n    # Get abbreviated syntagma for the leading prefix\n    syntagma_type = entry.get('syntagmaType', '')\n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # PROMPT SECTION: Unified continuous prompt layout\n    prompt_y = info_bar_y + info_bar_height + 10\n    \n    # Build the full continuous prompt from the entry components\n    prefix = syntagma_abbrev if syntagma_abbrev != \"??\" else \"??\"\n    prompt_text = build_full_prompt(entry)\n    \n    # Draw the prefix (syntagma abbreviation)\n    draw.text((10, prompt_y), prefix, font=header_font, fill=AMBER)\n    \n    # Draw the main prompt with wrapping - prefix width is about 30px\n    text_lines = wrap_text(prompt_text, prompt_font, BASE_WIDTH - 40)\n    \n    for i, line in enumerate(text_lines):\n        line_position_y = prompt_y + i * 22\n        # Check if the text is going beyond the header area\n        if line_position_y + 22 > HEADER_HEIGHT:\n            # Mark overflow text in red to indicate it doesn't fit\n            draw.text((40, line_position_y), line, font=prompt_font, fill=(255, 0, 0))\n        else:\n            draw.text((40, line_position_y), line, font=prompt_font, fill=AMBER)\n    \n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # Calculate how much space the prompt needs\n    required_height = prompt_y + len(text_lines) * 22 + 10  # +10 for padding\n    overflow = max(0, required_height - HEADER_HEIGHT)\n    \n    # Draw required height indicator\n    if overflow > 0:\n        draw.text((10, HEADER_HEIGHT - 20), f\"OVERFLOW: {overflow}px\", font=header_font, fill=(255, 0, 0))\n    else:\n        draw.text((10, HEADER_HEIGHT - 20), f\"FITS WITH {HEADER_HEIGHT - required_height}px TO SPARE\", \n                 font=header_font, fill=(0, 255, 0))\n    \n    # 16:9 Image area - Fill with dark gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=DARK_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=img_placeholder_font, fill=LIGHT_GRAY, anchor=\"mm\")\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (BORDER_WIDTH, BORDER_WIDTH))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered test card: {output_path}\")\n    \n    return {\n        \"required_height\": required_height,\n        \"overflow\": overflow,\n        \"text_lines\": len(text_lines)\n    }\n\ndef main():\n    \"\"\"Main function to test rendering with longest entries.\"\"\"\n    print(f\"Loading test entry from: {TEST_ENTRY_PATH}\")\n    test_entry = load_test_entry(TEST_ENTRY_PATH)\n    \n    if not test_entry:\n        print(\"No test entry found. Exiting.\")\n        return\n    \n    print(\"Testing with various header heights to find optimal size...\")\n    \n    # Test with different header heights\n    heights_to_test = [165, 180, 200, 220, 250]\n    results = {}\n    \n    for height in heights_to_test:\n        output_path = os.path.join(OUTPUT_DIR, f\"test_height_{height}.png\")\n        result = render_test_card(test_entry, output_path, height)\n        results[height] = result\n    \n    # Print report\n    print(\"\\n--- TEST RESULTS ---\")\n    print(f\"Full prompt requires {results[heights_to_test[0]]['text_lines']} lines of text\")\n    \n    for height, result in results.items():\n        if result['overflow'] > 0:\n            print(f\"Height {height}px: OVERFLOW by {result['overflow']}px\")\n        else:\n            print(f\"Height {height}px: FITS with {-result['overflow']}px to spare\")\n    \n    # Find the smallest height that fits\n    smallest_fit = None\n    for height in sorted(heights_to_test):\n        if results[height]['overflow'] <= 0:\n            smallest_fit = height\n            break\n    \n    if smallest_fit:\n        print(f\"\\nRECOMMENDATION: Use header height of {smallest_fit}px\")\n    else:\n        print(f\"\\nRECOMMENDATION: Need more than {max(heights_to_test)}px for header\")\n        \n    print(f\"\\nTest card images saved to: {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "longest_test_entry.json",
      "test_height_{height}.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 165  # May need adjustment based on testing\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (50, 50, 50)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nTEST_ENTRY_PATH = os.path.join(SCRIPT_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ", \n                 font=header_font, fill=(0, 255, 0))\n    \n    # 16:9 Image area - Fill with dark gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=DARK_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), "
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Clapper23 Test - Testing with the longest entries to ensure proper fit"
  },
  {
    "path": "HONEYBADGER/HIVE/clapper22.py",
    "size": 10391,
    "lines": 272,
    "source": "#!/usr/bin/env python3\n\"\"\"\nClapper22 - Enhanced Film Style Timeline Visualization\n\nThis script renders timeline entries in a professional film clapperboard style \nwith a clean layout that includes the full prompt information and a 16:9 image area.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 180  # Slightly taller header for better prompt display\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (50, 50, 50)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_clapper22')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\"\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\", \n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef wrap_text(text, font, max_width):\n    \"\"\"Wrap text to fit within a given width.\"\"\"\n    if not text:\n        return [\"\"]\n        \n    words = text.split()\n    lines = []\n    current_line = []\n    \n    for word in words:\n        # Try adding the word to the current line\n        test_line = \" \".join(current_line + [word])\n        text_width = font.getbbox(test_line)[2]\n        \n        if text_width <= max_width:\n            current_line.append(word)\n        else:\n            # Start a new line\n            lines.append(\" \".join(current_line))\n            current_line = [word]\n    \n    # Add the last line\n    if current_line:\n        lines.append(\" \".join(current_line))\n    \n    return lines\n\ndef render_card(entry, output_path):\n    \"\"\"Render a clean data card in film clapperboard style with full prompt text.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH + 2*BORDER_WIDTH, TOTAL_HEIGHT + 2*BORDER_WIDTH), BLUE)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font = find_font(HEADER_FONT_PATH, 16)\n    body_font = find_font(BODY_FONT_PATH, 14)\n    label_font = find_font(HEADER_FONT_PATH, 14)\n    prompt_font = find_font(BODY_FONT_PATH, 16)\n    \n    # Calculate row heights for the header section\n    top_row_height = 30\n    second_row_height = 30\n    info_bar_height = 25\n    prompt_section_height = HEADER_HEIGHT - (top_row_height + second_row_height + info_bar_height)\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = f\"FRM: (---)\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), \"TT: 0 DESCRIPTIVE IMAGE\", font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height), (BASE_WIDTH, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_content = entry.get('content', '---')\n    line_text = f\"LINE: \\\"{line_content}\\\"\"\n    \n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    draw.text((500, top_row_height + 8), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height + second_row_height), (BASE_WIDTH, top_row_height + second_row_height)], fill=WHITE, width=1)\n    \n    # INFO BAR: Column headers - AMBER BAR\n    info_bar_y = top_row_height + second_row_height\n    draw.rectangle([(0, info_bar_y), (BASE_WIDTH, info_bar_y + info_bar_height)], fill=AMBER)\n    \n    # Add column headers in the amber bar\n    draw.text((10, info_bar_y + 4), \"SYNTAGMA\", font=label_font, fill=BLACK)\n    draw.text((150, info_bar_y + 4), \"CINEOSIS FUNCTION\", font=label_font, fill=BLACK)\n    draw.text((550, info_bar_y + 4), \"OPERATIVE EKPHRASIS\", font=label_font, fill=BLACK)\n    \n    # PROMPT SECTION: Clean layout for the full prompt information\n    syntagma_type = entry.get('syntagmaType', '---')\n    cineosis_func = entry.get('cineosisFunction', '---')\n    operative_ekphrasis = entry.get('operativeEkphrasis', '')\n    style_conditioning = entry.get('styleConditioning', '')\n    \n    # Get abbreviated syntagma\n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # Calculate positioning for prompt content\n    prompt_y = info_bar_y + info_bar_height + 10\n    col1_x = 10\n    col2_x = 150\n    col3_x = 550\n    text_line_height = 20\n    \n    # Draw syntagma type\n    draw.text((col1_x, prompt_y), f\"{syntagma_abbrev}\", font=prompt_font, fill=AMBER)\n    \n    # Draw cineosis function with bullet point and wrap text if needed\n    cineosis_bullet = f\"\u00b7 {cineosis_func}\"\n    cineosis_lines = wrap_text(cineosis_bullet, prompt_font, 380)\n    for i, line in enumerate(cineosis_lines):\n        draw.text((col2_x, prompt_y + i * text_line_height), line, font=prompt_font, fill=AMBER)\n    \n    # Draw operative ekphrasis with bullet point and wrap text if needed\n    ekphrasis_bullet = f\"\u00b7 {operative_ekphrasis}\" if operative_ekphrasis else \"\"\n    ekphrasis_lines = wrap_text(ekphrasis_bullet, prompt_font, 460)\n    for i, line in enumerate(ekphrasis_lines):\n        draw.text((col3_x, prompt_y + i * text_line_height), line, font=prompt_font, fill=AMBER)\n    \n    # Add style conditioning if present\n    if style_conditioning:\n        style_y = prompt_y + max(len(cineosis_lines), len(ekphrasis_lines)) * text_line_height + 10\n        draw.line([(0, style_y - 5), (BASE_WIDTH, style_y - 5)], fill=DARK_AMBER, width=1)\n        \n        # Add style conditioning header\n        draw.text((10, style_y), \"STYLE:\", font=label_font, fill=AMBER)\n        \n        # Add style conditioning text with wrapping\n        style_lines = wrap_text(style_conditioning, prompt_font, BASE_WIDTH - 80)\n        for i, line in enumerate(style_lines):\n            draw.text((80, style_y + i * text_line_height), line, font=prompt_font, fill=AMBER)\n    \n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # 16:9 Image area - Fill with dark gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=DARK_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=img_placeholder_font, fill=LIGHT_GRAY, anchor=\"mm\")\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (BORDER_WIDTH, BORDER_WIDTH))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_clapper.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    print(f\"Header height: {HEADER_HEIGHT}px\")\n    print(f\"Image height (16:9): {IMAGE_HEIGHT}px\")\n    \n    # Render first 10 entries as samples\n    render_all_entries(timeline, OUTPUT_DIR, 10)\n    \n    # Also render a demo card showing the layout\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"SH001\",\n        \"timestamp\": \"00:03:51\",\n        \"poem\": \"Out of Life\",\n        \"content\": \"What we've made-\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"A symbolic tableau rendered through chiaroscuro\",\n        \"styleConditioning\": \"High contrast black and white photography with deep shadows and bright highlights. Dramatic lighting with silhouettes and volumetric light effects.\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_clapper.png\")\n    render_card(demo_entry, demo_path)\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {BASE_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {BASE_WIDTH}x{TOTAL_HEIGHT}px\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_clapper.png",
      "demo_clapper.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 180  # Slightly taller header for better prompt display\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (50, 50, 50)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ", font=label_font, fill=AMBER)\n        \n        # Add style conditioning text with wrapping\n        style_lines = wrap_text(style_conditioning, prompt_font, BASE_WIDTH - 80)\n        for i, line in enumerate(style_lines):\n            draw.text((80, style_y + i * text_line_height), line, font=prompt_font, fill=AMBER)\n    \n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # 16:9 Image area - Fill with dark gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=DARK_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), ",
      "Rendered {i+1}/{len(entries_to_render)} cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Clapper22 - Enhanced Film Style Timeline Visualization\n\nThis script renders timeline entries in a professional film clapperboard style \nwith a clean layout that includes the full prompt information and a 16:9 image area."
  },
  {
    "path": "HONEYBADGER/HIVE/font_glyph_tester.py",
    "size": 5047,
    "lines": 135,
    "source": "#!/usr/bin/env python3\n\"\"\"\nFont Glyph Tester - Renders a test sheet for genome report glyphs with various fonts.\n\"\"\"\n\nimport os\nimport re\nfrom PIL import Image, ImageDraw, ImageFont\n\n# --- Configuration ---\nOUTPUT_IMAGE_NAME = \"font_glyph_test_sheet.png\"\nIMAGE_WIDTH = 800\nLINE_HEIGHT = 20       # For font name labels\nGLYPH_LINE_HEIGHT = 18 # For rendering glyph lines\nGLYPH_FONT_SIZE = 12\nLABEL_FONT_SIZE = 14\nTOP_MARGIN = 20\nLEFT_MARGIN = 20\nSECTION_SPACING = 15 # Space between each font test section\n\nFONTS_TO_TEST = [\n    \"DejaVu Sans Mono\",\n    \"Courier New\",\n    \"Menlo\",\n    \"Monaco\",\n    \"Andale Mono\",\n    \"SF Mono\", # Apple's San Francisco Mono (may not be universally available)\n    \"Lucida Console\",\n    \"Consolas\",\n    \"Noto Sans Mono\", # Google Noto font, very comprehensive\n    \"Source Code Pro\",\n    \"monospace\" # Generic family name\n]\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nGRAY = (150, 150, 150)\nRED = (255, 100, 100)\n\n# --- Genome Snippet Loading Logic (from clapper_genome_exp.py) ---\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nGENOME_REPORT_PATH = os.path.join(os.path.dirname(BASE_DIR), \"NOTATION\", \"SYMBOLIC_GENOME_REPORT.md\")\nPRELOADED_GENOME_LINES = []\n\ndef load_fixed_genome_report_snippet():\n    global PRELOADED_GENOME_LINES\n    try:\n        with open(GENOME_REPORT_PATH, 'r', encoding='utf-8') as f:\n            content = f.read()\n        match = re.search(r\"\\u250c[\\u2500\\u252c]+\\u2510\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 S\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 I\\s*\\n\\s*\\\u2502([^\u2502\\n]+)\u2502 C\\s*\\n\\s*\\u2514\", content, re.MULTILINE)\n        if match:\n            PRELOADED_GENOME_LINES = [\n                match.group(1).strip() + \" S\", \n                match.group(2).strip() + \" I\", \n                match.group(3).strip() + \" C\"\n            ]\n        else:\n            PRELOADED_GENOME_LINES = [\"Genome report snippet not found in MD file.\", \"\", \"\"]\n            print(f\"Warning: Could not find genome block in {GENOME_REPORT_PATH}\")\n    except FileNotFoundError:\n        PRELOADED_GENOME_LINES = [f\"{os.path.basename(GENOME_REPORT_PATH)} not found.\", \"\", \"\"]\n        print(f\"Error: {GENOME_REPORT_PATH} not found.\")\n    except Exception as e:\n        PRELOADED_GENOME_LINES = [f\"Error loading report: {e}\", \"\", \"\"]\n        print(f\"Error loading genome report: {e}\")\n\ndef get_test_font(font_name, size):\n    \"\"\"Attempts to load a font by name, trying common extensions.\"\"\"\n    common_styles = [\"\", \" Regular\", \"-Regular\"] # Some fonts might have style in name\n    bold_styles = [\" Bold\", \"-Bold\", \"_Bold\"]\n    extensions = [\".ttf\", \".otf\"]\n    \n    # Try base name + common styles\n    for style in common_styles:\n        for ext in extensions:\n            try:\n                return ImageFont.truetype(font_name + style + ext, size)\n            except IOError:\n                continue\n    # Try bold styles if base not found (though less likely for mono glyphs)\n    # for style in bold_styles:\n    #     for ext in extensions:\n    #         try: return ImageFont.truetype(font_name + style + ext, size)\n    #         except IOError: continue\n    return None\n\ndef main():\n    load_fixed_genome_report_snippet()\n\n    if not PRELOADED_GENOME_LINES or \"not found\" in PRELOADED_GENOME_LINES[0]:\n        print(\"Could not load genome lines for testing. Aborting.\")\n        return\n\n    num_glyph_lines = len(PRELOADED_GENOME_LINES)\n    estimated_section_height = LINE_HEIGHT + (num_glyph_lines * GLYPH_LINE_HEIGHT) + SECTION_SPACING\n    image_height = TOP_MARGIN * 2 + len(FONTS_TO_TEST) * estimated_section_height\n\n    img = Image.new('RGB', (IMAGE_WIDTH, image_height), WHITE)\n    draw = ImageDraw.Draw(img)\n    \n    # Default font for labels if others fail\n    try: default_label_font = ImageFont.load_default(size=LABEL_FONT_SIZE) \n    except TypeError: default_label_font = ImageFont.load_default()\n\n    current_y = TOP_MARGIN\n\n    for font_name_to_test in FONTS_TO_TEST:\n        # Draw font name label\n        draw.text((LEFT_MARGIN, current_y), f\"Font: {font_name_to_test}\", font=default_label_font, fill=BLACK)\n        current_y += LINE_HEIGHT\n\n        glyph_font = get_test_font(font_name_to_test, GLYPH_FONT_SIZE)\n\n        if glyph_font:\n            for line in PRELOADED_GENOME_LINES:\n                draw.text((LEFT_MARGIN + 10, current_y), line, font=glyph_font, fill=BLACK)\n                current_y += GLYPH_LINE_HEIGHT\n        else:\n            draw.text((LEFT_MARGIN + 10, current_y), \"Font not found on system.\", font=default_label_font, fill=RED)\n            current_y += GLYPH_LINE_HEIGHT * num_glyph_lines # Keep spacing consistent\n        \n        current_y += SECTION_SPACING\n        draw.line([(LEFT_MARGIN, current_y - SECTION_SPACING//2), (IMAGE_WIDTH - LEFT_MARGIN, current_y - SECTION_SPACING//2)], fill=GRAY, width=1)\n\n    output_path = os.path.join(BASE_DIR, OUTPUT_IMAGE_NAME)\n    try:\n        img.save(output_path)\n        print(f\"Font glyph test sheet saved to: {output_path}\")\n    except Exception as e:\n        print(f\"Error saving image: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "font_glyph_test_sheet.png",
      ", font=default_label_font, fill=RED)\n            current_y += GLYPH_LINE_HEIGHT * num_glyph_lines # Keep spacing consistent\n        \n        current_y += SECTION_SPACING\n        draw.line([(LEFT_MARGIN, current_y - SECTION_SPACING//2), (IMAGE_WIDTH - LEFT_MARGIN, current_y - SECTION_SPACING//2)], fill=GRAY, width=1)\n\n    output_path = os.path.join(BASE_DIR, OUTPUT_IMAGE_NAME)\n    try:\n        img.save(output_path)\n        print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "PIL"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Font Glyph Tester - Renders a test sheet for genome report glyphs with various fonts."
  },
  {
    "path": "HONEYBADGER/HIVE/render_minimal.py",
    "size": 6424,
    "lines": 175,
    "source": "#!/usr/bin/env python3\n\"\"\"\nMinimal data card renderer focused on maximizing prompt visibility\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Constants\nBASE_WIDTH = 1200\nASPECT_16_9 = 16/9\nASPECT_3_2 = 3/2\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_minimal')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\n\n# Basic loading function\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card focused on prompt visibility.\"\"\"\n    # Create base image with blue border\n    img = Image.new('RGB', (BASE_WIDTH+8, TOTAL_HEIGHT+8), BLUE_BORDER)\n    canvas = Image.new('RGB', (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    tiny_font = find_font(BODY_FONT_PATH, 14)\n    header_font = find_font(HEADER_FONT_PATH, 16)\n    prompt_font = find_font(BODY_FONT_PATH, 18)\n    \n    # Fill header area black, image area gray\n    draw.rectangle([(0, 0), (BASE_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Draw dividing line between header and image\n    draw.line([(0, HEADER_HEIGHT-1), (BASE_WIDTH, HEADER_HEIGHT-1)], fill=WHITE, width=2)\n    \n    # Placeholder text for image area\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n              \"16:9 Image Area\", font=header_font, fill=BLACK, anchor=\"mm\")\n    \n    # Ultra compact top info section - just 20% of header height\n    info_height = HEADER_HEIGHT * 0.2\n    \n    # Row 1: ID | TIME | Metadata\n    basic_info_y = 4\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    \n    draw.text((5, basic_info_y), id_text, font=tiny_font, fill=WHITE)\n    draw.text((200, basic_info_y), time_text, font=tiny_font, fill=WHITE)\n    draw.text((400, basic_info_y), poem_text, font=tiny_font, fill=WHITE)\n    \n    # Row 2: Line content (truncated if needed)\n    line_content = entry.get('content', '---')\n    if len(line_content) > 70:\n        line_content = line_content[:67] + \"...\"\n    line_text = f\"LINE: \\\"{line_content}\\\"\"\n    \n    draw.text((5, basic_info_y + 20), line_text, font=tiny_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, info_height), (BASE_WIDTH, info_height)], fill=WHITE, width=1)\n    \n    # PROMPT SECTION - 80% of header height\n    prompt_area_y = info_height + 5\n    \n    # Get all prompt components\n    syntagma_type = entry.get('syntagmaType', '---')\n    syntagma_abbrev = syntagma_type[:2] if syntagma_type else \"??\"\n    cineosis_func = entry.get('cineosisFunction', '---')\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    style_conditioning = entry.get('styleConditioning', '---')\n    image_type = entry.get('imageType', '---')\n    \n    # Format the full prompt with clear spacing and section indicators \n    prompt_parts = []\n    prompt_parts.append(f\"[SYNTAGMA]: {syntagma_type}\")\n    prompt_parts.append(f\"[IMAGE]: {image_type}\")\n    prompt_parts.append(f\"[CINEOSIS]: {cineosis_func}\")\n    prompt_parts.append(f\"[EKPHRASIS]: {operative_ekphrasis}\")\n    if style_conditioning:\n        prompt_parts.append(f\"[STYLE]: {style_conditioning}\")\n    \n    # Draw prompt parts with clear separation\n    current_y = prompt_area_y\n    line_spacing = 22\n    \n    for part in prompt_parts:\n        wrapped_lines = textwrap.wrap(part, width=85)  # Wider wrapping for maximum text\n        for line in wrapped_lines:\n            if current_y < HEADER_HEIGHT - 5:  # Stay within header bounds\n                draw.text((5, current_y), line, font=prompt_font, fill=WHITE)\n                current_y += line_spacing\n    \n    # Paste canvas onto bordered image\n    img.paste(canvas, (4, 4))\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    \n    # Render first 10 entries as samples\n    for i, entry in enumerate(timeline[:10]):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(OUTPUT_DIR, f\"{entry_id}_card.png\")\n        render_card(entry, output_path)\n    \n    # Demo entry\n    demo_entry = {\n        \"id\": \"DEMO\",\n        \"timestamp\": \"00:00:00\",\n        \"poem\": \"Layout Example\",\n        \"content\": \"3:2 overall with 16:9 image\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"This is a demo card showing maximized prompt visibility with full details. All prompt components are clearly labeled and given priority in the layout.\",\n        \"styleConditioning\": \"High contrast black and white photography with deep shadows and dramatic lighting.\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_layout.png\")\n    render_card(demo_entry, demo_path)\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_card.png",
      "demo_layout.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Constants\nBASE_WIDTH = 1200\nASPECT_16_9 = 16/9\nASPECT_3_2 = 3/2\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      ", (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    tiny_font = find_font(BODY_FONT_PATH, 14)\n    header_font = find_font(HEADER_FONT_PATH, 16)\n    prompt_font = find_font(BODY_FONT_PATH, 18)\n    \n    # Fill header area black, image area gray\n    draw.rectangle([(0, 0), (BASE_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Draw dividing line between header and image\n    draw.line([(0, HEADER_HEIGHT-1), (BASE_WIDTH, HEADER_HEIGHT-1)], fill=WHITE, width=2)\n    \n    # Placeholder text for image area\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n              "
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Minimal data card renderer focused on maximizing prompt visibility"
  },
  {
    "path": "HONEYBADGER/HIVE/clapper32.py",
    "size": 21180,
    "lines": 444,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER32 - Film Clapperboard Style with Color-Coded Prompts (from clapper31)\n- Image integration from image_path field in timeline data \n- Poem and Line Content on same line\n- Updated header labels: \"IMAGE\" and \"FRAME\"\n- Color-coded prompt components: syntagma, cineosis, ekphrasis (bold amber), style.\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\n\n# Constants\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 200  # Increased height for text\nCARD_HEIGHT = HEADER_HEIGHT + 576  # 576 is the height for a 16:9 aspect ratio of 1024 width\nIMAGE_HEIGHT = 576  # 16:9 aspect ratio\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\n# Colors for different prompt components - from clapper31, for readability\nSYNTAGMA_COLOR = (100, 200, 255)  # Brighter blue\nCINEOSIS_COLOR = (230, 150, 255)  # Lighter purple\nEKPHRASIS_COLOR = AMBER           # Same as LINE color (amber), for operative ekphrasis\nSTYLE_COLOR = (150, 255, 180)     # Lighter green\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\") # Using the complete timeline with image paths\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper32\")\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR)) # Base directory for relative image paths\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Helper function to wrap text\ndef wrap_text(text, font, max_width):\n    \"\"\"Wrap text to fit within a given width.\"\"\"\n    words = text.split()\n    wrapped_lines = []\n    current_line = []\n    \n    for word in words:\n        # Test with current word added\n        test_line = ' '.join(current_line + [word])\n        line_width = font.getbbox(test_line)[2]\n        \n        if line_width <= max_width:\n            current_line.append(word)\n        else:\n            # If the current line has words, complete it\n            if current_line:\n                wrapped_lines.append(' '.join(current_line))\n                current_line = [word]\n            else:\n                # If the word itself is too long, force it on its own line\n                wrapped_lines.append(word)\n                current_line = []\n    \n    # Add the last line if there's anything left\n    if current_line:\n        wrapped_lines.append(' '.join(current_line))\n    \n    return wrapped_lines\n\n# Helper function to draw colored text and return the new position (adapted from clapper31)\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    \"\"\"Draw text with specified color and font, handling wrapping. text_font_for_metrics is used for width calculation if provided.\"\"\"\n    if not text or text.strip() == '---' or text.strip() == '':  # Also check for placeholder\n        return y\n\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n\n    words = text.split()\n    current_line = []\n    current_width = 0\n    start_x = x\n\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line else 0\n\n        if max_width is not None and current_line and current_width + word_width + space_width > max_width:\n            line_text = ' '.join(current_line)\n            draw.text((start_x, y), line_text, font=font, fill=color)\n            y += 16  # Move to next line (assuming 16px line height)\n            current_line = [word]\n            current_width = word_width\n        else:\n            current_line.append(word)\n            current_width += word_width + space_width\n\n    if current_line:\n        line_text = ' '.join(current_line)\n        draw.text((start_x, y), line_text, font=font, fill=color)\n        y += 16  # Add space after this component\n\n    return y\n\ndef get_image_path(entry):\n    \"\"\"Get the image path from the entry data or return None.\"\"\"\n    # Check if image_path exists in the entry\n    if 'image_path' in entry and entry['image_path']:\n        # Construct absolute path from relative path in the timeline data\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        # Try TIGER directory as alternative\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    \n    # Fallback: search in TIGER directory by ID\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        # Try to find any file with ID prefix\n        for root, _, files in os.walk(base_dir):\n            for file in files:\n                if file.startswith(entry_id + '__') and file.lower().endswith('.png'):\n                    return os.path.join(root, file)\n    \n    return None\n\ndef render_card(entry, output_path):\n    \"\"\"Render a single data card with the entry information.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH+8, CARD_HEIGHT+8), BLUE_BORDER)\n    \n    # Create inner black image (main card)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    \n    draw = ImageDraw.Draw(img)\n    \n    # Load fonts\n    try:\n        header_font = ImageFont.truetype(\"Courier New Bold.ttf\", 16)  \n        text_font = ImageFont.truetype(\"Courier New.ttf\", 16)\n    except IOError:\n        # Fallback to default font if Courier New is not available\n        header_font = ImageFont.load_default()\n        text_font = ImageFont.load_default()\n        print(\"Warning: Courier New font not found, using default font.\")\n    \n    top_row_height = 30  # Height of the top row\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    \n    # Get frame position information if available\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    \n    # Image type without the frame number\n    image_type = entry.get('imageType', 'Descriptive Image')\n    image_text = f\"IMAGE: {image_type}\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), image_text, font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(4, top_row_height), (BASE_WIDTH+4, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line with proper wrapping for long content\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    \n    # Calculate poem width\n    poem_width = header_font.getbbox(poem_text)[2]\n    \n    # Draw poem name\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # LINE CONTENT: Highlighted with amber to give it emphasis\n    # Calculate start position for line content (after poem with some spacing)\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)  # Either after poem or at half width\n    \n    # Draw the line content with amber highlighting\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    \n    # Draw prefix in white\n    draw.text((line_start_x, top_row_height + 8), line_prefix, font=header_font, fill=WHITE)\n    \n    # Available width for line content\n    available_width = BASE_WIDTH - line_start_x - line_prefix_width - 20\n    \n    # Wrap the line content if needed\n    wrapped_content = wrap_text(line_content, header_font, available_width)\n    \n    for i, line_part in enumerate(wrapped_content):\n        y_pos = top_row_height + 8 + (i * 16)\n        \n        # Add closing quote to last line\n        if i == len(wrapped_content) - 1:\n            line_part += \"\\\"\"\n        \n        # Draw the line content in amber\n        draw.text((line_start_x + line_prefix_width, y_pos), line_part, font=header_font, fill=AMBER)\n    \n    # Calculate actual height used by the line content\n    line_content_actual_height = 20 + max(1, len(wrapped_content)) * 16  # Base + line height\n    \n    # Draw horizontal divider after line content\n    divider_y = top_row_height + line_content_actual_height\n    draw.line([(4, divider_y), (BASE_WIDTH+4, divider_y)], fill=WHITE, width=1)\n    \n    # PROMPT ROW: Color-coded components (adapted from clapper31)\n    try:\n        bold_font = ImageFont.truetype(\"Courier New Bold.ttf\", 16)\n    except IOError:\n        bold_font = header_font # Fallback to header_font if bold not available\n\n    # Extract or construct prompt components\n    if 'full_prompt' in entry and entry['full_prompt']:\n        parts = entry['full_prompt'].split(' \u00b7 ')\n        if len(parts) >= 4:\n            syntagma_abbr = parts[0]\n            operative_ekphrasis = parts[2] # Ekphrasis is 3rd part in original construction\n            cineosis_function = parts[1]   # Cineosis is 2nd part\n            style_conditioning = parts[3]\n        elif len(parts) == 3: # Assuming older format: syntagma, ekphrasis, style\n            syntagma_abbr = parts[0]\n            operative_ekphrasis = parts[1]\n            cineosis_function = entry.get('cineosisFunction', '---') # Get from field if missing\n            style_conditioning = parts[2]\n        else: # Fallback if parts aren't properly separated or too few\n            syntagma_type_full = entry.get('syntagmaType', '')\n            if '(' in syntagma_type_full and ')' in syntagma_type_full:\n                syntagma_abbr = syntagma_type_full.split('(')[1].split(')')[0]\n            else: syntagma_abbr = ''.join(word[0] for word in syntagma_type_full.split() if word) or 'SS'\n            operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n            cineosis_function = entry.get('cineosisFunction', '---')\n            style_conditioning = entry.get('styleConditioning', '---')\n    else:\n        syntagma_type_full = entry.get('syntagmaType', '')\n        if '(' in syntagma_type_full and ')' in syntagma_type_full:\n            syntagma_abbr = syntagma_type_full.split('(')[1].split(')')[0]\n        else: syntagma_abbr = ''.join(word[0] for word in syntagma_type_full.split() if word) or 'SS'\n        operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n        cineosis_function = entry.get('cineosisFunction', '---')\n        style_conditioning = entry.get('styleConditioning', '---')\n\n    current_y = divider_y + 8\n    left_margin = 10\n    prompt_max_width = BASE_WIDTH - (2 * left_margin)\n\n    # Draw PROMPT: label\n    draw.text((left_margin, current_y), \"PROMPT:\", font=header_font, fill=WHITE)\n    current_y += 18 # Give space after PROMPT: label\n\n    # Draw Syntagma Abbreviation\n    current_y = draw_colored_text(draw, f\"Syntagma: {syntagma_abbr}\", left_margin, current_y, SYNTAGMA_COLOR, text_font, prompt_max_width, text_font_for_metrics=text_font)\n    # Draw Cineosis Function\n    current_y = draw_colored_text(draw, f\"Cineosis: {cineosis_function}\", left_margin, current_y, CINEOSIS_COLOR, text_font, prompt_max_width, text_font_for_metrics=text_font)\n    # Draw Operative Ekphrasis (bold amber)\n    current_y = draw_colored_text(draw, f\"Ekphrasis: {operative_ekphrasis}\", left_margin, current_y, EKPHRASIS_COLOR, bold_font, prompt_max_width, text_font_for_metrics=bold_font)\n    # Draw Style Conditioning\n    current_y = draw_colored_text(draw, f\"Style: {style_conditioning}\", left_margin, current_y, STYLE_COLOR, text_font, prompt_max_width, text_font_for_metrics=text_font)\n\n    prompt_actual_height = current_y - (divider_y + 8) # Total height of the prompt section content\n\n    # Calculate Y offset for the image, ensuring it's below all text\n    image_y_offset = divider_y + prompt_actual_height + 4 # Small padding after prompt section\n    \n    # Ensure image_y_offset is at least HEADER_HEIGHT, or if text is too long, cap it.\n    if image_y_offset < HEADER_HEIGHT:\n        image_y_offset = HEADER_HEIGHT\n    elif image_y_offset > CARD_HEIGHT - IMAGE_HEIGHT - 4: # Cap to prevent image going off card\n        image_y_offset = CARD_HEIGHT - IMAGE_HEIGHT - 4 \n        # print(f\"Warning: Text content for entry {entry.get('id')} too long, image might be clipped or prompt truncated.\")\n\n\n    # Get the actual image file path from the entry\n    image_path = get_image_path(entry)\n    \n    if image_path and os.path.exists(image_path):\n        try:\n            card_image = Image.open(image_path)\n            # Resize image to fit BASE_WIDTH x IMAGE_HEIGHT while maintaining aspect ratio\n            card_image.thumbnail((BASE_WIDTH, IMAGE_HEIGHT), Image.Resampling.LANCZOS)\n            \n            # Create a black background for the image area\n            img_bg = Image.new('RGB', (BASE_WIDTH, IMAGE_HEIGHT), BLACK)\n            \n            # Calculate position to center the image\n            pos_x = (BASE_WIDTH - card_image.width) // 2\n            pos_y = (IMAGE_HEIGHT - card_image.height) // 2\n            \n            img_bg.paste(card_image, (pos_x, pos_y))\n            img.paste(img_bg, (4, image_y_offset + 4)) # Paste into main image, considering border\n        except Exception as e:\n            print(f\"Error loading or processing image {image_path} for entry {entry.get('id', 'N/A')}: {e}\")\n            # Draw a placeholder if image fails to load\n            draw.rectangle([4, image_y_offset + 4, BASE_WIDTH + 4, image_y_offset + IMAGE_HEIGHT + 4], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_y_offset + IMAGE_HEIGHT // 2), \"Image Error\", fill=BLACK, font=header_font)\n    else:\n        # Draw a placeholder if no image path or image doesn't exist\n        draw.rectangle([4, image_y_offset + 4, BASE_WIDTH + 4, image_y_offset + IMAGE_HEIGHT + 4], fill=LIGHT_GRAY)\n        no_image_text = \"No Image Available\"\n        text_bbox = draw.textbbox((0,0), no_image_text, font=header_font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        draw.text(((BASE_WIDTH - text_width) // 2 + 4, image_y_offset + (IMAGE_HEIGHT - text_height) // 2 + 4), no_image_text, fill=BLACK, font=header_font)\n\n    # Save the image\n    img.save(output_path)\n    # print(f\"Rendered card: {output_path}\")\n\ndef render_demo_card():\n    \"\"\"Render a demo card with sample data.\"\"\"\n    demo_entry = {\n        \"id\": \"DEMO001\",\n        \"timestamp\": \"00:00:00\",\n        \"frame_position\": 1,\n        \"frame_total\": 1,\n        \"imageType\": \"Synthesized Image\",\n        \"poem\": \"The Great Work\",\n        \"content\": \"This is a demonstration line of poetic text for the clapperboard card.\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"operativeEkphrasis\": \"A lone figure stands on a neon-lit street corner, rain reflecting the city lights.\",\n        \"cineosisFunction\": \"Establishing Shot - Urban Despair\",\n        \"styleConditioning\": \"Shot on Kodak Vision3 500T, anamorphic lens, moody, atmospheric, Blade Runner aesthetic\",\n        \"image_path\": None # No image for demo\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_clapper32.png\")\n    render_card(demo_entry, demo_output_path)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\n\ndef render_extreme_test_case():\n    \"\"\"Render a test card with the longest entries to test layout boundaries.\"\"\"\n    pass # Ensure correct indentation for the first line of the function body\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        # Create a dummy longest entry if file not found\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Very Long Image Type Description That Pushes Boundaries\",\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Ultra Detailed Complex Syntagma Type (UDCST)\",\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista of an alien megacity at twilight, with towering, bio-luminescent skyscrapers, intricate sky-bridges teeming with exotic vehicles, and a sky filled with multiple moons and nebulae. The level of detail is immense, capturing every nuance of this fantastical urban landscape, from the smallest architectural features to the grand scale of the celestial backdrop. This description aims to be as long as possible to truly stress test the system.\",\n            \"cineosisFunction\": \"Establishing Shot - Grand Scale Alien Metropolis - Introduction to World Setting and Atmosphere - Evoking Wonder and Awe\",\n            \"styleConditioning\": \"Rendered in Unreal Engine 5 with Lumen and Nanite, 8K resolution, cinematic depth of field, volumetric lighting, ray-traced reflections and shadows, color graded with a rich, vibrant palette, inspired by the works of Syd Mead and Moebius, aiming for photorealistic yet fantastical quality.\",\n            \"image_path\": None\n        }\n    else:\n        with open(longest_entry_path, 'r') as f:\n            longest_entry = json.load(f)\n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_CLAPPER32.png\")\n    render_card(longest_entry, output_path)\n    print(f\"Rendered extreme test card: {output_path}\")\n\n\ndef calculate_frame_counts(timeline_data):\n    \"\"\"Calculate the total number of frames per poem and frame position for each entry.\"\"\"\n    poem_frames = {}\n    # First pass: count total frames for each poem\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name:\n            poem_frames[poem_name] = poem_frames.get(poem_name, 0) + 1\n            \n    # Second pass: assign frame_total and frame_position\n    current_poem_counts = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name:\n            entry['frame_total'] = poem_frames[poem_name]\n            current_poem_counts[poem_name] = current_poem_counts.get(poem_name, 0) + 1\n            entry['frame_position'] = current_poem_counts[poem_name]\n        else:\n            # Handle entries without a poem name (e.g., global metadata)\n            entry['frame_total'] = 0\n            entry['frame_position'] = 0\n    return timeline_data\n\ndef main():\n    \"\"\"Main function to load timeline and render all cards.\"\"\"\n    # Load timeline data\n    if not os.path.exists(TIMELINE_PATH):\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\")\n        # Create a dummy timeline if not found\n        timeline_data = [\n            {\n                \"id\": \"DUMMY001\", \"timestamp\": \"00:00:10\", \"imageType\": \"Placeholder Image\",\n                \"poem\": \"Dummy Poem\", \"content\": \"This is a line from a dummy poem.\",\n                \"syntagmaType\": \"Generic Syntagma (GS)\", \"operativeEkphrasis\": \"A simple placeholder scene.\",\n                \"cineosisFunction\": \"Filler\", \"styleConditioning\": \"Basic style\", \"image_path\": None\n            },\n            {\n                \"id\": \"DUMMY002\", \"timestamp\": \"00:00:20\", \"imageType\": \"Another Placeholder\",\n                \"poem\": \"Dummy Poem\", \"content\": \"Another line from the same dummy poem.\",\n                \"syntagmaType\": \"Generic Syntagma (GS)\", \"operativeEkphrasis\": \"A different placeholder scene.\",\n                \"cineosisFunction\": \"More Filler\", \"styleConditioning\": \"Slightly different basic style\", \"image_path\": None\n            }\n        ]\n        print(\"Using dummy timeline data as fallback.\")\n    else:\n        with open(TIMELINE_PATH, 'r') as f:\n            timeline_data = json.load(f)\n\n    # Calculate frame counts\n    timeline_data = calculate_frame_counts(timeline_data)\n\n    # Render demo card\n    render_demo_card()\n    \n    # Render extreme test card\n    render_extreme_test_case()\n\n    # Render cards for each entry in the timeline\n    total_entries = len(timeline_data)\n    for i, entry in enumerate(timeline_data):\n        entry_id = entry.get('id', f'unknown_id_{i+1}')\n        output_filename = f\"{entry_id}_clapper32.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        \n        # print(f\"Rendering card {i+1}/{total_entries}: {output_filename}\") # Verbose logging\n        render_card(entry, output_path)\n    \n    print(f\"\\nAll {total_entries} cards rendered successfully in {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "_DEMO_CARD_clapper32.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_CLAPPER32.png",
      "{entry_id}_clapper32.png",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    \n    # Calculate poem width\n    poem_width = header_font.getbbox(poem_text)[2]\n    \n    # Draw poem name\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    \n    # LINE CONTENT: Highlighted with amber to give it emphasis\n    # Calculate start position for line content (after poem with some spacing)\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)  # Either after poem or at half width\n    \n    # Draw the line content with amber highlighting\n    line_prefix = ",
      ", (BASE_WIDTH, IMAGE_HEIGHT), BLACK)\n            \n            # Calculate position to center the image\n            pos_x = (BASE_WIDTH - card_image.width) // 2\n            pos_y = (IMAGE_HEIGHT - card_image.height) // 2\n            \n            img_bg.paste(card_image, (pos_x, pos_y))\n            img.paste(img_bg, (4, image_y_offset + 4)) # Paste into main image, considering border\n        except Exception as e:\n            print(f",
      "N/A",
      ")\n            # Draw a placeholder if image fails to load\n            draw.rectangle([4, image_y_offset + 4, BASE_WIDTH + 4, image_y_offset + IMAGE_HEIGHT + 4], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_y_offset + IMAGE_HEIGHT // 2), ",
      "\n        text_bbox = draw.textbbox((0,0), no_image_text, font=header_font)\n        text_width = text_bbox[2] - text_bbox[0]\n        text_height = text_bbox[3] - text_bbox[1]\n        draw.text(((BASE_WIDTH - text_width) // 2 + 4, image_y_offset + (IMAGE_HEIGHT - text_height) // 2 + 4), no_image_text, fill=BLACK, font=header_font)\n\n    # Save the image\n    img.save(output_path)\n    # print(f",
      "Rendering card {i+1}/{total_entries}: {output_filename}"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER32 - Film Clapperboard Style with Color-Coded Prompts (from clapper31)\n- Image integration from image_path field in timeline data \n- Poem and Line Content on same line\n- Updated header labels: \"IMAGE\" and \"FRAME\"\n- Color-coded prompt components: syntagma, cineosis, ekphrasis (bold amber), style."
  },
  {
    "path": "HONEYBADGER/HIVE/clapper23.py",
    "size": 10431,
    "lines": 284,
    "source": "#!/usr/bin/env python3\n\"\"\"\nClapper23 - Film Style Timeline Visualization with Continuous Prompt Layout\n\nThis script renders timeline entries in a film clapperboard style with a more\ncontinuous, unified prompt layout that maintains the film production aesthetics\nwhile presenting the full prompt content as a cohesive whole.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 165  # Slightly adjusted for better balance\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (50, 50, 50)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_clapper23')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\"\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\", \n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef wrap_text(text, font, max_width):\n    \"\"\"Wrap text to fit within a given width.\"\"\"\n    if not text:\n        return []\n        \n    words = text.split()\n    lines = []\n    current_line = []\n    \n    for word in words:\n        # Try adding the word to the current line\n        test_line = \" \".join(current_line + [word])\n        text_width = font.getbbox(test_line)[2]\n        \n        if text_width <= max_width:\n            current_line.append(word)\n        else:\n            # Start a new line\n            lines.append(\" \".join(current_line))\n            current_line = [word]\n    \n    # Add the last line\n    if current_line:\n        lines.append(\" \".join(current_line))\n    \n    return lines\n\ndef build_full_prompt(entry):\n    \"\"\"Build a single continuous prompt from entry components.\"\"\"\n    syntagma_type = entry.get('syntagmaType', '')\n    cineosis_func = entry.get('cineosisFunction', '')\n    operative_ekphrasis = entry.get('operativeEkphrasis', '')\n    style_conditioning = entry.get('styleConditioning', '')\n    \n    # Remove any redundant \"(XX)\" from syntagma type\n    syntagma_cleaned = syntagma_type.split('(')[0].strip() if '(' in syntagma_type else syntagma_type\n    \n    # Build the continuous prompt\n    prompt_parts = []\n    if syntagma_cleaned:\n        prompt_parts.append(syntagma_cleaned)\n    if cineosis_func:\n        prompt_parts.append(cineosis_func)\n    if operative_ekphrasis:\n        prompt_parts.append(operative_ekphrasis)\n    \n    # Join the main prompt components\n    main_prompt = \". \".join(prompt_parts)\n    if main_prompt and not main_prompt.endswith('.'):\n        main_prompt += '.'\n    \n    # Add style conditioning as a separate component but still part of the unified prompt\n    if style_conditioning:\n        prompt = f\"{main_prompt} Style: {style_conditioning}\"\n    else:\n        prompt = main_prompt\n        \n    return prompt\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card in film clapperboard style with continuous prompt text.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (BASE_WIDTH + 2*BORDER_WIDTH, TOTAL_HEIGHT + 2*BORDER_WIDTH), BLUE)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    header_font = find_font(HEADER_FONT_PATH, 16)\n    body_font = find_font(BODY_FONT_PATH, 14)\n    label_font = find_font(HEADER_FONT_PATH, 14)\n    prompt_font = find_font(BODY_FONT_PATH, 16)\n    \n    # Calculate row heights for the header section\n    top_row_height = 30\n    second_row_height = 30\n    info_bar_height = 25\n    prompt_section_height = HEADER_HEIGHT - (top_row_height + second_row_height + info_bar_height)\n    \n    # TOP ROW: ID, Type, Time, Frame\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = f\"FRM: (---)\"\n    \n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), \"TT: 0 DESCRIPTIVE IMAGE\", font=header_font, fill=WHITE)\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height), (BASE_WIDTH, top_row_height)], fill=WHITE, width=1)\n    \n    # SECOND ROW: Poem and Line\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_content = entry.get('content', '---')\n    line_text = f\"LINE: \\\"{line_content}\\\"\"\n    \n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n    draw.text((500, top_row_height + 8), line_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal divider\n    draw.line([(0, top_row_height + second_row_height), (BASE_WIDTH, top_row_height + second_row_height)], fill=WHITE, width=1)\n    \n    # INFO BAR: AMBER BAR\n    info_bar_y = top_row_height + second_row_height\n    draw.rectangle([(0, info_bar_y), (BASE_WIDTH, info_bar_y + info_bar_height)], fill=AMBER)\n    \n    # Add title in the amber bar\n    draw.text((10, info_bar_y + 4), \"SYNTAGMA\", font=label_font, fill=BLACK)\n    \n    # Get abbreviated syntagma for the leading prefix\n    syntagma_type = entry.get('syntagmaType', '')\n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # PROMPT SECTION: Unified continuous prompt layout\n    prompt_y = info_bar_y + info_bar_height + 10\n    \n    # Default values for demonstration/example\n    prefix = \"DS\"\n    prompt_text = \"Mood Environment Stabilizer. A symbolic tableau rendered through chiaroscuro.\"\n    \n    # Use actual data if available\n    if entry:\n        prefix = syntagma_abbrev if syntagma_abbrev != \"??\" else \"DS\"\n        # Build the full continuous prompt from the entry components\n        prompt_text = build_full_prompt(entry)\n    \n    # Draw the prefix (syntagma abbreviation)\n    draw.text((10, prompt_y), prefix, font=header_font, fill=AMBER)\n    \n    # Draw the main prompt with wrapping - prefix width is about 30px\n    text_lines = wrap_text(prompt_text, prompt_font, BASE_WIDTH - 40)\n    for i, line in enumerate(text_lines):\n        draw.text((40, prompt_y + i * 22), line, font=prompt_font, fill=AMBER)\n    \n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # 16:9 Image area - Fill with dark gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=DARK_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=img_placeholder_font, fill=LIGHT_GRAY, anchor=\"mm\")\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (BORDER_WIDTH, BORDER_WIDTH))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_clapper.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    print(f\"Header height: {HEADER_HEIGHT}px\")\n    print(f\"Image height (16:9): {IMAGE_HEIGHT}px\")\n    \n    # Render first 10 entries as samples\n    render_all_entries(timeline, OUTPUT_DIR, 10)\n    \n    # Also render a demo card showing the layout\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"SH001\",\n        \"timestamp\": \"00:03:51\",\n        \"poem\": \"Out of Life\",\n        \"content\": \"What we've made-\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"A symbolic tableau rendered through chiaroscuro\",\n        \"styleConditioning\": \"High contrast black and white photography with deep shadows and bright highlights. Dramatic lighting with silhouettes and volumetric light effects.\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_clapper.png\")\n    render_card(demo_entry, demo_path)\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {BASE_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {BASE_WIDTH}x{TOTAL_HEIGHT}px\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_clapper.png",
      "demo_clapper.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants\nASPECT_16_9 = 16/9  # Image area\n\n# Constants for dimensions\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 165  # Slightly adjusted for better balance\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height for 16:9 image area\nTOTAL_HEIGHT = HEADER_HEIGHT + IMAGE_HEIGHT\nBORDER_WIDTH = 3\n\n# Colors - Film clapperboard style\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nDARK_AMBER = (204, 153, 0)\nBLUE = (0, 0, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (50, 50, 50)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ")\n    \n    # PROMPT SECTION: Unified continuous prompt layout\n    prompt_y = info_bar_y + info_bar_height + 10\n    \n    # Default values for demonstration/example\n    prefix = ",
      "\n        # Build the full continuous prompt from the entry components\n        prompt_text = build_full_prompt(entry)\n    \n    # Draw the prefix (syntagma abbreviation)\n    draw.text((10, prompt_y), prefix, font=header_font, fill=AMBER)\n    \n    # Draw the main prompt with wrapping - prefix width is about 30px\n    text_lines = wrap_text(prompt_text, prompt_font, BASE_WIDTH - 40)\n    for i, line in enumerate(text_lines):\n        draw.text((40, prompt_y + i * 22), line, font=prompt_font, fill=AMBER)\n    \n    # Draw dividing line between metadata area and image area\n    draw.line([(0, HEADER_HEIGHT), (BASE_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # 16:9 Image area - Fill with dark gray as placeholder\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=DARK_GRAY)\n    \n    # Add placeholder text in the image area\n    img_placeholder_font = find_font(HEADER_FONT_PATH, 24)\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), ",
      "Rendered {i+1}/{len(entries_to_render)} cards"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Clapper23 - Film Style Timeline Visualization with Continuous Prompt Layout\n\nThis script renders timeline entries in a film clapperboard style with a more\ncontinuous, unified prompt layout that maintains the film production aesthetics\nwhile presenting the full prompt content as a cohesive whole."
  },
  {
    "path": "HONEYBADGER/HIVE/clapper33.py",
    "size": 17957,
    "lines": 366,
    "source": "#!/usr/bin/env python3\n\"\"\"\nCLAPPER33 - Film Clapperboard Style with Glyphs and Color-Coded Prompts\n- Incorporates glyphs for Syntagma Type, Image Type, and Cineosis Function.\n- Image integration from image_path field in timeline data \n- Poem and Line Content on same line\n- Updated header labels: \"IMAGE\" and \"FRAME\"\n- Color-coded prompt components: syntagma, cineosis, ekphrasis (bold amber), style.\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\n\n# Constants\nBASE_WIDTH = 1024\nHEADER_HEIGHT = 200  # Height for header text area, image starts below this\nCARD_HEIGHT = HEADER_HEIGHT + 576  # 576 is the height for a 16:9 aspect ratio of 1024 width\nIMAGE_HEIGHT = 576  # 16:9 aspect ratio\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\n\nSYNTAGMA_COLOR = (100, 200, 255)  # Brighter blue\nCINEOSIS_COLOR = (230, 150, 255)  # Lighter purple\nEKPHRASIS_COLOR = AMBER           # Same as LINE color (amber), for operative ekphrasis\nSTYLE_COLOR = (150, 255, 180)     # Lighter green\n\n# Glyph Definitions\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\\u2591\",  # \u2591\n    \"Chronological Syntagma (CS)\": \"\\u2592\", # \u2592\n    \"Crystal Syntagma (XS)\": \"\\u2662\",       # \u2662\n    \"Descriptive Syntagma (DS)\": \"\\u259e\",   # \u259e\n    \"Flashback Syntagma (FS)\": \"\\u2599\",     # \u2599\n    \"Thematic Montage (TM)\": \"\\u2588\",       # \u2588\n    \"---\": \" \"  # Default for placeholder\n}\n\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\\u25ba\",          # \u25ba\n    \"Affection-Image\": \"\\u2639\",       # \u2639\n    \"Crystal-Image\": \"\\u2b27\",        # \u2b27\n    \"Descriptive Image\": \"\\u263c\",     # \u263c\n    \"Opsign\": \"\\u2a00\",               # \u2a00\n    \"Perception-Image\": \"\\u2691\",      # \u2691\n    \"Recollection-Image\": \"\\u2302\",    # \u2302\n    \"Sonsign\": \"\\u266c\",              # \u266c\n    \"Thematic Montage\": \"\\u2263\",      # \u2263 (Strictly equivalent, close to original symbol)\n    \"---\": \" \"  # Default for placeholder\n}\n\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\\u266a\",      # \u266a\n    \"Causal Motion Trigger\": \"\\u2794\",     # \u2794\n    \"Emotion Relay\": \"\\u2661\",           # \u2661\n    \"Event Pause Invocation\": \"\\u2016\",    # \u2016\n    \"Memory Storage Retrieval\": \"\\u21bb\",  # \u21bb\n    \"Mood Environment Stabilizer\": \"\\u25ff\",# \u25ff\n    \"Narrative Modifier\": \"\\u2726\",       # \u2726\n    \"Subjective Frame Recalibration\": \"\\u223f\", # \u223f\n    \"Temporal Reflection Loop\": \"\\u2318\",   # \u2318\n    \"---\": \" \"  # Default for placeholder\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nTIMELINE_PATH = os.path.join(os.path.dirname(BASE_DIR), \"COMPLETE-TIMELINE.json\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"cards_clapper33\")\nBASE_IMAGE_DIR = os.path.dirname(os.path.dirname(BASE_DIR))\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef wrap_text(text, font, max_width):\n    words = text.split()\n    wrapped_lines = []\n    current_line = []\n    for word in words:\n        test_line = ' '.join(current_line + [word])\n        line_width = font.getbbox(test_line)[2]\n        if line_width <= max_width:\n            current_line.append(word)\n        else:\n            if current_line:\n                wrapped_lines.append(' '.join(current_line))\n                current_line = [word]\n            else:\n                wrapped_lines.append(word)\n                current_line = []\n    if current_line:\n        wrapped_lines.append(' '.join(current_line))\n    return wrapped_lines\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line = []\n    current_width = 0\n    start_x = x\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line else 0\n        if max_width is not None and current_line and current_width + word_width + space_width > max_width:\n            line_text = ' '.join(current_line)\n            draw.text((start_x, y), line_text, font=font, fill=color)\n            y += 16\n            current_line = [word]\n            current_width = word_width\n        else:\n            current_line.append(word)\n            current_width += word_width + space_width\n    if current_line:\n        line_text = ' '.join(current_line)\n        draw.text((start_x, y), line_text, font=font, fill=color)\n        y += 16\n    return y\n\ndef get_image_path(entry):\n    if 'image_path' in entry and entry['image_path']:\n        image_path = os.path.join(BASE_IMAGE_DIR, entry['image_path'])\n        if os.path.exists(image_path):\n            return image_path\n        tiger_path = os.path.join(BASE_IMAGE_DIR, 'TIGER', os.path.basename(entry['image_path']))\n        if os.path.exists(tiger_path):\n            return tiger_path\n    entry_id = entry.get('id', '')\n    if entry_id:\n        base_dir = os.path.join(BASE_IMAGE_DIR, 'TIGER')\n        for root, _, files in os.walk(base_dir):\n            for file in files:\n                if file.startswith(entry_id + '__') and file.lower().endswith('.png'):\n                    return os.path.join(root, file)\n    return None\n\ndef render_card(entry, output_path):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    try:\n        header_font = ImageFont.truetype(\"Courier New Bold.ttf\", 16)\n        text_font = ImageFont.truetype(\"Courier New.ttf\", 16)\n        text_font_bold = ImageFont.truetype(\"Courier New Bold.ttf\", 16) # For bold ekphrasis\n    except IOError:\n        header_font = ImageFont.load_default()\n        text_font = ImageFont.load_default()\n        text_font_bold = ImageFont.load_default()\n        print(\"Warning: Courier New font not found, using default font.\")\n\n    top_row_height = 30\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position = entry.get('frame_position', 0)\n    frame_total = entry.get('frame_total', 0)\n    frame_text = f\"FRAME: ({frame_position}/{frame_total})\"\n    \n    image_type_str = entry.get('imageType', '---')\n    image_glyph = IMAGE_TYPE_GLYPHS.get(image_type_str, \" \")\n    image_text = f\"IMAGE: {image_glyph} {image_type_str}\"\n\n    draw.text((10, 8), id_text, font=header_font, fill=WHITE)\n    draw.text((280, 8), image_text, font=header_font, fill=WHITE) # Adjusted x for potentially wider text\n    draw.text((650, 8), time_text, font=header_font, fill=WHITE)\n    draw.text((850, 8), frame_text, font=header_font, fill=WHITE)\n    draw.line([(4, top_row_height), (BASE_WIDTH + 4, top_row_height)], fill=WHITE, width=1)\n\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name}\"\n    line_content = entry.get('content', '---')\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = \"LINE: \\\"\"\n    line_prefix_width = header_font.getbbox(line_prefix)[2]\n    draw.text((line_start_x, top_row_height + 8), line_prefix, font=header_font, fill=WHITE)\n    \n    line_content_start_x = line_start_x + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_start_x - 10 # 10 for right margin\n    \n    # Use draw_colored_text for the line content itself to handle wrapping and return y_after_line\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_start_x, top_row_height + 8, AMBER, text_font_bold, line_available_width, text_font_bold)\n    current_y = y_after_line # Start prompt section below the line content\n    \n    # If line content was empty or just '---', y_after_line would be top_row_height + 8. Add a default space.\n    if line_content.strip() == '---' or line_content.strip() == '':\n        current_y = top_row_height + 8 + 16 # Add one line height if content is empty\n    else:\n        current_y = max(y_after_line, top_row_height + 8 + 16) # Ensure at least one line height used by poem/line\n\n    prompt_start_y = current_y + 8 # Add some padding before prompt section\n    current_y = prompt_start_y\n    prompt_max_width = BASE_WIDTH - 20 # Max width for prompt values\n\n    # PROMPT SECTION - Syntagma\n    syntagma_type_str = entry.get('syntagmaType', '---')\n    syntagma_glyph = SYNTAGMA_GLYPHS.get(syntagma_type_str, \" \")\n    syntagma_label_text = f\"{syntagma_glyph} Syntagma:\"\n    draw.text((10, current_y), syntagma_label_text, font=header_font, fill=WHITE)\n    syntagma_label_width = header_font.getbbox(syntagma_label_text)[2]\n    current_y = draw_colored_text(draw, syntagma_type_str, 10 + syntagma_label_width + 5, current_y, SYNTAGMA_COLOR, text_font, prompt_max_width - (syntagma_label_width + 5), text_font)\n\n    # PROMPT SECTION - Cineosis\n    cineosis_func_str = entry.get('cineosisFunction', '---')\n    cineosis_glyph = CINEOSIS_FUNCTION_GLYPHS.get(cineosis_func_str, \" \")\n    cineosis_label_text = f\"{cineosis_glyph} Cineosis:\"\n    draw.text((10, current_y), cineosis_label_text, font=header_font, fill=WHITE)\n    cineosis_label_width = header_font.getbbox(cineosis_label_text)[2]\n    current_y = draw_colored_text(draw, cineosis_func_str, 10 + cineosis_label_width + 5, current_y, CINEOSIS_COLOR, text_font, prompt_max_width - (cineosis_label_width + 5), text_font)\n\n    # PROMPT SECTION - Ekphrasis (Operative)\n    ekphrasis_label_text = \"Ekphrasis:\"\n    draw.text((10, current_y), ekphrasis_label_text, font=header_font, fill=WHITE)\n    ekphrasis_label_width = header_font.getbbox(ekphrasis_label_text)[2]\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    current_y = draw_colored_text(draw, operative_ekphrasis, 10 + ekphrasis_label_width + 5, current_y, EKPHRASIS_COLOR, text_font_bold, prompt_max_width - (ekphrasis_label_width + 5), text_font_bold)\n\n    # PROMPT SECTION - Style (Conditioning)\n    style_label_text = \"Style:\"\n    draw.text((10, current_y), style_label_text, font=header_font, fill=WHITE)\n    style_label_width = header_font.getbbox(style_label_text)[2]\n    style_conditioning = entry.get('styleConditioning', '---')\n    current_y = draw_colored_text(draw, style_conditioning, 10 + style_label_width + 5, current_y, STYLE_COLOR, text_font, prompt_max_width - (style_label_width + 5), text_font)\n\n    # Image rendering (below prompt section)\n    image_y_offset = current_y + 8 # Add padding after last prompt item\n    if image_y_offset > HEADER_HEIGHT:\n        # This case should ideally not happen if prompt is constrained or HEADER_HEIGHT is sufficient\n        # For now, we'll cap it, but it means prompt might overlap image area if too long\n        print(f\"Warning: Prompt section for {entry.get('id')} is too tall ({image_y_offset}px) and may overlap image. Max header height is {HEADER_HEIGHT}px.\")\n        image_y_offset = HEADER_HEIGHT\n    else:\n        # If prompt is short, ensure image is still placed at HEADER_HEIGHT\n        image_y_offset = HEADER_HEIGHT\n\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_HEIGHT))\n            img.paste(card_image, (4, image_y_offset + 4)) # +4 for border\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            # Draw a placeholder if image fails to load\n            draw.rectangle([(4, image_y_offset + 4), (BASE_WIDTH + 3, image_y_offset + IMAGE_HEIGHT + 3)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_y_offset + IMAGE_HEIGHT // 2), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        # Draw a placeholder if no image path\n        draw.rectangle([(4, image_y_offset + 4), (BASE_WIDTH + 3, image_y_offset + IMAGE_HEIGHT + 3)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_y_offset + IMAGE_HEIGHT // 2), \"No Image Available\", font=header_font, fill=BLACK)\n\n    img.save(output_path)\n\ndef render_demo_card():\n    demo_entry = {\n        \"id\": \"DEMO001\", \"timestamp\": \"00:00:00\", \"frame_position\": 1, \"frame_total\": 100,\n        \"imageType\": \"Descriptive Image\",\n        \"poem\": \"Demo Poem Title\",\n        \"content\": \"This is a sample line of poetic content for the demo card.\",\n        \"syntagmaType\": \"Autonomous Syntagma (AS)\",\n        \"operativeEkphrasis\": \"A beautiful sunset over a calm ocean, rendered in vibrant colors.\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"styleConditioning\": \"Impressionistic, oil painting style, 4K resolution.\",\n        \"image_path\": None # No image for demo\n    }\n    demo_output_path = os.path.join(OUTPUT_DIR, \"_DEMO_CARD_clapper33.png\")\n    render_card(demo_entry, demo_output_path)\n    print(f\"Rendered demo card: {demo_output_path}\")\n\ndef render_extreme_test_case():\n    \"\"\"Render a test card with the longest entries to test layout boundaries.\"\"\"\n    pass # Ensure correct indentation for the first line of the function body\n    longest_entry_path = os.path.join(BASE_DIR, \"longest_test_entry.json\")\n    if not os.path.exists(longest_entry_path):\n        print(f\"Longest test entry file not found: {longest_entry_path}\")\n        longest_entry = {\n            \"id\": \"EXTREME01\", \"timestamp\": \"01:23:45\", \"frame_position\": 999, \"frame_total\": 999,\n            \"imageType\": \"Very Long Image Type Description That Pushes Boundaries\",\n            \"poem\": \"An Epic Poem With An Excessively Long Title That Spans Multiple Lines And Challenges All Known Layout Constraints For Testing Purposes\",\n            \"content\": \"This is an exceptionally long line of poetic content designed specifically to test the text wrapping capabilities of the rendering engine, ensuring that even the most verbose and loquacious descriptions are handled gracefully without breaking the visual integrity of the card. It goes on and on, seemingly forever, just to make sure every edge case is covered.\",\n            \"syntagmaType\": \"Ultra Detailed Complex Syntagma Type (UDCST)\", # This won't have a glyph unless added\n            \"operativeEkphrasis\": \"A sprawling, panoramic vista of an alien megacity at twilight, with towering, bio-luminescent skyscrapers, intricate sky-bridges teeming with exotic vehicles, and a sky filled with multiple moons and nebulae. The level of detail is immense, capturing every nuance of this fantastical urban landscape, from the smallest architectural features to the grand scale of the celestial backdrop. This description aims to be as long as possible to truly stress test the system.\",\n            \"cineosisFunction\": \"Establishing Shot - Grand Scale Alien Metropolis - Introduction to World Setting and Atmosphere - Evoking Wonder and Awe\", # This won't have a glyph\n            \"styleConditioning\": \"Rendered in Unreal Engine 5 with Lumen and Nanite, 8K resolution, cinematic depth of field, volumetric lighting, ray-traced reflections and shadows, color graded with a rich, vibrant palette, inspired by the works of Syd Mead and Moebius, aiming for photorealistic yet fantastical quality.\",\n            \"image_path\": None\n        }\n    else:\n        with open(longest_entry_path, 'r') as f:\n            longest_entry = json.load(f)\n    \n    # Ensure the extreme test entry uses known glyph types or '---' if not applicable\n    if longest_entry.get('syntagmaType') not in SYNTAGMA_GLYPHS:\n        longest_entry['syntagmaType'] = '---'\n    if longest_entry.get('imageType') not in IMAGE_TYPE_GLYPHS:\n        longest_entry['imageType'] = '---'\n    if longest_entry.get('cineosisFunction') not in CINEOSIS_FUNCTION_GLYPHS:\n        longest_entry['cineosisFunction'] = '---'\n        \n    output_path = os.path.join(OUTPUT_DIR, \"_EXTREME_TEST_CARD_CLAPPER33.png\")\n    render_card(longest_entry, output_path)\n    print(f\"Rendered extreme test card: {output_path}\")\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in poem_frames:\n            poem_frames[poem_name] = 0\n        poem_frames[poem_name] += 1\n    \n    processed_data = []\n    current_poem_frame_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name not in current_poem_frame_count:\n            current_poem_frame_count[poem_name] = 0\n        current_poem_frame_count[poem_name] += 1\n        \n        new_entry = entry.copy()\n        new_entry['frame_position'] = current_poem_frame_count[poem_name]\n        new_entry['frame_total'] = poem_frames[poem_name]\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting CLAPPER33 card generation...\")\n    try:\n        with open(TIMELINE_PATH, 'r') as f:\n            timeline_data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\")\n        return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\")\n        return\n\n    # Calculate frame counts before rendering\n    processed_timeline_data = calculate_frame_counts(timeline_data)\n\n    render_demo_card()\n    render_extreme_test_case() # Ensure this uses values present in glyph maps or '---'\n\n    count = 0\n    for entry in processed_timeline_data:\n        entry_id = entry.get('id', f'unknown_{count+1}')\n        output_filename = f\"{entry_id}_clapper33.png\"\n        output_path = os.path.join(OUTPUT_DIR, output_filename)\n        render_card(entry, output_path)\n        count += 1\n    \n    print(f\"All {count} cards rendered successfully in {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "COMPLETE-TIMELINE.json",
      "_DEMO_CARD_clapper33.png",
      "longest_test_entry.json",
      "_EXTREME_TEST_CARD_CLAPPER33.png",
      "{entry_id}_clapper33.png",
      "FRAME: ({frame_position}/{frame_total})",
      ")\n    poem_width = header_font.getbbox(poem_text)[2]\n    draw.text((10, top_row_height + 8), poem_text, font=header_font, fill=WHITE)\n\n    line_start_x = min(poem_width + 40, BASE_WIDTH // 2)\n    line_prefix = ",
      ":\n        current_y = top_row_height + 8 + 16 # Add one line height if content is empty\n    else:\n        current_y = max(y_after_line, top_row_height + 8 + 16) # Ensure at least one line height used by poem/line\n\n    prompt_start_y = current_y + 8 # Add some padding before prompt section\n    current_y = prompt_start_y\n    prompt_max_width = BASE_WIDTH - 20 # Max width for prompt values\n\n    # PROMPT SECTION - Syntagma\n    syntagma_type_str = entry.get(",
      ")\n            # Draw a placeholder if image fails to load\n            draw.rectangle([(4, image_y_offset + 4), (BASE_WIDTH + 3, image_y_offset + IMAGE_HEIGHT + 3)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_y_offset + IMAGE_HEIGHT // 2), ",
      ", font=header_font, fill=BLACK)\n    else:\n        # Draw a placeholder if no image path\n        draw.rectangle([(4, image_y_offset + 4), (BASE_WIDTH + 3, image_y_offset + IMAGE_HEIGHT + 3)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_y_offset + IMAGE_HEIGHT // 2), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "CLAPPER33 - Film Clapperboard Style with Glyphs and Color-Coded Prompts\n- Incorporates glyphs for Syntagma Type, Image Type, and Cineosis Function.\n- Image integration from image_path field in timeline data \n- Poem and Line Content on same line\n- Updated header labels: \"IMAGE\" and \"FRAME\"\n- Color-coded prompt components: syntagma, cineosis, ekphrasis (bold amber), style."
  },
  {
    "path": "HONEYBADGER/HIVE/menlo_crystal_syntagma_visibility_test.py",
    "size": 5419,
    "lines": 121,
    "source": "from PIL import Image, ImageDraw, ImageFont\nimport os\n\n# Configuration\nGLYPH_NAME = \"Crystal Syntagma\"\nORIGINAL_GLYPH = \"\\u2662\"  # \u2662 WHITE DIAMOND SUIT\nCANDIDATE_GLYPHS = [\n    (\"\\u25C7\", \"U+25C7 WHITE DIAMOND\"),\n    (\"\\u2666\", \"U+2666 BLACK DIAMOND SUIT\"),\n    (\"\\u2727\", \"U+2727 WHITE FOUR POINTED STAR\"),\n    (\"\\u2726\", \"U+2726 BLACK FOUR POINTED STAR\"),\n    (\"\\u2756\", \"U+2756 BLACK DIAMOND MINUS WHITE X\"),\n]\n\nFONT_SIZE = 48\nGLYPH_AREA_WIDTH = 80\nGLYPH_AREA_HEIGHT = 80\nPADDING = 20\nPAGE_BACKGROUND_COLOR = \"gray\" # Neutral page background\nWHITE_BG_COLOR = \"white\"\nBLACK_BG_COLOR = \"black\"\nTEXT_COLOR_ON_WHITE = \"black\"\nTEXT_COLOR_ON_BLACK = \"white\"\nFONT_PATH_MENLO = \"/System/Library/Fonts/Menlo.ttc\"\nFONT_PATH_COURIER = \"/System/Library/Fonts/Courier New.ttf\"\nOUTPUT_FILENAME = \"menlo_crystal_syntagma_visibility.png\"\n\ndef get_font(font_path, size):\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        return None\n\ndef main():\n    font_menlo = get_font(FONT_PATH_MENLO, FONT_SIZE)\n    font_courier = get_font(FONT_PATH_COURIER, FONT_SIZE)\n    font_default = ImageFont.load_default()\n\n    font = font_menlo\n    if font is None:\n        print(f\"Warning: Menlo font not found at {FONT_PATH_MENLO}. Trying Courier New.\")\n        font = font_courier\n    if font is None:\n        print(f\"Warning: Courier New font not found at {FONT_PATH_COURIER}. Using Pillow default font for glyphs.\")\n        font = font_default\n\n    label_font = font \n\n    all_glyphs_info = [\n        (ORIGINAL_GLYPH, f\"Original {GLYPH_NAME}: {ORIGINAL_GLYPH} (U+{ord(ORIGINAL_GLYPH):04X})\")] + \\\n        [(glyph, f\"Candidate: {glyph} ({desc})\") for glyph, desc in CANDIDATE_GLYPHS]\n\n    # Estimate max label width for image sizing\n    max_label_width = 0\n    # Create a temporary draw object for textbbox calculation if needed\n    temp_image_for_measure = Image.new(\"RGB\", (1,1))\n    temp_draw_for_measure = ImageDraw.Draw(temp_image_for_measure)\n    for _, label_text in all_glyphs_info:\n        bbox = temp_draw_for_measure.textbbox((0,0), label_text, font=label_font, anchor='lt')\n        max_label_width = max(max_label_width, bbox[2] - bbox[0])\n    del temp_draw_for_measure, temp_image_for_measure\n\n    img_width = PADDING + GLYPH_AREA_WIDTH + PADDING + GLYPH_AREA_WIDTH + PADDING + max_label_width + PADDING\n    img_height = len(all_glyphs_info) * (GLYPH_AREA_HEIGHT + PADDING) + PADDING\n\n    image = Image.new(\"RGB\", (int(img_width), int(img_height)), PAGE_BACKGROUND_COLOR)\n    draw = ImageDraw.Draw(image)\n\n    current_y = PADDING\n\n    for i, (glyph_char, label_text) in enumerate(all_glyphs_info):\n        # --- Render on White Background ---\n        bg_white_x0 = PADDING\n        bg_white_y0 = current_y\n        bg_white_x1 = bg_white_x0 + GLYPH_AREA_WIDTH\n        bg_white_y1 = bg_white_y0 + GLYPH_AREA_HEIGHT\n        draw.rectangle([bg_white_x0, bg_white_y0, bg_white_x1, bg_white_y1], fill=WHITE_BG_COLOR)\n\n        glyph_bbox_white = draw.textbbox((0, 0), glyph_char, font=font, anchor=\"lt\")\n        glyph_width_white = glyph_bbox_white[2] - glyph_bbox_white[0]\n        glyph_height_white = glyph_bbox_white[3] - glyph_bbox_white[1]\n        glyph_x_white = bg_white_x0 + (GLYPH_AREA_WIDTH - glyph_width_white) / 2 - glyph_bbox_white[0]\n        glyph_y_white = bg_white_y0 + (GLYPH_AREA_HEIGHT - glyph_height_white) / 2 - glyph_bbox_white[1]\n        try:\n            draw.text((glyph_x_white, glyph_y_white), glyph_char, font=font, fill=TEXT_COLOR_ON_WHITE, anchor=\"lt\")\n        except UnicodeEncodeError:\n            draw.text((glyph_x_white, glyph_y_white), \"?\", font=font, fill=\"red\", anchor=\"lt\")\n\n        # --- Render on Black Background ---\n        bg_black_x0 = PADDING + GLYPH_AREA_WIDTH + PADDING\n        bg_black_y0 = current_y\n        bg_black_x1 = bg_black_x0 + GLYPH_AREA_WIDTH\n        bg_black_y1 = bg_black_y0 + GLYPH_AREA_HEIGHT\n        draw.rectangle([bg_black_x0, bg_black_y0, bg_black_x1, bg_black_y1], fill=BLACK_BG_COLOR)\n\n        glyph_bbox_black = draw.textbbox((0, 0), glyph_char, font=font, anchor=\"lt\") # bbox should be same\n        glyph_width_black = glyph_bbox_black[2] - glyph_bbox_black[0]\n        glyph_height_black = glyph_bbox_black[3] - glyph_bbox_black[1]\n        glyph_x_black = bg_black_x0 + (GLYPH_AREA_WIDTH - glyph_width_black) / 2 - glyph_bbox_black[0]\n        glyph_y_black = bg_black_y0 + (GLYPH_AREA_HEIGHT - glyph_height_black) / 2 - glyph_bbox_black[1]\n        try:\n            draw.text((glyph_x_black, glyph_y_black), glyph_char, font=font, fill=TEXT_COLOR_ON_BLACK, anchor=\"lt\")\n        except UnicodeEncodeError:\n            draw.text((glyph_x_black, glyph_y_black), \"?\", font=font, fill=\"red\", anchor=\"lt\")\n\n        # --- Draw Label ---\n        label_bbox = draw.textbbox((0,0), label_text, font=label_font, anchor=\"lt\")\n        label_height_val = label_bbox[3] - label_bbox[1]\n        label_x = PADDING + GLYPH_AREA_WIDTH + PADDING + GLYPH_AREA_WIDTH + PADDING\n        label_y = current_y + (GLYPH_AREA_HEIGHT - label_height_val) / 2 - label_bbox[1]\n        draw.text((label_x, label_y), label_text, font=label_font, fill=TEXT_COLOR_ON_WHITE, anchor=\"lt\") # Label text on page bg\n        \n        current_y += GLYPH_AREA_HEIGHT + PADDING\n\n    output_path = os.path.join(os.path.dirname(__file__), OUTPUT_FILENAME)\n    image.save(output_path)\n    print(f\"Successfully generated image: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "menlo_crystal_syntagma_visibility.png",
      "/System/Library/Fonts/Menlo.ttc",
      "/System/Library/Fonts/Courier New.ttf",
      ")\n        glyph_width_white = glyph_bbox_white[2] - glyph_bbox_white[0]\n        glyph_height_white = glyph_bbox_white[3] - glyph_bbox_white[1]\n        glyph_x_white = bg_white_x0 + (GLYPH_AREA_WIDTH - glyph_width_white) / 2 - glyph_bbox_white[0]\n        glyph_y_white = bg_white_y0 + (GLYPH_AREA_HEIGHT - glyph_height_white) / 2 - glyph_bbox_white[1]\n        try:\n            draw.text((glyph_x_white, glyph_y_white), glyph_char, font=font, fill=TEXT_COLOR_ON_WHITE, anchor=",
      ") # bbox should be same\n        glyph_width_black = glyph_bbox_black[2] - glyph_bbox_black[0]\n        glyph_height_black = glyph_bbox_black[3] - glyph_bbox_black[1]\n        glyph_x_black = bg_black_x0 + (GLYPH_AREA_WIDTH - glyph_width_black) / 2 - glyph_bbox_black[0]\n        glyph_y_black = bg_black_y0 + (GLYPH_AREA_HEIGHT - glyph_height_black) / 2 - glyph_bbox_black[1]\n        try:\n            draw.text((glyph_x_black, glyph_y_black), glyph_char, font=font, fill=TEXT_COLOR_ON_BLACK, anchor=",
      ")\n        label_height_val = label_bbox[3] - label_bbox[1]\n        label_x = PADDING + GLYPH_AREA_WIDTH + PADDING + GLYPH_AREA_WIDTH + PADDING\n        label_y = current_y + (GLYPH_AREA_HEIGHT - label_height_val) / 2 - label_bbox[1]\n        draw.text((label_x, label_y), label_text, font=label_font, fill=TEXT_COLOR_ON_WHITE, anchor="
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "HONEYBADGER/HIVE/render_data_cards.py",
    "size": 7667,
    "lines": 219,
    "source": "#!/usr/bin/env python3\n\"\"\"\nRender Timeline Data Cards\n\nThis script renders timeline entries from a JSON file into styled data cards\nthat visually represent syntagma types, image types, and cineosis functions\nwith appropriate iconography and formatting.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Constants\nCARD_WIDTH = 800\nCARD_HEIGHT = 600\nCARD_BG = (25, 25, 25)\nHEADER_BG = (0, 0, 0)\nSECTION_BG = (40, 40, 40)\nLIGHT_SECTION_BG = (200, 200, 200)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nTEAL = (0, 128, 128)\nMAGENTA = (255, 0, 255)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = SCRIPT_DIR\nFONT_DIR = os.path.join(BASE_DIR, 'HIVE', 'fonts')\n\n# Create fonts directory if it doesn't exist\nos.makedirs(FONT_DIR, exist_ok=True)\n\n# Default system fonts - we'll use these initially\n# If you have custom fonts, you can place them in the FONT_DIR\n# and update the paths accordingly\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\" \n\n# Define glyphs for different types\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card for a timeline entry.\"\"\"\n    # Create a new image\n    img = Image.new('RGB', (CARD_WIDTH, CARD_HEIGHT), CARD_BG)\n    draw = ImageDraw.Draw(img)\n    \n    # Load fonts\n    header_font = find_font(HEADER_FONT_PATH, 24)\n    subheader_font = find_font(HEADER_FONT_PATH, 20)\n    body_font = find_font(BODY_FONT_PATH, 18)\n    symbol_font = find_font(SYMBOL_FONT_PATH, 24)\n    \n    # Header section (black bar)\n    draw.rectangle([(0, 0), (CARD_WIDTH, 100)], fill=HEADER_BG)\n    \n    # ID, timestamp and frame info\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = \"FRM: (---)\"\n    \n    draw.text((40, 30), id_text, font=header_font, fill=WHITE)\n    draw.text((300, 30), time_text, font=header_font, fill=WHITE)\n    draw.text((600, 30), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, 100), (CARD_WIDTH, 100)], fill=WHITE, width=1)\n    \n    # Poem and line section\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_text = f\"LINE: \"{entry.get('content', '---')}\"\"\n    \n    draw.text((40, 110), poem_text, font=subheader_font, fill=WHITE)\n    draw.text((500, 110), line_text, font=subheader_font, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, 150), (CARD_WIDTH, 150)], fill=WHITE, width=1)\n    \n    # Syntagma, Image Type, and Cineosis Function with glyphs\n    syntagma_type = entry.get('syntagmaType', '---')\n    image_type = entry.get('imageType', '---')\n    cineosis_func = entry.get('cineosisFunction', '---')\n    \n    syntagma_glyph = get_glyph(SYNTAGMA_GLYPHS, syntagma_type, \"\u25a1\")\n    image_glyph = get_glyph(IMAGE_GLYPHS, image_type, \"\u25a1\")\n    cineosis_glyph = get_glyph(CINEOSIS_GLYPHS, cineosis_func, \"\u25ef\")\n    \n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # Draw icons and labels for syntagma, image type, and cineosis\n    draw.text((40, 160), f\"SY: {syntagma_glyph} {syntagma_abbrev}\", font=subheader_font, fill=WHITE)\n    draw.text((300, 160), f\"IT: {image_glyph} {image_type.split('-')[0].upper()}\", font=subheader_font, fill=WHITE)\n    draw.text((570, 160), f\"CF: {cineosis_glyph} {cineosis_func.split(' ')[0].upper()}\", font=subheader_font, fill=WHITE)\n    if len(cineosis_func.split(' ')) > 1:\n        draw.text((570, 185), f\"{' '.join(cineosis_func.split(' ')[1:]).upper()}\", font=body_font, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, 200), (CARD_WIDTH, 200)], fill=WHITE, width=1)\n    \n    # Raw prompt section\n    draw.rectangle([(0, 200), (CARD_WIDTH, 240)], fill=LIGHT_SECTION_BG)\n    draw.text((40, 210), \"RAW PROMPT (MACHINE INPUT):\", font=subheader_font, fill=(0, 0, 0))\n    \n    # Format the prompt\n    prompt_parts = []\n    prompt_parts.append(f\"{syntagma_abbrev} \u00b7 {cineosis_func} \u00b7 {entry.get('operativeEkphrasis', '---')}\")\n    prompt_parts.append(f\"{entry.get('styleConditioning', '---')}\")\n    \n    # Draw the prompt text in the black area below\n    y_pos = 250\n    for part in prompt_parts:\n        # Wrap text to fit the card\n        wrapped_text = textwrap.wrap(part, width=70)\n        for line in wrapped_text:\n            draw.text((40, y_pos), line, font=body_font, fill=WHITE)\n            y_pos += 25\n        y_pos += 10  # Extra space between parts\n    \n    # If there's an image, we would place it here\n    # For now, we'll leave a gray placeholder space\n    draw.rectangle([(0, 450), (CARD_WIDTH, CARD_HEIGHT)], fill=(150, 150, 150))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir):\n    \"\"\"Render all timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Render a sample of entries (first 10) for testing\n    for i, entry in enumerate(timeline[:10]):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_card.png\")\n        render_card(entry, output_path)\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    render_all_entries(timeline, OUTPUT_DIR)\n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_card.png",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "HONEYBADGER/HIVE/render_data_cards_fixed.py",
    "size": 7669,
    "lines": 219,
    "source": "#!/usr/bin/env python3\n\"\"\"\nRender Timeline Data Cards\n\nThis script renders timeline entries from a JSON file into styled data cards\nthat visually represent syntagma types, image types, and cineosis functions\nwith appropriate iconography and formatting.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Constants\nCARD_WIDTH = 800\nCARD_HEIGHT = 600\nCARD_BG = (25, 25, 25)\nHEADER_BG = (0, 0, 0)\nSECTION_BG = (40, 40, 40)\nLIGHT_SECTION_BG = (200, 200, 200)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nTEAL = (0, 128, 128)\nMAGENTA = (255, 0, 255)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = SCRIPT_DIR\nFONT_DIR = os.path.join(BASE_DIR, 'HIVE', 'fonts')\n\n# Create fonts directory if it doesn't exist\nos.makedirs(FONT_DIR, exist_ok=True)\n\n# Default system fonts - we'll use these initially\n# If you have custom fonts, you can place them in the FONT_DIR\n# and update the paths accordingly\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\" \n\n# Define glyphs for different types\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card for a timeline entry.\"\"\"\n    # Create a new image\n    img = Image.new('RGB', (CARD_WIDTH, CARD_HEIGHT), CARD_BG)\n    draw = ImageDraw.Draw(img)\n    \n    # Load fonts\n    header_font = find_font(HEADER_FONT_PATH, 24)\n    subheader_font = find_font(HEADER_FONT_PATH, 20)\n    body_font = find_font(BODY_FONT_PATH, 18)\n    symbol_font = find_font(SYMBOL_FONT_PATH, 24)\n    \n    # Header section (black bar)\n    draw.rectangle([(0, 0), (CARD_WIDTH, 100)], fill=HEADER_BG)\n    \n    # ID, timestamp and frame info\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_text = \"FRM: (---)\"\n    \n    draw.text((40, 30), id_text, font=header_font, fill=WHITE)\n    draw.text((300, 30), time_text, font=header_font, fill=WHITE)\n    draw.text((600, 30), frame_text, font=header_font, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, 100), (CARD_WIDTH, 100)], fill=WHITE, width=1)\n    \n    # Poem and line section\n    poem_text = f\"POEM: {entry.get('poem', '---')}\"\n    line_text = f\"LINE: \\\"{entry.get('content', '---')}\\\"\"\n    \n    draw.text((40, 110), poem_text, font=subheader_font, fill=WHITE)\n    draw.text((500, 110), line_text, font=subheader_font, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, 150), (CARD_WIDTH, 150)], fill=WHITE, width=1)\n    \n    # Syntagma, Image Type, and Cineosis Function with glyphs\n    syntagma_type = entry.get('syntagmaType', '---')\n    image_type = entry.get('imageType', '---')\n    cineosis_func = entry.get('cineosisFunction', '---')\n    \n    syntagma_glyph = get_glyph(SYNTAGMA_GLYPHS, syntagma_type, \"\u25a1\")\n    image_glyph = get_glyph(IMAGE_GLYPHS, image_type, \"\u25a1\")\n    cineosis_glyph = get_glyph(CINEOSIS_GLYPHS, cineosis_func, \"\u25ef\")\n    \n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # Draw icons and labels for syntagma, image type, and cineosis\n    draw.text((40, 160), f\"SY: {syntagma_glyph} {syntagma_abbrev}\", font=subheader_font, fill=WHITE)\n    draw.text((300, 160), f\"IT: {image_glyph} {image_type.split('-')[0].upper()}\", font=subheader_font, fill=WHITE)\n    draw.text((570, 160), f\"CF: {cineosis_glyph} {cineosis_func.split(' ')[0].upper()}\", font=subheader_font, fill=WHITE)\n    if len(cineosis_func.split(' ')) > 1:\n        draw.text((570, 185), f\"{' '.join(cineosis_func.split(' ')[1:]).upper()}\", font=body_font, fill=WHITE)\n    \n    # Draw horizontal line\n    draw.line([(0, 200), (CARD_WIDTH, 200)], fill=WHITE, width=1)\n    \n    # Raw prompt section\n    draw.rectangle([(0, 200), (CARD_WIDTH, 240)], fill=LIGHT_SECTION_BG)\n    draw.text((40, 210), \"RAW PROMPT (MACHINE INPUT):\", font=subheader_font, fill=(0, 0, 0))\n    \n    # Format the prompt\n    prompt_parts = []\n    prompt_parts.append(f\"{syntagma_abbrev} \u00b7 {cineosis_func} \u00b7 {entry.get('operativeEkphrasis', '---')}\")\n    prompt_parts.append(f\"{entry.get('styleConditioning', '---')}\")\n    \n    # Draw the prompt text in the black area below\n    y_pos = 250\n    for part in prompt_parts:\n        # Wrap text to fit the card\n        wrapped_text = textwrap.wrap(part, width=70)\n        for line in wrapped_text:\n            draw.text((40, y_pos), line, font=body_font, fill=WHITE)\n            y_pos += 25\n        y_pos += 10  # Extra space between parts\n    \n    # If there's an image, we would place it here\n    # For now, we'll leave a gray placeholder space\n    draw.rectangle([(0, 450), (CARD_WIDTH, CARD_HEIGHT)], fill=(150, 150, 150))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir):\n    \"\"\"Render all timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Render a sample of entries (first 10) for testing\n    for i, entry in enumerate(timeline[:10]):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_card.png\")\n        render_card(entry, output_path)\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    \n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    \n    print(f\"Found {len(timeline)} timeline entries\")\n    render_all_entries(timeline, OUTPUT_DIR)\n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_card.png",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Render Timeline Data Cards\n\nThis script renders timeline entries from a JSON file into styled data cards\nthat visually represent syntagma types, image types, and cineosis functions\nwith appropriate iconography and formatting."
  },
  {
    "path": "HONEYBADGER/HIVE/update_report_stats_headers.py",
    "size": 1386,
    "lines": 40,
    "source": "import os\nimport re\n\nGENOME_REPORT_PATH = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/NOTATION/SYMBOLIC_GENOME_REPORT.md\"\n\ndef update_statistics_headers(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            lines = f.readlines()\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return\n\n    updated_lines = []\n    changes_made = 0\n\n    for line in lines:\n        # Look for headers like \"### Some Poem Name - Statistics\"\n        # and change them to \"### \\\"Some Poem Name\\\"\"\n        match = re.match(r\"^(### )(.*?)( - Statistics)$\", line)\n        if match:\n            poem_name = match.group(2)\n            updated_lines.append(f'{match.group(1)}\"{poem_name}\"\\n')\n            changes_made += 1\n        else:\n            updated_lines.append(line)\n\n    if changes_made > 0:\n        try:\n            with open(file_path, 'w', encoding='utf-8') as f:\n                f.writelines(updated_lines)\n            print(f\"Successfully updated {changes_made} statistics headers in {file_path} to new format.\")\n        except IOError:\n            print(f\"Error: Could not write changes back to {file_path}\")\n    else:\n        print(\"No statistics headers matching the format '### [Poem Name] - Statistics' found or no changes needed.\")\n\nif __name__ == \"__main__\":\n    update_statistics_headers(GENOME_REPORT_PATH)\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/NOTATION/SYMBOLIC_GENOME_REPORT.md"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files"
    ],
    "docstring": null
  },
  {
    "path": "HONEYBADGER/HIVE/render_elegant_cards.py",
    "size": 11297,
    "lines": 318,
    "source": "#!/usr/bin/env python3\n\"\"\"\nRender Timeline Data Cards - Elegant Spatial Design\n\nThis script renders timeline entries with clear spatial separation between elements\nand proper visual hierarchy, inspired by the provided examples.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Base dimensions and aspect ratios\nBASE_WIDTH = 1200\nASPECT_16_9 = 16/9\nASPECT_3_2 = 3/2\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (30, 30, 30)\nBLUE_BORDER = (0, 0, 255)\nGOLD = (212, 175, 55)\nCYAN = (0, 183, 235)\nSOFT_GREEN = (144, 238, 144)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_elegant')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\" \n\n# Define glyphs for different types\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2662\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u2a00\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2661\",\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u223f\",\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef draw_horizontal_divider(draw, y_position, width, color=WHITE):\n    \"\"\"Draw a horizontal divider line with proper padding.\"\"\"\n    draw.line([(0, y_position), (width, y_position)], fill=color, width=1)\n\ndef render_card(entry, output_path):\n    \"\"\"Render a data card with elegant spatial design.\"\"\"\n    # Create base image with blue border\n    img = Image.new('RGB', (BASE_WIDTH+8, TOTAL_HEIGHT+8), BLUE_BORDER)\n    canvas = Image.new('RGB', (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    id_font = find_font(HEADER_FONT_PATH, 22)\n    header_font = find_font(HEADER_FONT_PATH, 20)\n    label_font = find_font(HEADER_FONT_PATH, 18)\n    body_font = find_font(BODY_FONT_PATH, 18)\n    small_font = find_font(BODY_FONT_PATH, 16)\n    \n    # Fill header area black, image area light gray\n    draw.rectangle([(0, 0), (BASE_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Draw dividing line between header and image\n    draw.line([(0, HEADER_HEIGHT-1), (BASE_WIDTH, HEADER_HEIGHT-1)], fill=WHITE, width=2)\n    \n    # Placeholder text for image area\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n              \"16:9 Image Area\", font=header_font, fill=BLACK, anchor=\"mm\")\n    \n    # Get entry data\n    entry_id = entry.get('id', '---')\n    timestamp = entry.get('timestamp', '--:--:--')\n    progress = entry.get('progress', '(--/--)')\n    poem = entry.get('poem', '---')\n    line_content = entry.get('content', '---')\n    syntagma_type = entry.get('syntagmaType', '---')\n    image_type = entry.get('imageType', '---')\n    cineosis_func = entry.get('cineosisFunction', '---')\n    operative_ekphrasis = entry.get('operativeEkphrasis', '---')\n    style_conditioning = entry.get('styleConditioning', '---')\n    \n    # Get glyphs\n    syntagma_glyph = get_glyph(SYNTAGMA_GLYPHS, syntagma_type, \"\u25a1\")\n    image_glyph = get_glyph(IMAGE_GLYPHS, image_type, \"\u25a1\")\n    cineosis_glyph = get_glyph(CINEOSIS_GLYPHS, cineosis_func, \"\u25ef\")\n    syntagma_abbrev = SYNTAGMA_ABBREV.get(syntagma_type, \"??\")\n    \n    # ---- TOP ROW: ID, Timestamp, Progress ----\n    padding = 20\n    top_y = padding\n    \n    # ID and timestamp left-aligned\n    id_text = f\"ID: {entry_id}\"\n    time_text = f\"Time: {timestamp}\"\n    prog_text = f\"Prog: {progress}\"\n    \n    draw.text((padding, top_y), id_text, font=id_font, fill=WHITE)\n    draw.text((padding + 300, top_y), time_text, font=id_font, fill=WHITE)\n    draw.text((padding + 600, top_y), prog_text, font=id_font, fill=WHITE)\n    \n    # Draw first divider\n    first_divider_y = top_y + 40\n    draw_horizontal_divider(draw, first_divider_y, BASE_WIDTH)\n    \n    # ---- SECOND ROW: Poem name ----\n    poem_y = first_divider_y + 10\n    \n    # Poem name - italicized look\n    poem_text = f\"Poem: {poem}\"\n    draw.text((padding, poem_y), poem_text, font=header_font, fill=WHITE)\n    \n    # Draw second divider\n    second_divider_y = poem_y + 30\n    draw_horizontal_divider(draw, second_divider_y, BASE_WIDTH)\n    \n    # ---- THIRD ROW: Syntagma and Image types (side by side) ----\n    types_y = second_divider_y + 10\n    half_width = BASE_WIDTH // 2\n    \n    # Left side: Syntagma with glyph\n    sy_text = f\"SY: {syntagma_type}\"\n    draw.text((padding, types_y), sy_text, font=header_font, fill=GOLD)\n    \n    # Right side: Image type with glyph\n    it_text = f\"IT: {image_type}\"\n    draw.text((half_width + padding, types_y), it_text, font=header_font, fill=GOLD)\n    \n    # Draw third divider\n    third_divider_y = types_y + 30\n    draw_horizontal_divider(draw, third_divider_y, BASE_WIDTH)\n    \n    # ---- FOURTH ROW: Line content ----\n    line_y = third_divider_y + 10\n    \n    # Line label in cyan for emphasis\n    line_label = \"LINE: \"\n    draw.text((padding, line_y), line_label, font=label_font, fill=CYAN)\n    \n    # Line content after the label\n    label_width = 70\n    draw.text((padding + label_width, line_y), line_content, font=body_font, fill=WHITE)\n    \n    # Draw fourth divider\n    fourth_divider_y = line_y + 30\n    draw_horizontal_divider(draw, fourth_divider_y, BASE_WIDTH)\n    \n    # ---- FIFTH ROW: Ekphrasis ----\n    ekphrasis_y = fourth_divider_y + 10\n    \n    # Ekphrasis label in cyan\n    ekphrasis_label = \"EKPHRASIS: \"\n    draw.text((padding, ekphrasis_y), ekphrasis_label, font=label_font, fill=CYAN)\n    \n    # Ekphrasis content after label, wrapped if needed\n    wrapped_ekphrasis = textwrap.wrap(operative_ekphrasis, width=85)\n    for i, line in enumerate(wrapped_ekphrasis):\n        draw.text((padding + label_width, ekphrasis_y + i*25), line, font=body_font, fill=WHITE)\n    \n    # Determine height based on ekphrasis content\n    ekphrasis_height = max(30, len(wrapped_ekphrasis) * 25)\n    \n    # Draw fifth divider\n    fifth_divider_y = ekphrasis_y + ekphrasis_height + 5\n    draw_horizontal_divider(draw, fifth_divider_y, BASE_WIDTH)\n    \n    # ---- SIXTH ROW: Full Prompt ----\n    prompt_y = fifth_divider_y + 10\n    \n    # Prompt label in soft green\n    prompt_label = \"PROMPT: \"\n    draw.text((padding, prompt_y), prompt_label, font=label_font, fill=SOFT_GREEN)\n    \n    # Format full prompt with all components\n    full_prompt = f\"{syntagma_abbrev} \u00b7 {cineosis_func} \u00b7 {operative_ekphrasis}\"\n    if style_conditioning:\n        full_prompt += f\"\\n{style_conditioning}\"\n    \n    # Wrap and draw prompt content\n    wrapped_prompt = textwrap.wrap(full_prompt, width=90)\n    for i, line in enumerate(wrapped_prompt):\n        current_y = prompt_y + i*25\n        if current_y < HEADER_HEIGHT - padding:  # Stay within bounds\n            draw.text((padding + label_width, current_y), line, font=body_font, fill=WHITE)\n    \n    # Paste canvas onto bordered image\n    img.paste(canvas, (4, 4))\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_card.png\")\n        render_card(entry, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    try:\n        timeline = load_timeline(TIMELINE_PATH)\n        \n        if not timeline:\n            print(\"No timeline entries found. Exiting.\")\n            return\n        \n        print(f\"Found {len(timeline)} timeline entries\")\n        \n        print(f\"Rendering sample entries to: {OUTPUT_DIR}\")\n        # Render first 10 entries as samples\n        render_all_entries(timeline, OUTPUT_DIR, 10)\n        \n        # Demo entry\n        print(\"Creating demo card...\")\n        demo_entry = {\n            \"id\": \"DEMO\",\n            \"timestamp\": \"00:00:00\",\n            \"progress\": \"(45/99)\",\n            \"poem\": \"Example Layout\",\n            \"content\": \"This is a demonstration of the new elegant spatial layout\",\n            \"syntagmaType\": \"Crystal Syntagma (XS)\",\n            \"imageType\": \"Crystal-Image\",\n            \"cineosisFunction\": \"Temporal Reflection Loop\",\n            \"operativeEkphrasis\": \"Infinity symbol made of pale light, slowly fraying at both ends.\",\n            \"styleConditioning\": \"Desert aerial; biolum road coils \u221e, emerald aurora swirls overhead, surreal stillness\"\n        }\n        demo_path = os.path.join(OUTPUT_DIR, \"demo_layout.png\")\n        render_card(demo_entry, demo_path)\n        \n        print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n        print(f\"Check the output directory: {OUTPUT_DIR}\")\n    except Exception as e:\n        print(f\"ERROR: An exception occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "{entry_id}_card.png",
      "demo_layout.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Base dimensions and aspect ratios\nBASE_WIDTH = 1200\nASPECT_16_9 = 16/9\nASPECT_3_2 = 3/2\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nLIGHT_GRAY = (200, 200, 200)\nDARK_GRAY = (30, 30, 30)\nBLUE_BORDER = (0, 0, 255)\nGOLD = (212, 175, 55)\nCYAN = (0, 183, 235)\nSOFT_GREEN = (144, 238, 144)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      ", (BASE_WIDTH, TOTAL_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    id_font = find_font(HEADER_FONT_PATH, 22)\n    header_font = find_font(HEADER_FONT_PATH, 20)\n    label_font = find_font(HEADER_FONT_PATH, 18)\n    body_font = find_font(BODY_FONT_PATH, 18)\n    small_font = find_font(BODY_FONT_PATH, 16)\n    \n    # Fill header area black, image area light gray\n    draw.rectangle([(0, 0), (BASE_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    draw.rectangle([(0, HEADER_HEIGHT), (BASE_WIDTH, TOTAL_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Draw dividing line between header and image\n    draw.line([(0, HEADER_HEIGHT-1), (BASE_WIDTH, HEADER_HEIGHT-1)], fill=WHITE, width=2)\n    \n    # Placeholder text for image area\n    draw.text((BASE_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \n              ",
      "(--/--)",
      "\n    draw.text((padding, poem_y), poem_text, font=header_font, fill=WHITE)\n    \n    # Draw second divider\n    second_divider_y = poem_y + 30\n    draw_horizontal_divider(draw, second_divider_y, BASE_WIDTH)\n    \n    # ---- THIRD ROW: Syntagma and Image types (side by side) ----\n    types_y = second_divider_y + 10\n    half_width = BASE_WIDTH // 2\n    \n    # Left side: Syntagma with glyph\n    sy_text = f",
      "Rendered {i+1}/{len(entries_to_render)} cards",
      "(45/99)"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Render Timeline Data Cards - Elegant Spatial Design\n\nThis script renders timeline entries with clear spatial separation between elements\nand proper visual hierarchy, inspired by the provided examples."
  },
  {
    "path": "HONEYBADGER/HIVE/render_cards_clapperboard_genome.py",
    "size": 13324,
    "lines": 325,
    "source": "#!/usr/bin/env python3\n\"\"\"\nRender Timeline Data Cards as Clapperboard-style Headers\n\nThis script renders timeline entries with a structured clapperboard-style header\nthat sits above a 16:9 image area, creating an overall 3:2 aspect ratio.\n\"\"\"\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants (based on user requirements)\nASPECT_16_9 = 16/9  # Image area\nASPECT_3_2 = 3/2    # Overall frame\n\n# Calculate dimensions to maintain proper aspect ratios\nBASE_WIDTH = 1200\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height of the 16:9 image area\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)   # Total height for 3:2 aspect ratio\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT   # Height of the header area\n\n# Card dimensions\nCARD_WIDTH = BASE_WIDTH\nCARD_HEIGHT = TOTAL_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nDARK_GRAY = (40, 40, 40)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\nAMBER = (255, 191, 0)\nTEAL = (0, 128, 128)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, 'SEQUENCE-FIXED-TIMELINE.json')\nOUTPUT_DIR = os.path.join(SCRIPT_DIR, 'cards_clapperboard_genome_output')\nGENOME_DATA_PATH = os.path.join(BASE_DIR, 'HIVE', 'symbolic_genome_data.json')\n\n# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Font paths (system fonts - we'll use these initially)\nHEADER_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New Bold.ttf\"\nBODY_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\"\nSYMBOL_FONT_PATH = \"/System/Library/Fonts/Supplemental/Symbol.ttf\"\nMONO_FONT_PATH = \"/System/Library/Fonts/Supplemental/Courier New.ttf\" # For genome lines\n\n# Define glyphs for different types (incorporating updates from memory)\nSYNTAGMA_GLYPHS = {\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (XS)\": \"\u2756\",  # Updated from \u2662 (U+2662) to \u2756 (U+2756)\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n    \"Thematic Montage (TM)\": \"\u2588\"\n}\n\nIMAGE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u2b27\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",  # Updated from \u2a00 (U+2A00) to \u25ce (U+25CE)\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\"\n}\n\nCINEOSIS_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",  # Updated from \u2661 (U+2661) to \u2765 (U+2765)\n    \"Event Pause Invocation\": \"\u2016\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",  # Updated from \u223f (U+223F) to \u25a1 (U+25A1)\n    \"Temporal Reflection Loop\": \"\u2318\"\n}\n\n# Abbreviations for display\nSYNTAGMA_ABBREV = {\n    \"Autonomous Syntagma (AS)\": \"AS\",\n    \"Chronological Syntagma (CS)\": \"CS\",\n    \"Crystal Syntagma (XS)\": \"XS\",\n    \"Descriptive Syntagma (DS)\": \"DS\",\n    \"Flashback Syntagma (FS)\": \"FS\",\n    \"Thematic Montage (TM)\": \"TM\"\n}\n\ndef load_timeline(filepath):\n    \"\"\"Load timeline data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r', encoding='utf-8') as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"Error loading timeline: {e}\")\n        return []\n\ndef load_genome_data(filepath):\n    \"\"\"Load genome data from JSON file.\"\"\"\n    try:\n        with open(filepath, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n            # Convert list of poems to a dictionary keyed by title for easy lookup\n            genome_map = {poem['title']: poem for poem in data}\n            return genome_map\n    except Exception as e:\n        print(f\"Error loading genome data: {e}\")\n        return {}\n\ndef get_glyph(mapping, key, default=\"?\"):\n    \"\"\"Get glyph for a key or return default if not found.\"\"\"\n    return mapping.get(key, default)\n\ndef find_font(font_path, size):\n    \"\"\"Try to load a font, falling back to default if necessary.\"\"\"\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        print(f\"Could not load font: {font_path}, falling back to default\")\n        return ImageFont.load_default()\n\ndef render_card(entry, s_line, i_line, c_line, output_path):\n    \"\"\"Render a data card with a clapperboard-style header above a 16:9 image area.\"\"\"\n    # Create a new image with blue border\n    img = Image.new('RGB', (CARD_WIDTH+8, CARD_HEIGHT+8), BLUE_BORDER)\n    \n    # Create the main canvas inside the border\n    canvas = Image.new('RGB', (CARD_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    # Adjusted fonts for new header layout\n    font_h1 = find_font(HEADER_FONT_PATH, 20) # For ID, Poem, Time\n    font_genome = find_font(MONO_FONT_PATH, 16)    # For S, I, C lines\n    font_glyphs = find_font(HEADER_FONT_PATH, 18) # For SY, IT, CF glyphs row\n    # Original symbol_font might still be useful if specific symbols are needed outside glyphs\n    # body_font is no longer used in the header\n    \n    # Fill the header area black\n    draw.rectangle([(0, 0), (CARD_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    \n    # Fill the image area light gray (placeholder)\n    draw.rectangle([(0, HEADER_HEIGHT), (CARD_WIDTH, CARD_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Add text to the image area (placeholder)\n    draw.text((CARD_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), \"16:9 Image Area\", \n              font=font_h1, fill=BLACK, anchor=\"mm\")\n    \n    # Draw horizontal dividing line\n    draw.line([(0, HEADER_HEIGHT), (CARD_WIDTH, HEADER_HEIGHT)], fill=WHITE, width=2)\n    \n    # New header layout integrating S, I, C genome lines\n    padding = 20\n\n    def _truncate_text(text, font, max_width, prefix=\"\"):\n        if text is None: text = \"---\"\n        current_text_to_check = text # Store original text for ... logic\n        if prefix:\n            full_display_text = prefix + current_text_to_check\n        else:\n            full_display_text = current_text_to_check\n\n        if font.getlength(full_display_text) <= max_width:\n            return full_display_text\n        \n        # If too long, truncate the 'text' part and add '...'\n        # Max width for 'text' part is max_width - prefix_width - ellipsis_width\n        prefix_width = font.getlength(prefix) if prefix else 0\n        ellipsis_width = font.getlength(\"...\")\n        \n        available_width_for_text = max_width - prefix_width - ellipsis_width\n        if available_width_for_text < 0: # Prefix + ellipsis already too long\n             return (prefix + \"...\")[:int(max_width/font.getlength('M')) if font.getlength('M') > 0 else 0] # Crude truncate of prefix+...\n\n        for i in range(len(current_text_to_check) - 1, 0, -1):\n            if font.getlength(current_text_to_check[:i]) <= available_width_for_text:\n                return prefix + current_text_to_check[:i] + \"...\"\n        return prefix + \"...\" # Fallback if even one char + ... is too long\n\n    # Row 1: ID, POEM (truncated), TIME (y_offset = 5)\n    y_pos = 5\n    id_text = f\"ID: {entry.get('id', '---')}\"\n    poem_title_full = entry.get('poem', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n\n    id_width = font_h1.getlength(id_text)\n    time_width = font_h1.getlength(time_text)\n    poem_max_width = CARD_WIDTH - id_width - time_width - (padding * 4) # Extra padding between elements\n    poem_text_truncated = _truncate_text(poem_title_full, font_h1, poem_max_width, prefix=\"POEM: \")\n    poem_width = font_h1.getlength(poem_text_truncated)\n\n    draw.text((padding, y_pos), id_text, font=font_h1, fill=WHITE)\n    draw.text((padding + id_width + padding, y_pos), poem_text_truncated, font=font_h1, fill=WHITE)\n    draw.text((CARD_WIDTH - padding - time_width, y_pos), time_text, font=font_h1, fill=WHITE)\n    bbox_m_h1 = font_h1.getbbox(\"M\")\n    y_pos += (bbox_m_h1[3] - bbox_m_h1[1]) + 5 # Approx line height + spacing\n\n    # Divider Line\n    draw.line([(0, y_pos), (CARD_WIDTH, y_pos)], fill=WHITE, width=1)\n    y_pos += 5\n\n    # Row 2, 3, 4: S, I, C lines (truncated, Amber color)\n    genome_lines = [\n        (\"S: \", s_line),\n        (\"I: \", i_line),\n        (\"C: \", c_line)\n    ]\n    genome_max_width = CARD_WIDTH - (padding * 2)\n\n    for prefix, line_content in genome_lines:\n        line_display_text = _truncate_text(line_content, font_genome, genome_max_width, prefix=prefix)\n        draw.text((padding, y_pos), line_display_text, font=font_genome, fill=AMBER)\n        bbox_m_genome = font_genome.getbbox(\"M\")\n        y_pos += (bbox_m_genome[3] - bbox_m_genome[1]) + 2 # Approx line height + spacing\n    y_pos += 3 # Extra space before next divider\n\n    # Divider Line\n    draw.line([(0, y_pos), (CARD_WIDTH, y_pos)], fill=WHITE, width=1)\n    y_pos += 5\n\n    # Row 5: SY, IT, CF Glyphs only\n    syntagma_glyph = get_glyph(SYNTAGMA_GLYPHS, entry.get('syntagmaType', ''), \"-\")\n    image_glyph = get_glyph(IMAGE_GLYPHS, entry.get('imageType', ''), \"-\")\n    cineosis_glyph = get_glyph(CINEOSIS_GLYPHS, entry.get('cineosisFunction', ''), \"-\")\n    \n    sy_text = f\"SY: {syntagma_glyph}\"\n    it_text = f\"IT: {image_glyph}\"\n    cf_text = f\"CF: {cineosis_glyph}\"\n\n    sy_width = font_glyphs.getlength(sy_text)\n    it_width = font_glyphs.getlength(it_text)\n    # cf_width = font_glyphs.getsize(cf_text)[0] # Not needed for positioning here\n\n    # Distribute them: left, center-ish, right-ish\n    gap_width = (CARD_WIDTH - sy_width - it_width - font_glyphs.getlength(cf_text) - (padding*2)) // 2\n    if gap_width < padding : gap_width = padding # Ensure minimum gap\n\n    draw.text((padding, y_pos), sy_text, font=font_glyphs, fill=WHITE)\n    draw.text((padding + sy_width + gap_width, y_pos), it_text, font=font_glyphs, fill=WHITE)\n    draw.text((CARD_WIDTH - padding - font_glyphs.getlength(cf_text), y_pos), cf_text, font=font_glyphs, fill=WHITE)\n    # y_pos += font_glyphs.getsize(\"M\")[1] + 5 # This would be the end of content within header\n    # The main divider is drawn at HEADER_HEIGHT fixed position\n    \n    # Paste the canvas onto the blue-bordered image\n    img.paste(canvas, (4, 4))\n    \n    # Save the image\n    img.save(output_path)\n    print(f\"Rendered card: {output_path}\")\n\ndef render_all_entries(timeline, genome_data_map, output_dir, limit=None):\n    \"\"\"Render timeline entries to data cards.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Determine how many entries to render\n    entries_to_render = timeline[:limit] if limit else timeline\n    \n    for i, entry in enumerate(entries_to_render):\n        entry_id = entry.get('id', f'unknown_{i}')\n        output_path = os.path.join(output_dir, f\"{entry_id}_clapper_genome_card.png\")\n        poem_title = entry.get('poem')\n        genome_info = genome_data_map.get(poem_title, {})\n        s_line = genome_info.get('s_line', '--- GENOME S-LINE NOT FOUND ---')\n        i_line = genome_info.get('i_line', '--- GENOME I-LINE NOT FOUND ---')\n        c_line = genome_info.get('c_line', '--- GENOME C-LINE NOT FOUND ---')\n        render_card(entry, s_line, i_line, c_line, output_path)\n        \n        # Progress indicator for large batches\n        if (i+1) % 10 == 0:\n            print(f\"Rendered {i+1}/{len(entries_to_render)} cards\")\n\ndef main():\n    \"\"\"Main function to render data cards from timeline.\"\"\"\n    print(f\"Loading timeline from: {TIMELINE_PATH}\")\n    timeline = load_timeline(TIMELINE_PATH)\n    if not timeline:\n        print(\"No timeline entries found. Exiting.\")\n        return\n    print(f\"Found {len(timeline)} timeline entries\")\n\n    print(f\"Loading genome data from: {GENOME_DATA_PATH}\")\n    genome_data_map = load_genome_data(GENOME_DATA_PATH)\n    if not genome_data_map:\n        print(\"No genome data found. S,I,C lines will be placeholders.\")\n    \n    # Render first 10 entries as samples (or all if fewer than 10)\n    # Use min(10, len(timeline)) if you want to avoid error on small timeline\n    render_limit = 10 \n    render_all_entries(timeline, genome_data_map, OUTPUT_DIR, render_limit)\n    \n    # Also render a demo card showing the layout with dimensions\n    print(\"Rendering demo layout card\")\n    demo_entry = {\n        \"id\": \"DEMO\",\n        \"timestamp\": \"00:00:00\",\n        \"poem\": \"Layout Example\",\n        \"content\": \"3:2 overall with 16:9 image\",\n        \"syntagmaType\": \"Descriptive Syntagma (DS)\",\n        \"imageType\": \"Descriptive Image\",\n        \"cineosisFunction\": \"Mood Environment Stabilizer\",\n        \"operativeEkphrasis\": \"This is a demo card showing the layout structure with proper aspect ratios\"\n    }\n    demo_path = os.path.join(OUTPUT_DIR, \"demo_layout_genome.png\")\n    # Provide dummy S,I,C lines for the demo card\n    demo_s = \"S: \u25ce\u25ce\u25ce \u2756\u2756\u2756 \u25a1\u25a1\u25a1 \u2765\u2765\u2765 \u266a\u266a\u266a\"\n    demo_i = \"I: \u25ba\u25ba\u25ba \u2639\u2639\u2639 \u2b27\u2b27\u2b27 \u263c\u263c\u263c \u2691\u2691\u2691\"\n    demo_c = \"C: \u2794\u2794\u2794 \u2016</\u2016</\u2016</ \u21bb\u21bb\u21bb \u25ff\u25ff\u25ff \u2726\u2726\u2726\"\n    render_card(demo_entry, demo_s, demo_i, demo_c, demo_path)\n    \n    print(f\"Completed rendering data cards to: {OUTPUT_DIR}\")\n    print(f\"- Header height: {HEADER_HEIGHT}px\")\n    print(f\"- Image area: {CARD_WIDTH}x{IMAGE_HEIGHT}px (16:9)\")\n    print(f\"- Total dimensions: {CARD_WIDTH}x{CARD_HEIGHT}px (3:2)\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "SEQUENCE-FIXED-TIMELINE.json",
      "symbolic_genome_data.json",
      "{entry_id}_clapper_genome_card.png",
      "demo_layout_genome.png",
      "\n\nimport json\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\n\n# Aspect ratio constants (based on user requirements)\nASPECT_16_9 = 16/9  # Image area\nASPECT_3_2 = 3/2    # Overall frame\n\n# Calculate dimensions to maintain proper aspect ratios\nBASE_WIDTH = 1200\nIMAGE_HEIGHT = int(BASE_WIDTH / ASPECT_16_9)  # Height of the 16:9 image area\nTOTAL_HEIGHT = int(BASE_WIDTH / ASPECT_3_2)   # Total height for 3:2 aspect ratio\nHEADER_HEIGHT = TOTAL_HEIGHT - IMAGE_HEIGHT   # Height of the header area\n\n# Card dimensions\nCARD_WIDTH = BASE_WIDTH\nCARD_HEIGHT = TOTAL_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nDARK_GRAY = (40, 40, 40)\nLIGHT_GRAY = (200, 200, 200)\nBLUE_BORDER = (0, 0, 255)\nAMBER = (255, 191, 0)\nTEAL = (0, 128, 128)\n\n# Paths\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nBASE_DIR = os.path.dirname(SCRIPT_DIR)\nTIMELINE_PATH = os.path.join(BASE_DIR, ",
      "/System/Library/Fonts/Supplemental/Courier New Bold.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      "/System/Library/Fonts/Supplemental/Symbol.ttf",
      "/System/Library/Fonts/Supplemental/Courier New.ttf",
      ", (CARD_WIDTH, CARD_HEIGHT), BLACK)\n    draw = ImageDraw.Draw(canvas)\n    \n    # Load fonts\n    # Adjusted fonts for new header layout\n    font_h1 = find_font(HEADER_FONT_PATH, 20) # For ID, Poem, Time\n    font_genome = find_font(MONO_FONT_PATH, 16)    # For S, I, C lines\n    font_glyphs = find_font(HEADER_FONT_PATH, 18) # For SY, IT, CF glyphs row\n    # Original symbol_font might still be useful if specific symbols are needed outside glyphs\n    # body_font is no longer used in the header\n    \n    # Fill the header area black\n    draw.rectangle([(0, 0), (CARD_WIDTH, HEADER_HEIGHT)], fill=BLACK)\n    \n    # Fill the image area light gray (placeholder)\n    draw.rectangle([(0, HEADER_HEIGHT), (CARD_WIDTH, CARD_HEIGHT)], fill=LIGHT_GRAY)\n    \n    # Add text to the image area (placeholder)\n    draw.text((CARD_WIDTH//2, HEADER_HEIGHT + IMAGE_HEIGHT//2), ",
      ")[:int(max_width/font.getlength(",
      "\n\n    sy_width = font_glyphs.getlength(sy_text)\n    it_width = font_glyphs.getlength(it_text)\n    # cf_width = font_glyphs.getsize(cf_text)[0] # Not needed for positioning here\n\n    # Distribute them: left, center-ish, right-ish\n    gap_width = (CARD_WIDTH - sy_width - it_width - font_glyphs.getlength(cf_text) - (padding*2)) // 2\n    if gap_width < padding : gap_width = padding # Ensure minimum gap\n\n    draw.text((padding, y_pos), sy_text, font=font_glyphs, fill=WHITE)\n    draw.text((padding + sy_width + gap_width, y_pos), it_text, font=font_glyphs, fill=WHITE)\n    draw.text((CARD_WIDTH - padding - font_glyphs.getlength(cf_text), y_pos), cf_text, font=font_glyphs, fill=WHITE)\n    # y_pos += font_glyphs.getsize(",
      "Rendered {i+1}/{len(entries_to_render)} cards",
      "C: \u2794\u2794\u2794 \u2016</\u2016</\u2016</ \u21bb\u21bb\u21bb \u25ff\u25ff\u25ff \u2726\u2726\u2726"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "os",
      "PIL",
      "textwrap"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Render Timeline Data Cards as Clapperboard-style Headers\n\nThis script renders timeline entries with a structured clapperboard-style header\nthat sits above a 16:9 image area, creating an overall 3:2 aspect ratio."
  },
  {
    "path": "HONEYBADGER/HIVE/menlo_subjective_frame_candidates_test.py",
    "size": 4222,
    "lines": 102,
    "source": "from PIL import Image, ImageDraw, ImageFont\nimport os\n\n# Configuration\nGLYPH_NAME = \"Subjective Frame Recalibration\"\nORIGINAL_GLYPH = \"\\u223F\"  # \u223f SINE WAVE\nCANDIDATE_GLYPHS = [\n    (\"\\u25A1\", \"U+25A1 WHITE SQUARE\"),\n    (\"\\u25FB\", \"U+25FB WHITE MEDIUM SQUARE\"),\n    (\"\\u25A2\", \"U+25A2 WHITE SQUARE WITH ROUNDED CORNERS\"),\n    (\"\\u2395\", \"U+2395 APL FUNCTIONAL SYMBOL QUAD\"),\n    (\"\\u2610\", \"U+2610 BALLOT BOX\"),\n    (\"\\u26F6\", \"U+26F6 SQUARE FOUR CORNERS\"),\n]\n\nFONT_SIZE = 48\nGLYPH_AREA_WIDTH = 80\nGLYPH_AREA_HEIGHT = 80\nPADDING = 20\nBACKGROUND_COLOR = \"white\"\nTEXT_COLOR = \"black\"\nFONT_PATH_MENLO = \"/System/Library/Fonts/Menlo.ttc\"\nFONT_PATH_COURIER = \"/System/Library/Fonts/Courier New.ttf\"\nOUTPUT_FILENAME = \"menlo_subjective_frame_candidates.png\"\n\ndef get_font(font_path, size):\n    try:\n        return ImageFont.truetype(font_path, size)\n    except IOError:\n        return None\n\ndef main():\n    font_menlo = get_font(FONT_PATH_MENLO, FONT_SIZE)\n    font_courier = get_font(FONT_PATH_COURIER, FONT_SIZE)\n    font_default = ImageFont.load_default() # Fallback for labels if main font fails for some reason\n\n    font = font_menlo\n    if font is None:\n        print(f\"Warning: Menlo font not found at {FONT_PATH_MENLO}. Trying Courier New.\")\n        font = font_courier\n    if font is None:\n        print(f\"Warning: Courier New font not found at {FONT_PATH_COURIER}. Using Pillow default font for glyphs.\")\n        font = font_default # Pillow default for glyphs as last resort\n\n    label_font = font # Use the same font for labels to avoid encoding issues with glyphs in labels\n\n    all_glyphs_info = [\n        (ORIGINAL_GLYPH, f\"Original {GLYPH_NAME}: {ORIGINAL_GLYPH} (U+{ord(ORIGINAL_GLYPH):04X})\")] + \\\n        [(glyph, f\"Candidate: {glyph} ({desc})\") for glyph, desc in CANDIDATE_GLYPHS]\n\n    # Calculate image dimensions\n    # Max label width estimation\n    max_label_width = 0\n    for _, label_text in all_glyphs_info:\n        try:\n            bbox = draw.textbbox((0,0), label_text, font=label_font, anchor='lt') # Temp draw to measure\n            max_label_width = max(max_label_width, bbox[2] - bbox[0])\n        except NameError: # draw not defined yet, use a simpler estimate or define dummy draw\n            temp_image = Image.new(\"RGB\", (1,1))\n            temp_draw = ImageDraw.Draw(temp_image)\n            bbox = temp_draw.textbbox((0,0), label_text, font=label_font, anchor='lt')\n            max_label_width = max(max_label_width, bbox[2] - bbox[0])\n            del temp_draw, temp_image\n\n    img_width = PADDING + GLYPH_AREA_WIDTH + PADDING + max_label_width + PADDING\n    img_height = len(all_glyphs_info) * (GLYPH_AREA_HEIGHT + PADDING) + PADDING\n\n    image = Image.new(\"RGB\", (int(img_width), int(img_height)), BACKGROUND_COLOR)\n    draw = ImageDraw.Draw(image)\n\n    current_y = PADDING\n\n    for i, (glyph_char, label_text) in enumerate(all_glyphs_info):\n        glyph_bbox = draw.textbbox((0, 0), glyph_char, font=font, anchor=\"lt\")\n        glyph_width = glyph_bbox[2] - glyph_bbox[0]\n        glyph_height = glyph_bbox[3] - glyph_bbox[1]\n        \n        glyph_x = PADDING + (GLYPH_AREA_WIDTH - glyph_width) / 2 - glyph_bbox[0]\n        glyph_y = current_y + (GLYPH_AREA_HEIGHT - glyph_height) / 2 - glyph_bbox[1]\n\n        try:\n            draw.text((glyph_x, glyph_y), glyph_char, font=font, fill=TEXT_COLOR, anchor=\"lt\")\n        except UnicodeEncodeError:\n            print(f\"Error: Could not render glyph {glyph_char} ({label_text}) with the selected font.\")\n            draw.text((glyph_x, glyph_y), \"?\", font=font, fill=\"red\", anchor=\"lt\")\n\n        label_bbox = draw.textbbox((0,0), label_text, font=label_font, anchor=\"lt\")\n        label_height_val = label_bbox[3] - label_bbox[1]\n        label_x = PADDING + GLYPH_AREA_WIDTH + PADDING\n        label_y = current_y + (GLYPH_AREA_HEIGHT - label_height_val) / 2 - label_bbox[1]\n        \n        draw.text((label_x, label_y), label_text, font=label_font, fill=TEXT_COLOR, anchor=\"lt\")\n        \n        current_y += GLYPH_AREA_HEIGHT + PADDING\n\n    output_path = os.path.join(os.path.dirname(__file__), OUTPUT_FILENAME)\n    image.save(output_path)\n    print(f\"Successfully generated image: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "menlo_subjective_frame_candidates.png",
      "/System/Library/Fonts/Menlo.ttc",
      "/System/Library/Fonts/Courier New.ttf",
      ")\n        glyph_width = glyph_bbox[2] - glyph_bbox[0]\n        glyph_height = glyph_bbox[3] - glyph_bbox[1]\n        \n        glyph_x = PADDING + (GLYPH_AREA_WIDTH - glyph_width) / 2 - glyph_bbox[0]\n        glyph_y = current_y + (GLYPH_AREA_HEIGHT - glyph_height) / 2 - glyph_bbox[1]\n\n        try:\n            draw.text((glyph_x, glyph_y), glyph_char, font=font, fill=TEXT_COLOR, anchor=",
      ")\n        label_height_val = label_bbox[3] - label_bbox[1]\n        label_x = PADDING + GLYPH_AREA_WIDTH + PADDING\n        label_y = current_y + (GLYPH_AREA_HEIGHT - label_height_val) / 2 - label_bbox[1]\n        \n        draw.text((label_x, label_y), label_text, font=label_font, fill=TEXT_COLOR, anchor="
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "HONEYBADGER/HIVE/menlo_crystal_glyph_candidates_test.py",
    "size": 5498,
    "lines": 124,
    "source": "from PIL import Image, ImageDraw, ImageFont\nimport os\n\n# --- Configuration ---\nOUTPUT_IMAGE_NAME = \"menlo_crystal_glyph_candidates.png\"\nOUTPUT_DIR = \"/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/\"\n\nFONT_NAME_PRIMARY = \"Menlo\"\nFONT_NAME_FALLBACK = \"Courier New\"\nFONT_SIZE = 24\nLABEL_FONT_SIZE = 12\n\nPADDING = 20\nGLYPH_SPACING = 15\nLINE_HEIGHT = 40\n\nBACKGROUND_COLOR = (240, 240, 240)  # Light gray\nTEXT_COLOR = (20, 20, 20)        # Dark gray\nLABEL_COLOR = (100, 100, 100)    # Medium gray\nERROR_COLOR = (255, 0, 0)        # Red\n\nORIGINAL_GLYPH = {'char': '\u2b27', 'label': 'Original (U+2B27)'}\nCANDIDATE_GLYPHS = [\n    {'char': '\u25c7', 'label': '\u25c7 WHITE DIAMOND (U+25C7)'},\n    {'char': '\u25c6', 'label': '\u25c6 BLACK DIAMOND (U+25C6)'},\n    {'char': '\u25c8', 'label': '\u25c8 WHITE DIAMOND CONTAINING BLACK SMALL DIAMOND (U+25C8)'},\n    {'char': '\u2727', 'label': '\u2727 WHITE FOUR POINTED STAR (U+2727)'},\n    {'char': '\u2726', 'label': '\u2726 BLACK FOUR POINTED STAR (U+2726)'},\n    {'char': '\u2736', 'label': '\u2736 SIX POINTED BLACK STAR (U+2736)'}\n]\n\nALL_GLYPHS_TO_TEST = [ORIGINAL_GLYPH] + CANDIDATE_GLYPHS\n\ndef get_font(font_name, fallback_name, size):\n    try:\n        return ImageFont.truetype(font_name, size)\n    except IOError:\n        print(f\"Warning: Font '{font_name}' not found.\")\n        try:\n            return ImageFont.truetype(fallback_name, size)\n        except IOError:\n            print(f\"Warning: Fallback font '{fallback_name}' not found. Using Pillow's default font.\")\n            return ImageFont.load_default()\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    output_path = os.path.join(OUTPUT_DIR, OUTPUT_IMAGE_NAME)\n\n    glyph_font = get_font(FONT_NAME_PRIMARY, FONT_NAME_FALLBACK, FONT_SIZE)\n    label_font = get_font(FONT_NAME_PRIMARY, FONT_NAME_FALLBACK, LABEL_FONT_SIZE)\n\n    # Calculate image dimensions\n    num_glyphs = len(ALL_GLYPHS_TO_TEST)\n    img_height = PADDING * 2 + num_glyphs * LINE_HEIGHT\n    max_label_width = 0\n    fixed_glyph_area_width = FONT_SIZE * 2  # Allocate a fixed width area for the glyph character itself\n\n    dummy_img = Image.new(\"RGB\", (1, 1))\n    draw_context = ImageDraw.Draw(dummy_img)\n    for item in ALL_GLYPHS_TO_TEST:\n        try:\n            # Use anchor='lt' for textbbox to get dimensions from (0,0) top-left\n            bbox = draw_context.textbbox((0, 0), item['label'], font=label_font, anchor=\"lt\")\n            max_label_width = max(max_label_width, bbox[2] - bbox[0])\n        except UnicodeEncodeError:\n            pass\n\n    img_width = PADDING * 2 + fixed_glyph_area_width + GLYPH_SPACING + max_label_width\n\n    image = Image.new(\"RGB\", (int(img_width), int(img_height)), BACKGROUND_COLOR)\n    draw = ImageDraw.Draw(image)\n\n    current_y = PADDING  # This is the top of the current line's allocated slot\n\n    for item in ALL_GLYPHS_TO_TEST:\n        glyph_char = item['char']\n        glyph_label = item['label']\n\n        # --- Draw Glyph ---\n        try:\n            glyph_bbox_origin = draw.textbbox((0, 0), glyph_char, font=glyph_font, anchor=\"lt\")\n            glyph_actual_width = glyph_bbox_origin[2] - glyph_bbox_origin[0]\n            glyph_actual_height = glyph_bbox_origin[3] - glyph_bbox_origin[1]\n\n            draw_glyph_top_y = current_y + (LINE_HEIGHT - glyph_actual_height) / 2\n            draw_glyph_left_x = PADDING + (fixed_glyph_area_width - glyph_actual_width) / 2\n\n            draw.text((draw_glyph_left_x, draw_glyph_top_y), glyph_char, font=glyph_font, fill=TEXT_COLOR, anchor=\"lt\")\n        except UnicodeEncodeError:\n            error_indicator = \"[!]\"\n            error_bbox_origin = draw.textbbox((0,0), error_indicator, font=glyph_font, anchor=\"lt\")\n            error_width = error_bbox_origin[2] - error_bbox_origin[0]\n            error_height = error_bbox_origin[3] - error_bbox_origin[1]\n            draw_error_top_y = current_y + (LINE_HEIGHT - error_height) / 2\n            draw_error_left_x = PADDING + (fixed_glyph_area_width - error_width) / 2\n            draw.text((draw_error_left_x, draw_error_top_y), error_indicator, font=glyph_font, fill=ERROR_COLOR, anchor=\"lt\")\n            print(f\"Menlo (or fallback) could not render: {glyph_char} ({glyph_label})\")\n        except Exception as e:\n            print(f\"Other error rendering {glyph_char}: {e}\")\n            # Fallback error display if specific measurement failed\n            draw.text((PADDING, current_y + (LINE_HEIGHT - FONT_SIZE)/2), \"[E]\", font=glyph_font, fill=ERROR_COLOR, anchor=\"lt\")\n\n        # --- Draw Label ---\n        label_x_start = PADDING + fixed_glyph_area_width + GLYPH_SPACING\n        try:\n            label_bbox_origin = draw.textbbox((0, 0), glyph_label, font=label_font, anchor=\"lt\")\n            label_actual_height = label_bbox_origin[3] - label_bbox_origin[1]\n            draw_label_top_y = current_y + (LINE_HEIGHT - label_actual_height) / 2\n            draw.text((label_x_start, draw_label_top_y), glyph_label, font=label_font, fill=LABEL_COLOR, anchor=\"lt\")\n        except UnicodeEncodeError:\n            draw_label_error_top_y = current_y + (LINE_HEIGHT - LABEL_FONT_SIZE) / 2 # Approx centering for error label\n            draw.text((label_x_start, draw_label_error_top_y), \"[Label Error]\", font=label_font, fill=ERROR_COLOR, anchor=\"lt\")\n\n        current_y += LINE_HEIGHT\n\n    try:\n        image.save(output_path)\n        print(f\"Successfully generated glyph candidate test image: {output_path}\")\n    except Exception as e:\n        print(f\"Error saving image: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "menlo_crystal_glyph_candidates.png",
      "/Users/gaia/resurrecting atlantis/HONEYBADGER/HIVE/",
      ")\n            glyph_actual_width = glyph_bbox_origin[2] - glyph_bbox_origin[0]\n            glyph_actual_height = glyph_bbox_origin[3] - glyph_bbox_origin[1]\n\n            draw_glyph_top_y = current_y + (LINE_HEIGHT - glyph_actual_height) / 2\n            draw_glyph_left_x = PADDING + (fixed_glyph_area_width - glyph_actual_width) / 2\n\n            draw.text((draw_glyph_left_x, draw_glyph_top_y), glyph_char, font=glyph_font, fill=TEXT_COLOR, anchor=",
      ")\n            error_width = error_bbox_origin[2] - error_bbox_origin[0]\n            error_height = error_bbox_origin[3] - error_bbox_origin[1]\n            draw_error_top_y = current_y + (LINE_HEIGHT - error_height) / 2\n            draw_error_left_x = PADDING + (fixed_glyph_area_width - error_width) / 2\n            draw.text((draw_error_left_x, draw_error_top_y), error_indicator, font=glyph_font, fill=ERROR_COLOR, anchor=",
      ")\n            # Fallback error display if specific measurement failed\n            draw.text((PADDING, current_y + (LINE_HEIGHT - FONT_SIZE)/2), ",
      ")\n            label_actual_height = label_bbox_origin[3] - label_bbox_origin[1]\n            draw_label_top_y = current_y + (LINE_HEIGHT - label_actual_height) / 2\n            draw.text((label_x_start, draw_label_top_y), glyph_label, font=label_font, fill=LABEL_COLOR, anchor=",
      ")\n        except UnicodeEncodeError:\n            draw_label_error_top_y = current_y + (LINE_HEIGHT - LABEL_FONT_SIZE) / 2 # Approx centering for error label\n            draw.text((label_x_start, draw_label_error_top_y), "
    ],
    "subprocess_calls": [],
    "imports": [
      "PIL",
      "os"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "HONEYBADGER/HIVE/extended_unicode_test.py",
    "size": 7726,
    "lines": 181,
    "source": "#!/usr/bin/env python3\n\"\"\"\nExtended Unicode Test - Renders a diverse set of Unicode characters with various fonts.\n\"\"\"\n\nimport os\nfrom PIL import Image, ImageDraw, ImageFont\n\n# --- Configuration ---\nOUTPUT_IMAGE_NAME = \"extended_unicode_test_sheet.png\"\nIMAGE_WIDTH = 800\nLINE_HEIGHT = 20       # For font name labels\nGLYPH_LINE_HEIGHT = 18 # For rendering glyph lines\nGLYPH_FONT_SIZE = 12\nLABEL_FONT_SIZE = 14\nTOP_MARGIN = 20\nLEFT_MARGIN = 20\nRIGHT_MARGIN = 20\nSECTION_SPACING = 25 # Space between each font test section\n\nFONTS_TO_TEST_INFO = [\n    {\"name\": \"Courier New\", \"type\": \"system\"},\n    {\"name\": \"Menlo\", \"type\": \"system\"}, # Common macOS IDE font\n    {\"name\": \"Pillow Default\", \"type\": \"pillow_default\"}\n]\n\nUNICODE_TEST_STRING = \"AaBbCc 123 \u0393\u0394\u0398\u039b\u039e\u03a0\u03a3\u03a6\u03a8\u03a9 \u03b1\u03b2\u03b3\u03b4\u03b5\u03b6\u03b7\u03b8\u03b9\u03ba\u03bb\u03bc\u03bd\u03be\u03bf\u03c0\u03c1\u03c3\u03c4\u03c5\u03c6\u03c7\u03c8\u03c9 \u2591\u2592\u2593\u2588 \u28c0\u28c4\u28e4\u28fe\u28ff \u2502\u2503\u2506\u250a \u25b2\u25b6\u25bc\u25c0 \u25c6\u25c7\u25c8\u25c9\u25ca\u25cb\u25cf\u25e6 \u25a1\u25a0\u25aa\u25ab\u25ac\u25ad\u25ae\u25af \u2794\u2192\u2799\u279e \u2605\u2606\u2714\u2715\u2728 \u2300\u2318\u23ce \u2669\u266a\u266b\u266c\u266d\u266e\u266f \u2211\u222b\u221a\u221e \u00b1\u2260\u2264\u2265\u2264\u2265 \u2295\u2297\u2234\u2235\"\n\n# Colors\nBLACK = (0, 0, 0)\nWHITE = (255, 255, 255)\nGRAY = (150, 150, 150)\nRED = (255, 100, 100)\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n\ndef get_system_font(font_name, size):\n    \"\"\"Attempts to load a system font by name, trying common extensions/variations.\"\"\"\n    variations = [font_name, f\"{font_name} Regular\", f\"{font_name}-Regular\"]\n    extensions = [\".ttf\", \".otf\"]\n    for var in variations:\n        for ext in extensions:\n            try:\n                return ImageFont.truetype(var + ext, size)\n            except IOError:\n                continue\n    # Fallback for some system fonts that might be directly addressable without extension\n    try:\n        return ImageFont.truetype(font_name, size)\n    except IOError:\n        return None\n\ndef wrap_text(draw_context, text, font, max_width):\n    \"\"\"Simple text wrapper that breaks by words.\"\"\"\n    lines = []\n    if not text:\n        return lines\n\n    words = text.split()\n    current_line = \"\"\n\n    for word in words:\n        if not current_line: # First word for this line\n            test_line_candidate = word\n        else:\n            test_line_candidate = f\"{current_line} {word}\"\n\n        try:\n            # Try to measure test_line_candidate\n            line_width = 0\n            try:\n                bbox = draw_context.textbbox((0,0), test_line_candidate.strip(), font=font)\n                line_width = bbox[2] - bbox[0]\n            except AttributeError: # Fallback for older Pillow\n                line_width = draw_context.textsize(test_line_candidate.strip(), font=font)[0]\n            except UnicodeEncodeError: # Handle error during measurement itself\n                # This word or sequence is unrenderable by this font.\n                if current_line: # Add any preceding valid text as a line\n                    lines.append(current_line)\n                lines.append(word) # Problematic word becomes its own line\n                current_line = \"\"    # Reset current line\n                continue          # Move to the next word\n\n            if line_width <= max_width:\n                current_line = test_line_candidate.strip()\n            else: # Word doesn't fit on the current line\n                lines.append(current_line) # Finalize current_line\n                current_line = word      # Start new line with the current word\n        \n        except UnicodeEncodeError: # Should be caught by inner try-except, but as a safeguard\n            if current_line:\n                lines.append(current_line)\n            lines.append(word)\n            current_line = \"\"\n            continue\n    \n    if current_line: # Append the last line\n        lines.append(current_line)\n    \n    return lines\n\ndef main():\n    # Estimate image height dynamically\n    # Create a temporary draw object to measure text for height estimation\n    temp_img = Image.new('RGB', (1,1), WHITE)\n    temp_draw = ImageDraw.Draw(temp_img)\n    \n    total_estimated_text_height = 0\n    try: default_label_font = ImageFont.load_default(size=LABEL_FONT_SIZE) \n    except TypeError: default_label_font = ImageFont.load_default()\n\n    for font_info in FONTS_TO_TEST_INFO:\n        total_estimated_text_height += LINE_HEIGHT # For font label\n        glyph_font = None\n        if font_info[\"type\"] == \"system\":\n            glyph_font = get_system_font(font_info[\"name\"], GLYPH_FONT_SIZE)\n        elif font_info[\"type\"] == \"pillow_default\":\n            try: glyph_font = ImageFont.load_default(size=GLYPH_FONT_SIZE)\n            except TypeError: glyph_font = ImageFont.load_default()\n\n        if glyph_font:\n            wrapped_lines = wrap_text(temp_draw, UNICODE_TEST_STRING, glyph_font, IMAGE_WIDTH - LEFT_MARGIN - RIGHT_MARGIN)\n            total_estimated_text_height += len(wrapped_lines) * GLYPH_LINE_HEIGHT\n        else:\n            total_estimated_text_height += GLYPH_LINE_HEIGHT # For 'font not found' message\n        total_estimated_text_height += SECTION_SPACING\n\n    image_height = TOP_MARGIN * 2 + total_estimated_text_height\n    img = Image.new('RGB', (IMAGE_WIDTH, int(image_height)), WHITE)\n    draw = ImageDraw.Draw(img)\n\n    current_y = TOP_MARGIN\n\n    for font_info in FONTS_TO_TEST_INFO:\n        font_display_name = font_info[\"name\"]\n        font_type = font_info[\"type\"]\n\n        draw.text((LEFT_MARGIN, current_y), f\"Font: {font_display_name} (Size: {GLYPH_FONT_SIZE}pt)\", font=default_label_font, fill=BLACK)\n        current_y += LINE_HEIGHT\n\n        glyph_font = None\n        if font_type == \"system\":\n            glyph_font = get_system_font(font_info[\"name\"], GLYPH_FONT_SIZE)\n        elif font_type == \"pillow_default\":\n            try: glyph_font = ImageFont.load_default(size=GLYPH_FONT_SIZE)\n            except TypeError: glyph_font = ImageFont.load_default()\n\n        if glyph_font:\n            wrapped_lines = wrap_text(draw, UNICODE_TEST_STRING, glyph_font, IMAGE_WIDTH - LEFT_MARGIN - RIGHT_MARGIN)\n            for line_idx, line_text in enumerate(wrapped_lines):\n                try:\n                    draw.text((LEFT_MARGIN + 10, current_y), line_text, font=glyph_font, fill=BLACK)\n                except UnicodeEncodeError:\n                    error_message = \"Error: Line contains unrenderable chars (UnicodeEncodeError)\"\n                    draw.text((LEFT_MARGIN + 10, current_y), error_message, font=default_label_font, fill=RED)\n                    if line_idx == 0 and font_type == \"pillow_default\":\n                        print(f\"Note: Pillow default font failed for '{font_display_name}' due to UnicodeEncodeError.\")\n                except Exception as e:\n                    error_message = f\"Error rendering line: {type(e).__name__}\"\n                    draw.text((LEFT_MARGIN + 10, current_y), error_message, font=default_label_font, fill=RED)\n                    if line_idx == 0 and font_type == \"pillow_default\":\n                        print(f\"Note: Pillow default font for '{font_display_name}' encountered: {e}\")\n                current_y += GLYPH_LINE_HEIGHT\n        else:\n            draw.text((LEFT_MARGIN + 10, current_y), f\"{font_display_name} not found on system.\", font=default_label_font, fill=RED)\n            current_y += GLYPH_LINE_HEIGHT\n        \n        current_y += SECTION_SPACING\n        if font_info != FONTS_TO_TEST_INFO[-1]:\n            draw.line([(LEFT_MARGIN, current_y - SECTION_SPACING//2), (IMAGE_WIDTH - LEFT_MARGIN, current_y - SECTION_SPACING//2)], fill=GRAY, width=1)\n\n    output_path = os.path.join(BASE_DIR, OUTPUT_IMAGE_NAME)\n    try:\n        img.save(output_path)\n        print(f\"Extended Unicode test sheet saved to: {output_path}\")\n    except Exception as e:\n        print(f\"Error saving image: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "extended_unicode_test_sheet.png",
      "Attempts to load a system font by name, trying common extensions/variations.",
      ", font=default_label_font, fill=RED)\n            current_y += GLYPH_LINE_HEIGHT\n        \n        current_y += SECTION_SPACING\n        if font_info != FONTS_TO_TEST_INFO[-1]:\n            draw.line([(LEFT_MARGIN, current_y - SECTION_SPACING//2), (IMAGE_WIDTH - LEFT_MARGIN, current_y - SECTION_SPACING//2)], fill=GRAY, width=1)\n\n    output_path = os.path.join(BASE_DIR, OUTPUT_IMAGE_NAME)\n    try:\n        img.save(output_path)\n        print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "PIL"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [],
    "docstring": "Extended Unicode Test - Renders a diverse set of Unicode characters with various fonts."
  },
  {
    "path": "PRIME/scripts/hm_special_video_generator.py",
    "size": 12714,
    "lines": 357,
    "source": "#!/usr/bin/env python3\n\"\"\"\nHM Special Video Generator\nThis script specifically generates the HM (Hot Minute) video using the images from HM directory\nand the shot information from HM_prompts.md file.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport shutil\nimport subprocess\nfrom datetime import datetime\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nJELLYFISH_DIR = os.path.join(BASE_DIR, \"JELLYFISH\")\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nAUDIO_DIR = os.path.join(BASE_DIR, \"MANTA\", \"audio\")\nVIDEO_OUTPUT_DIR = os.path.join(JELLYFISH_DIR, \"video_output\")\nHORSE_DIR = os.path.join(BASE_DIR, \"HORSE\")\nHM_DIR = os.path.join(TIGER_DIR, \"HM\")\n\n# Source files\nHM_ASSEMBLY_JSON = os.path.join(HM_DIR, \"HM_assembly.json\")\nHM_PROMPTS = os.path.join(HM_DIR, \"HM_prompts.md\")\n\n# HM section timestamp based on the numbered list (11_HM)\nHM_HOUR_OFFSET = 24  # 24 hours offset for position 11\nHM_VIDEO_NUMBER = \"11\"\nSECTION_DURATION = 2 * 60 + 11  # 2:11 in seconds\n\ndef log(message):\n    \"\"\"Log a message to console.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    log_message = f\"{timestamp} - {message}\"\n    print(log_message)\n    \n    # Also write to log file\n    log_dir = os.path.join(BASE_DIR, \"PRIME\", \"logs\")\n    os.makedirs(log_dir, exist_ok=True)\n    log_file = os.path.join(log_dir, f\"hm_video_generator_{datetime.now().strftime('%Y%m%d')}.log\")\n    with open(log_file, \"a\") as f:\n        f.write(log_message + \"\\n\")\n\ndef extract_hm_shots_from_prompts():\n    \"\"\"Extract HM shots in order from the prompts file.\"\"\"\n    shots = []\n    \n    if not os.path.exists(HM_PROMPTS):\n        log(f\"Error: HM prompts file not found at {HM_PROMPTS}\")\n        return shots\n    \n    with open(HM_PROMPTS, 'r') as file:\n        content = file.read()\n        \n        # Extract each shot line by line - HM prompts don't have timestamps in brackets\n        # Format: HM001 \u00b7 DS \u00b7 Mood Environment Stabilizer \u00b7 Title flickers like...\n        shot_pattern = r'HM(\\d+)\\s*\u00b7\\s*([A-Z]+)\\s*\u00b7\\s*([^\u00b7]+)\u00b7\\s*(.+)'\n        for match in re.finditer(shot_pattern, content):\n            shot_num, category, function, description = match.groups()\n            shot_id = f\"HM{shot_num}\"\n            shots.append({\n                'id': shot_id,\n                'number': shot_num,\n                'timestamp': \"00:00:00\",  # We'll calculate this later\n                'category': category.strip(),\n                'function': function.strip(),\n                'description': description.strip()\n            })\n    \n    log(f\"Extracted {len(shots)} HM shots from prompts in official order\")\n    return shots\n\ndef extract_assembly_data():\n    \"\"\"Extract assembly data from the HM assembly JSON file.\"\"\"\n    assembly_data = {}\n    \n    # Try to use HM_assembly.json\n    if os.path.exists(HM_ASSEMBLY_JSON):\n        try:\n            with open(HM_ASSEMBLY_JSON, 'r') as file:\n                assembly_list = json.load(file)\n                for item in assembly_list:\n                    shot_id = item.get('id')\n                    if shot_id:\n                        assembly_data[shot_id] = item\n                \n                log(f\"Loaded {len(assembly_data)} assemblies from HM_assembly.json\")\n        except Exception as e:\n            log(f\"Error loading HM_assembly.json: {e}\")\n    \n    return assembly_data\n\ndef find_hm_images():\n    \"\"\"Find HM images in the HM directory.\"\"\"\n    hm_images = {}\n    \n    # Get list of all image files in HM directory\n    image_files = []\n    for file in os.listdir(HM_DIR):\n        if file.endswith(('.png', '.jpg', '.jpeg')) and file.startswith('HM'):\n            image_files.append(file)\n    \n    # Group images by shot ID\n    for filename in image_files:\n        shot_match = re.match(r'(HM\\d+)__', filename)\n        if shot_match:\n            shot_id = shot_match.group(1)\n            # We need the relative path from TIGER directory, not the absolute path\n            relative_path = f\"HM/{filename}\"\n            \n            if shot_id not in hm_images:\n                hm_images[shot_id] = []\n            \n            hm_images[shot_id].append(relative_path)\n    \n    log(f\"Found {len(hm_images)} HM shots with images directly in HM directory\")\n    return hm_images\n\ndef calculate_timestamps(shot_count):\n    \"\"\"Calculate evenly distributed timestamps for HM section.\"\"\"\n    interval = SECTION_DURATION / shot_count\n    \n    timestamps = []\n    for i in range(shot_count):\n        seconds = i * interval\n        minutes = int(seconds / 60)\n        remaining_seconds = int(seconds % 60)\n        timestamp = f\"{HM_HOUR_OFFSET:02d}:{minutes:02d}:{remaining_seconds:02d}\"\n        timestamps.append(timestamp)\n    \n    return timestamps\n\ndef generate_hm_codex_entries(shots, images, assembly_data):\n    \"\"\"Generate ordered codex entries for HM shots with complete metadata.\"\"\"\n    entries = []\n    shots_with_images = 0\n    shots_without_images = 0\n    used_shots = []\n    \n    # Calculate timestamps\n    timestamps = calculate_timestamps(len(shots))\n    \n    for i, shot in enumerate(shots):\n        shot_id = shot['id']\n        shot_number = shot['number']\n        timestamp = timestamps[i]\n        \n        # Check if we have an image for this shot\n        if shot_id in images and images[shot_id]:\n            image_path = images[shot_id][0]  # Use the first available image\n            shots_with_images += 1\n            \n            # Get assembly data if available\n            assembly = assembly_data.get(shot_id, {})\n            if not assembly:\n                # Create assembly from shot data if not found\n                assembly = {\n                    \"id\": shot_id,\n                    \"poem\": \"Hot Minute\",\n                    \"content\": shot['description'][:30] + \"...\",\n                    \"syntagmaType\": f\"{shot['category']}\",\n                    \"cineosisFunction\": f\"{shot['function']}\",\n                    \"imageType\": \"midjourney\"\n                }\n            else:\n                # Ensure we have all required fields\n                if \"syntagmaType\" not in assembly:\n                    assembly[\"syntagmaType\"] = f\"{shot['category']}\"\n                if \"cineosisFunction\" not in assembly:\n                    assembly[\"cineosisFunction\"] = f\"{shot['function']}\"\n                if \"imageType\" not in assembly:\n                    assembly[\"imageType\"] = \"midjourney\"\n            \n            # Generate the codex entry with EXACT format as expected by the parser\n            entry = f\"### {shot_id} [{timestamp}]\\n\\n\"\n            entry += f\"**Image:** `{image_path}`\\n\\n\"\n            entry += \"**Assembly Source:**\\n```json\\n\"\n            entry += json.dumps(assembly, indent=2)\n            entry += \"\\n```\\n\\n\"\n            entry += f\"**Prompt:** {shot['category']} \u00b7 {shot['function']} \u00b7 {shot['description']}\"\n            entry += \"\\n\\n---\\n\\n\"\n            \n            entries.append(entry)\n            used_shots.append(shot_id)\n        else:\n            log(f\"Warning: No image found for {shot_id}, skipping\")\n            shots_without_images += 1\n    \n    log(f\"Total entries: {len(entries)}\")\n    log(f\"Shots with images: {shots_with_images}\")\n    log(f\"Shots without images: {shots_without_images}\")\n    if used_shots:\n        log(f\"Used shots: {used_shots[0]}...{used_shots[-1]} ({len(used_shots)} total)\")\n    \n    return entries\n\ndef write_codex_to_file(entries):\n    \"\"\"Write HM codex entries to file.\"\"\"\n    output_file = os.path.join(TIGER_DIR, \"HM_ordered_codex_entries.md\")\n    \n    with open(output_file, 'w') as file:\n        file.write(\"# HM Hot Minute - Ordered Codex Entries\\n\\n\")\n        file.write(\"*Generated on \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"*\\n\\n\")\n        file.write(\"These entries are in EXACT shot order from the prompts markdown.\\n\\n\")\n        file.write(\"\".join(entries))\n    \n    log(f\"Generated ordered codex entries written to {output_file}\")\n    return output_file\n\ndef backup_existing_video():\n    \"\"\"Backup existing HM videos with timestamp suffix.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Backup video without audio\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"HM_header_prompt.mp4\")\n    if os.path.exists(video_file):\n        backup_file = os.path.join(VIDEO_OUTPUT_DIR, f\"HM_header_prompt_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_file, backup_file)\n        log(f\"Backing up existing HM video to {backup_file}\")\n    \n    # Backup video with audio\n    video_audio_file = os.path.join(VIDEO_OUTPUT_DIR, f\"HM_header_prompt_with_audio.mp4\")\n    if os.path.exists(video_audio_file):\n        backup_audio_file = os.path.join(VIDEO_OUTPUT_DIR, f\"HM_header_prompt_with_audio_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_audio_file, backup_audio_file)\n        log(f\"Backing up existing HM video with audio to {backup_audio_file}\")\n\ndef generate_hm_video(codex_file):\n    \"\"\"Generate HM video with ordered shot sequence.\"\"\"\n    log(f\"Generating HM video with ordered shot sequence...\")\n    \n    # Check if ordered codex file exists\n    if not os.path.exists(codex_file):\n        log(f\"Error: Ordered codex file {codex_file} not found.\")\n        return False\n    \n    # Run the FL video generator with HM prefix and ordered codex\n    cmd = [\n        \"python3\",\n        os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\"),\n        \"--prefix\", \"HM\",\n        \"--ordered-codex\", codex_file\n    ]\n    \n    log(f\"Using ordered codex file: {codex_file}\")\n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        log(f\"Error: Failed to generate HM video.\")\n        return False\n    \n    log(f\"Successfully generated HM video\")\n    return True\n\ndef add_audio_to_video():\n    \"\"\"Add audio to the generated HM video.\"\"\"\n    log(f\"Adding audio to HM video...\")\n    \n    # Check if video file exists\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"HM_header_prompt.mp4\")\n    if not os.path.exists(video_file):\n        log(f\"Error: Video file {video_file} not found.\")\n        return False\n    \n    # Check if audio file exists\n    audio_file = os.path.join(AUDIO_DIR, \"HM_audio.wav\")\n    if not os.path.exists(audio_file):\n        log(f\"Warning: Audio file {audio_file} not found. Skipping audio addition.\")\n        return False\n    \n    # Output file path\n    output_file = os.path.join(VIDEO_OUTPUT_DIR, \"HM_header_prompt_with_audio.mp4\")\n    \n    # FFmpeg command to merge video and audio\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-map\", \"0:v\", \"-map\", \"1:a\",\n        \"-c:v\", \"copy\", \"-c:a\", \"aac\",\n        \"-shortest\",\n        output_file\n    ]\n    \n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        log(f\"Error: Failed to add audio to HM video.\")\n        return False\n    \n    log(f\"Successfully created HM video with audio: {output_file}\")\n    return True\n\ndef copy_to_horse_directory():\n    \"\"\"Copy the new HM video to the HORSE directory with both naming conventions.\"\"\"\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"HM_header_prompt.mp4\")\n    \n    # Copy with original name\n    horse_file = os.path.join(HORSE_DIR, \"HM_header_prompt.mp4\")\n    shutil.copy2(video_file, horse_file)\n    log(f\"Copied HM video to {horse_file}\")\n    \n    # Copy with special naming convention\n    special_file = os.path.join(HORSE_DIR, f\"{HM_VIDEO_NUMBER}_HM_HotMinute_230000_header_prompt.mp4\")\n    shutil.copy2(video_file, special_file)\n    log(f\"Copied HM video to {special_file}\")\n    \n    return True\n\ndef main():\n    \"\"\"Main function to generate HM video.\"\"\"\n    log(\"=== HM Special Video Generator ===\")\n    \n    # Backup existing videos\n    backup_existing_video()\n    \n    # Extract shot order from prompts\n    shots = extract_hm_shots_from_prompts()\n    if not shots:\n        log(\"Error: No HM shots found in prompts file.\")\n        return False\n    \n    # Extract assembly data\n    assembly_data = extract_assembly_data()\n    \n    # Find images\n    hm_images = find_hm_images()\n    if not hm_images:\n        log(\"Error: No HM images found.\")\n        return False\n    \n    # Generate ordered codex entries\n    entries = generate_hm_codex_entries(shots, hm_images, assembly_data)\n    if not entries:\n        log(\"Error: No valid HM entries generated.\")\n        return False\n    \n    # Write to codex file\n    codex_file = write_codex_to_file(entries)\n    \n    # Generate HM video\n    success = generate_hm_video(codex_file)\n    if success:\n        # Add audio to video\n        add_audio_to_video()\n        \n        # Copy to HORSE directory\n        copy_to_horse_directory()\n    \n    log(\"=== HM Special Video Generator Complete ===\")\n    return success\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "HM_assembly.json",
      "Loaded {len(assembly_data)} assemblies from HM_assembly.json",
      "HM_header_prompt.mp4",
      "HM_header_prompt_MISFIT_{timestamp}.mp4",
      "HM_header_prompt_with_audio.mp4",
      "HM_header_prompt_with_audio_MISFIT_{timestamp}.mp4",
      "HM_header_prompt.mp4",
      "HM_audio.wav",
      "HM_header_prompt_with_audio.mp4",
      "HM_header_prompt.mp4",
      "HM_header_prompt.mp4",
      "{HM_VIDEO_NUMBER}_HM_HotMinute_230000_header_prompt.mp4",
      "HM/{filename}",
      "\n    interval = SECTION_DURATION / shot_count\n    \n    timestamps = []\n    for i in range(shot_count):\n        seconds = i * interval\n        minutes = int(seconds / 60)\n        remaining_seconds = int(seconds % 60)\n        timestamp = f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "re",
      "json",
      "shutil",
      "subprocess",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "HM Special Video Generator\nThis script specifically generates the HM (Hot Minute) video using the images from HM directory\nand the shot information from HM_prompts.md file."
  },
  {
    "path": "PRIME/scripts/at_special_video_generator.py",
    "size": 15050,
    "lines": 409,
    "source": "#!/usr/bin/env python3\n\"\"\"\nAT Special Video Generator\nThis script specifically generates the AT (Atlantis) video using the images from AT_shots directory\nand the information from ImageSequence.md file.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport shutil\nimport subprocess\nfrom datetime import datetime\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nJELLYFISH_DIR = os.path.join(BASE_DIR, \"JELLYFISH\")\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nAUDIO_DIR = os.path.join(BASE_DIR, \"MANTA\", \"audio\")\nVIDEO_OUTPUT_DIR = os.path.join(JELLYFISH_DIR, \"video_output\")\nHORSE_DIR = os.path.join(BASE_DIR, \"HORSE\")\nAT_SHOTS_DIR = os.path.join(TIGER_DIR, \"AT\", \"AT_shots\")\n\n# Source files\nIMAGE_SEQUENCE = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_ImageSequence.md\")\nASSEMBLIES = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_assemblies.md\")\nPROMPTS = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_prompts.md\")\nAT_ASSEMBLY_JSON = os.path.join(TIGER_DIR, \"AT\", \"AT_assembly.json\")\n\n# AT section timestamp\nAT_HOUR_OFFSET = 11  # 11 hours offset\nSECTION_DURATION = 2 * 60 + 11  # 2:11 in seconds\n\ndef log(message):\n    \"\"\"Log a message to console.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    log_message = f\"{timestamp} - {message}\"\n    print(log_message)\n\ndef extract_at_shots_from_prompts():\n    \"\"\"Extract AT shots in order from the prompts file.\"\"\"\n    shots = []\n    \n    with open(PROMPTS, 'r') as file:\n        content = file.read()\n        \n        # Find AT section in prompts file\n        section_pattern = r'## \\d+_AT - .+?(?=## |\\Z)'\n        section_match = re.search(section_pattern, content, re.DOTALL)\n        if not section_match:\n            log(\"No AT section found in prompts file.\")\n            return shots\n        \n        section_content = section_match.group(0)\n        \n        # Extract each shot line by line\n        shot_pattern = r'AT(\\d+)\\s*\\[([\\d:]+)\\]\\s*\u00b7\\s*([A-Z]+)\\s*\u00b7\\s*([^\u00b7]+)\u00b7\\s*(.+)'\n        for match in re.finditer(shot_pattern, section_content):\n            shot_num, timestamp, category, function, description = match.groups()\n            shot_id = f\"AT{shot_num}\"\n            shots.append({\n                'id': shot_id,\n                'number': shot_num,\n                'timestamp': timestamp,\n                'category': category.strip(),\n                'function': function.strip(),\n                'description': description.strip()\n            })\n    \n    log(f\"Extracted {len(shots)} AT shots from prompts in official order\")\n    return shots\n\ndef extract_assembly_data():\n    \"\"\"Extract assembly data from the AT assembly JSON file.\"\"\"\n    assembly_data = {}\n    \n    # Try to use AT_assembly.json first\n    if os.path.exists(AT_ASSEMBLY_JSON):\n        try:\n            with open(AT_ASSEMBLY_JSON, 'r') as file:\n                assembly_list = json.load(file)\n                for item in assembly_list:\n                    shot_id = item.get('id')\n                    if shot_id:\n                        assembly_data[shot_id] = item\n                \n                log(f\"Loaded {len(assembly_data)} assemblies from AT_assembly.json\")\n        except Exception as e:\n            log(f\"Error loading AT_assembly.json: {e}\")\n    \n    # If no assemblies loaded, try the COLLECTION assemblies file\n    if not assembly_data:\n        with open(ASSEMBLIES, 'r') as file:\n            content = file.read()\n            \n            # Find AT section in assemblies file\n            section_match = re.search(r'## \\d+_AT - .+?\\n```json\\n(.*?)```', content, re.DOTALL)\n            if section_match:\n                try:\n                    assembly_list = json.loads(section_match.group(1))\n                    for item in assembly_list:\n                        shot_id = item.get('id')\n                        if shot_id:\n                            assembly_data[shot_id] = item\n                    \n                    log(f\"Loaded {len(assembly_data)} assemblies from COLLECTION assemblies\")\n                except json.JSONDecodeError as e:\n                    log(f\"Error parsing assembly JSON: {e}\")\n    \n    return assembly_data\n\ndef find_at_images_from_sequence():\n    \"\"\"Find AT images referenced in the ImageSequence.md file.\"\"\"\n    at_images = {}\n    \n    with open(IMAGE_SEQUENCE, 'r') as file:\n        content = file.read()\n        \n        # Use regex to find AT shot sections and their images\n        at_section_pattern = r'### (AT\\d+).*?\\n\\n.*?(?=###|\\Z)'\n        for section_match in re.finditer(at_section_pattern, content, re.DOTALL):\n            section_content = section_match.group(0)\n            shot_id = section_match.group(1)\n            \n            # Extract image paths\n            image_paths = []\n            for image_match in re.finditer(r'- `([^`]+)`', section_content):\n                image_path = image_match.group(1)\n                if image_path.startswith('AT/AT_shots/'):\n                    image_paths.append(image_path)\n            \n            if image_paths:\n                at_images[shot_id] = image_paths\n    \n    log(f\"Found {len(at_images)} AT shots with images in ImageSequence.md\")\n    return at_images\n\ndef find_at_images_from_directory():\n    \"\"\"Find all AT images in the AT_shots directory.\"\"\"\n    at_images = {}\n    \n    # Get list of all image files in AT_shots\n    image_files = []\n    for root, _, files in os.walk(AT_SHOTS_DIR):\n        for file in files:\n            if file.endswith(('.png', '.jpg', '.jpeg')):\n                image_files.append(os.path.join(root, file))\n    \n    # Group images by shot ID\n    for image_path in image_files:\n        filename = os.path.basename(image_path)\n        shot_match = re.match(r'(AT\\d+)__', filename)\n        if shot_match:\n            shot_id = shot_match.group(1)\n            relative_path = f\"AT/AT_shots/{filename}\"\n            \n            if shot_id not in at_images:\n                at_images[shot_id] = []\n            \n            at_images[shot_id].append(relative_path)\n    \n    log(f\"Found {len(at_images)} AT shots with images directly in AT_shots directory\")\n    return at_images\n\ndef calculate_timestamps(shot_count):\n    \"\"\"Calculate evenly distributed timestamps for AT section.\"\"\"\n    interval = SECTION_DURATION / shot_count\n    \n    timestamps = []\n    for i in range(shot_count):\n        seconds = i * interval\n        minutes = int(seconds / 60)\n        remaining_seconds = int(seconds % 60)\n        timestamp = f\"{AT_HOUR_OFFSET:02d}:{minutes:02d}:{remaining_seconds:02d}\"\n        timestamps.append(timestamp)\n    \n    return timestamps\n\ndef generate_at_codex_entries(shots, images, assembly_data):\n    \"\"\"Generate ordered codex entries for AT shots with complete metadata.\"\"\"\n    entries = []\n    shots_with_images = 0\n    shots_without_images = 0\n    used_shots = []\n    \n    # Calculate timestamps\n    timestamps = calculate_timestamps(len(shots))\n    \n    for i, shot in enumerate(shots):\n        shot_id = shot['id']\n        shot_number = shot['number']\n        timestamp = timestamps[i]\n        \n        # Check if we have an image for this shot\n        if shot_id in images and images[shot_id]:\n            image_path = images[shot_id][0]  # Use the first available image\n            shots_with_images += 1\n            \n            # Get assembly data if available\n            assembly = assembly_data.get(shot_id, {})\n            if not assembly:\n                # Create assembly from shot data if not found\n                assembly = {\n                    \"id\": shot_id,\n                    \"poem\": \"Resurrecting Atlantis\",\n                    \"content\": shot['description'][:30] + \"...\",\n                    \"syntagmaType\": f\"{shot['category']}\",\n                    \"cineosisFunction\": f\"{shot['function']}\",\n                    \"imageType\": \"midjourney\"\n                }\n            else:\n                # Ensure we have all required fields\n                if \"syntagmaType\" not in assembly:\n                    assembly[\"syntagmaType\"] = f\"{shot['category']}\"\n                if \"cineosisFunction\" not in assembly:\n                    assembly[\"cineosisFunction\"] = f\"{shot['function']}\"\n                if \"imageType\" not in assembly:\n                    assembly[\"imageType\"] = \"midjourney\"\n            \n            # Generate the codex entry with EXACT format as expected by the parser\n            entry = f\"### {shot_id} [{timestamp}]\\n\\n\"\n            entry += f\"**Image:** `{image_path}`\\n\\n\"\n            entry += \"**Assembly Source:**\\n```json\\n\"\n            entry += json.dumps(assembly, indent=2)\n            entry += \"\\n```\\n\\n\"\n            entry += f\"**Prompt:** {shot['category']} \u00b7 {shot['function']} \u00b7 {shot['description']}\"\n            entry += \"\\n\\n---\\n\\n\"\n            \n            entries.append(entry)\n            used_shots.append(shot_id)\n        else:\n            log(f\"Warning: No image found for {shot_id}, skipping\")\n            shots_without_images += 1\n    \n    log(f\"Total entries: {len(entries)}\")\n    log(f\"Shots with images: {shots_with_images}\")\n    log(f\"Shots without images: {shots_without_images}\")\n    if used_shots:\n        log(f\"Used shots: {used_shots[0]}...{used_shots[-1]} ({len(used_shots)} total)\")\n    \n    return entries\n\ndef write_codex_to_file(entries):\n    \"\"\"Write AT codex entries to file.\"\"\"\n    output_file = os.path.join(TIGER_DIR, \"AT_ordered_codex_entries.md\")\n    \n    with open(output_file, 'w') as file:\n        file.write(\"# AT Resurrecting Atlantis - Ordered Codex Entries\\n\\n\")\n        file.write(\"*Generated on \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"*\\n\\n\")\n        file.write(\"These entries are in EXACT shot order from the prompts markdown.\\n\\n\")\n        file.write(\"\".join(entries))\n    \n    log(f\"Generated ordered codex entries written to {output_file}\")\n    return output_file\n\ndef backup_existing_video():\n    \"\"\"Backup existing AT videos with timestamp suffix.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Backup video without audio\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"AT_header_prompt.mp4\")\n    if os.path.exists(video_file):\n        backup_file = os.path.join(VIDEO_OUTPUT_DIR, f\"AT_header_prompt_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_file, backup_file)\n        log(f\"Backing up existing AT video to {backup_file}\")\n    \n    # Backup video with audio\n    video_audio_file = os.path.join(VIDEO_OUTPUT_DIR, f\"AT_header_prompt_with_audio.mp4\")\n    if os.path.exists(video_audio_file):\n        backup_audio_file = os.path.join(VIDEO_OUTPUT_DIR, f\"AT_header_prompt_with_audio_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_audio_file, backup_audio_file)\n        log(f\"Backing up existing AT video with audio to {backup_audio_file}\")\n\ndef generate_at_video(codex_file):\n    \"\"\"Generate AT video with ordered shot sequence.\"\"\"\n    log(f\"Generating AT video with ordered shot sequence...\")\n    \n    # Check if ordered codex file exists\n    if not os.path.exists(codex_file):\n        log(f\"Error: Ordered codex file {codex_file} not found.\")\n        return False\n    \n    # Run the FL video generator with AT prefix and ordered codex\n    cmd = [\n        \"python3\",\n        os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\"),\n        \"--prefix\", \"AT\",\n        \"--ordered-codex\", codex_file\n    ]\n    \n    log(f\"Using ordered codex file: {codex_file}\")\n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        log(f\"Error: Failed to generate AT video.\")\n        return False\n    \n    log(f\"Successfully generated AT video\")\n    return True\n\ndef add_audio_to_video():\n    \"\"\"Add audio to the generated AT video.\"\"\"\n    log(f\"Adding audio to AT video...\")\n    \n    # Check if video file exists\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"AT_header_prompt.mp4\")\n    if not os.path.exists(video_file):\n        log(f\"Error: Video file {video_file} not found.\")\n        return False\n    \n    # Check if audio file exists\n    audio_file = os.path.join(AUDIO_DIR, \"AT_audio.wav\")\n    if not os.path.exists(audio_file):\n        log(f\"Warning: Audio file {audio_file} not found. Skipping audio addition.\")\n        return False\n    \n    # Output file path\n    output_file = os.path.join(VIDEO_OUTPUT_DIR, \"AT_header_prompt_with_audio.mp4\")\n    \n    # FFmpeg command to merge video and audio\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-map\", \"0:v\", \"-map\", \"1:a\",\n        \"-c:v\", \"copy\", \"-c:a\", \"aac\",\n        \"-shortest\",\n        output_file\n    ]\n    \n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        log(f\"Error: Failed to add audio to AT video.\")\n        return False\n    \n    log(f\"Successfully created AT video with audio: {output_file}\")\n    return True\n\ndef copy_to_horse_directory():\n    \"\"\"Copy the new AT video to the HORSE directory with both naming conventions.\"\"\"\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, \"AT_header_prompt.mp4\")\n    \n    # Copy with original name\n    horse_file = os.path.join(HORSE_DIR, \"AT_header_prompt.mp4\")\n    shutil.copy2(video_file, horse_file)\n    log(f\"Copied AT video to {horse_file}\")\n    \n    # Copy with special naming convention\n    special_file = os.path.join(HORSE_DIR, \"06_AT_ResurrectingAtlantis_105500_header_prompt.mp4\")\n    shutil.copy2(video_file, special_file)\n    log(f\"Copied AT video to {special_file}\")\n    \n    return True\n\ndef main():\n    \"\"\"Main function to generate AT video.\"\"\"\n    log(\"=== AT Special Video Generator ===\")\n    \n    # Backup existing videos\n    backup_existing_video()\n    \n    # Extract shot order from prompts\n    shots = extract_at_shots_from_prompts()\n    if not shots:\n        log(\"Error: No AT shots found in prompts file.\")\n        return False\n    \n    # Extract assembly data\n    assembly_data = extract_assembly_data()\n    \n    # Find images from both sources and combine\n    sequence_images = find_at_images_from_sequence()\n    directory_images = find_at_images_from_directory()\n    \n    # Merge image sources, prioritizing sequence images\n    all_images = {}\n    for shot_id in set(list(sequence_images.keys()) + list(directory_images.keys())):\n        if shot_id in sequence_images:\n            all_images[shot_id] = sequence_images[shot_id]\n        else:\n            all_images[shot_id] = directory_images[shot_id]\n    \n    log(f\"Combined {len(all_images)} AT shots with images\")\n    \n    # Generate ordered codex entries\n    entries = generate_at_codex_entries(shots, all_images, assembly_data)\n    if not entries:\n        log(\"Error: No valid AT entries generated.\")\n        return False\n    \n    # Write to codex file\n    codex_file = write_codex_to_file(entries)\n    \n    # Generate AT video\n    success = generate_at_video(codex_file)\n    if success:\n        # Add audio to video\n        add_audio_to_video()\n        \n        # Copy to HORSE directory\n        copy_to_horse_directory()\n    \n    log(\"=== AT Special Video Generator Complete ===\")\n    return success\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "AT_assembly.json",
      "Loaded {len(assembly_data)} assemblies from AT_assembly.json",
      "AT_header_prompt.mp4",
      "AT_header_prompt_MISFIT_{timestamp}.mp4",
      "AT_header_prompt_with_audio.mp4",
      "AT_header_prompt_with_audio_MISFIT_{timestamp}.mp4",
      "AT_header_prompt.mp4",
      "AT_audio.wav",
      "AT_header_prompt_with_audio.mp4",
      "AT_header_prompt.mp4",
      "AT_header_prompt.mp4",
      "06_AT_ResurrectingAtlantis_105500_header_prompt.mp4",
      "AT/AT_shots/",
      "AT/AT_shots/{filename}",
      "\n    interval = SECTION_DURATION / shot_count\n    \n    timestamps = []\n    for i in range(shot_count):\n        seconds = i * interval\n        minutes = int(seconds / 60)\n        remaining_seconds = int(seconds % 60)\n        timestamp = f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "re",
      "json",
      "shutil",
      "subprocess",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "AT Special Video Generator\nThis script specifically generates the AT (Atlantis) video using the images from AT_shots directory\nand the information from ImageSequence.md file."
  },
  {
    "path": "PRIME/scripts/multi_section_video_generator.py",
    "size": 16192,
    "lines": 401,
    "source": "#!/usr/bin/env python3\n\"\"\"\nMulti-Section Video Generator\nThis script generates videos for multiple poem sections (BE, AT, DJ, NS, YH) using the \nauthoritative source files and following the official shot orders.\n\"\"\"\n\nimport os\nimport re\nimport json\nimport shutil\nimport argparse\nimport subprocess\nfrom datetime import datetime\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nJELLYFISH_DIR = os.path.join(BASE_DIR, \"JELLYFISH\")\nTIGER_DIR = os.path.join(BASE_DIR, \"TIGER\")\nAUDIO_DIR = os.path.join(BASE_DIR, \"MANTA\", \"audio\")\nVIDEO_OUTPUT_DIR = os.path.join(JELLYFISH_DIR, \"video_output\")\nPRIME_DIR = os.path.join(BASE_DIR, \"PRIME\")\nLOGS_DIR = os.path.join(PRIME_DIR, \"logs\")\n\n# Source files\nIMAGE_SEQUENCE = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_ImageSequence.md\")\nASSEMBLIES = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_assemblies.md\")\nSIMPLIFIED = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_Simplified.md\")\nPROMPTS = os.path.join(TIGER_DIR, \"COLLECTION_WhereYouGoWhenYouLeave_prompts.md\")\n\n# Section durations based on the collection (all 2:11)\nSECTION_DURATION = 2 * 60 + 11  # 2:11 in seconds\n\n# Section metadata\nSECTION_INFO = {\n    'BE': {'name': 'Because Of You', 'hour_offset': 9},\n    'AT': {'name': 'Atlantis', 'hour_offset': 11},\n    'DJ': {'name': 'Disc Jockey', 'hour_offset': 13},\n    'NS': {'name': 'New Schema', 'hour_offset': 15},\n    'YH': {'name': 'You Are Here', 'hour_offset': 17}\n}\n\n# Ensure directories exist\nos.makedirs(LOGS_DIR, exist_ok=True)\nos.makedirs(os.path.join(PRIME_DIR, \"output\"), exist_ok=True)\n\n# Set up logging\ndef log(message, section=None):\n    \"\"\"Log a message to console and log file.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    section_prefix = f\"[{section}] \" if section else \"\"\n    log_message = f\"{timestamp} - {section_prefix}{message}\"\n    print(log_message)\n    \n    log_file = os.path.join(LOGS_DIR, \"video_generation.log\")\n    with open(log_file, \"a\") as f:\n        f.write(log_message + \"\\n\")\n\ndef extract_assembly_data(section_prefix):\n    \"\"\"Extract assembly data from the assemblies file for a specific section.\"\"\"\n    assembly_data = {}\n    \n    with open(ASSEMBLIES, 'r') as file:\n        content = file.read()\n        \n        # Find section in assemblies file\n        section_match = re.search(rf'## \\d+_{section_prefix} - .+?\\n```json\\n(.*?)```', content, re.DOTALL)\n        if not section_match:\n            log(f\"No assembly data found for section {section_prefix}\", section_prefix)\n            return assembly_data\n        \n        # Parse assembly JSON\n        assembly_json = section_match.group(1)\n        try:\n            assembly_list = json.loads(assembly_json)\n            \n            # Convert assembly list to dict for lookup\n            for item in assembly_list:\n                shot_id = item.get('id')\n                if shot_id:\n                    assembly_data[shot_id] = item\n            \n            log(f\"Extracted {len(assembly_data)} assembly items for {section_prefix}\", section_prefix)\n        except json.JSONDecodeError as e:\n            log(f\"Error parsing assembly JSON for {section_prefix}: {e}\", section_prefix)\n    \n    return assembly_data\n\ndef extract_shot_order(section_prefix):\n    \"\"\"Extract shots in exact order from the prompts file for a specific section.\"\"\"\n    shots = []\n    \n    with open(PROMPTS, 'r') as file:\n        content = file.read()\n        \n        # Find section in prompts file\n        section_pattern = rf'## \\d+_{section_prefix} - .+?(?=## |\\Z)'\n        section_match = re.search(section_pattern, content, re.DOTALL)\n        if not section_match:\n            log(f\"No prompts found for section {section_prefix}\", section_prefix)\n            return shots\n        \n        section_content = section_match.group(0)\n        \n        # Extract each shot line by line\n        shot_pattern = rf'{section_prefix}(\\d+)\\s*\\[([\\d:]+)\\]\\s*\u00b7\\s*([A-Z]+)\\s*\u00b7\\s*([^\u00b7]+)\u00b7\\s*(.+)'\n        for match in re.finditer(shot_pattern, section_content):\n            shot_num, timestamp, category, function, description = match.groups()\n            shot_id = f\"{section_prefix}{shot_num}\"\n            shots.append({\n                'id': shot_id,\n                'number': shot_num,\n                'timestamp': timestamp,\n                'category': category.strip(),\n                'function': function.strip(),\n                'description': description.strip()\n            })\n    \n    log(f\"Extracted {len(shots)} {section_prefix} shots from prompts in official order\", section_prefix)\n    return shots\n\ndef extract_existing_images(section_prefix):\n    \"\"\"Extract the list of existing images from the ImageSequence file for a specific section.\"\"\"\n    images = {}\n    \n    with open(IMAGE_SEQUENCE, 'r') as file:\n        content = file.read()\n        \n        # Find section in the sequence file\n        section_pattern = rf'### {section_prefix}\\d+.*?(?=###|\\Z)'\n        section_match = re.search(section_pattern, content, re.DOTALL)\n        if section_match:\n            section_content = section_match.group(0)\n            \n            # Extract image paths\n            current_shot_id = None\n            for line in section_content.split('\\n'):\n                # Match the shot ID line\n                shot_match = re.match(rf'### ({section_prefix}\\d+)', line)\n                if shot_match:\n                    current_shot_id = shot_match.group(1)\n                \n                # Match image paths\n                image_match = re.match(r'- `([^`]+)`', line)\n                if image_match and current_shot_id:\n                    image_path = image_match.group(1)\n                    if current_shot_id not in images:\n                        images[current_shot_id] = []\n                    images[current_shot_id].append(image_path)\n    \n    # Also check the section directory for any images not in the sequence file\n    section_dir = os.path.join(TIGER_DIR, section_prefix)\n    if os.path.exists(section_dir):\n        for filename in os.listdir(section_dir):\n            if filename.endswith(('.png', '.jpg', '.jpeg')):\n                shot_match = re.match(rf'({section_prefix}\\d+)', filename)\n                if shot_match:\n                    shot_id = shot_match.group(1)\n                    image_path = f\"{section_prefix}/{filename}\"\n                    if shot_id not in images:\n                        images[shot_id] = []\n                    if image_path not in images[shot_id]:\n                        images[shot_id].append(image_path)\n    \n    log(f\"Found images for {len(images)} {section_prefix} shots\", section_prefix)\n    return images\n\ndef calculate_timestamps(section_prefix, shot_count):\n    \"\"\"Calculate evenly distributed timestamps for a section.\"\"\"\n    hour_offset = SECTION_INFO[section_prefix]['hour_offset']\n    interval = SECTION_DURATION / shot_count\n    \n    timestamps = []\n    for i in range(shot_count):\n        seconds = i * interval\n        minutes = int(seconds / 60)\n        remaining_seconds = int(seconds % 60)  # No milliseconds for compatibility\n        timestamp = f\"{hour_offset:02d}:{minutes:02d}:{remaining_seconds:02d}\"\n        timestamps.append(timestamp)\n    \n    return timestamps\n\ndef generate_ordered_codex_entries(section_prefix, shots, images, assembly_data):\n    \"\"\"Generate ordered Codex entries for shots with complete metadata.\"\"\"\n    entries = []\n    shots_with_images = 0\n    shots_without_images = 0\n    used_shots = []\n    \n    # Calculate timestamps\n    timestamps = calculate_timestamps(section_prefix, len(shots))\n    \n    for i, shot in enumerate(shots):\n        shot_id = shot['id']\n        shot_number = shot['number']\n        timestamp = timestamps[i]\n        \n        # Check if we have an image for this shot\n        if shot_id in images and images[shot_id]:\n            image_path = images[shot_id][0]  # Use the first available image\n            shots_with_images += 1\n            \n            # Get assembly data if available\n            assembly = assembly_data.get(shot_id, {})\n            if not assembly:\n                # Create assembly from shot data if not found in assembly_data\n                assembly = {\n                    \"poem\": SECTION_INFO[section_prefix]['name'],\n                    \"content\": shot['description'][:30] + \"...\",\n                    \"syntagmaType\": f\"{shot['category']}\",\n                    \"cineosisFunction\": f\"{shot['function']}\"\n                }\n            else:\n                # Ensure we have all required fields\n                if \"syntagmaType\" not in assembly:\n                    assembly[\"syntagmaType\"] = f\"{shot['category']}\"\n                if \"cineosisFunction\" not in assembly:\n                    assembly[\"cineosisFunction\"] = f\"{shot['function']}\"\n            \n            # Generate the codex entry with EXACT format as expected by the parser\n            entry = f\"### {shot_id} [{timestamp}]\\n\\n\"\n            entry += f\"**Image:** `{image_path}`\\n\\n\"\n            entry += \"**Assembly Source:**\\n```json\\n\"\n            entry += json.dumps(assembly, indent=2)\n            entry += \"\\n```\\n\\n\"\n            entry += f\"**Prompt:** {shot['category']} \u00b7 {shot['function']} \u00b7 {shot['description']}\"\n            entry += \"\\n\\n---\\n\\n\"\n            \n            entries.append(entry)\n            used_shots.append(shot_id)\n        else:\n            log(f\"Warning: No image found for {shot_id}, skipping this shot\", section_prefix)\n            shots_without_images += 1\n    \n    log(f\"Total entries: {len(entries)}\", section_prefix)\n    log(f\"Shots with images: {shots_with_images}\", section_prefix)\n    log(f\"Shots without images: {shots_without_images}\", section_prefix)\n    if used_shots:\n        log(f\"Used shots: {used_shots[0]}...{used_shots[-1]} ({len(used_shots)} total)\", section_prefix)\n    \n    return entries\n\ndef write_codex_to_file(section_prefix, entries):\n    \"\"\"Write entries to output file.\"\"\"\n    output_file = os.path.join(TIGER_DIR, f\"{section_prefix}_ordered_codex_entries.md\")\n    \n    with open(output_file, 'w') as file:\n        file.write(f\"# {section_prefix} {SECTION_INFO[section_prefix]['name']} - Ordered Codex Entries\\n\\n\")\n        file.write(\"*Generated on \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"*\\n\\n\")\n        file.write(\"These entries are in EXACT shot order from the prompts markdown.\\n\\n\")\n        file.write(\"\".join(entries))\n    \n    log(f\"Generated ordered codex entries written to {output_file}\", section_prefix)\n    return output_file\n\ndef backup_existing_video(section_prefix):\n    \"\"\"Backup existing section videos with timestamp suffix.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Backup video without audio\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, f\"{section_prefix}_header_prompt.mp4\")\n    if os.path.exists(video_file):\n        backup_file = os.path.join(VIDEO_OUTPUT_DIR, f\"{section_prefix}_header_prompt_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_file, backup_file)\n        log(f\"Backing up existing {section_prefix} video to {backup_file}\", section_prefix)\n    \n    # Backup video with audio\n    video_audio_file = os.path.join(VIDEO_OUTPUT_DIR, f\"{section_prefix}_header_prompt_with_audio.mp4\")\n    if os.path.exists(video_audio_file):\n        backup_audio_file = os.path.join(VIDEO_OUTPUT_DIR, f\"{section_prefix}_header_prompt_with_audio_MISFIT_{timestamp}.mp4\")\n        shutil.copy2(video_audio_file, backup_audio_file)\n        log(f\"Backing up existing {section_prefix} video with audio to {backup_audio_file}\", section_prefix)\n\ndef generate_section_video(section_prefix, codex_file):\n    \"\"\"Generate section video with ordered shot sequence.\"\"\"\n    log(f\"Generating {section_prefix} video with ordered shot sequence...\", section_prefix)\n    \n    # Check if ordered codex file exists\n    if not os.path.exists(codex_file):\n        log(f\"Error: Ordered codex file {codex_file} not found.\", section_prefix)\n        return False\n    \n    # Run the FL video generator with section prefix and ordered codex\n    cmd = [\n        \"python3\",\n        os.path.join(JELLYFISH_DIR, \"fl_video_generator_header_prompt.py\"),\n        \"--prefix\", section_prefix,\n        \"--ordered-codex\", codex_file\n    ]\n    \n    log(f\"Using ordered codex file: {codex_file}\", section_prefix)\n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        log(f\"Error: Failed to generate {section_prefix} video.\", section_prefix)\n        return False\n    \n    log(f\"Successfully generated {section_prefix} video\", section_prefix)\n    return True\n\ndef add_audio_to_video(section_prefix):\n    \"\"\"Add audio to the generated video.\"\"\"\n    log(f\"Adding audio to {section_prefix} video...\", section_prefix)\n    \n    # Check if video file exists\n    video_file = os.path.join(VIDEO_OUTPUT_DIR, f\"{section_prefix}_header_prompt.mp4\")\n    if not os.path.exists(video_file):\n        log(f\"Error: Video file {video_file} not found.\", section_prefix)\n        return False\n    \n    # Check if audio file exists\n    audio_file = os.path.join(AUDIO_DIR, f\"{section_prefix}_audio.wav\")\n    if not os.path.exists(audio_file):\n        log(f\"Warning: Audio file {audio_file} not found. Skipping audio addition.\", section_prefix)\n        return False\n    \n    # Output file path\n    output_file = os.path.join(VIDEO_OUTPUT_DIR, f\"{section_prefix}_header_prompt_with_audio.mp4\")\n    \n    # FFmpeg command to merge video and audio\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", video_file,\n        \"-i\", audio_file,\n        \"-map\", \"0:v\", \"-map\", \"1:a\",\n        \"-c:v\", \"copy\", \"-c:a\", \"aac\",\n        \"-shortest\",\n        output_file\n    ]\n    \n    result = subprocess.run(cmd)\n    \n    if result.returncode != 0:\n        log(f\"Error: Failed to add audio to {section_prefix} video.\", section_prefix)\n        return False\n    \n    log(f\"Successfully created {section_prefix} video with audio: {output_file}\", section_prefix)\n    return True\n\ndef process_section(section_prefix):\n    \"\"\"Process a single poem section from start to finish.\"\"\"\n    log(f\"=== Processing {section_prefix} ({SECTION_INFO[section_prefix]['name']}) ===\", section_prefix)\n    \n    # Backup existing videos\n    backup_existing_video(section_prefix)\n    \n    # Extract assembly data\n    assembly_data = extract_assembly_data(section_prefix)\n    \n    # Extract shot order\n    shots = extract_shot_order(section_prefix)\n    if not shots:\n        log(f\"Error: No shots found for {section_prefix}, skipping section\", section_prefix)\n        return False\n    \n    # Extract existing images\n    images = extract_existing_images(section_prefix)\n    \n    # Generate ordered entries with complete metadata\n    entries = generate_ordered_codex_entries(section_prefix, shots, images, assembly_data)\n    if not entries:\n        log(f\"Error: No valid entries generated for {section_prefix}, skipping section\", section_prefix)\n        return False\n    \n    # Write to codex file\n    codex_file = write_codex_to_file(section_prefix, entries)\n    \n    # Generate section video\n    success = generate_section_video(section_prefix, codex_file)\n    if success:\n        # Add audio to video\n        add_audio_to_video(section_prefix)\n    \n    return success\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Generate videos for multiple poem sections\")\n    parser.add_argument(\"--sections\", nargs=\"+\", default=[\"BE\", \"AT\", \"DJ\", \"NS\", \"YH\"], \n                        help=\"Section prefixes to process (default: BE AT DJ NS YH)\")\n    args = parser.parse_args()\n    \n    log(f\"=== Multi-Section Video Generator ===\")\n    log(f\"Processing sections: {', '.join(args.sections)}\")\n    \n    results = {}\n    for section in args.sections:\n        if section in SECTION_INFO:\n            results[section] = process_section(section)\n        else:\n            log(f\"Error: Unknown section prefix '{section}'\")\n    \n    # Report summary\n    log(\"=== Processing Summary ===\")\n    for section, success in results.items():\n        status = \"SUCCESS\" if success else \"FAILED\"\n        log(f\"{section} ({SECTION_INFO[section]['name']}): {status}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "{section_prefix}_header_prompt.mp4",
      "{section_prefix}_header_prompt_MISFIT_{timestamp}.mp4",
      "{section_prefix}_header_prompt_with_audio.mp4",
      "{section_prefix}_header_prompt_with_audio_MISFIT_{timestamp}.mp4",
      "{section_prefix}_header_prompt.mp4",
      "{section_prefix}_audio.wav",
      "{section_prefix}_header_prompt_with_audio.mp4",
      "{section_prefix}/{filename}",
      "]\n    interval = SECTION_DURATION / shot_count\n    \n    timestamps = []\n    for i in range(shot_count):\n        seconds = i * interval\n        minutes = int(seconds / 60)\n        remaining_seconds = int(seconds % 60)  # No milliseconds for compatibility\n        timestamp = f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "cmd"
      },
      {
        "type": "run",
        "snippet": "cmd"
      }
    ],
    "imports": [
      "os",
      "re",
      "json",
      "shutil",
      "argparse",
      "subprocess",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "Multi-Section Video Generator\nThis script generates videos for multiple poem sections (BE, AT, DJ, NS, YH) using the \nauthoritative source files and following the official shot orders."
  },
  {
    "path": "DOG/HOUSE/update_bloodline_timestamps.py",
    "size": 5432,
    "lines": 134,
    "source": "import json\nimport math\nimport re\nimport os\n\ndef get_id_numeric_part(entry_id_str):\n    \"\"\"Extracts the numeric part of an ID like 'BL001', 'BL077'.\"\"\"\n    if not isinstance(entry_id_str, str):\n        return -1 \n    match = re.search(r'(\\d+)$', entry_id_str)\n    if match:\n        return int(match.group(1))\n    print(f\"Warning: Could not parse numeric part from ID: {entry_id_str}\")\n    return -1 # Fallback if no numeric part found\n\ndef update_timeline_timestamps(file_path):\n    \"\"\"\n    Reads a JSON timeline file, updates timestamps for entries, and overwrites the file.\n    - Timestamps are in HH:MM:SS:mmm format.\n    - Entries start at 08:43:59:000.\n    - All entries in the file are spread evenly over a total duration of 131 seconds.\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            original_content = f.read()\n            data = json.loads(original_content)\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return False\n    except json.JSONDecodeError as e:\n        print(f\"Error decoding JSON from {file_path}: {e}\")\n        return False\n    except Exception as e:\n        print(f\"Error reading file {file_path}: {e}\")\n        return False\n\n    if not isinstance(data, list):\n        print(\"Error: JSON data in the file is not a list of entries.\")\n        return False\n\n    num_entries = len(data)\n    if num_entries == 0:\n        print(\"No entries found in the JSON data. No changes made.\")\n        return True # No error, but nothing to do\n\n    # Sort entries by the numeric part of their 'id' field\n    try:\n        # Ensure all entries have an 'id' field and it's a string\n        for i, entry in enumerate(data):\n            if 'id' not in entry:\n                print(f\"Error: Entry at index {i} (before sort) is missing an 'id' field: {entry}\")\n                return False\n            if not isinstance(entry.get('id'), str):\n                print(f\"Error: Entry at index {i} (before sort) has a non-string 'id': {entry.get('id')}\")\n                return False\n        \n        data.sort(key=lambda x: get_id_numeric_part(x['id']))\n    except Exception as e:\n        print(f\"Error sorting entries by ID: {e}\")\n        return False\n\n    # Explicit start time calculation as per user's formula\n    # \"08:43:059\" means 8 minutes, 43 seconds, 59 milliseconds\n    start_calc_m, start_calc_s, start_calc_ms_val = 8, 43, 59\n    start_time_total_ms = (start_calc_m * 60 * 1000) + \\\n                          (start_calc_s * 1000) + \\\n                          start_calc_ms_val\n    \n    total_duration_ms = 131 * 1000  # 131 seconds\n\n    increment_ms_per_entry = 0.0\n    if num_entries > 1:\n        # The time difference between the start of the first entry and the start of the last entry is total_duration_ms.\n        # So, there are (num_entries - 1) intervals.\n        increment_ms_per_entry = float(total_duration_ms) / (num_entries - 1)\n    \n    updated_entries_count = 0\n    for i, entry in enumerate(data):\n        current_offset_ms = i * increment_ms_per_entry\n        current_entry_total_ms = start_time_total_ms + current_offset_ms\n        \n        time_val = current_entry_total_ms\n        \n        # Convert Tn_ms back to MM:SS:mmm\n        # Tn_ms is current_entry_total_ms\n        m_calc = math.floor(current_entry_total_ms / 60000)\n        remainder_ms_after_minutes = current_entry_total_ms % 60000\n        s_calc = math.floor(remainder_ms_after_minutes / 1000)\n        ms_calc = round(remainder_ms_after_minutes % 1000) # Round milliseconds\n\n        if ms_calc == 1000:\n            ms_calc = 0\n            s_calc += 1\n            if s_calc == 60:\n                s_calc = 0\n                m_calc += 1\n        # Note: m_calc can exceed 59 if total duration makes it so (e.g. 654059ms -> 10:54:059)\n        \n        entry['timestamp'] = f\"{int(m_calc):02d}:{int(s_calc):02d}:{int(ms_calc):03d}\"\n        updated_entries_count +=1\n\n    try:\n        with open(file_path, 'w', encoding='utf-8') as f:\n            json.dump(data, f, indent=4, ensure_ascii=False)\n        print(f\"Successfully updated timestamps for {updated_entries_count} entries in {file_path}.\")\n        if num_entries > 0:\n            print(f\"First entry ID {data[0].get('id', 'N/A')}, timestamp: {data[0]['timestamp']}\")\n        if num_entries > 1:\n            print(f\"Last entry ID {data[-1].get('id', 'N/A')}, timestamp: {data[-1]['timestamp']}\")\n        print(f\"Total entries processed: {num_entries}\")\n        return True\n    except Exception as e:\n        print(f\"Error writing updated JSON to {file_path}: {e}\")\n        return False\n\nif __name__ == '__main__':\n    # Path to the target JSON file\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    target_file_name = \"BL-TIMELINE-ADDENDUM.json\"\n    \n    target_file_path = \"/Users/gaia/resurrecting atlantis/DOG/HOUSE/BL-TIMELINE-ADDENDUM.json\"\n    \n    if not os.path.exists(target_file_path):\n        print(f\"File not found at hardcoded path: {target_file_path}\")\n        alt_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), target_file_name)\n        if os.path.exists(alt_path):\n            print(f\"Found file at alternative relative path: {alt_path}\")\n            target_file_path = alt_path\n        else:\n            print(f\"Also not found at {alt_path}. Please check the file path.\")\n            exit(1)\n            \n    update_timeline_timestamps(target_file_path)\n",
    "file_references": [
      "BL-TIMELINE-ADDENDUM.json",
      "/Users/gaia/resurrecting atlantis/DOG/HOUSE/BL-TIMELINE-ADDENDUM.json",
      " means 8 minutes, 43 seconds, 59 milliseconds\n    start_calc_m, start_calc_s, start_calc_ms_val = 8, 43, 59\n    start_time_total_ms = (start_calc_m * 60 * 1000) + \\\n                          (start_calc_s * 1000) + \\\n                          start_calc_ms_val\n    \n    total_duration_ms = 131 * 1000  # 131 seconds\n\n    increment_ms_per_entry = 0.0\n    if num_entries > 1:\n        # The time difference between the start of the first entry and the start of the last entry is total_duration_ms.\n        # So, there are (num_entries - 1) intervals.\n        increment_ms_per_entry = float(total_duration_ms) / (num_entries - 1)\n    \n    updated_entries_count = 0\n    for i, entry in enumerate(data):\n        current_offset_ms = i * increment_ms_per_entry\n        current_entry_total_ms = start_time_total_ms + current_offset_ms\n        \n        time_val = current_entry_total_ms\n        \n        # Convert Tn_ms back to MM:SS:mmm\n        # Tn_ms is current_entry_total_ms\n        m_calc = math.floor(current_entry_total_ms / 60000)\n        remainder_ms_after_minutes = current_entry_total_ms % 60000\n        s_calc = math.floor(remainder_ms_after_minutes / 1000)\n        ms_calc = round(remainder_ms_after_minutes % 1000) # Round milliseconds\n\n        if ms_calc == 1000:\n            ms_calc = 0\n            s_calc += 1\n            if s_calc == 60:\n                s_calc = 0\n                m_calc += 1\n        # Note: m_calc can exceed 59 if total duration makes it so (e.g. 654059ms -> 10:54:059)\n        \n        entry[",
      "N/A",
      "N/A",
      "/Users/gaia/resurrecting atlantis/DOG/HOUSE/BL-TIMELINE-ADDENDUM.json"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "math",
      "re",
      "os"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "DOG/LEG/scripts/prepend_intros.py",
    "size": 6448,
    "lines": 126,
    "source": "import os\nimport subprocess\n\nMAIN_VIDEO_DIR = \"/Users/gaia/resurrecting atlantis/DOG/LEG/output_videos/\"\nINTRO_VIDEO_DIR = \"/Users/gaia/resurrecting atlantis/IMPALA/IBEX/intros_with_headers/\"\nOUTPUT_DIR = \"/Users/gaia/resurrecting atlantis/DOG/LEG/output_videos_with_intros/\"\nCONCAT_TEMP_FILE_NAME = \"_temp_concat_list.txt\"\n\nVIDEO_MAPPING = {\n    \"out-of-life.mp4\": \"01_SH_OutOfLife_000000_2101217636, 01_SH_OutofLife_0000, M 5_header.mp4\",\n    \"flashing-lights.mp4\": \"02_FL_FlashingLights_021100_2424344039, 02_FL_FlashingLights, M 5_header.mp4\",\n    \"how-to-break-off-an-engagement.mp4\": \"03_HT_HowToBreakOffAnEngagement_042200_03_HT_HowtoBreakOffa, M 5_header.mp4\",\n    \"nevermore.mp4\": \"04_NM_Nevermore_063300_CinePrompt \u201cNEVERM, image-prompt, M 5_header.mp4\",\n    \"bloodline.mp4\": \"05_BE_Bloodline_084400_2436216441 (10)_header.mp4\",\n    \"resurrecting-atlantis.mp4\": \"06_AT_ResurrectingAtlantis_105500_503796468, 06_AT_ResurrectingAt, M 5_header.mp4\",\n    \"dj-turn-me-up.mp4\": \"07_DJ_DJTurnMeUp_130600_2436216441 (13)_header.mp4\",\n    \"newly-single.mp4\": \"08_NS_NewlySingle_151700_2436216441_header.mp4\",\n    \"yet-heard.mp4\": \"09_YH_YetHeard_172800_2436216441 (12)_header.mp4\",\n    \"magic-ride.mp4\": \"10_MR_MagicRide_193900_2436216441 (5)_header.mp4\",\n    \"reunion.mp4\": \"12_RU_Reunion_215000_2436216441 (11)_header.mp4\",\n    \"how-to-win-my-heart.mp4\": \"13_HW_HowToWinMyHeart_240100_2436216441 (9)_header.mp4\",\n    \"hot-minute.mp4\": \"14_HM_HotMinute_261200_20250528_2220_Blend Video_blend_01jwcxeddkehsae18pnwgh2hq2_header.mp4\"\n}\n\ndef main():\n    print(f\"Starting to prepend intro videos.\")\n    print(f\"Main video directory: {MAIN_VIDEO_DIR}\")\n    print(f\"Intro video directory: {INTRO_VIDEO_DIR}\")\n    print(f\"Output directory: {OUTPUT_DIR}\")\n\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f\"Created output directory: {OUTPUT_DIR}\")\n\n    success_count = 0\n    fail_count = 0\n\n    for main_video_filename, intro_video_filename in VIDEO_MAPPING.items():\n        main_video_path = os.path.join(MAIN_VIDEO_DIR, main_video_filename)\n        intro_video_path = os.path.join(INTRO_VIDEO_DIR, intro_video_filename)\n        output_video_path = os.path.join(OUTPUT_DIR, main_video_filename) # Keep original main video name\n        temp_concat_file = os.path.join(OUTPUT_DIR, CONCAT_TEMP_FILE_NAME) # Temp file in output dir\n\n        print(f\"\\nProcessing: {main_video_filename}\")\n        print(f\"  Intro video: {intro_video_path}\")\n        print(f\"  Main video: {main_video_path}\")\n        print(f\"  Output video: {output_video_path}\")\n\n        if not os.path.exists(main_video_path):\n            print(f\"  Error: Main video not found: {main_video_path}\")\n            fail_count += 1\n            continue\n        \n        if not os.path.exists(intro_video_path):\n            print(f\"  Error: Intro video not found: {intro_video_path}\")\n            fail_count += 1\n            continue\n\n        try:\n            with open(temp_concat_file, 'w') as f:\n                f.write(f\"file '{os.path.abspath(intro_video_path)}'\\n\")\n                f.write(f\"file '{os.path.abspath(main_video_path)}'\\n\")\n            \n            # Attempt to concatenate. If streams are incompatible, this might fail or produce poor results.\n            # A more robust solution would involve re-encoding to common parameters.\n            ffmpeg_command = [\n                \"ffmpeg\",\n                \"-y\",\n                \"-f\", \"concat\",\n                \"-safe\", \"0\",\n                \"-i\", temp_concat_file,\n                \"-c\", \"copy\", # Attempt to copy streams directly\n                output_video_path\n            ]\n            \n            print(f\"  Running ffmpeg command: {' '.join(ffmpeg_command)}\")\n            process = subprocess.run(ffmpeg_command, capture_output=True, text=True, check=False) # check=False to inspect errors\n\n            if process.returncode != 0:\n                print(f\"  Warning: ffmpeg (copy codecs) failed for {main_video_filename}. Return code: {process.returncode}\")\n                print(f\"  ffmpeg stdout:\\n{process.stdout}\")\n                print(f\"  ffmpeg stderr:\\n{process.stderr}\")\n                print(f\"  Attempting re-encode for {main_video_filename}...\")\n                # Fallback: Re-encode. This is a basic re-encode, might need adjustments for specific quality/resolution.\n                # Using -vf scale to a common height (e.g., 720p) and -video_track_timescale 25000 for broader compatibility\n                # Assuming main video resolution is desired, but we need to handle potential mismatches.\n                # For simplicity, let's try to set a common resolution like 1032x844 (from LEG videos) if possible, or 720p height.\n                # This re-encoding part can be complex due to varying input resolutions/SAR/DAR.\n                # A simpler re-encode without scaling, but ensuring compatible codecs:\n                ffmpeg_command_reencode = [\n                    \"ffmpeg\",\n                    \"-y\",\n                    \"-f\", \"concat\",\n                    \"-safe\", \"0\",\n                    \"-i\", temp_concat_file,\n                    \"-c:v\", \"libx264\",\n                    \"-preset\", \"medium\",\n                    \"-crf\", \"23\",\n                    \"-c:a\", \"aac\",\n                    \"-b:a\", \"192k\",\n                    \"-pix_fmt\", \"yuv420p\", # Common pixel format\n                    output_video_path\n                ]\n                print(f\"  Running ffmpeg re-encode command: {' '.join(ffmpeg_command_reencode)}\")\n                process_reencode = subprocess.run(ffmpeg_command_reencode, capture_output=True, text=True, check=True)\n                print(f\"  Successfully re-encoded and created video with intro: {output_video_path}\")\n            else:\n                print(f\"  Successfully copied streams and created video with intro: {output_video_path}\")\n            success_count += 1\n\n        except subprocess.CalledProcessError as e:\n            print(f\"  Error during ffmpeg execution for {main_video_filename}: {e}\")\n            print(f\"  ffmpeg stdout:\\n{e.stdout}\")\n            print(f\"  ffmpeg stderr:\\n{e.stderr}\")\n            fail_count += 1\n        finally:\n            if os.path.exists(temp_concat_file):\n                os.remove(temp_concat_file)\n\n    print(f\"\\nFinished processing.\")\n    print(f\"Successfully processed: {success_count} videos.\")\n    print(f\"Failed to process: {fail_count} videos.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "out-of-life.mp4",
      "01_SH_OutOfLife_000000_2101217636, 01_SH_OutofLife_0000, M 5_header.mp4",
      "flashing-lights.mp4",
      "02_FL_FlashingLights_021100_2424344039, 02_FL_FlashingLights, M 5_header.mp4",
      "how-to-break-off-an-engagement.mp4",
      "03_HT_HowToBreakOffAnEngagement_042200_03_HT_HowtoBreakOffa, M 5_header.mp4",
      "nevermore.mp4",
      "04_NM_Nevermore_063300_CinePrompt \u201cNEVERM, image-prompt, M 5_header.mp4",
      "bloodline.mp4",
      "05_BE_Bloodline_084400_2436216441 (10)_header.mp4",
      "resurrecting-atlantis.mp4",
      "06_AT_ResurrectingAtlantis_105500_503796468, 06_AT_ResurrectingAt, M 5_header.mp4",
      "dj-turn-me-up.mp4",
      "07_DJ_DJTurnMeUp_130600_2436216441 (13)_header.mp4",
      "newly-single.mp4",
      "08_NS_NewlySingle_151700_2436216441_header.mp4",
      "yet-heard.mp4",
      "09_YH_YetHeard_172800_2436216441 (12)_header.mp4",
      "magic-ride.mp4",
      "10_MR_MagicRide_193900_2436216441 (5)_header.mp4",
      "reunion.mp4",
      "12_RU_Reunion_215000_2436216441 (11)_header.mp4",
      "how-to-win-my-heart.mp4",
      "13_HW_HowToWinMyHeart_240100_2436216441 (9)_header.mp4",
      "hot-minute.mp4",
      "14_HM_HotMinute_261200_20250528_2220_Blend Video_blend_01jwcxeddkehsae18pnwgh2hq2_header.mp4",
      "/Users/gaia/resurrecting atlantis/DOG/LEG/output_videos/",
      "/Users/gaia/resurrecting atlantis/IMPALA/IBEX/intros_with_headers/",
      "/Users/gaia/resurrecting atlantis/DOG/LEG/output_videos_with_intros/",
      ")\n                # Fallback: Re-encode. This is a basic re-encode, might need adjustments for specific quality/resolution.\n                # Using -vf scale to a common height (e.g., 720p) and -video_track_timescale 25000 for broader compatibility\n                # Assuming main video resolution is desired, but we need to handle potential mismatches.\n                # For simplicity, let"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "ffmpeg_command, capture_output=True, text=True, check=False"
      },
      {
        "type": "run",
        "snippet": "ffmpeg_command_reencode, capture_output=True, text=True, check=True"
      }
    ],
    "imports": [
      "os",
      "subprocess"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "DOG/LEG/scripts/meta_assembler.py",
    "size": 3555,
    "lines": 97,
    "source": "import os\nimport subprocess\n\nVIDEO_BASE_DIR = \"/Users/gaia/resurrecting atlantis/DOG/EAR/\"  # Changed to use videos with audio\nOUTPUT_VIDEO_NAME = \"PROJECT_ATLANTIS_COMPLETE_SEQUENCE_WITH_AUDIO.mp4\"  # New name for sequence with audio\nCONCAT_FILE_NAME = \"concat_list.txt\"\n\n# Titles in the desired order, derived from user's list\n# 01_SH_OutOfLife -> outoflife.mp4\n# 02_FL_FlashingLights -> flashinglights.mp4\n# 03_BE_HowToBreakOffAnEngagement -> howtobreakoffanengagement.mp4\n# 04_NM_Nevermore -> nevermore.mp4\n# 05_BL_Bloodline -> bloodline.mp4\n# 06_AT_ResurrectingAtlantis -> resurrectingatlantis.mp4\n# 07_DJ_DJTurnMeUp -> djturnmeup.mp4\n# 08_NS_NewlySingle -> newlysingle.mp4\n# 09_YH_YetHeard -> yetheard.mp4\n# 10_MR_MagicRide -> magicride.mp4\n# 12_RU_Reunion -> reunion.mp4\n# 13_HW_HowToWinMyHeart -> howtowinmyheart.mp4\n# 14_HM_HotMinute -> hotminute.mp4\n\nORDERED_VIDEO_FILENAMES = [\n    \"out-of-life.mp4\",\n    \"flashing-lights.mp4\",\n    \"how-to-break-off-an-engagement.mp4\",\n    \"nevermore.mp4\",\n    \"bloodline.mp4\",\n    \"resurrecting-atlantis.mp4\",\n    \"dj-turn-me-up.mp4\",\n    \"newly-single.mp4\",\n    \"yet-heard.mp4\",\n    \"magic-ride.mp4\",\n    \"reunion.mp4\",\n    \"how-to-win-my-heart.mp4\",\n    \"hot-minute.mp4\"\n]\n\ndef main():\n    print(f\"Starting meta-assembly of {len(ORDERED_VIDEO_FILENAMES)} videos.\")\n    print(f\"Input video directory: {VIDEO_BASE_DIR}\")\n    \n    concat_file_path = os.path.join(VIDEO_BASE_DIR, CONCAT_FILE_NAME) # Place concat list in the new video base dir (DOG/EAR/)\n    output_video_path = os.path.join(VIDEO_BASE_DIR, OUTPUT_VIDEO_NAME)\n\n    video_paths_for_concat = []\n    for video_filename in ORDERED_VIDEO_FILENAMES:\n        full_path = os.path.join(VIDEO_BASE_DIR, video_filename)\n        if not os.path.exists(full_path):\n            print(f\"Error: Video file not found: {full_path}\")\n            print(\"Aborting assembly.\")\n            return\n        video_paths_for_concat.append(full_path)\n\n    print(f\"All {len(video_paths_for_concat)} video files found.\")\n\n    try:\n        with open(concat_file_path, 'w') as f:\n            for path in video_paths_for_concat:\n                # FFmpeg concat demuxer requires 'file' keyword and proper quoting for paths\n                f.write(f\"file '{path}'\\n\")\n        print(f\"Successfully created concat list: {concat_file_path}\")\n\n        ffmpeg_command = [\n            \"ffmpeg\",\n            \"-y\",  # Overwrite output file if it exists\n            \"-f\", \"concat\",\n            \"-safe\", \"0\", # Allows unsafe file paths (though we use absolute)\n            \"-i\", concat_file_path,\n            \"-c\", \"copy\", # Copy codecs, no re-encoding\n            output_video_path\n        ]\n\n        print(f\"Running ffmpeg command: {' '.join(ffmpeg_command)}\")\n        process = subprocess.run(ffmpeg_command, capture_output=True, text=True, check=True)\n        print(\"ffmpeg stdout:\")\n        print(process.stdout)\n        print(\"ffmpeg stderr:\")\n        print(process.stderr)\n        print(f\"Successfully assembled videos into: {output_video_path}\")\n\n    except FileNotFoundError:\n        print(f\"Error: Could not create concat list file at {concat_file_path}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during ffmpeg execution: {e}\")\n        print(\"ffmpeg stdout:\")\n        print(e.stdout)\n        print(\"ffmpeg stderr:\")\n        print(e.stderr)\n    finally:\n        if os.path.exists(concat_file_path):\n            os.remove(concat_file_path)\n            print(f\"Cleaned up temporary concat list: {concat_file_path}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "PROJECT_ATLANTIS_COMPLETE_SEQUENCE_WITH_AUDIO.mp4",
      "out-of-life.mp4",
      "flashing-lights.mp4",
      "how-to-break-off-an-engagement.mp4",
      "nevermore.mp4",
      "bloodline.mp4",
      "resurrecting-atlantis.mp4",
      "dj-turn-me-up.mp4",
      "newly-single.mp4",
      "yet-heard.mp4",
      "magic-ride.mp4",
      "reunion.mp4",
      "how-to-win-my-heart.mp4",
      "hot-minute.mp4",
      "/Users/gaia/resurrecting atlantis/DOG/EAR/",
      ")\n    \n    concat_file_path = os.path.join(VIDEO_BASE_DIR, CONCAT_FILE_NAME) # Place concat list in the new video base dir (DOG/EAR/)\n    output_video_path = os.path.join(VIDEO_BASE_DIR, OUTPUT_VIDEO_NAME)\n\n    video_paths_for_concat = []\n    for video_filename in ORDERED_VIDEO_FILENAMES:\n        full_path = os.path.join(VIDEO_BASE_DIR, video_filename)\n        if not os.path.exists(full_path):\n            print(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "ffmpeg_command, capture_output=True, text=True, check=True"
      }
    ],
    "imports": [
      "os",
      "subprocess"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "DOG/LEG/scripts/clean_ibex_filenames.py",
    "size": 4516,
    "lines": 105,
    "source": "import os\nimport re\n\nTARGET_DIR = \"/Users/gaia/resurrecting atlantis/IMPALA/IBEX/\"\n\n# This map defines the EXACT desired filename stem (without .mp4 extension)\n# based on the \"XX_CODE\" prefix.\nFILENAME_STEM_MAP = {\n    \"01_SH\": \"01_SH_OutOfLife\",\n    \"02_FL\": \"02_FL_FlashingLights\",\n    \"03_HT\": \"03_HT_HowToBreakOffAnEngagement\",\n    \"04_NM\": \"04_NM_Nevermore\",\n    \"05_BE\": \"05_BE_Bloodline\",\n    \"06_AT\": \"06_AT_ResurrectingAtlantis\",\n    \"07_DJ\": \"07_DJ_DJTurnMeUp\",\n    \"08_NS\": \"08_NS_NewlySingle\",\n    \"09_YH\": \"09_YH_YetHeard\",\n    \"10_MR\": \"10_MR_MagicRide\",\n    # No \"11_XX\" in the original list of videos or title maps.\n    \"12_RU\": \"12_RU_Reunion\",\n    \"13_HW\": \"13_HW_HowToWinMyHeart\",\n    \"14_HM\": \"14_HM_HotMinute\"\n}\n\ndef clean_filenames():\n    print(f\"Scanning directory: {TARGET_DIR}\")\n    renamed_count = 0\n    skipped_count = 0\n    unparsed_count = 0\n    processed_mp4_count = 0\n\n    # Get a list of items to iterate over to avoid issues if directory content changes mid-loop\n    try:\n        dir_items = os.listdir(TARGET_DIR)\n    except FileNotFoundError:\n        print(f\"Error: Directory not found: {TARGET_DIR}\")\n        return\n    except OSError as e:\n        print(f\"Error listing directory {TARGET_DIR}: {e}\")\n        return\n\n    for current_filename in dir_items:\n        old_path = os.path.join(TARGET_DIR, current_filename)\n\n        if not os.path.isfile(old_path) or not current_filename.lower().endswith('.mp4'):\n            # Optionally, log skipped non-target files/dirs if needed for debugging,\n            # but generally, we only care about MP4s.\n            # Example: if current_filename not in ['.DS_Store', 'SUNFLOWER', 'intros_with_headers']:\n            # print(f\"Skipping non-MP4 file or directory: {current_filename}\")\n            continue\n        \n        processed_mp4_count += 1\n        # Try to extract the \"XX_CODE\" prefix (e.g., \"01_SH\")\n        match = re.match(r\"^(\\d{2}_[A-Z]{2})\", current_filename)\n\n        if match:\n            prefix = match.group(1)\n            if prefix in FILENAME_STEM_MAP:\n                desired_stem = FILENAME_STEM_MAP[prefix]\n                new_filename = desired_stem + \".mp4\"\n\n                if new_filename != current_filename:\n                    new_path = os.path.join(TARGET_DIR, new_filename)\n\n                    if os.path.exists(new_path):\n                        try:\n                            if os.path.samefile(old_path, new_path):\n                                # print(f\"File '{current_filename}' is already effectively named '{new_filename}'. Skipping rename operation.\")\n                                skipped_count +=1\n                                continue # Already correctly named effectively\n                            else:\n                                print(f\"Warning: Target filename '{new_filename}' already exists and is a different file. Skipping rename of '{current_filename}'.\")\n                                skipped_count += 1\n                                continue\n                        except FileNotFoundError: # Should not happen if old_path exists, but good practice\n                            print(f\"Error checking samefile for '{old_path}' and '{new_path}'. Skipping.\")\n                            skipped_count += 1\n                            continue\n                    \n                    try:\n                        os.rename(old_path, new_path)\n                        print(f\"Successfully renamed '{current_filename}' to '{new_filename}'\")\n                        renamed_count += 1\n                    except OSError as e:\n                        print(f\"Error renaming '{current_filename}' to '{new_filename}': {e}\")\n                        skipped_count += 1\n                else:\n                    # Filename is already correct\n                    skipped_count += 1\n            else:\n                print(f\"Warning: Prefix '{prefix}' from '{current_filename}' not found in FILENAME_STEM_MAP. Skipping.\")\n                unparsed_count += 1\n        else:\n            print(f\"Warning: Could not parse prefix from '{current_filename}'. Skipping.\")\n            unparsed_count += 1\n            \n    print(\"\\n--- Renaming Summary ---\")\n    print(f\"MP4 files found and processed: {processed_mp4_count}\")\n    print(f\"Successfully renamed: {renamed_count} files\")\n    print(f\"Skipped (already correct, target exists, or error): {skipped_count} files\")\n    print(f\"Could not parse or map: {unparsed_count} files\")\n\nif __name__ == \"__main__\":\n    clean_filenames()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/IMPALA/IBEX/",
      "):\n            # Optionally, log skipped non-target files/dirs if needed for debugging,\n            # but generally, we only care about MP4s.\n            # Example: if current_filename not in ["
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "DOG/LEG/scripts/leg_generator_v1.py",
    "size": 32671,
    "lines": 619,
    "source": "#!/usr/bin/env python3\n\"\"\"\nLEG Generator - Processes a timeline JSON file (e.g., ORDERED-TIMELINE-FIXED.json).\n- For each poem in the timeline, generates image cards for its entries.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Outputs cards into poem-specific subdirectories under a main output folder.\n- Adapted from CLAPPER61PRIMEPLUS.\n\"\"\"\n\nimport os\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport textwrap\nimport glob\nimport re\nimport random\nimport unicodedata # Added for slugify\n\n# --- slugify function ---\ndef slugify(value, allow_unicode=False):\n    \"\"\"\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n    dashes to single dashes. Remove characters that aren't alphanumerics,\n    underscores, or hyphens. Convert to lowercase. Also strip leading and\n    trailing whitespace, dashes, and underscores.\n    \"\"\"\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n\n# --- get_flexible_glyph function ---\ndef get_flexible_glyph(data_string, glyph_dict, default_glyph=\"?\"):\n    \"\"\"Gets a glyph from a dictionary, trying exact match first, then flexible match (ignoring suffix in parentheses).\"\"\"\n    if not data_string:\n        return default_glyph\n\n    # 1. Try exact match first\n    glyph = glyph_dict.get(data_string)\n    if glyph:\n        return glyph.strip()\n\n    # 2. Try flexible match (strip suffix and whitespace)\n    #    e.g., \"Autonomous Syntagma\" from data vs \"Autonomous Syntagma (AS)\" in dict\n    data_string_base = re.sub(r'\\s*\\(.*\\)$', '', data_string).strip()\n    \n    for key, val in glyph_dict.items():\n        key_base = re.sub(r'\\s*\\(.*\\)$', '', key).strip()\n        if key_base.lower() == data_string_base.lower():\n            return val.strip()\n            \n    return default_glyph\n\n# Constants\nBASE_WIDTH = 1024\nGENOME_REPORT_HEIGHT = 60\nHEADER_HEIGHT = 200\nIMAGE_DISPLAY_HEIGHT = 576\nCARD_HEIGHT = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + IMAGE_DISPLAY_HEIGHT\n\n# Colors\nBLACK = (0, 0, 0)\nBLUE_BORDER = (50, 150, 255)\nWHITE = (255, 255, 255)\nAMBER = (255, 191, 0)\nLIGHT_GRAY = (200, 200, 200)\nSYNTAGMA_COLOR = (100, 200, 255)\nCINEOSIS_COLOR = (230, 150, 255)\nEKPHRASIS_COLOR = AMBER\nSTYLE_COLOR = (150, 255, 180)\nIMAGE_TYPE_COLOR = (173, 216, 230)\n\n# Glyph Definitions\nSYNTAGMA_GLYPHS = {\n    \"Chronological Syntagma (CS)\": \"\u2592\",\n    \"Crystal Syntagma (CS)\": \"\u2756\",\n    \"Descriptive Syntagma (DS)\": \"\u259e\",\n    \"Thematic Montage (TM)\": \"\u2588\",\n    \"Autonomous Syntagma (AS)\": \"\u2591\",\n    \"Flashback Syntagma (FS)\": \"\u2599\",\n}\nIMAGE_TYPE_GLYPHS = {\n    \"Action-Image\": \"\u25ba\",\n    \"Affection-Image\": \"\u2639\",\n    \"Crystal-Image\": \"\u25c8\",\n    \"Descriptive Image\": \"\u263c\",\n    \"Opsign\": \"\u25ce\",\n    \"Perception-Image\": \"\u2691\",\n    \"Recollection-Image\": \"\u2302\",\n    \"Sonsign\": \"\u266c\",\n    \"Thematic Montage\": \"\u2263\",\n}\nCINEOSIS_FUNCTION_GLYPHS = {\n    \"Aural-Echo Extension\": \"\u266a\",\n    \"Causal Motion Trigger\": \"\u2794\",\n    \"Emotion Relay\": \"\u2765\",\n    \"Event Pause Invocation\": \"\u275a\",\n    \"Memory Storage Retrieval\": \"\u21bb\",\n    \"Mood Environment Stabilizer\": \"\u25ff\",\n    \"Narrative Modifier\": \"\u2726\",\n    \"Subjective Frame Recalibration\": \"\u25a1\",\n    \"Temporal Reflection Loop\": \"\u2318\",\n}\n\n# Paths\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n# Path to the ordered timeline JSON file\nTIMELINE_PATH = \"/Users/gaia/resurrecting atlantis/DOG/ORDERED-TIMELINE-FIXED.json\"\n\n# Path to the symbolic genome data (assumed to be in the same directory as the 'scripts' folder, i.e., DOG/LEG/)\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), \"symbolic_genome_data.json\")\n\n# Base directory for all poem-specific output folders (will be created if it doesn't exist)\nOUTPUT_DIR_BASE = os.path.join(os.path.dirname(BASE_DIR), \"output_poems_sequenced\")\n\n# Base directory for finding source images (e.g., SH/, BL/, MR_IMAGES/)\n# This should point to the root of your 'resurrecting atlantis' project.\n# BASE_DIR is the directory of this script. os.path.join(BASE_DIR, \"..\", \"..\", \"..\") navigates up three levels.\n# For /Users/gaia/resurrecting atlantis/DOG/LEG/scripts, this becomes /Users/gaia/resurrecting atlantis/TIGER/\nBASE_IMAGE_DIR = os.path.normpath(os.path.join(BASE_DIR, \"..\", \"..\", \"..\", \"TIGER\"))\n\n# Note: Poem-specific output directories and the OUTPUT_DIR_BASE will be created in main() as needed.\n\ndef get_font(preferred_name, fallback_name, size, is_bold_preferred=False, is_bold_fallback=False):\n    font_name_to_try = preferred_name\n    if is_bold_preferred:\n        font_name_to_try = preferred_name + \" Bold\"\n    try:\n        return ImageFont.truetype(font_name_to_try, size)\n    except IOError:\n        try:\n            return ImageFont.truetype(font_name_to_try + \".ttf\", size)\n        except IOError:\n            try:\n                return ImageFont.truetype(font_name_to_try + \".otf\", size)\n            except IOError:\n                if preferred_name != fallback_name or is_bold_preferred != is_bold_fallback:\n                    print(f\"Warning: Preferred font '{font_name_to_try}' not found. Trying fallback '{fallback_name}'.\")\n                fallback_name_to_try = fallback_name\n                if is_bold_fallback:\n                    fallback_name_to_try = fallback_name + \" Bold\"\n                try:\n                    return ImageFont.truetype(fallback_name_to_try, size)\n                except IOError:\n                    try:\n                        return ImageFont.truetype(fallback_name_to_try + \".ttf\", size)\n                    except IOError:\n                        try:\n                            return ImageFont.truetype(fallback_name_to_try + \".otf\", size)\n                        except IOError:\n                            print(f\"Warning: Fallback font '{fallback_name_to_try}' also not found. Using default font for size {size}.\")\n                            try:\n                                return ImageFont.load_default(size=size)\n                            except TypeError:\n                                return ImageFont.load_default()\n\ndef draw_colored_text(draw, text, x, y, color, font, max_width=None, text_font_for_metrics=None):\n    if not text or text.strip() == '---' or text.strip() == '':\n        return y\n    metrics_font = text_font_for_metrics if text_font_for_metrics else font\n    words = text.split()\n    current_line_text = []\n    current_width = 0\n    start_x = x\n    line_height_bbox = metrics_font.getbbox(\"Ay\")\n    line_height = line_height_bbox[3] - line_height_bbox[1] + 4\n    for word in words:\n        word_bbox = metrics_font.getbbox(word)\n        word_width = word_bbox[2] - word_bbox[0]\n        space_bbox = metrics_font.getbbox(' ')\n        space_width = space_bbox[2] - space_bbox[0] if current_line_text else 0\n        if max_width is not None and current_line_text and current_width + word_width + space_width > max_width:\n            draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n            y += line_height\n            current_line_text = [word]\n            current_width = word_width\n        else:\n            current_line_text.append(word)\n            current_width += word_width + space_width\n    if current_line_text:\n        draw.text((start_x, y), ' '.join(current_line_text), font=font, fill=color)\n        y += line_height\n    return y\n\ndef get_image_path(entry):\n    relative_path = entry.get('image_path')\n    if not relative_path:\n        print(f\"Warning: 'image_path' missing in entry: {entry.get('id', 'Unknown ID')}\")\n        return None\n\n    # Assume relative_path from JSON is directly usable relative to BASE_IMAGE_DIR\n    # e.g., if image_path is \"SH_IMAGES/foo.png\", and BASE_IMAGE_DIR is \"/Users/gaia/resurrecting atlantis/\",\n    # it will look for \"/Users/gaia/resurrecting atlantis/SH_IMAGES/foo.png\".\n    adjusted_relative_path = relative_path\n    full_image_path = os.path.join(BASE_IMAGE_DIR, adjusted_relative_path)\n    \n    if os.path.exists(full_image_path):\n        return os.path.normpath(full_image_path)\n    else:\n        # Fallback if TIGER/MR path not found, try original MR/ path directly under BASE_IMAGE_DIR\n        if relative_path.startswith(\"MR/\"):\n            original_mr_path = os.path.join(BASE_IMAGE_DIR, relative_path)\n            if os.path.exists(original_mr_path):\n                return os.path.normpath(original_mr_path)\n        \n        # Broader fallback search using entry ID (adapted from original clapper61prime)\n        entry_id_val = entry.get('id', '')\n        if entry_id_val:\n            search_dirs = [BASE_IMAGE_DIR] # General search in base image dir\n            if relative_path.startswith(\"MR/\"):\n                 # If it was an MR path, also specifically check TIGER/MR and TIGER folders\n                search_dirs.insert(0, os.path.join(BASE_IMAGE_DIR, 'TIGER', 'MR'))\n                search_dirs.insert(1, os.path.join(BASE_IMAGE_DIR, 'TIGER'))\n\n            for search_dir_base in search_dirs:\n                if os.path.exists(search_dir_base):\n                    for root, _, files in os.walk(search_dir_base):\n                        for file_name in files:\n                            if file_name.startswith(entry_id_val + '__') and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n                                return os.path.normpath(os.path.join(root, file_name))\n                            # Check for exact filename match from image_path as well\n                            if os.path.basename(relative_path) == file_name:\n                                return os.path.normpath(os.path.join(root, file_name))\n        \n        print(f\"Error: Image not found for entry {entry.get('id', 'Unknown ID')}. Path attempted: {full_image_path}\")\n        return None\n\ndef load_genome_data_from_json(file_path):\n    genome_map_dict = {}\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            poems_data_list = json.load(f)\n        if not isinstance(poems_data_list, list):\n            print(f\"Error: Genome data in {file_path} is not a list. Found {type(poems_data_list)}.\")\n            return {}\n        for item in poems_data_list:\n            if isinstance(item, dict) and 'title' in item:\n                poem_name = item['title'].strip().lower()\n                genome_map_dict[poem_name] = {\n                    's_line': item.get('s_line', ''),\n                    'i_line': item.get('i_line', ''),\n                    'c_line': item.get('c_line', '')\n                }\n    except FileNotFoundError:\n        print(f\"Error: JSON data file not found at {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}.\")\n    return genome_map_dict\n\ndef get_original_char_pos(text, non_space_target_idx):\n    \"\"\"Given a text string and a 0-indexed target for non-space characters,\n    returns the original character index in the text string.\n    Returns -1 if non_space_target_idx is out of bounds.\n    \"\"\"\n    if non_space_target_idx < 0:\n        return -1\n    \n    current_non_space_count = 0\n    for original_idx, char in enumerate(text):\n        if char != ' ':\n            if current_non_space_count == non_space_target_idx:\n                return original_idx\n            current_non_space_count += 1\n    \n    # Target index not found (e.g., non_space_target_idx >= total non-space chars in text)\n    return -1\n\ndef get_non_space_char_index(text, original_char_pos):\n    count = -1\n    if original_char_pos >= len(text):\n        return -1\n    for i in range(original_char_pos + 1):\n        if text[i] != ' ':\n            count += 1\n    return count\n\ndef render_card(entry, output_path, fonts, genome_map):\n    img = Image.new('RGB', (BASE_WIDTH + 8, CARD_HEIGHT + 8), BLUE_BORDER)\n    inner = Image.new('RGB', (BASE_WIDTH, CARD_HEIGHT), BLACK)\n    img.paste(inner, (4, 4))\n    draw = ImageDraw.Draw(img)\n\n    header_font = fonts['header']\n    text_font = fonts['text']\n    text_font_bold = fonts['text_bold']\n    genome_font_small = fonts['genome_small']\n    genome_font_tiny = fonts['genome_tiny']\n\n    narrow_label_area_width = header_font.getbbox(\"ID\")[2] + 10\n    value_start_x = 10 + narrow_label_area_width\n    current_y_offset = 4\n    genome_section_y_on_img = current_y_offset\n    PLACEHOLDER_CHAR = '.'\n    PLACEHOLDER_COLOR = (70, 70, 70)\n    genome_content_x1_on_img = 10\n    poem_title_from_timeline = entry.get('poem', '').strip().lower()\n    genome_data_entry = genome_map.get(poem_title_from_timeline)\n    i_line_data_full = genome_data_entry.get('i_line', 'I_LINE_N/A') if genome_data_entry else 'I_LINE_N/A'\n    s_line_data_full = genome_data_entry.get('s_line', 'S_LINE_N/A') if genome_data_entry else 'S_LINE_N/A'\n    c_line_data_full = genome_data_entry.get('c_line', 'C_LINE_N/A') if genome_data_entry else 'C_LINE_N/A'\n    genome_lines_to_render = [i_line_data_full, s_line_data_full, c_line_data_full]\n    frame_position = entry.get('frameNumber', 0)\n    highlight_index = int(frame_position) - 1 if isinstance(frame_position, (int, float)) and frame_position > 0 else -1\n    \n    genome_line_height_small_bbox = genome_font_small.getbbox(\"Ay\")\n    actual_line_height_small = (genome_line_height_small_bbox[3] - genome_line_height_small_bbox[1]) + 2\n    genome_line_height_tiny_bbox = genome_font_tiny.getbbox(\"Ay\")\n    actual_line_height_tiny = (genome_line_height_tiny_bbox[3] - genome_line_height_tiny_bbox[1]) + 1\n    chosen_genome_font = genome_font_small\n    chosen_genome_line_height = actual_line_height_small\n    if len(genome_lines_to_render) * chosen_genome_line_height > GENOME_REPORT_HEIGHT - 8:\n        chosen_genome_font = genome_font_tiny\n        chosen_genome_line_height = actual_line_height_tiny\n        if len(genome_lines_to_render) * chosen_genome_line_height > GENOME_REPORT_HEIGHT - 8:\n            print(f\"Warning: Genome text for '{entry.get('poem', 'Unknown Poem')}' (ID: {entry.get('id')}) too tall.\")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    if genome_y_start < genome_section_y_on_img + 4: genome_y_start = genome_section_y_on_img + 4\n\n    max_render_width = BASE_WIDTH - 10 # Allow a small margin on the right\n    avg_char_width = chosen_genome_font.getlength(\"X\") if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(\"X\")[2] - chosen_genome_font.getbbox(\"X\")[0]\n    if avg_char_width <= 0: avg_char_width = 1\n    placeholder_char_width = chosen_genome_font.getlength(PLACEHOLDER_CHAR) if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[2] - chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[0]\n    if placeholder_char_width <= 0: placeholder_char_width = 1\n\n    for line_idx, original_line_text in enumerate(genome_lines_to_render):\n        current_x = genome_content_x1_on_img\n        chars_that_fit_total_approx = int(max_render_width // avg_char_width)\n        if chars_that_fit_total_approx <= 0: chars_that_fit_total_approx = 1 # Avoid division by zero or negative\n        target_highlight_offset_in_view = chars_that_fit_total_approx // 4\n\n        display_start_index = 0\n        scrolled_from_left = False\n\n        if highlight_index >= 0: # Ensure we have a valid highlight_index\n            if len(original_line_text) <= chars_that_fit_total_approx:\n                # Line fits entirely, no scrolling needed\n                display_start_index = 0\n                scrolled_from_left = False\n            else:\n                # Line is longer than available space, scrolling is needed\n                actual_pos_of_highlight_char_in_string = get_original_char_pos(original_line_text, highlight_index)\n\n                if actual_pos_of_highlight_char_in_string != -1:\n                    scrolled_from_left = True\n                    # Default start: try to position the highlighted character (at actual_pos_of_highlight_char_in_string)\n                    # such that it appears target_highlight_offset_in_view characters from the left of the display window.\n                    # This means the display window should start target_highlight_offset_in_view characters *before* actual_pos_of_highlight_char_in_string.\n                    # Note: target_highlight_offset_in_view is a count of characters, not a direct string index offset.\n                    # This subtraction is an approximation but works with the pinning logic below.\n                    tentative_display_start_index = max(0, actual_pos_of_highlight_char_in_string - target_highlight_offset_in_view)\n                    \n                    # Adjust if scrolling near the end of the line\n                    # Ensure the window doesn't go past the end of the string\n                    if tentative_display_start_index + chars_that_fit_total_approx > len(original_line_text):\n                        display_start_index = max(0, len(original_line_text) - chars_that_fit_total_approx)\n                    else:\n                        display_start_index = tentative_display_start_index\n                else:\n                    # Highlight character is NOT found (highlight_index is out of bounds for this line's non-space chars)\n                    # Check if the line, despite no highlight, is too wide in pixels and needs to scroll to the end.\n                    actual_pixel_width_of_line = chosen_genome_font.getlength(original_line_text)\n                    # max_render_width is already defined earlier (BASE_WIDTH - 10)\n                    if actual_pixel_width_of_line <= max_render_width:\n                        display_start_index = 0 # Line fits, show from beginning\n                        scrolled_from_left = False\n                    else:\n                        # Line is too wide in pixels, scroll to show the end\n                        display_start_index = max(0, len(original_line_text) - chars_that_fit_total_approx)\n                        scrolled_from_left = display_start_index > 0\n        else: # No valid highlight_index (e.g. -1 from frame_position calculation), just show from the beginning\n            display_start_index = 0\n            scrolled_from_left = False\n            \n        # Prefix display has been removed to maximize space for genome characters.\n        # The scrolled_from_left flag is still determined above to correctly set display_start_index.\n        for idx_in_original in range(display_start_index, len(original_line_text)):\n            char_to_draw = original_line_text[idx_in_original]\n            char_width = chosen_genome_font.getlength(char_to_draw) if hasattr(chosen_genome_font, 'getlength') else chosen_genome_font.getbbox(char_to_draw)[2] - chosen_genome_font.getbbox(char_to_draw)[0]\n            if char_width <=0: char_width = avg_char_width # Fallback if char_width is bad\n            \n            # Check if the current character fits\n            if current_x + char_width > max_render_width:\n                break # Stop drawing this line if the character doesn't fit\n\n            # Determine default color based on line index\n            if line_idx == 0: # I-line (corresponds to Image Type)\n                default_char_color = IMAGE_TYPE_COLOR\n            elif line_idx == 1: # S-line (corresponds to Syntagma Type)\n                default_char_color = SYNTAGMA_COLOR\n            elif line_idx == 2: # C-line (corresponds to Cineosis Function)\n                default_char_color = CINEOSIS_COLOR\n            else: # Fallback, though should not happen with 3 lines\n                default_char_color = LIGHT_GRAY # Fallback to a neutral color\n\n            char_color = default_char_color\n            \n            # Highlighted character is white, overriding line color\n            if char_to_draw != ' ': # Only check for highlight if not a space\n                current_char_overall_non_space_idx = get_non_space_char_index(original_line_text, idx_in_original)\n                if current_char_overall_non_space_idx == highlight_index:\n                    char_color = AMBER\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n        drawn_width = current_x - genome_content_x1_on_img\n        remaining_line_width = max_render_width - drawn_width\n        if remaining_line_width > 0 and placeholder_char_width > 0:\n            num_padding_chars = int(remaining_line_width // placeholder_char_width)\n            if num_padding_chars > 0:\n                draw.text((current_x, genome_y_start), PLACEHOLDER_CHAR * num_padding_chars, font=chosen_genome_font, fill=PLACEHOLDER_COLOR)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 4\n    header_content_y = current_y_offset + 4\n    top_row_internal_height = 30\n    id_label_text = \"ID:\"\n    id_value_text = entry.get('id', '---')\n    time_text = f\"TIME: {entry.get('timestamp', '--:--:--')}\"\n    frame_position_val = entry.get('frameNumber', 0)\n    frame_total_val = entry.get('totalFrames', 0)\n    frame_text = f\"FRAME: ({frame_position_val}/{frame_total_val})\"\n    poem_name = entry.get('poem', '---')\n    poem_text = f\"POEM: {poem_name[:25]}\"\n    draw.text((10, header_content_y), id_label_text, font=header_font, fill=WHITE)\n    draw.text((value_start_x, header_content_y), id_value_text, font=header_font, fill=WHITE)\n    draw.text((280, header_content_y), poem_text, font=header_font, fill=WHITE)\n    draw.text((650, header_content_y), time_text, font=header_font, fill=WHITE)\n    draw.text((850, header_content_y), frame_text, font=header_font, fill=WHITE)\n    current_y_offset += top_row_internal_height\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8\n    initial_content_row_y = current_y_offset\n    image_type_str_raw = entry.get('imageType', '---')\n    image_glyph = get_flexible_glyph(image_type_str_raw, IMAGE_TYPE_GLYPHS)\n    line_content = entry.get('content', '---')\n    glyph_column_x = 10\n    draw.text((glyph_column_x, initial_content_row_y), image_glyph, font=chosen_genome_font, fill=AMBER)\n    draw.text((value_start_x, initial_content_row_y), image_type_str_raw, font=text_font, fill=IMAGE_TYPE_COLOR)\n    image_text_width_bbox = text_font.getbbox(image_type_str_raw)\n    image_text_width = image_text_width_bbox[2] - image_text_width_bbox[0]\n    line_field_x_start = max(int(BASE_WIDTH * 0.35), min(value_start_x + image_text_width + 30, int(BASE_WIDTH * 0.50)))\n    line_prefix = \"LINE: \\\"\"\n    draw.text((line_field_x_start, initial_content_row_y), line_prefix, font=header_font, fill=WHITE)\n    line_prefix_width_bbox = header_font.getbbox(line_prefix)\n    line_prefix_width = line_prefix_width_bbox[2] - line_prefix_width_bbox[0]\n    line_content_actual_x_start = line_field_x_start + line_prefix_width\n    line_available_width = BASE_WIDTH - line_content_actual_x_start - 10\n    y_after_line = draw_colored_text(draw, line_content + \"\\\"\", line_content_actual_x_start, initial_content_row_y, AMBER, text_font_bold, line_available_width, text_font_bold)\n    ONE_TEXT_LINE_HEIGHT = (text_font.getbbox(\"Ay\")[3] - text_font.getbbox(\"Ay\")[1] + 4) if hasattr(text_font, 'getbbox') else 16\n    current_y_offset = max(y_after_line, initial_content_row_y + ONE_TEXT_LINE_HEIGHT) if line_content.strip() and line_content.strip() != '---' else initial_content_row_y + ONE_TEXT_LINE_HEIGHT\n    current_y_offset += 4\n    divider_start_x = glyph_column_x - 4 if glyph_column_x > 4 else 6\n    draw.line([(divider_start_x, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 8\n    prompt_value_max_width = BASE_WIDTH - value_start_x - 10\n    syntagma_type_str_raw = entry.get('syntagmaType', '---')\n    syntagma_glyph = get_flexible_glyph(syntagma_type_str_raw, SYNTAGMA_GLYPHS)\n    draw.text((glyph_column_x, current_y_offset), syntagma_glyph, font=chosen_genome_font, fill=AMBER)\n    current_y_offset = draw_colored_text(draw, syntagma_type_str_raw, value_start_x, current_y_offset, SYNTAGMA_COLOR, text_font, prompt_value_max_width, text_font)\n    cineosis_func_str_raw = entry.get('cineosisFunction', '---')\n    cineosis_glyph = get_flexible_glyph(cineosis_func_str_raw, CINEOSIS_FUNCTION_GLYPHS)\n    draw.text((glyph_column_x, current_y_offset), cineosis_glyph, font=chosen_genome_font, fill=AMBER)\n    current_y_offset = draw_colored_text(draw, cineosis_func_str_raw, value_start_x, current_y_offset, CINEOSIS_COLOR, text_font, prompt_value_max_width, text_font)\n    ekphrasis_str = entry.get('operativeEkphrasis', '---')\n    current_y_offset = draw_colored_text(draw, ekphrasis_str, value_start_x, current_y_offset, EKPHRASIS_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n    style_str = entry.get('styleConditioning', '---')\n    current_y_offset = draw_colored_text(draw, style_str, value_start_x, current_y_offset, STYLE_COLOR, text_font_bold, prompt_value_max_width, text_font_bold)\n    image_placement_y = GENOME_REPORT_HEIGHT + HEADER_HEIGHT + 4\n    image_path = get_image_path(entry)\n    if image_path:\n        try:\n            card_image = Image.open(image_path).convert(\"RGB\")\n            card_image = card_image.resize((BASE_WIDTH, IMAGE_DISPLAY_HEIGHT))\n            img.paste(card_image, (4, image_placement_y))\n        except Exception as e:\n            print(f\"Error loading image {image_path} for entry {entry.get('id')}: {e}\")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"Image Error\", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), \"No Image Available\", font=header_font, fill=BLACK)\n    img.save(output_path)\n\ndef calculate_frame_counts(timeline_data):\n    poem_frames = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        if poem_name:\n            poem_frames[poem_name] = poem_frames.get(poem_name, 0) + 1\n    processed_data = []\n    current_poem_entry_count = {}\n    for entry in timeline_data:\n        poem_name = entry.get('poem')\n        new_entry = entry.copy()\n        if poem_name:\n            current_poem_entry_count[poem_name] = current_poem_entry_count.get(poem_name, 0) + 1\n            new_entry['frameNumber'] = entry.get('frameNumber', current_poem_entry_count[poem_name]) # Use existing or assign\n            new_entry['totalFrames'] = entry.get('totalFrames', poem_frames.get(poem_name, 0))\n        else:\n            new_entry['frameNumber'] = entry.get('frameNumber', 0)\n            new_entry['totalFrames'] = entry.get('totalFrames', 0)\n        processed_data.append(new_entry)\n    return processed_data\n\ndef main():\n    print(f\"Starting LEG Generator to process all poems from: {TIMELINE_PATH}\")\n    print(f\"Base output directory for poem sequences: {OUTPUT_DIR_BASE}\")\n    print(f\"Symbolic genome data from: {SYMBOLIC_GENOME_DATA_PATH}\")\n    print(f\"Base image directory: {BASE_IMAGE_DIR}\")\n\n    genome_map = load_genome_data_from_json(SYMBOLIC_GENOME_DATA_PATH)\n    if not genome_map:\n        print(f\"Warning: No genome data loaded from {SYMBOLIC_GENOME_DATA_PATH}. Proceeding without genome data on cards.\")\n        # genome_map will be an empty dict or None, render_card should handle this gracefully.\n\n    fonts = {\n        'header': get_font(\"Menlo\", \"Courier New\", 16),\n        'text':   get_font(\"Menlo\", \"Courier New\", 16),\n        'text_bold': get_font(\"Menlo\", \"Courier New\", 16, is_bold_preferred=False, is_bold_fallback=False),\n        'genome_small': get_font(\"Menlo\", \"Courier New\", 12),\n        'genome_tiny':  get_font(\"Menlo\", \"Courier New\", 10)\n    }\n\n    try:\n        with open(TIMELINE_PATH, 'r', encoding='utf-8') as f:\n            timeline_data_raw = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Timeline file not found at {TIMELINE_PATH}\"); return\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {TIMELINE_PATH}\"); return\n\n    # calculate_frame_counts might add per-poem frame numbers if enhanced.\n    # For now, assume it's a pass-through or provides data main() can use.\n    all_entries_processed = calculate_frame_counts(timeline_data_raw)\n\n    if not all_entries_processed:\n        print(\"No timeline entries found or processed. Exiting.\")\n        return\n\n    total_cards_rendered_overall = 0\n    poems_processed_count = 0\n    \n    # Group entries by poem first to process them together\n    entries_by_poem = {}\n    for entry in all_entries_processed:\n        poem_name = entry.get('poem', 'UnknownPoem').strip()\n        if not poem_name: poem_name = 'UnknownPoem' # Ensure there's always a poem name\n        entries_by_poem.setdefault(poem_name, []).append(entry)\n\n    # Create the base output directory if it doesn't exist\n    os.makedirs(OUTPUT_DIR_BASE, exist_ok=True)\n\n    for poem_name, entries_for_poem in entries_by_poem.items():\n        poems_processed_count += 1\n        if poem_name != \"Bloodline\":\n            print(f\"Skipping poem: '{poem_name}' (not Bloodline)\") # Optional: for verbosity\n            continue\n        poem_slug = slugify(poem_name)\n        current_poem_output_dir = os.path.join(OUTPUT_DIR_BASE, poem_slug)\n        os.makedirs(current_poem_output_dir, exist_ok=True)\n\n        print(f\"\\nProcessing poem: '{poem_name}' ({len(entries_for_poem)} entries) -> Outputting to: {current_poem_output_dir}\")\n        \n        def get_sort_key(e):\n            pfn = e.get('poem_frame_number') # Per-poem frame number from calculate_frame_counts\n            fn = e.get('frameNumber') # Original frame number\n            if isinstance(pfn, (int, float)): return pfn\n            if isinstance(fn, (int, float)): return fn\n            if isinstance(fn, str) and fn.isdigit(): return int(fn)\n            return float('inf') # Entries with no valid frame number go last\n\n        entries_for_poem.sort(key=get_sort_key)\n        if entries_for_poem: # If there are entries for this poem\n            pass # Test mode logic was previously here\n            # entries_for_poem = entries_for_poem[:1] # Keep only the first one for the test run\n\n        cards_rendered_for_poem = 0\n        for i, entry_data in enumerate(entries_for_poem):\n            entry_id = entry_data.get('id', f'{poem_slug}_unknown-id_{i+1}')\n            \n            frame_num_val = entry_data.get('poem_frame_number')\n            if not isinstance(frame_num_val, (int, float)):\n                frame_num_val = entry_data.get('frameNumber')\n                if not (isinstance(frame_num_val, (int, float)) or (isinstance(frame_num_val, str) and frame_num_val.isdigit())):\n                    frame_num_val = i + 1 # Fallback to simple enumeration\n            \n            frame_num_str = str(int(frame_num_val) if isinstance(frame_num_val, (int,float)) else frame_num_val).zfill(4)\n\n            output_filename = f\"frame_{frame_num_str}.png\"\n            output_path = os.path.join(current_poem_output_dir, output_filename)\n            \n            print(f\"  Rendering card for: {entry_id} (Frame: {frame_num_str}) -> {output_filename}\")\n            try:\n                render_card(entry_data, output_path, fonts, genome_map)\n                cards_rendered_for_poem += 1\n            except Exception as e:\n                print(f\"    ERROR rendering card for {entry_id} (Path: {output_path}): {e}\")\n        \n        print(f\"  Finished poem '{poem_name}'. Cards rendered for this poem: {cards_rendered_for_poem}\")\n        total_cards_rendered_overall += cards_rendered_for_poem\n\n    print(f\"\\n--- Script Finished ---\")\n    print(f\"Total poems processed: {poems_processed_count}\")\n    print(f\"Total cards rendered overall: {total_cards_rendered_overall}\")\n    print(f\"Output saved in subdirectories under: {OUTPUT_DIR_BASE}\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/DOG/ORDERED-TIMELINE-FIXED.json",
      "symbolic_genome_data.json",
      "SH_IMAGES/foo.png",
      "/Users/gaia/resurrecting atlantis/SH_IMAGES/foo.png",
      "frame_{frame_num_str}.png",
      "\n    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n    Convert to ASCII if ",
      "/Users/gaia/resurrecting atlantis/DOG/ORDERED-TIMELINE-FIXED.json",
      " folder, i.e., DOG/LEG/)\nSYMBOLIC_GENOME_DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), ",
      ")\n\n# Base directory for finding source images (e.g., SH/, BL/, MR_IMAGES/)\n# This should point to the root of your ",
      ") navigates up three levels.\n# For /Users/gaia/resurrecting atlantis/DOG/LEG/scripts, this becomes /Users/gaia/resurrecting atlantis/TIGER/\nBASE_IMAGE_DIR = os.path.normpath(os.path.join(BASE_DIR, ",
      "SH_IMAGES/foo.png",
      "/Users/gaia/resurrecting atlantis/",
      "/Users/gaia/resurrecting atlantis/SH_IMAGES/foo.png",
      "):\n                 # If it was an MR path, also specifically check TIGER/MR and TIGER folders\n                search_dirs.insert(0, os.path.join(BASE_IMAGE_DIR, ",
      "I_LINE_N/A",
      "I_LINE_N/A",
      "S_LINE_N/A",
      "S_LINE_N/A",
      "C_LINE_N/A",
      "C_LINE_N/A",
      ")\n\n    current_block_height = len(genome_lines_to_render) * chosen_genome_line_height\n    text_lines_y_start_offset = (GENOME_REPORT_HEIGHT - current_block_height) // 2\n    genome_y_start = genome_section_y_on_img + text_lines_y_start_offset\n    if genome_y_start < genome_section_y_on_img + 4: genome_y_start = genome_section_y_on_img + 4\n\n    max_render_width = BASE_WIDTH - 10 # Allow a small margin on the right\n    avg_char_width = chosen_genome_font.getlength(",
      ") else chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[2] - chosen_genome_font.getbbox(PLACEHOLDER_CHAR)[0]\n    if placeholder_char_width <= 0: placeholder_char_width = 1\n\n    for line_idx, original_line_text in enumerate(genome_lines_to_render):\n        current_x = genome_content_x1_on_img\n        chars_that_fit_total_approx = int(max_render_width // avg_char_width)\n        if chars_that_fit_total_approx <= 0: chars_that_fit_total_approx = 1 # Avoid division by zero or negative\n        target_highlight_offset_in_view = chars_that_fit_total_approx // 4\n\n        display_start_index = 0\n        scrolled_from_left = False\n\n        if highlight_index >= 0: # Ensure we have a valid highlight_index\n            if len(original_line_text) <= chars_that_fit_total_approx:\n                # Line fits entirely, no scrolling needed\n                display_start_index = 0\n                scrolled_from_left = False\n            else:\n                # Line is longer than available space, scrolling is needed\n                actual_pos_of_highlight_char_in_string = get_original_char_pos(original_line_text, highlight_index)\n\n                if actual_pos_of_highlight_char_in_string != -1:\n                    scrolled_from_left = True\n                    # Default start: try to position the highlighted character (at actual_pos_of_highlight_char_in_string)\n                    # such that it appears target_highlight_offset_in_view characters from the left of the display window.\n                    # This means the display window should start target_highlight_offset_in_view characters *before* actual_pos_of_highlight_char_in_string.\n                    # Note: target_highlight_offset_in_view is a count of characters, not a direct string index offset.\n                    # This subtraction is an approximation but works with the pinning logic below.\n                    tentative_display_start_index = max(0, actual_pos_of_highlight_char_in_string - target_highlight_offset_in_view)\n                    \n                    # Adjust if scrolling near the end of the line\n                    # Ensure the window doesn",
      ": # Only check for highlight if not a space\n                current_char_overall_non_space_idx = get_non_space_char_index(original_line_text, idx_in_original)\n                if current_char_overall_non_space_idx == highlight_index:\n                    char_color = AMBER\n            \n            draw.text((current_x, genome_y_start), char_to_draw, font=chosen_genome_font, fill=char_color)\n            current_x += char_width\n        drawn_width = current_x - genome_content_x1_on_img\n        remaining_line_width = max_render_width - drawn_width\n        if remaining_line_width > 0 and placeholder_char_width > 0:\n            num_padding_chars = int(remaining_line_width // placeholder_char_width)\n            if num_padding_chars > 0:\n                draw.text((current_x, genome_y_start), PLACEHOLDER_CHAR * num_padding_chars, font=chosen_genome_font, fill=PLACEHOLDER_COLOR)\n        genome_y_start += chosen_genome_line_height\n    \n    current_y_offset += GENOME_REPORT_HEIGHT\n    draw.line([(4, current_y_offset), (BASE_WIDTH + 4, current_y_offset)], fill=WHITE, width=1)\n    current_y_offset += 4\n    header_content_y = current_y_offset + 4\n    top_row_internal_height = 30\n    id_label_text = ",
      "FRAME: ({frame_position_val}/{frame_total_val})",
      ")\n            draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n            draw.text((BASE_WIDTH // 2 - 50, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), ",
      ", font=header_font, fill=BLACK)\n    else:\n        draw.rectangle([(4, image_placement_y), (BASE_WIDTH + 3, image_placement_y + IMAGE_DISPLAY_HEIGHT -1)], fill=LIGHT_GRAY)\n        draw.text((BASE_WIDTH // 2 - 70, image_placement_y + IMAGE_DISPLAY_HEIGHT // 2 - 8), "
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "json",
      "PIL",
      "textwrap",
      "glob",
      "re",
      "random",
      "unicodedata"
    ],
    "generates": [
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": "LEG Generator - Processes a timeline JSON file (e.g., ORDERED-TIMELINE-FIXED.json).\n- For each poem in the timeline, generates image cards for its entries.\n- Displays genome (S, I, C lines) using Menlo font, loaded from symbolic_genome_data.json.\n- Genome data is dynamically matched to the poem of the current timeline entry.\n- Outputs cards into poem-specific subdirectories under a main output folder.\n- Adapted from CLAPPER61PRIMEPLUS."
  },
  {
    "path": "DOG/LEG/scripts/add_audio_to_videos.py",
    "size": 12779,
    "lines": 214,
    "source": "import os\nimport subprocess\nimport json\n\nSOURCE_VIDEO_DIR = \"/Users/gaia/resurrecting atlantis/DOG/LEG/fused_videos_output/\"\nTARGET_AUDIO_VIDEO_DIR = \"/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS/\" # New base output directory\n\n# AUDIO_START_DELAYS defines the absolute start time of the main audio track\n# from the beginning of the fused video (intro included).\nAUDIO_START_DELAYS = [0, 1, 2, 3, 4, 5]  # Seconds\nAUDIO_SAMPLE_RATE = 44100 # For generated silence/drone\nAUDIO_CHANNEL_LAYOUT = \"stereo\" # For generated silence/drone\n\n# Mapping of base video filenames to their corresponding audio file paths\nVIDEO_TO_AUDIO_MAP = {\n    \"how-to-break-off-an-engagement.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/How to break off an engagement_The New Testament.mp3\",\n    \"bloodline.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/Bloodline_New Gospel of Creation.mp3\",\n    \"dj-turn-me-up.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/DJ_turn_me_up_Whispers of a Forgotten Love.mp3\",\n    \"flashing-lights.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/flashing-lights-potent-Digital Testament remix v1.2.1.2.mp3\",\n    \"hot-minute.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/Hot Minute_The New Covenant remix v2.mp3\",\n    \"how-to-win-my-heart.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/How To Win My Heart_Echoes of Longing remix v2.2.1.mp3\",\n    \"magic-ride.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/Magic ride_Awakening of the New Gospel remix v2.mp3\",\n    \"nevermore.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/nevermore-Reclaimed Truths remix v1.mp3\",\n    \"newly-single.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/Newly Single_The New Awakening.mp3\",\n    \"out-of-life.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/Out of Life_Echoes of Departure remix v1_GOOD.mp3\",\n    \"resurrecting-atlantis.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/RESURRECTING_ATLANTIS_Resonance of Futures.mp3\",\n    \"reunion.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/Reunion_The New Gospel.mp3\",\n    \"yet-heard.mp4\": \"/Users/gaia/resurrecting atlantis/MANTA/audio/Yet, Heard_The New Covenant.mp3\"\n}\n\ndef get_media_duration(filepath):\n    \"\"\"Gets the duration of a media file using ffprobe.\"\"\"\n    if filepath is None:\n        # This case should ideally not be reached if ffprobe check in main is done first.\n        # However, as a safeguard or if called directly with None:\n        print(\"  Error: get_media_duration called with None filepath.\")\n        return 0.0 # Or raise an error\n\n    ffprobe_command = [\n        \"ffprobe\",\n        \"-v\", \"quiet\",\n        \"-print_format\", \"json\",\n        \"-show_format\",\n        \"-show_streams\",\n        filepath\n    ]\n    try:\n        result = subprocess.run(ffprobe_command, capture_output=True, text=True, check=True)\n        metadata = json.loads(result.stdout)\n        if 'format' in metadata and 'duration' in metadata['format']:\n            return float(metadata['format']['duration'])\n        # Fallback for some files, check streams (audio/video typically have duration in their stream info)\n        if 'streams' in metadata:\n            for stream in metadata['streams']:\n                if stream.get('codec_type') in ['video', 'audio'] and 'duration' in stream:\n                    return float(stream['duration'])\n        print(f\"  Warning: Could not determine duration from ffprobe for {filepath}\")\n        return 0.0\n    except subprocess.CalledProcessError as e:\n        print(f\"  Error running ffprobe for {filepath}: {e.stderr}\")\n        return 0.0\n    # FileNotFoundError for ffprobe itself is handled in main's pre-check\n    except json.JSONDecodeError:\n        print(f\"  Error decoding ffprobe JSON output for {filepath}.\")\n        return 0.0\n\ndef main():\n    print(f\"Starting to add audio to videos.\")\n    print(f\"Source video directory: {SOURCE_VIDEO_DIR}\")\n    print(f\"Target base directory for audio delay versions: {TARGET_AUDIO_VIDEO_DIR}\")\n\n    # Check for ffprobe availability before starting\n    try:\n        subprocess.run([\"ffprobe\", \"-version\"], capture_output=True, text=True, check=True)\n        print(\"ffprobe found.\")\n    except FileNotFoundError:\n        print(\"  CRITICAL ERROR: ffprobe command not found. Please ensure ffmpeg (and ffprobe) is installed and in your PATH.\")\n        print(\"  Halting script.\")\n        return\n    except subprocess.CalledProcessError as e:\n        print(f\"  CRITICAL ERROR: ffprobe -version failed: {e.stderr}. Please ensure ffprobe is working correctly.\")\n        print(\"  Halting script.\")\n        return\n\n    if not os.path.exists(TARGET_AUDIO_VIDEO_DIR):\n        os.makedirs(TARGET_AUDIO_VIDEO_DIR)\n        print(f\"Created target base directory: {TARGET_AUDIO_VIDEO_DIR}\")\n\n    total_files_to_process = len(VIDEO_TO_AUDIO_MAP) * len(AUDIO_START_DELAYS)\n    success_count = 0\n    fail_count = 0\n    processed_count = 0\n\n    for video_filename_ext, audio_filepath in VIDEO_TO_AUDIO_MAP.items():\n        base_video_filename = video_filename_ext.rsplit('.', 1)[0]\n        source_video_path = os.path.join(SOURCE_VIDEO_DIR, video_filename_ext)\n\n        if not os.path.exists(source_video_path):\n            print(f\"\\nError: Source video not found: {source_video_path}. Skipping all versions for this video.\")\n            fail_count += len(AUDIO_START_DELAYS)\n            processed_count += len(AUDIO_START_DELAYS)\n            continue\n        \n        if not os.path.exists(audio_filepath):\n            print(f\"\\nError: Audio file not found: {audio_filepath}. Skipping all versions for {base_video_filename}.\")\n            fail_count += len(AUDIO_START_DELAYS)\n            processed_count += len(AUDIO_START_DELAYS)\n            continue\n\n        video_duration = get_media_duration(source_video_path)\n        main_audio_duration = get_media_duration(audio_filepath)\n\n        if video_duration == 0.0 or main_audio_duration == 0.0:\n            print(f\"\\nError: Could not get valid duration for {source_video_path} or {audio_filepath}. Skipping all versions for this video.\")\n            fail_count += len(AUDIO_START_DELAYS)\n            processed_count += len(AUDIO_START_DELAYS)\n            continue\n\n        for current_delay in AUDIO_START_DELAYS:\n            processed_count += 1\n            target_filename = f\"{base_video_filename}_delay_{current_delay}s.{video_filename_ext.rsplit('.', 1)[-1]}\"\n            target_video_path = os.path.join(TARGET_AUDIO_VIDEO_DIR, target_filename)\n\n            print(f\"\\n({processed_count}/{total_files_to_process}) Processing: {video_filename_ext} with audio delay: {current_delay}s\")\n            print(f\"  Source video: {source_video_path} (Duration: {video_duration:.2f}s)\")\n            print(f\"  Main audio: {audio_filepath} (Duration: {main_audio_duration:.2f}s)\")\n            print(f\"  Output video: {target_video_path}\")\n\n            fc_parts = []\n            # current_delay is the absolute start time for the main audio track.\n            # If current_delay is 0, main audio starts at t=0.\n            # If current_delay > 0, initial silence of 'current_delay' seconds is prepended.\n\n            audio_input_for_main_track = \"[1:a]\" # Main audio from input file 1 (ffmpeg input index 1, audio stream 'a')\n            current_audio_stream_label_post_delay = \"\"\n            current_constructed_audio_length = 0\n\n            if current_delay == 0:\n                # Main audio starts at t=0. Use acopy to give it a label for consistent filter graph.\n                fc_parts.append(f\"{audio_input_for_main_track}acopy[audio_post_delay];\")\n                current_audio_stream_label_post_delay = \"[audio_post_delay]\"\n                current_constructed_audio_length = main_audio_duration\n                print(f\"    Main audio starts at t=0s. Length before drone: {current_constructed_audio_length:.2f}s\")\n            else: # current_delay > 0\n                # Generate initial silence for the duration of current_delay\n                fc_parts.append(f\"anullsrc=channel_layout={AUDIO_CHANNEL_LAYOUT}:sample_rate={AUDIO_SAMPLE_RATE}:d={current_delay:.3f}[initial_silence];\")\n                # Concatenate this silence with the main audio track\n                fc_parts.append(f\"[initial_silence]{audio_input_for_main_track}concat=n=2:v=0:a=1[audio_post_delay];\")\n                current_audio_stream_label_post_delay = \"[audio_post_delay]\"\n                current_constructed_audio_length = current_delay + main_audio_duration\n                print(f\"    Main audio starts at t={current_delay}s (after {current_delay:.2f}s silence). Length before drone: {current_constructed_audio_length:.2f}s\")\n            \n            # Append drone if necessary to fill up to video_duration\n            drone_at_end_duration = video_duration - current_constructed_audio_length\n            final_audio_map_label = current_audio_stream_label_post_delay # Default if no drone needed or audio is already longer\n\n            if drone_at_end_duration > 0.01: # Drone needed at the end\n                print(f\"    Audio up to main track end ({current_constructed_audio_length:.2f}s) is shorter than video ({video_duration:.2f}s). Appending {drone_at_end_duration:.2f}s drone.\")\n                fc_parts.append(f\"aevalsrc=sin(100*2*PI*t)*0.05:d={drone_at_end_duration:.3f}:channel_layout={AUDIO_CHANNEL_LAYOUT}:sample_rate={AUDIO_SAMPLE_RATE}[drone_suffix];\")\n                # Concatenate the (silence + main audio) stream with the drone suffix\n                fc_parts.append(f\"{current_audio_stream_label_post_delay}[drone_suffix]concat=n=2:v=0:a=1[final_complete_audio];\")\n                final_audio_map_label = \"[final_complete_audio]\"\n            elif drone_at_end_duration < -0.01: # Audio (with initial silence/positioning) is already longer than video\n                print(f\"    Audio up to main track end ({current_constructed_audio_length:.2f}s) is longer than video ({video_duration:.2f}s). It will be truncated by -t.\")\n            else: # Lengths are very close\n                print(f\"    Audio up to main track end ({current_constructed_audio_length:.2f}s) matches video length ({video_duration:.2f}s) closely.\")\n\n            filter_complex_string = \"\".join(fc_parts)\n\n            ffmpeg_command = [\n                \"ffmpeg\", \"-y\",\n                \"-i\", source_video_path,    # Input 0 (fused video)\n                \"-i\", audio_filepath,       # Input 1 (main audio track)\n                \"-filter_complex\", filter_complex_string,\n                \"-map\", \"0:v\",                # Map video from input 0\n                \"-map\", final_audio_map_label, # Map the constructed audio stream\n                \"-c:v\", \"copy\",\n                \"-c:a\", \"aac\", \"-b:a\", \"192k\",\n                \"-t\", str(video_duration),      # Set output duration to match original fused video\n                target_video_path\n            ]\n            \n            print(f\"  Running ffmpeg command: {' '.join(ffmpeg_command)}\")\n            try:\n                process = subprocess.run(ffmpeg_command, capture_output=True, text=True, check=True)\n                print(f\"  Successfully created: {target_video_path}\")\n                success_count += 1\n            except subprocess.CalledProcessError as e:\n                print(f\"  Error during ffmpeg processing for {target_filename} (delay {current_delay}s):\")\n                print(f\"    Return code: {e.returncode}\")\n                print(f\"    Stderr: {e.stderr}\")\n                fail_count += 1\n            except FileNotFoundError: # Should be caught by the initial ffprobe check\n                print(\"  CRITICAL Error: ffmpeg command not found during processing loop. Halting.\")\n                # Mark all remaining files for this video and subsequent videos as failed\n                remaining_for_this_video = len(AUDIO_START_DELAYS) - (AUDIO_START_DELAYS.index(current_delay) +1)\n                fail_count += remaining_for_this_video\n                # Calculate remaining videos in VIDEO_TO_AUDIO_MAP\n                current_video_index = list(VIDEO_TO_AUDIO_MAP.keys()).index(video_filename_ext)\n                remaining_videos = len(VIDEO_TO_AUDIO_MAP) - (current_video_index + 1)\n                fail_count += remaining_videos * len(AUDIO_START_DELAYS)\n                # And then break all loops or return\n                print(f\"\\n--- Audio Addition Finished Prematurely ---\")\n                print(f\"Successfully processed: {success_count} video versions.\")\n                print(f\"Failed to process: {fail_count} video versions.\")\n                return # Exit main function\n\n    print(f\"\\n--- Audio Addition Finished ---\")\n    print(f\"Successfully processed: {success_count} video versions.\")\n    print(f\"Failed to process: {fail_count} video versions.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "how-to-break-off-an-engagement.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/How to break off an engagement_The New Testament.mp3",
      "bloodline.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Bloodline_New Gospel of Creation.mp3",
      "dj-turn-me-up.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/DJ_turn_me_up_Whispers of a Forgotten Love.mp3",
      "flashing-lights.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/flashing-lights-potent-Digital Testament remix v1.2.1.2.mp3",
      "hot-minute.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Hot Minute_The New Covenant remix v2.mp3",
      "how-to-win-my-heart.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/How To Win My Heart_Echoes of Longing remix v2.2.1.mp3",
      "magic-ride.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Magic ride_Awakening of the New Gospel remix v2.mp3",
      "nevermore.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/nevermore-Reclaimed Truths remix v1.mp3",
      "newly-single.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Newly Single_The New Awakening.mp3",
      "out-of-life.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Out of Life_Echoes of Departure remix v1_GOOD.mp3",
      "resurrecting-atlantis.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/RESURRECTING_ATLANTIS_Resonance of Futures.mp3",
      "reunion.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Reunion_The New Gospel.mp3",
      "yet-heard.mp4",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Yet, Heard_The New Covenant.mp3",
      "/Users/gaia/resurrecting atlantis/DOG/LEG/fused_videos_output/",
      "/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS/",
      " # For generated silence/drone\n\n# Mapping of base video filenames to their corresponding audio file paths\nVIDEO_TO_AUDIO_MAP = {\n    ",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/How to break off an engagement_The New Testament.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Bloodline_New Gospel of Creation.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/DJ_turn_me_up_Whispers of a Forgotten Love.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/flashing-lights-potent-Digital Testament remix v1.2.1.2.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Hot Minute_The New Covenant remix v2.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/How To Win My Heart_Echoes of Longing remix v2.2.1.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Magic ride_Awakening of the New Gospel remix v2.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/nevermore-Reclaimed Truths remix v1.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Newly Single_The New Awakening.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Out of Life_Echoes of Departure remix v1_GOOD.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/RESURRECTING_ATLANTIS_Resonance of Futures.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Reunion_The New Gospel.mp3",
      "/Users/gaia/resurrecting atlantis/MANTA/audio/Yet, Heard_The New Covenant.mp3",
      "])\n        # Fallback for some files, check streams (audio/video typically have duration in their stream info)\n        if ",
      "\\n({processed_count}/{total_files_to_process}) Processing: {video_filename_ext} with audio delay: {current_delay}s",
      "\n            elif drone_at_end_duration < -0.01: # Audio (with initial silence/positioning) is already longer than video\n                print(f"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "ffprobe_command, capture_output=True, text=True, check=True"
      },
      {
        "type": "run",
        "snippet": "[\"ffprobe\", \"-version\"], capture_output=True, text=True, check=True"
      },
      {
        "type": "run",
        "snippet": "ffmpeg_command, capture_output=True, text=True, check=True"
      }
    ],
    "imports": [
      "os",
      "subprocess",
      "json"
    ],
    "generates": [],
    "reads": [
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "DOG/LEG/scripts/video_assembler.py",
    "size": 5911,
    "lines": 156,
    "source": "import os\nimport glob\nimport subprocess\nimport shutil\n\nINPUT_DIR = \"/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS/\"\nOUTPUT_DIR = \"/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS/\"\nOUTPUT_FILENAME = \"DOG_NEW_AUDIO_ASSEMBLAGE_0s.mp4\"\nFILE_SUFFIX = \"_NEW-AUDIO_0s.mp4\"\nCONCAT_LIST_FILENAME = \"concat_list.txt\"\nNORMALIZED_TEMP_DIR_NAME = \"normalized_temp_for_assembly\"\nTARGET_FPS = 25.0\nTARGET_RESOLUTION = \"1032x844\" # WidthxHeight\n\n# Desired order of poems (case-insensitive for matching)\n# Based on MEMORY[7f53de71-f2cb-4fc1-a459-5af2e0f82bd6] and available processed files\nDESIRED_POEM_ORDER = [\n    \"out-of-life\",\n    \"flashing-lights\",\n    \"how-to-break-off-an-engagement\",\n    \"nevermore\",\n    \"bloodline\",\n    \"resurrecting-atlantis\",\n    \"dj-turn-me-up\",\n    \"newly-single\",\n    \"yet-heard\",\n    \"magic-ride\",\n    \"reunion\",\n    \"how-to-win-my-heart\",\n    \"hot-minute\",\n]\n\ndef get_poem_name_from_filename(filename):\n    \"\"\"Extracts the base poem name from the filename.\"\"\"\n    base = os.path.basename(filename)\n    if base.endswith(FILE_SUFFIX):\n        return base[:-len(FILE_SUFFIX)].lower()\n    return base.lower()\n\ndef sort_videos(video_files):\n    \"\"\"Sorts video files based on DESIRED_POEM_ORDER.\"\"\"\n    def sort_key(filepath):\n        poem_name = get_poem_name_from_filename(filepath)\n        try:\n            return DESIRED_POEM_ORDER.index(poem_name)\n        except ValueError:\n            # If poem_name is not in DESIRED_POEM_ORDER, put it at the end\n            return len(DESIRED_POEM_ORDER)\n    \n    return sorted(video_files, key=sort_key)\n\ndef main():\n    print(f\"Starting video assembly with normalization pass...\")\n    print(f\"Input directory: {INPUT_DIR}\")\n\n    normalized_temp_dir = os.path.join(INPUT_DIR, NORMALIZED_TEMP_DIR_NAME)\n    if os.path.exists(normalized_temp_dir):\n        shutil.rmtree(normalized_temp_dir) # Clean up from previous run if any\n    os.makedirs(normalized_temp_dir, exist_ok=True)\n    print(f\"Created temporary directory for normalized files: {normalized_temp_dir}\")\n\n    search_pattern = os.path.join(INPUT_DIR, f\"*{FILE_SUFFIX}\")\n    video_files = glob.glob(search_pattern)\n\n    if not video_files:\n        print(f\"No video files found matching pattern: {search_pattern}\")\n        if os.path.exists(normalized_temp_dir):\n            shutil.rmtree(normalized_temp_dir)\n        return\n\n    print(f\"Found {len(video_files)} video files to process.\")\n    sorted_video_files = sort_videos(video_files)\n    \n    normalized_file_paths = []\n    print(\"\\nNormalizing video files...\")\n    for i, video_file_path in enumerate(sorted_video_files):\n        base_name = os.path.basename(video_file_path)\n        normalized_output_path = os.path.join(normalized_temp_dir, base_name)\n        print(f\"Normalizing ({i+1}/{len(sorted_video_files)}): {base_name} -> {TARGET_FPS}fps, {TARGET_RESOLUTION}\")\n        \n        target_w, target_h = TARGET_RESOLUTION.split('x')\n\n        normalize_ffmpeg_cmd = [\n            \"ffmpeg\", \"-y\",\n            \"-i\", video_file_path,\n            \"-vf\", f\"fps={TARGET_FPS},scale={target_w}:{target_h}:force_original_aspect_ratio=decrease,pad={target_w}:{target_h}:(ow-iw)/2:(oh-ih)/2\",\n            \"-c:v\", \"libx264\",\n            \"-preset\", \"medium\",\n            \"-crf\", \"22\",\n            \"-c:a\", \"copy\",\n            \"-video_track_timescale\", str(int(TARGET_FPS * 1000)), # e.g., 25000 for 25fps\n            normalized_output_path\n        ]\n        \n        try:\n            subprocess.run(normalize_ffmpeg_cmd, check=True, capture_output=True, text=True)\n            normalized_file_paths.append(normalized_output_path)\n        except subprocess.CalledProcessError as e:\n            print(f\"Error normalizing {base_name}:\")\n            print(f\"Command: {' '.join(e.cmd)}\")\n            print(f\"Return code: {e.returncode}\")\n            print(f\"Stderr: {e.stderr}\")\n            # Decide if we should stop or continue\n            print(\"Skipping this file and continuing with others...\")\n            continue # or raise\n\n    if not normalized_file_paths:\n        print(\"No files were successfully normalized. Aborting assembly.\")\n        if os.path.exists(normalized_temp_dir):\n            shutil.rmtree(normalized_temp_dir)\n        return\n\n    print(\"\\nFiles to concatenate (normalized, in order):\")\n    for f_path in normalized_file_paths:\n        print(f\"  - {os.path.basename(f_path)}\")\n\n    concat_list_path = os.path.join(normalized_temp_dir, CONCAT_LIST_FILENAME) # Place concat list in temp dir\n    with open(concat_list_path, 'w') as f:\n        for norm_video_file in normalized_file_paths:\n            f.write(f\"file '{os.path.abspath(norm_video_file)}'\\n\")\n    print(f\"Generated concat list: {concat_list_path}\")\n\n    output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    final_assembly_ffmpeg_cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-f\", \"concat\",\n        \"-safe\", \"0\",\n        \"-i\", concat_list_path,\n        \"-c\", \"copy\",\n        output_path\n    ]\n\n    print(f\"\\nRunning final assembly ffmpeg command: {' '.join(final_assembly_ffmpeg_cmd)}\")\n    try:\n        process = subprocess.run(final_assembly_ffmpeg_cmd, check=True, capture_output=True, text=True)\n        print(\"FFmpeg assembly stdout:\")\n        print(process.stdout)\n        print(\"FFmpeg assembly stderr:\")\n        print(process.stderr)\n        print(f\"Successfully assembled video: {output_path}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during final ffmpeg assembly:\")\n        print(f\"Return code: {e.returncode}\")\n        print(f\"Stdout: {e.stdout}\")\n        print(f\"Stderr: {e.stderr}\")\n    finally:\n        print(f\"Cleaning up temporary directory: {normalized_temp_dir}\")\n        if os.path.exists(normalized_temp_dir):\n            shutil.rmtree(normalized_temp_dir)\n            print(f\"Removed temporary directory.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "DOG_NEW_AUDIO_ASSEMBLAGE_0s.mp4",
      "_NEW-AUDIO_0s.mp4",
      "/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS/",
      "/Users/gaia/resurrecting atlantis/DOG/EAR_AUDIO_DELAY_OPTIONS/",
      "Normalizing ({i+1}/{len(sorted_video_files)}): {base_name} -> {TARGET_FPS}fps, {TARGET_RESOLUTION}",
      "fps={TARGET_FPS},scale={target_w}:{target_h}:force_original_aspect_ratio=decrease,pad={target_w}:{target_h}:(ow-iw)/2:(oh-ih)/2"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "normalize_ffmpeg_cmd, check=True, capture_output=True, text=True"
      },
      {
        "type": "run",
        "snippet": "final_assembly_ffmpeg_cmd, check=True, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "glob",
      "subprocess",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "DOG/LEG/scripts/video_fusor.py",
    "size": 7093,
    "lines": 127,
    "source": "import os\nimport subprocess\n\nMAIN_VIDEO_DIR = \"/Users/gaia/resurrecting atlantis/DOG/LEG/output_videos/\"\nINTRO_VIDEO_DIR = \"/Users/gaia/resurrecting atlantis/IMPALA/IBEX/intros_with_headers/\"\nNEW_OUTPUT_DIR = \"/Users/gaia/resurrecting atlantis/DOG/LEG/fused_videos_output/\"  # New output directory\nCONCAT_TEMP_FILE_NAME = \"_temp_fuse_list.txt\"\n\nVIDEO_MAPPING = {\n    \"out-of-life.mp4\": \"01_SH_OutOfLife_000000_2101217636, 01_SH_OutofLife_0000, M 5_header.mp4\",\n    \"flashing-lights.mp4\": \"02_FL_FlashingLights_021100_2424344039, 02_FL_FlashingLights, M 5_header.mp4\",\n    \"how-to-break-off-an-engagement.mp4\": \"03_HT_HowToBreakOffAnEngagement_042200_03_HT_HowtoBreakOffa, M 5_header.mp4\",\n    \"nevermore.mp4\": \"04_NM_Nevermore_063300_CinePrompt \u201cNEVERM, image-prompt, M 5_header.mp4\",\n    \"bloodline.mp4\": \"05_BE_Bloodline_084400_2436216441 (10)_header.mp4\",\n    \"resurrecting-atlantis.mp4\": \"06_AT_ResurrectingAtlantis_105500_503796468, 06_AT_ResurrectingAt, M 5_header.mp4\",\n    \"dj-turn-me-up.mp4\": \"07_DJ_DJTurnMeUp_130600_2436216441 (13)_header.mp4\",\n    \"newly-single.mp4\": \"08_NS_NewlySingle_151700_2436216441_header.mp4\",\n    \"yet-heard.mp4\": \"09_YH_YetHeard_172800_2436216441 (12)_header.mp4\",\n    \"magic-ride.mp4\": \"10_MR_MagicRide_193900_2436216441 (5)_header.mp4\",\n    \"reunion.mp4\": \"12_RU_Reunion_215000_2436216441 (11)_header.mp4\",\n    \"how-to-win-my-heart.mp4\": \"13_HW_HowToWinMyHeart_240100_2436216441 (9)_header.mp4\",\n    \"hot-minute.mp4\": \"14_HM_HotMinute_261200_20250528_2220_Blend Video_blend_01jwcxeddkehsae18pnwgh2hq2_header.mp4\"\n}\n\ndef main():\n    print(f\"--- Starting Video Fusion Process ---\")\n    print(f\"Main video directory: {MAIN_VIDEO_DIR}\")\n    print(f\"Intro video directory: {INTRO_VIDEO_DIR}\")\n    print(f\"NEW Output directory: {NEW_OUTPUT_DIR}\")\n\n    if not os.path.exists(NEW_OUTPUT_DIR):\n        os.makedirs(NEW_OUTPUT_DIR)\n        print(f\"Created NEW output directory: {NEW_OUTPUT_DIR}\")\n    else:\n        print(f\"NEW output directory already exists: {NEW_OUTPUT_DIR}\")\n\n    verified_success_count = 0\n    processing_fail_count = 0 # For issues before ffmpeg (e.g., missing inputs)\n    verification_fail_count = 0 # For issues after ffmpeg (file not created/empty)\n\n    for main_video_filename, intro_video_filename in VIDEO_MAPPING.items():\n        main_video_path = os.path.join(MAIN_VIDEO_DIR, main_video_filename)\n        intro_video_path = os.path.join(INTRO_VIDEO_DIR, intro_video_filename)\n        output_video_path = os.path.join(NEW_OUTPUT_DIR, main_video_filename)\n        temp_concat_file = os.path.join(NEW_OUTPUT_DIR, CONCAT_TEMP_FILE_NAME)\n\n        print(f\"\\nProcessing: {main_video_filename}\")\n        print(f\"  Intro video: {intro_video_path}\")\n        print(f\"  Main video: {main_video_path}\")\n        print(f\"  Target output video: {output_video_path}\")\n\n        if not os.path.exists(main_video_path):\n            print(f\"  ERROR: Main video not found: {main_video_path}\")\n            processing_fail_count += 1\n            continue\n        \n        if not os.path.exists(intro_video_path):\n            print(f\"  ERROR: Intro video not found: {intro_video_path}\")\n            processing_fail_count += 1\n            continue\n\n        ffmpeg_return_code = -1\n        ffmpeg_executed_successfully = False\n\n        try:\n            print(f\"  Using ffmpeg concat filter for {main_video_filename}...\")\n            ffmpeg_command_filter = [\n                \"ffmpeg\", \"-y\",\n                \"-i\", intro_video_path,  # First input\n                \"-i\", main_video_path,   # Second input\n                \"-filter_complex\", \"[0:v][1:v]concat=n=2:v=1:a=0[v]\", # Concat video streams only\n                \"-map\", \"[v]\", # Map the concatenated video stream to output\n                \"-c:v\", \"libx264\", \"-preset\", \"medium\", \"-crf\", \"23\",\n                \"-pix_fmt\", \"yuv420p\", # Video specific options\n                output_video_path\n            ]\n            print(f\"  Running ffmpeg concat filter command: {' '.join(ffmpeg_command_filter)}\")\n            process_reencode = subprocess.run(ffmpeg_command_filter, capture_output=True, text=True, check=False)\n            ffmpeg_return_code = process_reencode.returncode\n\n            # Clean up temp file if it was created in a previous version (no longer needed)\n            temp_concat_file_old_path = os.path.join(NEW_OUTPUT_DIR, \"_temp_fuse_list.txt\")\n            if os.path.exists(temp_concat_file_old_path):\n                try:\n                    os.remove(temp_concat_file_old_path)\n                    print(f\"  Cleaned up old temp file: {temp_concat_file_old_path}\")\n                except OSError as e:\n                    print(f\"  Warning: Could not remove old temp file {temp_concat_file_old_path}: {e}\")\n            if process_reencode.returncode == 0:\n                print(f\"  ffmpeg (re-encode) successful for {main_video_filename}.\")\n                ffmpeg_executed_successfully = True\n            else:\n                print(f\"  ERROR: ffmpeg (re-encode) failed. Return code: {process_reencode.returncode}\")\n                print(f\"  ffmpeg stdout (re-encode):\\n{process_reencode.stdout}\")\n                print(f\"  ffmpeg stderr (re-encode):\\n{process_reencode.stderr}\")\n\n        except Exception as e:\n            print(f\"  CRITICAL ERROR during ffmpeg execution for {main_video_filename}: {e}\")\n            # This catches errors in subprocess.run itself or file I/O before ffmpeg\n        finally:\n            if os.path.exists(temp_concat_file):\n                os.remove(temp_concat_file)\n\n        # --- File Verification Step ---\n        if ffmpeg_executed_successfully:\n            if os.path.exists(output_video_path) and os.path.getsize(output_video_path) > 0:\n                print(f\"  VERIFIED: {main_video_filename} created successfully at {output_video_path}, size: {os.path.getsize(output_video_path)} bytes.\")\n                verified_success_count += 1\n            else:\n                print(f\"  VERIFICATION FAILED: {main_video_filename} NOT FOUND or IS EMPTY at {output_video_path} despite ffmpeg reporting success (code {ffmpeg_return_code}).\")\n                verification_fail_count += 1\n        elif ffmpeg_return_code != -1 : # ffmpeg was attempted but failed\n             print(f\"  VERIFICATION FAILED: {main_video_filename} not created due to ffmpeg error (code {ffmpeg_return_code}). File not found at {output_video_path}.\")\n             verification_fail_count += 1\n        # If ffmpeg_return_code is -1, it means an exception occurred before ffmpeg command was properly run or completed.\n        # processing_fail_count would have been incremented if it was an input file issue.\n\n    print(f\"\\n--- Video Fusion Process Finished ---\")\n    print(f\"Successfully processed and VERIFIED: {verified_success_count} videos.\")\n    if processing_fail_count > 0:\n        print(f\"Failed to process (due to missing inputs or pre-ffmpeg errors): {processing_fail_count} videos.\")\n    if verification_fail_count > 0:\n        print(f\"Failed VERIFICATION (file not created/empty post-ffmpeg): {verification_fail_count} videos.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "out-of-life.mp4",
      "01_SH_OutOfLife_000000_2101217636, 01_SH_OutofLife_0000, M 5_header.mp4",
      "flashing-lights.mp4",
      "02_FL_FlashingLights_021100_2424344039, 02_FL_FlashingLights, M 5_header.mp4",
      "how-to-break-off-an-engagement.mp4",
      "03_HT_HowToBreakOffAnEngagement_042200_03_HT_HowtoBreakOffa, M 5_header.mp4",
      "nevermore.mp4",
      "04_NM_Nevermore_063300_CinePrompt \u201cNEVERM, image-prompt, M 5_header.mp4",
      "bloodline.mp4",
      "05_BE_Bloodline_084400_2436216441 (10)_header.mp4",
      "resurrecting-atlantis.mp4",
      "06_AT_ResurrectingAtlantis_105500_503796468, 06_AT_ResurrectingAt, M 5_header.mp4",
      "dj-turn-me-up.mp4",
      "07_DJ_DJTurnMeUp_130600_2436216441 (13)_header.mp4",
      "newly-single.mp4",
      "08_NS_NewlySingle_151700_2436216441_header.mp4",
      "yet-heard.mp4",
      "09_YH_YetHeard_172800_2436216441 (12)_header.mp4",
      "magic-ride.mp4",
      "10_MR_MagicRide_193900_2436216441 (5)_header.mp4",
      "reunion.mp4",
      "12_RU_Reunion_215000_2436216441 (11)_header.mp4",
      "how-to-win-my-heart.mp4",
      "13_HW_HowToWinMyHeart_240100_2436216441 (9)_header.mp4",
      "hot-minute.mp4",
      "14_HM_HotMinute_261200_20250528_2220_Blend Video_blend_01jwcxeddkehsae18pnwgh2hq2_header.mp4",
      "/Users/gaia/resurrecting atlantis/DOG/LEG/output_videos/",
      "/Users/gaia/resurrecting atlantis/IMPALA/IBEX/intros_with_headers/",
      "/Users/gaia/resurrecting atlantis/DOG/LEG/fused_videos_output/",
      ")\n\n    verified_success_count = 0\n    processing_fail_count = 0 # For issues before ffmpeg (e.g., missing inputs)\n    verification_fail_count = 0 # For issues after ffmpeg (file not created/empty)\n\n    for main_video_filename, intro_video_filename in VIDEO_MAPPING.items():\n        main_video_path = os.path.join(MAIN_VIDEO_DIR, main_video_filename)\n        intro_video_path = os.path.join(INTRO_VIDEO_DIR, intro_video_filename)\n        output_video_path = os.path.join(NEW_OUTPUT_DIR, main_video_filename)\n        temp_concat_file = os.path.join(NEW_OUTPUT_DIR, CONCAT_TEMP_FILE_NAME)\n\n        print(f",
      ")\n            # This catches errors in subprocess.run itself or file I/O before ffmpeg\n        finally:\n            if os.path.exists(temp_concat_file):\n                os.remove(temp_concat_file)\n\n        # --- File Verification Step ---\n        if ffmpeg_executed_successfully:\n            if os.path.exists(output_video_path) and os.path.getsize(output_video_path) > 0:\n                print(f",
      "Failed VERIFICATION (file not created/empty post-ffmpeg): {verification_fail_count} videos."
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "ffmpeg_command_filter, capture_output=True, text=True, check=False"
      }
    ],
    "imports": [
      "os",
      "subprocess"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "DOG/LEG/scripts/leg_assembler_v1.py",
    "size": 3703,
    "lines": 85,
    "source": "#!/usr/bin/env python3\n\"\"\"\nLEG_ASSEMBLER_V1 - Creates a video from image cards for the LEG workflow.\n- Specifically designed to assemble images generated by leg_generator_v1.py for the 'Magic ride' poem.\n- Reads images from ELEPHANT/LEG/output_media/\n- Outputs a video file (e.g., leg_magic_ride_v1.mp4) to ELEPHANT/LEG/output_media/\n- Target video duration is 2 minutes 11 seconds (131 seconds).\n\"\"\"\n\nimport os\nimport glob\nfrom moviepy.editor import ImageSequenceClip\n\n# --- Configuration ---\n# Assumes this script is in ELEPHANT/TRUNK/\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nELEPHANT_DIR = os.path.dirname(SCRIPT_DIR)\n\nIMAGE_DIR_NAME = \"output_media\"\nIMAGE_SUBDIR = os.path.join(ELEPHANT_DIR, IMAGE_DIR_NAME)\n\nOUTPUT_VIDEO_FILENAME = \"leg_magic_ride_v1.mp4\"\nOUTPUT_VIDEO_PATH = os.path.join(IMAGE_SUBDIR, OUTPUT_VIDEO_FILENAME)\n\nTARGET_TOTAL_DURATION_SECONDS = 131  # 2 minutes 11 seconds\nIMAGE_FILE_PATTERN = \"magic-ride_frame_*.png\"\n\n# --- Main script ---\ndef main():\n    print(f\"Starting video assembly...\")\n    print(f\"Image source directory: {IMAGE_SUBDIR}\")\n    print(f\"Output video path: {OUTPUT_VIDEO_PATH}\")\n    print(f\"Target total duration: {TARGET_TOTAL_DURATION_SECONDS} seconds\")\n\n    image_search_pattern = os.path.join(IMAGE_SUBDIR, IMAGE_FILE_PATTERN)\n    image_files = sorted(glob.glob(image_search_pattern))\n\n    if not image_files:\n        print(f\"Error: No image files found matching pattern '{IMAGE_FILE_PATTERN}' in '{IMAGE_SUBDIR}'.\")\n        print(\"Please ensure 61primeplus.py has run successfully and generated images.\")\n        return\n\n    num_images = len(image_files)\n    print(f\"Found {num_images} image frames.\")\n\n    if num_images == 0:\n        print(\"Error: No images to process. Exiting.\")\n        return\n\n    # Calculate FPS required to achieve the target total duration with the given number of images.\n    # Each frame will be displayed for (TARGET_TOTAL_DURATION_SECONDS / num_images) seconds.\n    # FPS = 1 / duration_per_frame = num_images / TARGET_TOTAL_DURATION_SECONDS.\n    fps = num_images / TARGET_TOTAL_DURATION_SECONDS\n\n    print(f\"Calculated FPS: {fps:.4f} (for a total duration of {TARGET_TOTAL_DURATION_SECONDS}s with {num_images} frames)\")\n    \n    try:\n        print(\"Creating video clip...\")\n        # Create the video clip using the image files and calculated fps\n        clip = ImageSequenceClip(image_files, fps=fps)\n\n        print(f\"Writing video file to {OUTPUT_VIDEO_PATH}...\")\n        # Write the video file to disk\n        # Common codecs: 'libx264' (for H.264, good quality/compression), 'mpeg4'\n        # preset options: ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow\n        # threads for parallel processing (can speed up encoding)\n        clip.write_videofile(OUTPUT_VIDEO_PATH, \n                             codec='libx264', \n                             fps=fps, \n                             preset='medium', \n                             threads=4, \n                             logger='bar') # Use 'bar' for progress bar, or None for less verbose output\n        \n        print(f\"\\nVideo '{OUTPUT_VIDEO_FILENAME}' created successfully in '{IMAGE_SUBDIR}'.\")\n        # moviepy might slightly adjust duration due to fps quantization, check actual duration.\n        print(f\"Actual video duration reported by moviepy: {clip.duration:.2f} seconds.\")\n\n    except Exception as e:\n        print(f\"An error occurred during video creation: {e}\")\n        print(\"Please ensure 'moviepy' is installed (e.g., 'pip install moviepy') and that FFmpeg is available on your system.\")\n        print(\"FFmpeg is a dependency for moviepy to read/write video files.\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_references": [
      "leg_magic_ride_v1.mp4",
      "magic-ride_frame_*.png",
      " poem.\n- Reads images from ELEPHANT/LEG/output_media/\n- Outputs a video file (e.g., leg_magic_ride_v1.mp4) to ELEPHANT/LEG/output_media/\n- Target video duration is 2 minutes 11 seconds (131 seconds).\n",
      "\n\nimport os\nimport glob\nfrom moviepy.editor import ImageSequenceClip\n\n# --- Configuration ---\n# Assumes this script is in ELEPHANT/TRUNK/\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\nELEPHANT_DIR = os.path.dirname(SCRIPT_DIR)\n\nIMAGE_DIR_NAME = ",
      ")\n        return\n\n    # Calculate FPS required to achieve the target total duration with the given number of images.\n    # Each frame will be displayed for (TARGET_TOTAL_DURATION_SECONDS / num_images) seconds.\n    # FPS = 1 / duration_per_frame = num_images / TARGET_TOTAL_DURATION_SECONDS.\n    fps = num_images / TARGET_TOTAL_DURATION_SECONDS\n\n    print(f",
      " (for H.264, good quality/compression), ",
      "FFmpeg is a dependency for moviepy to read/write video files."
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "glob",
      "moviepy.editor"
    ],
    "generates": [],
    "reads": [],
    "docstring": "LEG_ASSEMBLER_V1 - Creates a video from image cards for the LEG workflow.\n- Specifically designed to assemble images generated by leg_generator_v1.py for the 'Magic ride' poem.\n- Reads images from ELEPHANT/LEG/output_media/\n- Outputs a video file (e.g., leg_magic_ride_v1.mp4) to ELEPHANT/LEG/output_media/\n- Target video duration is 2 minutes 11 seconds (131 seconds)."
  },
  {
    "path": "DOG/LEG/scripts/add_scrolling_header_to_intros.py",
    "size": 18969,
    "lines": 373,
    "source": "#!/usr/bin/env python3\nprint(\"--- SCRIPT EXECUTION STARTED ---\")\n\"\"\"\nAdds a dynamic, scrolling header bar to intro videos.\n\nThe header bar displays:\n- Static poem title and sequence number on the left.\n- Three stacked lines of S, I, C genome glyphs scrolling right-to-left on the right.\n\nFinal video dimensions are 1032x844, with the original intro video content\nscaled and letterboxed below the 1032x80 header.\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nimport shutil # Standard library\nprint(\"--- DEBUG: About to import PIL.ImageFont ---\")\nfrom PIL import ImageFont # Problematic if Pillow not *really* there or corrupted\nprint(\"--- DEBUG: Successfully imported PIL.ImageFont ---\")\nimport re\n\n# --- Configuration ---\nBASE_DIR_PROJECT = \"/Users/gaia/resurrecting atlantis\"\nINTRO_VIDEOS_DIR = os.path.join(BASE_DIR_PROJECT, \"IMPALA\", \"IBEX\")\nOUTPUT_VIDEOS_DIR = os.path.join(INTRO_VIDEOS_DIR, \"intros_with_headers\")\nSYMBOLIC_GENOME_PATH = os.path.join(BASE_DIR_PROJECT, \"DOG\", \"LEG\", \"symbolic_genome_data.json\")\n\n# Video and Header Dimensions\nFINAL_WIDTH = 1032\nFINAL_HEIGHT = 844\nHEADER_HEIGHT = 80\nVIDEO_CONTENT_HEIGHT = FINAL_HEIGHT - HEADER_HEIGHT # 764\n\n# Header Content Layout\nTEXT_PADDING = 10 # General padding from edges\nGLYPH_LINE_HEIGHT_ALLOCATION = 20 # Approx vertical space for each S,I,C line including spacing\nTITLE_AREA_TOP_Y = GLYPH_LINE_HEIGHT_ALLOCATION * 3 # Glyphs take top 3*20 = 60px\nTITLE_HEIGHT_ALLOCATION = HEADER_HEIGHT - TITLE_AREA_TOP_Y # Remaining 20px for title + stats\n\n# Fonts (Using Menlo as per leg_generator_v1.py and discussion)\nFONT_FAMILY_MAIN = \"Menlo\"\nFONT_TITLE_SIZE = 22 # Made slightly bigger as requested\n# FONT_SEQUENCE_SIZE = 14 # Removed\nFONT_GENOME_SIZE = 18\nFONT_STATS_SIZE = 12\n\n# Colors (from leg_generator_v1.py where applicable)\nCOLOR_BLACK = \"0x000000\"\nCOLOR_WHITE = \"0xFFFFFF\"\nCOLOR_LIGHT_GRAY = \"0xC8C8C8\" # (200,200,200)\nCOLOR_SYNTAGMA = \"0x64C8FF\" # (100, 200, 255) - S-line\nCOLOR_IMAGE_TYPE = \"0xADE2E6\" # (173, 216, 230) - I-line\nCOLOR_CINEOSIS = \"0xE696FF\"   # (230, 150, 255) - C-line\n\nprint(\"--- DEBUG: About to call shutil.which for ffmpeg and ffprobe ---\")\nFFMPEG_PATH = shutil.which(\"ffmpeg\")\nFFPROBE_PATH = shutil.which(\"ffprobe\")\n\nif not FFMPEG_PATH:\n    print(\"Error: ffmpeg not found in PATH.\")\n    exit(1)\nif not FFPROBE_PATH: # This would print an error\n    print(\"Error: ffprobe not found in PATH.\")\n    exit(1)\nprint(\"--- DEBUG: Successfully called shutil.which for ffmpeg and ffprobe ---\")\n\ndef get_poem_title_from_filename(filename):\n    # This map is crucial for converting 2-letter codes to full titles\n    # that match the keys in symbolic_genome_data.json\n    title_code_map = {\n        \"SH\": \"Out of Life\",\n        \"FL\": \"Flashing Lights\",\n        \"HT\": \"How to break off an engagement\",\n        \"NM\": \"Nevermore\",\n        \"BE\": \"Bloodline\",\n        \"AT\": \"Resurrecting Atlantis\",\n        \"DJ\": \"DJ Turn Me Up\",\n        \"NS\": \"Newly Single\",\n        \"YH\": \"Yet, Heard\",\n        \"MR\": \"Magic ride\",\n        \"RU\": \"Reunion\",\n        \"HW\": \"How To Win My Heart\",\n        \"HM\": \"Hot Minute\"\n    }\n\n    # Pattern for filenames like \"01_SH_OutOfLife_...mp4\"\n    # Extracts the two-letter code (e.g., \"SH\")\n    code_match = re.match(r\"^\\d{2}_([A-Z]{2})_.*\", filename)\n    if code_match:\n        code = code_match.group(1)\n        return title_code_map.get(code, code)\n\n    # If the above pattern doesn't match, assume the filename itself (without extension)\n    # is the title. This handles cases like \"Resurrecting Atlantis.mp4\".\n    filename_stem = os.path.splitext(filename)[0]\n\n    # A specific check for \"Resurrecting Atlantis\" as it's a known title that might appear as a filename stem.\n    if filename_stem.lower() == \"resurrecting atlantis\":\n        return \"Resurrecting Atlantis\"\n\n    # Fallback: if the stem itself is a key in title_code_map (e.g. filename was \"SH.mp4\")\n    # or if it's a direct title like \"Poem Name.mp4\" that is NOT \"Resurrecting Atlantis\".\n    # The primary use of title_code_map is for codes.\n    # So, for non-coded filenames, we return the stem.\n    # The .lower() for lookup happens later in main().\n    return filename_stem\n\ndef get_text_width_pil(text, font_path, font_size):\n    try:\n        print(\"--- DEBUG: main() - Font loading: About to load font ---\")\n        font = ImageFont.truetype(font_path, font_size)\n        print(\"--- DEBUG: main() - Font loading: Successfully loaded font ---\")\n        # getlength is available in newer Pillow versions\n        if hasattr(font, 'getlength'):\n            return int(font.getlength(text))\n        else:\n            # Fallback for older Pillow: getbbox\n            bbox = font.getbbox(text)\n            return int(bbox[2] - bbox[0]) # width = x1 - x0\n    except IOError:\n        print(f\"Warning: Font {font_path} not found. Using approximate width.\")\n        return len(text) * font_size * 0.6 # Rough approximation\n    except Exception as e:\n        print(f\"Error getting text width: {e}\")\n        return len(text) * font_size * 0.6\n\ndef get_glyph_stats_text(line_text, line_label):\n    if not line_text or line_text.endswith(\"_N/A\"):\n        return f\"{line_label}: N/A\"\n    \n    glyphs = [g for g in line_text if g != ' ']\n    if not glyphs:\n        return f\"{line_label}: (empty)\"\n    \n    total_glyphs = len(glyphs)\n    counts = {}\n    for glyph in glyphs:\n        counts[glyph] = counts.get(glyph, 0) + 1\n    \n    stats_parts = []\n    # Sort by glyph for consistent order, or could sort by count\n    for glyph, count in sorted(counts.items()):\n        percentage = (count / total_glyphs) * 100\n        stats_parts.append(f\"{glyph}{percentage:.0f}%\")\n    \n    return f\"{line_label}: {' '.join(stats_parts)}\"\n\n\ndef main():\n    print(\"--- DEBUG: Entered main() function ---\")\n    print(f\"--- DEBUG: INTRO_VIDEOS_DIR in main: {INTRO_VIDEOS_DIR}, type: {type(INTRO_VIDEOS_DIR)} ---\")\n    os.makedirs(OUTPUT_VIDEOS_DIR, exist_ok=True)\n\n    print(\"--- DEBUG: main() - About to try loading symbolic_genome_data.json ---\")\n    try:\n        with open(SYMBOLIC_GENOME_PATH, 'r', encoding='utf-8') as f:\n            genome_data_list = json.load(f)\n        print(\"--- DEBUG: main() - Successfully loaded symbolic_genome_data.json ---\")\n        genome_map = {item['title'].strip().lower(): item for item in genome_data_list}\n    except Exception as e:\n        print(f\"Error loading symbolic genome data: {e}\")\n        return\n\n    # Genome map loading seems fine, removed verbose key printing for now.\n    # --- Font Path Setup (Simplified: No Bold Variant) ---\n    font_path_menlo = None\n    print(\"--- DEBUG: main() - Font loading: Attempting to find Menlo font path ---\")\n    try:\n        # Try to get path via Pillow's font finding first (use a common size for path test)\n        font_path_menlo = ImageFont.truetype(\"Menlo\", FONT_GENOME_SIZE).path\n        print(f\"--- DEBUG: main() - Font loading: Found Menlo via Pillow at '{font_path_menlo}' ---\")\n    except IOError:\n        print(\"--- DEBUG: main() - Font loading: Menlo not found via Pillow. Trying explicit OS path. ---\")\n        explicit_os_path = \"/System/Library/Fonts/Menlo.ttc\"  # macOS specific\n        if os.path.exists(explicit_os_path):\n            font_path_menlo = explicit_os_path\n            print(f\"--- DEBUG: main() - Font loading: Found Menlo at explicit path '{font_path_menlo}' ---\")\n        else:\n            print(\"Error: Menlo font not found via Pillow or at common OS path /System/Library/Fonts/Menlo.ttc.\")\n            print(\"Please ensure Menlo font is installed. Using 'Menlo' as a fallback name for ffmpeg to find.\")\n            font_path_menlo = \"Menlo\"  # Fallback: let ffmpeg try to resolve \"Menlo\"\n\n    font_path_menlo_bold = font_path_menlo  # Use regular Menlo for \"bold\" title path\n    print(f\"--- DEBUG: main() - Font loading: font_path_menlo set to: {font_path_menlo} ---\")\n    print(f\"--- DEBUG: main() - Font loading: font_path_menlo_bold (same as regular) set to: {font_path_menlo_bold} ---\")\n\n    print(f\"Attempting to find intro videos in: {INTRO_VIDEOS_DIR}\")\n    if not os.path.isdir(INTRO_VIDEOS_DIR):\n        print(f\"Error: Intro videos directory not found: {INTRO_VIDEOS_DIR}\")\n        return\n\n    intro_files = sorted([f for f in os.listdir(INTRO_VIDEOS_DIR) if f.lower().endswith('.mp4')])\n    total_videos = len(intro_files)\n\n    if not intro_files:\n        print(f\"Warning: No .mp4 files found in {INTRO_VIDEOS_DIR}. Nothing to process.\")\n        print(\"Please ensure the directory exists and contains .mp4 intro videos.\")\n        return\n\n    print(f\"Found {total_videos} .mp4 files to process.\")\n\n    for i, filename in enumerate(intro_files):\n        input_path = os.path.join(INTRO_VIDEOS_DIR, filename)\n        output_filename = f\"{os.path.splitext(filename)[0]}_header.mp4\"\n        output_path = os.path.join(OUTPUT_VIDEOS_DIR, output_filename)\n        \n        print(f\"\\nProcessing video {i+1}/{total_videos}: {filename}\")\n\n        poem_key_from_filename = get_poem_title_from_filename(filename)\n        poem_lookup_key = poem_key_from_filename.lower()\n        print(f\"  Attempting to lookup genome data with derived key: '{poem_lookup_key}' (original from filename map: '{poem_key_from_filename}')\")\n        poem_data = genome_map.get(poem_lookup_key)\n\n        if not poem_data:\n            print(f\"  Warning: No genome data found for '{poem_key_from_filename}'. Skipping header genome text.\")\n            s_line, i_line, c_line = \"S_LINE_N/A\", \"I_LINE_N/A\", \"C_LINE_N/A\"\n        else:\n            s_line = poem_data.get('s_line', '').strip()\n            i_line = poem_data.get('i_line', '').strip()\n            c_line = poem_data.get('c_line', '').strip()\n\n        # Get text widths for scrolling speed calculation\n        print(\"--- DEBUG: main() - Font loading: About to create font_title object ---\")\n        font_title = ImageFont.truetype(font_path_menlo_bold, FONT_TITLE_SIZE)\n        print(\"--- DEBUG: main() - Font loading: Successfully created font_title. About to create font_genome_lines ---\")\n        font_genome_lines = ImageFont.truetype(font_path_menlo, FONT_GENOME_SIZE)\n        print(\"--- DEBUG: main() - Font loading: Successfully created font_genome_lines. About to create font_stats ---\")\n        font_stats = ImageFont.truetype(font_path_menlo, FONT_STATS_SIZE)\n        print(\"--- DEBUG: main() - Font loading: Successfully created font_stats. All font objects for this video loaded. ---\")\n        s_line_width = get_text_width_pil(s_line, font_path_menlo, FONT_GENOME_SIZE)\n        i_line_width = get_text_width_pil(i_line, font_path_menlo, FONT_GENOME_SIZE)\n        c_line_width = get_text_width_pil(c_line, font_path_menlo, FONT_GENOME_SIZE)\n\n        # --- Construct ffmpeg command ---\n        # Video duration for scrolling - assuming 5 seconds for intros\n        # A more robust way would be to ffprobe each video's duration\n        # but for now, let's use a fixed 5s as per earlier discussion.\n        scroll_duration = 5 \n\n        # Static text positions (approximations, can be refined)\n        # Title: Vertically centered in header, x from left padding\n        # Sequence: Below title\n        title_text = poem_key_from_filename.replace(\"_\", \" \") # Make it more readable\n        sequence_text = f\"Intro {i+1} of {total_videos}\"\n\n        # Vertical positioning for the 3 genome lines within the 80px header\n        # Approx 80px / 3 lines = 26.6px per line. Font size 18pt.\n        # Let's try to center them a bit. (Line height of 18pt Menlo is ~22px)\n        # Top of 1st line: (80 - (3*22))/2 = (80-66)/2 = 7px from top of header.\n        # Then each line is ~22px below the previous.\n        s_line_y = (HEADER_HEIGHT / 2) - (FONT_GENOME_SIZE * 1.5) # Center of 3 lines, then up one line height\n        i_line_y = (HEADER_HEIGHT / 2) - (FONT_GENOME_SIZE * 0.5) # Center line\n        c_line_y = (HEADER_HEIGHT / 2) + (FONT_GENOME_SIZE * 0.5) # Bottom line\n        # These Ys are relative to the header's top. FFmpeg drawtext y is from top of video frame.\n\n        # FFmpeg drawtext fontfile path needs to be escaped for Windows if it contains ':',\n        # but on macOS/Linux, direct path is usually fine.\n        # For Menlo.ttc, it's a TrueType Collection. FFmpeg needs to know which font index.\n        # Usually, Menlo Regular is index 0 in Menlo.ttc. Let's assume that.\n        # If bold is needed, it's often index 1 or 2.\n        ffmpeg_font_path = font_path_menlo.replace('\\\\', '/').replace(':', '\\\\:')\n        ffmpeg_font_file_option_title = f\"fontfile='{ffmpeg_font_path}':font='Menlo Bold':fontsize={FONT_TITLE_SIZE}\"\n        # ffmpeg_font_file_option_seq was removed\n        ffmpeg_font_file_option_genome = f\"fontfile='{ffmpeg_font_path}':font='Menlo':fontsize={FONT_GENOME_SIZE}\"\n        ffmpeg_font_file_option_stats = f\"fontfile='{ffmpeg_font_path}':font='Menlo':fontsize={FONT_STATS_SIZE}\"\n        # If using specific indices for .ttc: fontfile='{ffmpeg_font_path}:fontindex=0'\n        # For simplicity, let's try with font names first, ffmpeg might find them if fontconfig is set up.\n        # If not, we'll need to use fontconfig names or direct .ttf paths if Menlo Regular .ttf is available.\n        # For now, assuming Menlo.ttc with font name 'Menlo' works for regular.\n\n        filter_complex_parts = []\n        # 1. Scale original video and prepare it for bottom part of canvas\n        filter_complex_parts.append(\n            f\"[0:v]scale={FINAL_WIDTH}:{VIDEO_CONTENT_HEIGHT}:force_original_aspect_ratio=decrease,pad={FINAL_WIDTH}:{VIDEO_CONTENT_HEIGHT}:(ow-iw)/2:(oh-ih)/2:color=black[scaled_video]\"\n        )\n        # 2. Create header canvas\n        filter_complex_parts.append(\n            f\"color=c={COLOR_BLACK}:s={FINAL_WIDTH}x{HEADER_HEIGHT}:d={scroll_duration}[header_bg]\"\n        )\n        # 3. Draw static text on header_bg\n        # Title Text (centered vertically in header, x from left padding)\n        # Y for title: vertically centered in its allocated space at bottom of header\n        # Title area starts at TITLE_AREA_TOP_Y, height is TITLE_HEIGHT_ALLOCATION\n        # Effective font pixel height for centering (approximate)\n        font_pixel_height_title = FONT_TITLE_SIZE # Approximation\n        title_y_pos = TITLE_AREA_TOP_Y + (TITLE_HEIGHT_ALLOCATION - font_pixel_height_title) / 2\n        # Calculate stats text\n        s_stats_text = get_glyph_stats_text(s_line, \"S\")\n        i_stats_text = get_glyph_stats_text(i_line, \"I\")\n        c_stats_text = get_glyph_stats_text(c_line, \"C\")\n        stats_text = f\"{s_stats_text}  {i_stats_text}  {c_stats_text}\"\n        # Escape text for ffmpeg drawtext filter (specifically %)\n        stats_text_escaped = stats_text.replace('%', '%%')\n\n        # Draw Poem Title (left aligned)\n        filter_complex_parts.append(\n            f\"[header_bg]drawtext={ffmpeg_font_file_option_title}:text='{title_text}':x={TEXT_PADDING}:y={title_y_pos}:fontcolor={COLOR_WHITE}[header_with_title]\"\n        )\n        \n        # Draw Stats Text (to the right of the title, need to calculate X position)\n        # This is a rough estimate for x; precise positioning needs title width.\n        # For now, let's put it at a fixed offset or find a better way.\n        # A simpler way: put stats on a new line or fixed position if too complex.\n        # For now, placing it far right, may overlap if title is long.\n        # A better approach: fixed x for stats, e.g., half-width of header.\n        stats_x_pos = FINAL_WIDTH / 2.5 # Start stats text somewhat to the right\n        font_pixel_height_stats = FONT_STATS_SIZE\n        stats_y_pos = TITLE_AREA_TOP_Y + (TITLE_HEIGHT_ALLOCATION - font_pixel_height_stats) / 2\n\n        filter_complex_parts.append(\n            f\"[header_with_title]drawtext={ffmpeg_font_file_option_stats}:text='{stats_text_escaped}':x={stats_x_pos}:y={stats_y_pos}:fontcolor={COLOR_LIGHT_GRAY}[header_with_all_static_text]\"\n        )\n        \n        # 4. Draw scrolling genome lines (full width)\n        # Y positions for S,I,C lines within their allocated space at top of header\n        s_line_y_scroll = (GLYPH_LINE_HEIGHT_ALLOCATION - FONT_GENOME_SIZE) / 2\n        i_line_y_scroll = s_line_y_scroll + GLYPH_LINE_HEIGHT_ALLOCATION\n        c_line_y_scroll = i_line_y_scroll + GLYPH_LINE_HEIGHT_ALLOCATION\n\n        # Scrolling X formula: text starts with its left edge at FINAL_WIDTH (off-screen right)\n        # and scrolls until its right edge is at 0 (off-screen left).\n        # Total distance to travel = FINAL_WIDTH + text_width.\n        s_scroll_x = f\"'{FINAL_WIDTH} - (({s_line_width} + {FINAL_WIDTH}) / {scroll_duration}) * mod(t,{scroll_duration})'\"\n        filter_complex_parts.append(\n            f\"[header_with_all_static_text]drawtext={ffmpeg_font_file_option_genome}:text='{s_line}':x={s_scroll_x}:y={s_line_y_scroll}:fontcolor={COLOR_SYNTAGMA}[header_s]\"\n        )\n        i_scroll_x = f\"'{FINAL_WIDTH} - (({i_line_width} + {FINAL_WIDTH}) / {scroll_duration}) * mod(t,{scroll_duration})'\"\n        filter_complex_parts.append(\n            f\"[header_s]drawtext={ffmpeg_font_file_option_genome}:text='{i_line}':x={i_scroll_x}:y={i_line_y_scroll}:fontcolor={COLOR_IMAGE_TYPE}[header_si]\"\n        )\n        c_scroll_x = f\"'{FINAL_WIDTH} - (({c_line_width} + {FINAL_WIDTH}) / {scroll_duration}) * mod(t,{scroll_duration})'\"\n        filter_complex_parts.append(\n            f\"[header_si]drawtext={ffmpeg_font_file_option_genome}:text='{c_line}':x={c_scroll_x}:y={c_line_y_scroll}:fontcolor={COLOR_CINEOSIS}[header_sic]\"\n        )\n\n        # 5. Stack header and scaled video\n        filter_complex_parts.append(\n            f\"[header_sic][scaled_video]vstack=inputs=2[final_video]\"\n        )\n\n        ffmpeg_command = [\n            FFMPEG_PATH, '-y', # Overwrite output without asking\n            '-i', input_path,\n            '-filter_complex', \";\".join(filter_complex_parts),\n            '-map', '[final_video]',\n            # If original intro has audio, we might want to map it too: e.g., '-map', '0:a?',\n            # For now, assuming visual only or audio will be handled later.\n            '-c:v', 'libx264', # Or your preferred codec\n            '-preset', 'medium',\n            '-crf', '23',\n            '-r', '24', # Explicitly set frame rate for fluidity\n            '-t', str(scroll_duration), # Ensure output is also 5s\n            output_path\n        ]\n        \n        print(f\"  Executing ffmpeg command:\")\n        # print(\" \".join(ffmpeg_command)) # For debugging, can be very long\n        try:\n            subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n            print(f\"  Successfully created: {output_path}\")\n        except subprocess.CalledProcessError as e:\n            print(f\"  Error processing {filename}:\")\n            print(f\"    STDOUT: {e.stdout}\")\n            print(f\"    STDERR: {e.stderr}\")\n\n    print(\"\\n--- All videos processed ---\")\n\nif __name__ == \"__main__\":\n    print(\"--- DEBUG: Inside __main__ block, about to call main() ---\")\n    main()\n    print(\"--- DEBUG: Call to main() completed ---\")\n",
    "file_references": [
      "symbolic_genome_data.json",
      "01_SH_OutOfLife_...mp4",
      "Resurrecting Atlantis.mp4",
      "SH.mp4",
      "Poem Name.mp4",
      "{os.path.splitext(filename)[0]}_header.mp4",
      "/Users/gaia/resurrecting atlantis",
      "_N/A",
      "{line_label}: N/A",
      "\n    \n    total_glyphs = len(glyphs)\n    counts = {}\n    for glyph in glyphs:\n        counts[glyph] = counts.get(glyph, 0) + 1\n    \n    stats_parts = []\n    # Sort by glyph for consistent order, or could sort by count\n    for glyph, count in sorted(counts.items()):\n        percentage = (count / total_glyphs) * 100\n        stats_parts.append(f",
      "/System/Library/Fonts/Menlo.ttc",
      "Error: Menlo font not found via Pillow or at common OS path /System/Library/Fonts/Menlo.ttc.",
      "\\nProcessing video {i+1}/{total_videos}: {filename}",
      "S_LINE_N/A",
      "I_LINE_N/A",
      "C_LINE_N/A",
      "\n\n        # Vertical positioning for the 3 genome lines within the 80px header\n        # Approx 80px / 3 lines = 26.6px per line. Font size 18pt.\n        # Let",
      ",\n        # but on macOS/Linux, direct path is usually fine.\n        # For Menlo.ttc, it",
      "[0:v]scale={FINAL_WIDTH}:{VIDEO_CONTENT_HEIGHT}:force_original_aspect_ratio=decrease,pad={FINAL_WIDTH}:{VIDEO_CONTENT_HEIGHT}:(ow-iw)/2:(oh-ih)/2:color=black[scaled_video]",
      "\n        )\n        # 3. Draw static text on header_bg\n        # Title Text (centered vertically in header, x from left padding)\n        # Y for title: vertically centered in its allocated space at bottom of header\n        # Title area starts at TITLE_AREA_TOP_Y, height is TITLE_HEIGHT_ALLOCATION\n        # Effective font pixel height for centering (approximate)\n        font_pixel_height_title = FONT_TITLE_SIZE # Approximation\n        title_y_pos = TITLE_AREA_TOP_Y + (TITLE_HEIGHT_ALLOCATION - font_pixel_height_title) / 2\n        # Calculate stats text\n        s_stats_text = get_glyph_stats_text(s_line, ",
      "s put it at a fixed offset or find a better way.\n        # A simpler way: put stats on a new line or fixed position if too complex.\n        # For now, placing it far right, may overlap if title is long.\n        # A better approach: fixed x for stats, e.g., half-width of header.\n        stats_x_pos = FINAL_WIDTH / 2.5 # Start stats text somewhat to the right\n        font_pixel_height_stats = FONT_STATS_SIZE\n        stats_y_pos = TITLE_AREA_TOP_Y + (TITLE_HEIGHT_ALLOCATION - font_pixel_height_stats) / 2\n\n        filter_complex_parts.append(\n            f",
      "\n        )\n        \n        # 4. Draw scrolling genome lines (full width)\n        # Y positions for S,I,C lines within their allocated space at top of header\n        s_line_y_scroll = (GLYPH_LINE_HEIGHT_ALLOCATION - FONT_GENOME_SIZE) / 2\n        i_line_y_scroll = s_line_y_scroll + GLYPH_LINE_HEIGHT_ALLOCATION\n        c_line_y_scroll = i_line_y_scroll + GLYPH_LINE_HEIGHT_ALLOCATION\n\n        # Scrolling X formula: text starts with its left edge at FINAL_WIDTH (off-screen right)\n        # and scrolls until its right edge is at 0 (off-screen left).\n        # Total distance to travel = FINAL_WIDTH + text_width.\n        s_scroll_x = f",
      "{FINAL_WIDTH} - (({s_line_width} + {FINAL_WIDTH}) / {scroll_duration}) * mod(t,{scroll_duration})",
      "{FINAL_WIDTH} - (({i_line_width} + {FINAL_WIDTH}) / {scroll_duration}) * mod(t,{scroll_duration})",
      "{FINAL_WIDTH} - (({c_line_width} + {FINAL_WIDTH}) / {scroll_duration}) * mod(t,{scroll_duration})"
    ],
    "subprocess_calls": [
      {
        "type": "run",
        "snippet": "ffmpeg_command, check=True, capture_output=True, text=True"
      }
    ],
    "imports": [
      "os",
      "json",
      "subprocess",
      "shutil",
      "PIL",
      "re"
    ],
    "generates": [],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "ARKADU/kern/taxonomy_scan.py",
    "size": 6366,
    "lines": 188,
    "source": "#!/usr/bin/env python3\n\"\"\"\nARKADU Taxonomy Scanner\nMaps Kingdom\u2192Phylum\u2192Class\u2192Order\u2192Family\u2192Genus\u2192Species hierarchy\nBased on actual filesystem structure\n\"\"\"\n\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\n\n# Taxonomic rank names (Linnaean hierarchy)\nRANKS = [\n    'kingdom',   # depth 1: ANT, CAT, DOG, etc.\n    'phylum',    # depth 2: CAT/WHISKER, CAT/PAW\n    'class',     # depth 3: CAT/WHISKER/MEDIA\n    'order',     # depth 4: CAT/WHISKER/MEDIA/MR07\n    'family',    # depth 5: variant type (raw, export, etc.)\n    'genus',     # depth 6: sequence position\n    'species'    # file itself (extension)\n]\n\ndef scan_taxonomy(root_path):\n    \"\"\"\n    Walk filesystem and assign taxonomic ranks to each path.\n    \"\"\"\n    root = Path(root_path)\n    \n    # Track chambers (directories) and their contents\n    chambers = defaultdict(lambda: {\n        'path': None,\n        'rank': None,\n        'depth': 0,\n        'file_count': 0,\n        'total_bytes': 0,\n        'species': defaultdict(int),  # file types\n        'children': set()\n    })\n    \n    # Track all files\n    artifacts = []\n    \n    for path in root.rglob('*'):\n        # Skip venv/cache\n        if any(skip in str(path) for skip in ['.venv', 'venv', '__pycache__', '.git', 'node_modules']):\n            continue\n        \n        rel_path = path.relative_to(root)\n        parts = rel_path.parts\n        depth = len(parts)\n        \n        if path.is_file():\n            # This is a species (individual file)\n            species = path.suffix.lower() or 'no_ext'\n            \n            # Build taxonomic ID\n            tax_components = list(parts[:-1])  # All but filename\n            tax_id_parts = tax_components + [species]\n            tax_id = '.'.join(tax_id_parts)\n            \n            # Determine ranks for each level\n            taxonomy = {}\n            for i, component in enumerate(parts[:-1]):\n                if i < len(RANKS):\n                    taxonomy[RANKS[i]] = component\n            taxonomy['species'] = species\n            \n            artifact = {\n                'taxonomic_id': tax_id,\n                'path': str(rel_path),\n                'taxonomy': taxonomy,\n                'depth': depth,\n                'size': path.stat().st_size,\n                'species': species\n            }\n            artifacts.append(artifact)\n            \n            # Update parent chamber stats\n            if len(parts) > 1:\n                chamber_path = str(Path(*parts[:-1]))\n                chambers[chamber_path]['file_count'] += 1\n                chambers[chamber_path]['total_bytes'] += artifact['size']\n                chambers[chamber_path]['species'][species] += 1\n        \n        elif path.is_dir():\n            # This is a chamber (directory)\n            chamber_path = str(rel_path)\n            rank_idx = depth - 1\n            rank = RANKS[rank_idx] if rank_idx < len(RANKS) else 'species'\n            \n            chambers[chamber_path]['path'] = chamber_path\n            chambers[chamber_path]['rank'] = rank\n            chambers[chamber_path]['depth'] = depth\n            \n            # Track parent-child relationships\n            if depth > 1:\n                parent_path = str(Path(*parts[:-1]))\n                chambers[parent_path]['children'].add(chamber_path)\n    \n    # Convert sets to lists for JSON serialization\n    for chamber in chambers.values():\n        chamber['children'] = list(chamber['children'])\n    \n    return artifacts, dict(chambers)\n\ndef generate_chamber_summaries(chambers):\n    \"\"\"\n    Generate per-chamber summaries with dominant species.\n    \"\"\"\n    summaries = []\n    \n    for chamber_path, data in chambers.items():\n        if data['file_count'] > 0:\n            # Find dominant species (top 3 file types)\n            top_species = sorted(\n                data['species'].items(),\n                key=lambda x: x[1],\n                reverse=True\n            )[:3]\n            \n            summary = {\n                'chamber': chamber_path,\n                'rank': data['rank'],\n                'depth': data['depth'],\n                'file_count': data['file_count'],\n                'total_bytes': data['total_bytes'],\n                'total_mb': round(data['total_bytes'] / 1024 / 1024, 2),\n                'dominant_species': [s[0] for s in top_species],\n                'species_distribution': dict(data['species']),\n                'child_count': len(data['children'])\n            }\n            summaries.append(summary)\n    \n    return sorted(summaries, key=lambda x: x['total_bytes'], reverse=True)\n\n# Run scanner\nif __name__ == '__main__':\n    print(\"ARKADU Taxonomy Scanner v1.0\")\n    print(\"=\" * 50)\n    print(\"Scanning filesystem hierarchy...\")\n    \n    artifacts, chambers = scan_taxonomy('.')\n    \n    print(f\"\\n\u2713 Scanned {len(artifacts)} artifacts\")\n    print(f\"\u2713 Found {len(chambers)} chambers\")\n    \n    # Generate summaries\n    print(\"\\nGenerating chamber summaries...\")\n    summaries = generate_chamber_summaries(chambers)\n    \n    # Create output directory\n    Path('ARKADU/sys').mkdir(parents=True, exist_ok=True)\n    \n    # Write artifacts (taxonomic IDs for each file)\n    with open('ARKADU/sys/taxonomy.jsonl', 'w') as f:\n        for artifact in artifacts:\n            f.write(json.dumps(artifact) + '\\n')\n    \n    print(f\"\u2713 Output: ARKADU/sys/taxonomy.jsonl ({len(artifacts)} entries)\")\n    \n    # Write chamber summaries\n    with open('ARKADU/sys/chambers.jsonl', 'w') as f:\n        for summary in summaries:\n            f.write(json.dumps(summary) + '\\n')\n    \n    print(f\"\u2713 Output: ARKADU/sys/chambers.jsonl ({len(summaries)} entries)\")\n    \n    # Show top kingdoms (depth 1)\n    print(\"\\n\" + \"=\" * 50)\n    print(\"TOP KINGDOMS (Depth 1)\")\n    print(\"=\" * 50)\n    \n    kingdoms = [s for s in summaries if s['depth'] == 1]\n    for k in sorted(kingdoms, key=lambda x: x['total_bytes'], reverse=True)[:15]:\n        mb = k['total_mb']\n        bar = '\u2588' * min(int(mb / 100), 40)\n        print(f\"{k['chamber']:15s} {k['file_count']:5d} files  {mb:8.1f} MB  {bar}\")\n    \n    # Show sample taxonomic IDs\n    print(\"\\n\" + \"=\" * 50)\n    print(\"SAMPLE TAXONOMIC IDs\")\n    print(\"=\" * 50)\n    \n    for artifact in artifacts[:5]:\n        print(f\"\\n{artifact['path']}\")\n        print(f\"  Tax ID: {artifact['taxonomic_id']}\")\n        print(f\"  Taxonomy: {artifact['taxonomy']}\")\n",
    "file_references": [
      ",    # depth 2: CAT/WHISKER, CAT/PAW\n    ",
      ",     # depth 3: CAT/WHISKER/MEDIA\n    ",
      ",     # depth 4: CAT/WHISKER/MEDIA/MR07\n    ",
      "):\n        # Skip venv/cache\n        if any(skip in str(path) for skip in [",
      "] / 1024 / 1024, 2),\n                ",
      "ARKADU/sys",
      "ARKADU/sys/taxonomy.jsonl",
      "\u2713 Output: ARKADU/sys/taxonomy.jsonl ({len(artifacts)} entries)",
      "ARKADU/sys/chambers.jsonl",
      "\u2713 Output: ARKADU/sys/chambers.jsonl ({len(summaries)} entries)",
      " * min(int(mb / 100), 40)\n        print(f"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "pathlib",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [],
    "docstring": "ARKADU Taxonomy Scanner\nMaps Kingdom\u2192Phylum\u2192Class\u2192Order\u2192Family\u2192Genus\u2192Species hierarchy\nBased on actual filesystem structure"
  },
  {
    "path": "ARKADU/kern/deep_scan.py",
    "size": 9182,
    "lines": 277,
    "source": "#!/usr/bin/env python3\n\"\"\"\nARKADU Deep Scanner\nComprehensive analysis of ALL files:\n- Full media catalog (from original manifest)\n- Deep code analysis (read every .py file)\n- JSON content analysis (read every .json file)\n- Dependency mapping (which files reference which)\n- Generation tracking (which code generates which files)\n\"\"\"\n\nimport json\nimport re\nimport ast\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef load_original_manifest():\n    \"\"\"Load the original full media-manifest.json\"\"\"\n    try:\n        with open('media-manifest.json') as f:\n            data = json.load(f)\n        return data\n    except:\n        return {}\n\ndef deep_scan_python_file(py_path):\n    \"\"\"\n    Deep analysis of Python file:\n    - Read full source code\n    - Extract file references (paths in strings)\n    - Find subprocess calls (ffmpeg, etc.)\n    - Parse imports\n    - Find output file generation\n    \"\"\"\n    try:\n        source = py_path.read_text(encoding='utf-8', errors='ignore')\n    except:\n        return None\n    \n    analysis = {\n        'path': str(py_path),\n        'size': py_path.stat().st_size,\n        'lines': len(source.split('\\n')),\n        'source': source,  # Full source code\n        'file_references': [],\n        'subprocess_calls': [],\n        'imports': [],\n        'generates': [],\n        'reads': []\n    }\n    \n    # Extract docstring\n    try:\n        tree = ast.parse(source)\n        analysis['docstring'] = ast.get_docstring(tree)\n    except:\n        analysis['docstring'] = None\n    \n    # Find file path references (any string that looks like a path)\n    for match in re.finditer(r'[\"\\']([^\"\\']+\\.(mp4|png|jpg|wav|json|mp3|mov|avi))[\"\\']', source, re.IGNORECASE):\n        path = match.group(1)\n        analysis['file_references'].append(path)\n    \n    # Find directory references\n    for match in re.finditer(r'[\"\\']([^\"\\']+/[^\"\\']+)[\"\\']', source):\n        path = match.group(1)\n        if '/' in path and not path.startswith('http'):\n            analysis['file_references'].append(path)\n    \n    # Find subprocess calls\n    if 'subprocess' in source or 'ffmpeg' in source:\n        for match in re.finditer(r'subprocess\\.(run|call|Popen)\\s*\\((.*?)\\)', source, re.DOTALL):\n            analysis['subprocess_calls'].append({\n                'type': match.group(1),\n                'snippet': match.group(2)[:200]\n            })\n    \n    # Find imports\n    for match in re.finditer(r'^(?:from|import)\\s+(\\S+)', source, re.MULTILINE):\n        analysis['imports'].append(match.group(1))\n    \n    # Detect file generation (open for writing, write, save, export)\n    if re.search(r'open\\([^)]+[\"\\']w[\"\\']', source):\n        analysis['generates'].append('writes_files')\n    if re.search(r'\\.save\\(|\\.write\\(|\\.export\\(', source):\n        analysis['generates'].append('generates_output')\n    \n    # Detect file reading\n    if re.search(r'open\\([^)]+[\"\\']r[\"\\']', source):\n        analysis['reads'].append('reads_files')\n    if re.search(r'json\\.load|\\.read\\(', source):\n        analysis['reads'].append('reads_data')\n    \n    return analysis\n\ndef deep_scan_json_file(json_path):\n    \"\"\"\n    Deep analysis of JSON file:\n    - Read full content\n    - Detect structure (array, object)\n    - Count entries\n    - Sample data\n    - Find prompts/ekphrasis\n    \"\"\"\n    try:\n        content = json_path.read_text(encoding='utf-8', errors='ignore')\n        data = json.loads(content)\n    except:\n        return None\n    \n    analysis = {\n        'path': str(json_path),\n        'size': json_path.stat().st_size,\n        'content': content,  # Full JSON content\n        'data': data,  # Parsed data\n        'structure': type(data).__name__,\n        'has_prompts': 'operativeEkphrasis' in content,\n        'file_references': []\n    }\n    \n    # If it's an array, count entries\n    if isinstance(data, list):\n        analysis['entry_count'] = len(data)\n        analysis['sample'] = data[:3] if len(data) > 0 else []\n        \n        # Check for prompts in entries\n        if len(data) > 0 and isinstance(data[0], dict):\n            if 'operativeEkphrasis' in data[0]:\n                analysis['prompts'] = [\n                    item.get('operativeEkphrasis', '')[:100]\n                    for item in data[:5]\n                    if 'operativeEkphrasis' in item\n                ]\n    \n    # Find file path references in JSON\n    for match in re.finditer(r'[\"\\']([^\"\\']+\\.(mp4|png|jpg|wav|mp3|json))[\"\\']', content, re.IGNORECASE):\n        path = match.group(1)\n        analysis['file_references'].append(path)\n    \n    return analysis\n\ndef build_dependency_graph(py_analyses, json_analyses, media_manifest):\n    \"\"\"\n    Build complete dependency graph:\n    - Which Python files reference which JSON files\n    - Which Python files generate which media files\n    - Which JSON files reference which media files\n    \"\"\"\n    graph = {\n        'nodes': [],\n        'edges': []\n    }\n    \n    # Add Python nodes\n    for py in py_analyses:\n        graph['nodes'].append({\n            'id': py['path'],\n            'type': 'python',\n            'label': Path(py['path']).name,\n            'lines': py['lines'],\n            'generates': len(py['generates']) > 0,\n            'reads': len(py['reads']) > 0\n        })\n    \n    # Add JSON nodes\n    for js in json_analyses:\n        graph['nodes'].append({\n            'id': js['path'],\n            'type': 'json',\n            'label': Path(js['path']).name,\n            'size': js['size'],\n            'has_prompts': js.get('has_prompts', False)\n        })\n    \n    # Add media nodes (from manifest)\n    if 'images' in media_manifest:\n        for img in media_manifest['images'][:100]:  # Sample\n            graph['nodes'].append({\n                'id': img['path'],\n                'type': 'image',\n                'label': Path(img['path']).name\n            })\n    \n    # Build edges (Python \u2192 JSON references)\n    for py in py_analyses:\n        for ref in py['file_references']:\n            # Find matching JSON\n            matching = [js for js in json_analyses if ref in js['path'] or Path(ref).name in js['path']]\n            for match in matching:\n                graph['edges'].append({\n                    'source': py['path'],\n                    'target': match['path'],\n                    'type': 'references'\n                })\n    \n    return graph\n\ndef run_deep_scan():\n    \"\"\"Run complete deep scan\"\"\"\n    print(\"ARKADU Deep Scanner\")\n    print(\"=\" * 60)\n    \n    # 1. Load original manifest\n    print(\"\\n[1/4] Loading original media manifest...\")\n    manifest = load_original_manifest()\n    print(f\"  Loaded manifest with {len(manifest.get('images', []))} images, \"\n          f\"{len(manifest.get('videos', []))} videos\")\n    \n    # 2. Deep scan all Python files\n    print(\"\\n[2/4] Deep scanning Python files...\")\n    py_files = list(Path('.').rglob('*.py'))\n    py_files = [f for f in py_files if '.venv' not in str(f) and 'site-packages' not in str(f)]\n    \n    py_analyses = []\n    for i, py_file in enumerate(py_files):\n        if i % 50 == 0:\n            print(f\"  Scanned {i}/{len(py_files)} Python files...\")\n        analysis = deep_scan_python_file(py_file)\n        if analysis:\n            py_analyses.append(analysis)\n    \n    print(f\"  \u2713 Analyzed {len(py_analyses)} Python files\")\n    \n    # 3. Deep scan all JSON files\n    print(\"\\n[3/4] Deep scanning JSON files...\")\n    json_files = list(Path('.').rglob('*.json'))\n    json_files = [f for f in json_files if '.venv' not in str(f) and 'node_modules' not in str(f)]\n    \n    json_analyses = []\n    for i, json_file in enumerate(json_files):\n        if i % 50 == 0:\n            print(f\"  Scanned {i}/{len(json_files)} JSON files...\")\n        analysis = deep_scan_json_file(json_file)\n        if analysis:\n            json_analyses.append(analysis)\n    \n    print(f\"  \u2713 Analyzed {len(json_analyses)} JSON files\")\n    \n    # 4. Build dependency graph\n    print(\"\\n[4/4] Building dependency graph...\")\n    graph = build_dependency_graph(py_analyses, json_analyses, manifest)\n    print(f\"  \u2713 Graph: {len(graph['nodes'])} nodes, {len(graph['edges'])} edges\")\n    \n    # Save outputs\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Saving outputs...\")\n    \n    Path('ARKADU/deep').mkdir(parents=True, exist_ok=True)\n    \n    # Save Python analyses\n    with open('ARKADU/deep/python_files.json', 'w') as f:\n        json.dump(py_analyses, f, indent=2)\n    print(f\"  \u2713 ARKADU/deep/python_files.json ({len(py_analyses)} files)\")\n    \n    # Save JSON analyses  \n    with open('ARKADU/deep/json_files.json', 'w') as f:\n        json.dump(json_analyses, f, indent=2)\n    print(f\"  \u2713 ARKADU/deep/json_files.json ({len(json_analyses)} files)\")\n    \n    # Save dependency graph\n    with open('ARKADU/deep/dependency_graph.json', 'w') as f:\n        json.dump(graph, f, indent=2)\n    print(f\"  \u2713 ARKADU/deep/dependency_graph.json\")\n    \n    # Copy original manifest\n    import shutil\n    shutil.copy('media-manifest.json', 'ARKADU/deep/media_manifest.json')\n    print(f\"  \u2713 ARKADU/deep/media_manifest.json (original preserved)\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"\u2713 Deep scan complete\")\n    print(\"=\" * 60)\n\nif __name__ == '__main__':\n    run_deep_scan()\n",
    "file_references": [
      "Load the original full media-manifest.json",
      "media-manifest.json",
      "*.json",
      "ARKADU/deep/python_files.json",
      "ARKADU/deep/json_files.json",
      "ARKADU/deep/dependency_graph.json",
      "  \u2713 ARKADU/deep/dependency_graph.json",
      "media-manifest.json",
      "ARKADU/deep/media_manifest.json",
      "]+/[^",
      "\n    Deep analysis of JSON file:\n    - Read full content\n    - Detect structure (array, object)\n    - Count entries\n    - Sample data\n    - Find prompts/ekphrasis\n    ",
      "\\n[1/4] Loading original media manifest...",
      "\\n[2/4] Deep scanning Python files...",
      "  Scanned {i}/{len(py_files)} Python files...",
      "\\n[3/4] Deep scanning JSON files...",
      "  Scanned {i}/{len(json_files)} JSON files...",
      "\\n[4/4] Building dependency graph...",
      "ARKADU/deep",
      "ARKADU/deep/python_files.json",
      "  \u2713 ARKADU/deep/python_files.json ({len(py_analyses)} files)",
      "ARKADU/deep/json_files.json",
      "  \u2713 ARKADU/deep/json_files.json ({len(json_analyses)} files)",
      "ARKADU/deep/dependency_graph.json",
      "  \u2713 ARKADU/deep/dependency_graph.json",
      "ARKADU/deep/media_manifest.json",
      "  \u2713 ARKADU/deep/media_manifest.json (original preserved)"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "re",
      "ast",
      "pathlib",
      "collections"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": "ARKADU Deep Scanner\nComprehensive analysis of ALL files:\n- Full media catalog (from original manifest)\n- Deep code analysis (read every .py file)\n- JSON content analysis (read every .json file)\n- Dependency mapping (which files reference which)\n- Generation tracking (which code generates which files)"
  },
  {
    "path": "ARKADU/kern/ekphrasis_trace.py",
    "size": 6574,
    "lines": 207,
    "source": "#!/usr/bin/env python3\n\"\"\"\nARKADU Ekphrasis Tracer\nMaps operative ekphrasis chains:\n  JSON (prompts) \u2192 Python (assembly scripts) \u2192 Media (generated output)\n\"\"\"\n\nimport json\nimport re\nimport ast\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef find_json_with_prompts():\n    \"\"\"\n    Find all JSON files containing operativeEkphrasis.\n    \"\"\"\n    prompt_files = []\n    root = Path('.')\n    \n    for json_file in root.rglob('*.json'):\n        # Skip venv/cache\n        if any(skip in str(json_file) for skip in ['.venv', 'node_modules', '__pycache__']):\n            continue\n        \n        try:\n            content = json_file.read_text(encoding='utf-8', errors='ignore')\n            if 'operativeEkphrasis' in content:\n                data = json.loads(content)\n                if isinstance(data, list):\n                    prompt_files.append({\n                        'path': str(json_file),\n                        'entry_count': len(data),\n                        'sample_prompts': [\n                            item.get('operativeEkphrasis', '')[:80]\n                            for item in data[:3]\n                            if 'operativeEkphrasis' in item\n                        ]\n                    })\n        except:\n            pass\n    \n    return prompt_files\n\ndef find_python_scripts():\n    \"\"\"\n    Find all Python scripts (exclude venv).\n    \"\"\"\n    scripts = []\n    root = Path('.')\n    \n    for py_file in root.rglob('*.py'):\n        # Skip venv/cache\n        if any(skip in str(py_file) for skip in ['.venv', 'venv', 'node_modules', '__pycache__', 'site-packages']):\n            continue\n        \n        scripts.append(py_file)\n    \n    return scripts\n\ndef trace_json_usage_in_script(script_path):\n    \"\"\"\n    Find JSON files referenced in Python script.\n    \"\"\"\n    try:\n        content = script_path.read_text(encoding='utf-8', errors='ignore')\n    except:\n        return []\n    \n    # Find JSON file references\n    json_refs = []\n    \n    # Pattern 1: direct file paths\n    for match in re.finditer(r'[\"\\']([^\"\\']+\\.json)[\"\\']', content):\n        json_path = match.group(1)\n        json_refs.append(json_path)\n    \n    # Pattern 2: variable assignments\n    for match in re.finditer(r'(\\w+)\\s*=\\s*[\"\\']([^\"\\']+\\.json)[\"\\']', content):\n        var_name = match.group(1)\n        json_path = match.group(2)\n        json_refs.append(json_path)\n    \n    return list(set(json_refs))\n\ndef trace_ffmpeg_usage_in_script(script_path):\n    \"\"\"\n    Find ffmpeg/subprocess calls in Python script.\n    \"\"\"\n    try:\n        content = script_path.read_text(encoding='utf-8', errors='ignore')\n    except:\n        return []\n    \n    ffmpeg_calls = []\n    \n    # Check for ffmpeg usage\n    if 'ffmpeg' in content or 'subprocess' in content:\n        # Extract docstring (intent)\n        try:\n            tree = ast.parse(content)\n            docstring = ast.get_docstring(tree)\n        except:\n            docstring = None\n        \n        # Check for drawtext (text overlay)\n        has_drawtext = 'drawtext' in content\n        \n        # Check for overlay operations\n        has_overlay = any(term in content for term in ['overlay', 'vf', 'filter'])\n        \n        ffmpeg_calls.append({\n            'script': str(script_path),\n            'intent': docstring[:200] if docstring else None,\n            'uses_drawtext': has_drawtext,\n            'uses_overlay': has_overlay,\n            'likely_generates_video': '.mp4' in content or '.mov' in content\n        })\n    \n    return ffmpeg_calls\n\ndef build_ekphrasis_chains():\n    \"\"\"\n    Connect JSON prompts \u2192 Python scripts \u2192 Video outputs.\n    \"\"\"\n    print(\"Finding JSON files with prompts...\")\n    prompt_files = find_json_with_prompts()\n    print(f\"  Found {len(prompt_files)} JSON files with prompts\")\n    \n    print(\"\\nFinding Python scripts...\")\n    scripts = find_python_scripts()\n    print(f\"  Found {len(scripts)} Python scripts\")\n    \n    print(\"\\nTracing connections...\")\n    chains = []\n    script_to_json = defaultdict(list)\n    script_to_ffmpeg = {}\n    \n    for script in scripts:\n        # Find JSON references\n        json_refs = trace_json_usage_in_script(script)\n        if json_refs:\n            script_to_json[str(script)] = json_refs\n        \n        # Find ffmpeg usage\n        ffmpeg_usage = trace_ffmpeg_usage_in_script(script)\n        if ffmpeg_usage:\n            script_to_ffmpeg[str(script)] = ffmpeg_usage[0]\n    \n    # Build chains\n    for script_path, json_refs in script_to_json.items():\n        # Check if this script uses ffmpeg\n        ffmpeg_info = script_to_ffmpeg.get(script_path)\n        \n        # Match JSON refs to actual prompt files\n        for json_ref in json_refs:\n            # Find matching prompt file\n            matching_prompts = [\n                pf for pf in prompt_files\n                if json_ref in pf['path'] or Path(json_ref).name in pf['path']\n            ]\n            \n            if matching_prompts:\n                for prompt_file in matching_prompts:\n                    chain = {\n                        'type': 'operative_ekphrasis_chain',\n                        'prompt_file': prompt_file['path'],\n                        'script': script_path,\n                        'sample_prompts': prompt_file['sample_prompts'],\n                        'uses_ffmpeg': ffmpeg_info is not None if ffmpeg_info else False,\n                        'uses_drawtext': ffmpeg_info['uses_drawtext'] if ffmpeg_info else False,\n                        'intent': ffmpeg_info['intent'] if ffmpeg_info else None\n                    }\n                    chains.append(chain)\n    \n    return chains\n\n# Run tracer\nif __name__ == '__main__':\n    print(\"ARKADU Ekphrasis Tracer v1.0\")\n    print(\"=\" * 50)\n    \n    chains = build_ekphrasis_chains()\n    \n    print(f\"\\n\u2713 Found {len(chains)} ekphrasis chains\")\n    \n    # Create output directory\n    Path('ARKADU/sys').mkdir(parents=True, exist_ok=True)\n    \n    # Write output\n    with open('ARKADU/sys/ekphrasis.jsonl', 'w') as f:\n        for chain in chains:\n            f.write(json.dumps(chain) + '\\n')\n    \n    print(f\"\u2713 Output: ARKADU/sys/ekphrasis.jsonl\")\n    \n    # Show samples\n    if chains:\n        print(f\"\\nSample chains:\")\n        for chain in chains[:3]:\n            print(f\"\\n  {Path(chain['prompt_file']).name}\")\n            print(f\"    \u2192 {Path(chain['script']).name}\")\n            if chain['sample_prompts']:\n                print(f\"    Prompt: {chain['sample_prompts'][0][:60]}...\")\n            if chain['uses_drawtext']:\n                print(f\"    Uses drawtext overlay \u2713\")\n",
    "file_references": [
      "*.json",
      "):\n        # Skip venv/cache\n        if any(skip in str(json_file) for skip in [",
      "):\n        # Skip venv/cache\n        if any(skip in str(py_file) for skip in [",
      "\n    Find ffmpeg/subprocess calls in Python script.\n    ",
      "ARKADU/sys",
      "ARKADU/sys/ekphrasis.jsonl",
      "\u2713 Output: ARKADU/sys/ekphrasis.jsonl"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "re",
      "ast",
      "pathlib",
      "collections"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": "ARKADU Ekphrasis Tracer\nMaps operative ekphrasis chains:\n  JSON (prompts) \u2192 Python (assembly scripts) \u2192 Media (generated output)"
  },
  {
    "path": "ARKADU/kern/primitive_scan.py",
    "size": 4172,
    "lines": 128,
    "source": "#!/usr/bin/env python3\n\"\"\"\nARKADU Primitive Scanner\nMost basic scan - no assumptions, just facts:\n- path, size, extension, depth, modified time\n- detect known filename patterns\n- identify JSON files with prompts\n\"\"\"\n\nimport json\nimport re\nfrom pathlib import Path\nfrom datetime import datetime\n\ndef scan_primitive(root_path):\n    \"\"\"\n    Most basic scan - captures raw file metadata.\n    \"\"\"\n    root = Path(root_path)\n    \n    for path in root.rglob('*'):\n        # Skip venv/cache\n        if any(skip in str(path) for skip in ['.venv', 'venv', '__pycache__', '.git', 'node_modules']):\n            continue\n            \n        if path.is_file():\n            yield {\n                'path': str(path.relative_to(root)),\n                'size': path.stat().st_size,\n                'ext': path.suffix.lower(),\n                'depth': len(path.relative_to(root).parts),\n                'mtime': datetime.fromtimestamp(path.stat().st_mtime).isoformat(),\n                'name': path.name\n            }\n\ndef detect_pattern(filename):\n    \"\"\"\n    Detect known filename patterns.\n    \"\"\"\n    # CAT/WHISKER pattern: WGY058_RI__Memory_Storage_Retrieval__POET_and_MOTHE_uuid_0.png\n    wgy_match = re.match(r'WGY(\\d+)_(\\w+)__([^_]+)__(.+)_([a-f0-9-]{36})_(\\d+)', filename)\n    if wgy_match:\n        return {\n            'schema': 'CAT_WHISKER',\n            'shot_num': wgy_match.group(1),\n            'operator': wgy_match.group(2),\n            'operation': wgy_match.group(3),\n            'uuid': wgy_match.group(5),\n            'variant': wgy_match.group(6)\n        }\n    \n    # HORSE pattern: 01_SH_OutOfLife_000000_header_prompt.mp4\n    horse_match = re.match(r'(\\d+)_(\\w+)_(.+?)_(\\d+)_header_prompt', filename)\n    if horse_match:\n        return {\n            'schema': 'HORSE_HEADER',\n            'track_num': horse_match.group(1),\n            'track_code': horse_match.group(2),\n            'title': horse_match.group(3),\n            'timestamp': horse_match.group(4)\n        }\n    \n    return {'schema': 'unknown'}\n\ndef check_for_prompts(path_obj, artifact):\n    \"\"\"\n    If JSON, check if it contains operativeEkphrasis prompts.\n    \"\"\"\n    if artifact['ext'] == '.json':\n        try:\n            content = path_obj.read_text(encoding='utf-8', errors='ignore')\n            if 'operativeEkphrasis' in content:\n                # Sample first operativeEkphrasis\n                data = json.loads(content)\n                if isinstance(data, list) and len(data) > 0:\n                    first_item = data[0]\n                    if 'operativeEkphrasis' in first_item:\n                        return {\n                            'has_prompts': True,\n                            'sample_prompt': first_item.get('operativeEkphrasis', '')[:100],\n                            'entry_count': len(data)\n                        }\n        except:\n            pass\n    return {'has_prompts': False}\n\n# Run scan\nif __name__ == '__main__':\n    print(\"ARKADU Primitive Scanner v1.0\")\n    print(\"=\" * 50)\n    print(\"Scanning...\")\n    \n    root = Path('.')\n    count = 0\n    prompt_files = []\n    \n    # Create output directory\n    (root / 'ARKADU' / 'sys').mkdir(parents=True, exist_ok=True)\n    \n    with open('ARKADU/sys/primitive.jsonl', 'w') as f:\n        for artifact in scan_primitive('.'):\n            path_obj = Path(artifact['path'])\n            \n            # Add pattern detection\n            artifact['pattern'] = detect_pattern(artifact['name'])\n            \n            # Check for prompts\n            artifact['prompts'] = check_for_prompts(path_obj, artifact)\n            \n            if artifact['prompts']['has_prompts']:\n                prompt_files.append(artifact['path'])\n            \n            f.write(json.dumps(artifact) + '\\n')\n            count += 1\n            \n            if count % 1000 == 0:\n                print(f\"  {count} files...\")\n    \n    print(\"=\" * 50)\n    print(f\"\u2713 Scanned {count} files\")\n    print(f\"\u2713 Found {len(prompt_files)} JSON files with prompts\")\n    print(f\"\u2713 Output: ARKADU/sys/primitive.jsonl\")\n    \n    if prompt_files:\n        print(f\"\\nSample prompt files:\")\n        for pf in prompt_files[:5]:\n            print(f\"  - {pf}\")\n",
    "file_references": [
      "):\n        # Skip venv/cache\n        if any(skip in str(path) for skip in [",
      "\n    # CAT/WHISKER pattern: WGY058_RI__Memory_Storage_Retrieval__POET_and_MOTHE_uuid_0.png\n    wgy_match = re.match(r",
      ")\n    count = 0\n    prompt_files = []\n    \n    # Create output directory\n    (root / ",
      " / ",
      "ARKADU/sys/primitive.jsonl",
      "\u2713 Output: ARKADU/sys/primitive.jsonl"
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "re",
      "pathlib",
      "datetime"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_data"
    ],
    "docstring": "ARKADU Primitive Scanner\nMost basic scan - no assumptions, just facts:\n- path, size, extension, depth, modified time\n- detect known filename patterns\n- identify JSON files with prompts"
  },
  {
    "path": "ARKADU/kern/analyze.py",
    "size": 5160,
    "lines": 159,
    "source": "#!/usr/bin/env python3\n\"\"\"\nARKADU Analyzer\nGenerates insights from scanned data\n\"\"\"\n\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef load_jsonl(path):\n    \"\"\"Load JSONL file into list.\"\"\"\n    data = []\n    with open(path) as f:\n        for line in f:\n            data.append(json.loads(line))\n    return data\n\ndef analyze_chambers():\n    \"\"\"Analyze chamber distribution.\"\"\"\n    chambers = load_jsonl('ARKADU/sys/chambers.jsonl')\n    \n    # Group by rank\n    by_rank = defaultdict(list)\n    for c in chambers:\n        by_rank[c['rank']].append(c)\n    \n    # Find largest chambers at each rank\n    print(\"=\" * 60)\n    print(\"LARGEST CHAMBERS BY RANK\")\n    print(\"=\" * 60)\n    \n    for rank in ['kingdom', 'phylum', 'class', 'order']:\n        if rank in by_rank:\n            print(f\"\\n{rank.upper()} (depth {['kingdom', 'phylum', 'class', 'order'].index(rank) + 1}):\")\n            top = sorted(by_rank[rank], key=lambda x: x['total_bytes'], reverse=True)[:5]\n            for c in top:\n                gb = c['total_bytes'] / 1024 / 1024 / 1024\n                print(f\"  {c['chamber']:45s} {c['file_count']:5d} files  {gb:6.2f} GB\")\n\ndef analyze_species():\n    \"\"\"Analyze species (file types) distribution.\"\"\"\n    artifacts = load_jsonl('ARKADU/sys/taxonomy.jsonl')\n    \n    # Count by species\n    species_count = defaultdict(int)\n    species_bytes = defaultdict(int)\n    \n    for a in artifacts:\n        species = a['species']\n        species_count[species] += 1\n        species_bytes[species] += a['size']\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"TOP SPECIES (File Types)\")\n    print(\"=\" * 60)\n    \n    top_species = sorted(species_bytes.items(), key=lambda x: x[1], reverse=True)[:15]\n    for species, total_bytes in top_species:\n        count = species_count[species]\n        gb = total_bytes / 1024 / 1024 / 1024\n        avg_mb = (total_bytes / count) / 1024 / 1024\n        print(f\"{species:10s} {count:6d} files  {gb:8.2f} GB  (avg: {avg_mb:6.1f} MB/file)\")\n\ndef analyze_ekphrasis():\n    \"\"\"Analyze operative ekphrasis chains.\"\"\"\n    chains = load_jsonl('ARKADU/sys/ekphrasis.jsonl')\n    \n    # Find chains with ffmpeg\n    ffmpeg_chains = [c for c in chains if c.get('uses_ffmpeg')]\n    drawtext_chains = [c for c in chains if c.get('uses_drawtext')]\n    \n    # Count unique prompts files\n    unique_prompts = set(c['prompt_file'] for c in chains)\n    unique_scripts = set(c['script'] for c in chains)\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"OPERATIVE EKPHRASIS ECOLOGY\")\n    print(\"=\" * 60)\n    print(f\"\\nTotal chains traced:        {len(chains)}\")\n    print(f\"Unique prompt files:        {len(unique_prompts)}\")\n    print(f\"Unique assembly scripts:    {len(unique_scripts)}\")\n    print(f\"Chains using ffmpeg:        {len(ffmpeg_chains)}\")\n    print(f\"Chains using drawtext:      {len(drawtext_chains)}\")\n    \n    # Show top scripts by chain count\n    script_count = defaultdict(int)\n    for c in chains:\n        script_name = Path(c['script']).name\n        script_count[script_name] += 1\n    \n    print(f\"\\nTop assembly scripts:\")\n    top_scripts = sorted(script_count.items(), key=lambda x: x[1], reverse=True)[:10]\n    for script, count in top_scripts:\n        print(f\"  {script:40s} {count:3d} chains\")\n\ndef analyze_patterns():\n    \"\"\"Analyze filename patterns.\"\"\"\n    artifacts = load_jsonl('ARKADU/sys/taxonomy.jsonl')\n    \n    # Count by pattern schema\n    pattern_count = defaultdict(int)\n    for a in artifacts:\n        if 'pattern' in a:\n            schema = a['pattern'].get('schema', 'unknown')\n            pattern_count[schema] += 1\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"FILENAME PATTERNS\")\n    print(\"=\" * 60)\n    \n    for schema, count in sorted(pattern_count.items(), key=lambda x: x[1], reverse=True):\n        if count > 5:  # Only show significant patterns\n            print(f\"{schema:20s} {count:6d} files\")\n\ndef generate_summary_stats():\n    \"\"\"Generate overall summary statistics.\"\"\"\n    artifacts = load_jsonl('ARKADU/sys/taxonomy.jsonl')\n    chambers = load_jsonl('ARKADU/sys/chambers.jsonl')\n    chains = load_jsonl('ARKADU/sys/ekphrasis.jsonl')\n    \n    total_bytes = sum(a['size'] for a in artifacts)\n    total_gb = total_bytes / 1024 / 1024 / 1024\n    \n    # Count prompts\n    prompt_count = 0\n    for a in artifacts:\n        if 'prompts' in a and a['prompts'].get('has_prompts'):\n            prompt_count += 1\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"ARKADU OS - SYSTEM SUMMARY\")\n    print(\"=\" * 60)\n    print(f\"\"\"\nTotal artifacts:           {len(artifacts):,}\nTotal chambers:            {len(chambers):,}\nTotal storage:             {total_gb:.2f} GB\nJSON files with prompts:   {prompt_count:,}\nEkphrasis chains traced:   {len(chains):,}\n\nDeepest taxonomy depth:    {max(a['depth'] for a in artifacts)}\nUnique species (types):    {len(set(a['species'] for a in artifacts))}\n\"\"\")\n\n# Run analysis\nif __name__ == '__main__':\n    print(\"\\nARKADU Analyzer v1.0\")\n    print(\"=\" * 60)\n    \n    generate_summary_stats()\n    analyze_chambers()\n    analyze_species()\n    analyze_patterns()\n    analyze_ekphrasis()\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"Analysis complete\")\n    print(\"=\" * 60)\n",
    "file_references": [
      "ARKADU/sys/chambers.jsonl",
      "] / 1024 / 1024 / 1024\n                print(f",
      "ARKADU/sys/taxonomy.jsonl",
      " * 60)\n    \n    top_species = sorted(species_bytes.items(), key=lambda x: x[1], reverse=True)[:15]\n    for species, total_bytes in top_species:\n        count = species_count[species]\n        gb = total_bytes / 1024 / 1024 / 1024\n        avg_mb = (total_bytes / count) / 1024 / 1024\n        print(f",
      "ARKADU/sys/ekphrasis.jsonl",
      "ARKADU/sys/taxonomy.jsonl",
      "ARKADU/sys/taxonomy.jsonl",
      "ARKADU/sys/chambers.jsonl",
      "ARKADU/sys/ekphrasis.jsonl",
      "] for a in artifacts)\n    total_gb = total_bytes / 1024 / 1024 / 1024\n    \n    # Count prompts\n    prompt_count = 0\n    for a in artifacts:\n        if "
    ],
    "subprocess_calls": [],
    "imports": [
      "json",
      "pathlib",
      "collections"
    ],
    "generates": [],
    "reads": [
      "reads_data"
    ],
    "docstring": "ARKADU Analyzer\nGenerates insights from scanned data"
  },
  {
    "path": "ANT/APEX/parse_poems.py",
    "size": 4023,
    "lines": 117,
    "source": "import os\nimport re\nfrom bs4 import BeautifulSoup\n\n# --- Configuration ---\nINPUT_FILE = 'CULTURAL SYSTEM FOR POEMS.htm'\nOUTPUT_DIR = 'poems'\n# Based on the user's feedback, poem titles are in h2 tags inside these divs.\nSPLIT_BY_CLASS = 'notion-sub_header-block'\n# -------------------\n\n# Get absolute paths\nbase_dir = os.path.dirname(os.path.abspath(__file__))\ninput_path = os.path.join(base_dir, INPUT_FILE)\noutput_path = os.path.join(base_dir, OUTPUT_DIR)\n\n# --- Helper Functions ---\ndef sanitize_filename(name):\n    \"\"\"Remove invalid characters from a string to make it a valid filename.\"\"\"\n    # Remove the leading number and period (e.g., \"1. \")\n    name = re.sub(r'^\\d+\\.\\s*', '', name).strip()\n    # Remove quotes and other punctuation\n    name = name.replace('\"', '')\n    # Replace spaces and special characters with underscores\n    return re.sub(r'[^a-zA-Z0-9_\\-]+', '_', name).lower()\n\n# --- Main Script ---\nif not os.path.exists(input_path):\n    print(f\"Error: Input file not found at {input_path}\")\n    exit()\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_path, exist_ok=True)\n\nprint(\"Reading and parsing the HTML file...\")\nwith open(input_path, 'r', encoding='utf-8') as f:\n    soup = BeautifulSoup(f, 'lxml')\n\n# Extract the head to preserve styles\nhead_content = soup.find('head')\nif not head_content:\n    print(\"Warning: Could not find <head> tag. Styles will not be preserved.\")\n\n# Find the main content div that contains all the poem blocks\nh1_title = soup.select_one('.notion-page-block > h1')\nif not h1_title:\n    print(\"Error: Could not find the H1 title inside .notion-page-block.\")\n    exit()\n\ncontent_div = h1_title.find_next_sibling('div')\nif not content_div:\n    print(\"Error: Could not find the content div next to the H1 title.\")\n    exit()\n\n# Find all poem title blocks\ntitle_blocks = content_div.find_all('div', class_=SPLIT_BY_CLASS, recursive=False)\n\nif not title_blocks:\n    print(f\"Error: No poem titles found. Could not find any divs with class '{SPLIT_BY_CLASS}'.\")\n    exit()\n\nprint(f\"Found {len(title_blocks)} poems. Processing...\")\n\nfor i, title_block in enumerate(title_blocks):\n    # Extract title from the h2 tag\n    h2 = title_block.find('h2')\n    if not h2:\n        print(f\"Warning: Skipping block {i+1} because it has no h2 tag.\")\n        continue\n    \n    poem_title = h2.get_text(strip=True)\n    file_name = sanitize_filename(poem_title) + '.html'\n    file_path = os.path.join(output_path, file_name)\n\n    # Start collecting content for this poem\n    poem_content_tags = [title_block]\n    for sibling in title_block.find_next_siblings():\n        # Stop when we hit the next title block\n        if sibling.name == 'div' and SPLIT_BY_CLASS in sibling.get('class', []):\n            break\n        poem_content_tags.append(sibling)\n\n    # Create a new HTML document for the poem\n    poem_soup = BeautifulSoup('<html><head></head><body></body></html>', 'lxml')\n    \n    # Add original head\n    if head_content:\n        poem_soup.head.replace_with(head_content)\n\n    # Create a body structure that mimics the original for styling\n    body = poem_soup.body\n    body['class'] = ['notion-body', 'dark']\n    main_container = poem_soup.new_tag('div', attrs={'class': 'notion-frame'})\n    body.append(main_container)\n    page_container = poem_soup.new_tag('div', attrs={'class': 'notion-page-scroller'})\n    main_container.append(page_container)\n\n    # Add a simplified header\n    header = poem_soup.new_tag('div', attrs={'class': 'notion-page-header'})\n    page_container.append(header)\n\n    # Create the content area\n    poem_body_container = poem_soup.new_tag('div', attrs={'class': 'notion-page-block'})\n    page_container.append(poem_body_container)\n\n    # Add all the collected tags for the poem\n    for tag in poem_content_tags:\n        poem_body_container.append(tag)\n\n    # Write the new HTML file\n    with open(file_path, 'w', encoding='utf-8') as f:\n        f.write(str(poem_soup))\n\n    print(f\"  - Created '{file_name}'\")\n\nprint(\"\\nProcessing complete.\")\n",
    "file_references": [
      "<html><head></head><body></body></html>"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "bs4"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files"
    ],
    "docstring": null
  },
  {
    "path": "ANT/ANTHOLOGY/convert_poems_v2.py",
    "size": 2308,
    "lines": 68,
    "source": "import os\nfrom bs4 import BeautifulSoup\n\n# --- Configuration ---\nSOURCE_DIR = 'generated_poems'\nTEMPLATE_PATH = 'ALPS/fl-00.html'\nOUTPUT_DIR = 'ALPS'\n\n# --- Main Script ---\ndef convert_poems():\n    \"\"\"\n    Reads original poem HTML files, extracts their body content, and injects it\n    into a new, standardized template.\n    \"\"\"\n    print('--- Starting Poem Conversion (V2) ---')\n\n    # 1. Read the new template file\n    try:\n        with open(TEMPLATE_PATH, 'r', encoding='utf-8') as f:\n            template_content = f.read()\n    except FileNotFoundError:\n        print(f\"ERROR: Template file not found at '{TEMPLATE_PATH}'\")\n        return\n\n    # Ensure the output directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # 2. Iterate through source poem files\n    source_files = [f for f in os.listdir(SOURCE_DIR) if f.startswith('poem_') and f.endswith('.html')]\n\n    if not source_files:\n        print(f\"No poem files found in '{SOURCE_DIR}'\")\n        return\n\n    for filename in source_files:\n        print(f\"Processing: {filename}\")\n        source_path = os.path.join(SOURCE_DIR, filename)\n\n        try:\n            with open(source_path, 'r', encoding='utf-8') as f:\n                soup = BeautifulSoup(f, 'html.parser')\n\n            # 3. Extract title and the entire body content\n            title_tag = soup.find('h1')\n            poem_title = title_tag.get_text(strip=True) if title_tag else 'Untitled Poem'\n            \n            body_tag = soup.find('body')\n            poem_body_html = ''.join(str(child) for child in body_tag.contents) if body_tag else '<p>Content not found.</p>'\n\n            # 4. Replace placeholders in the template\n            new_html = template_content.replace('__POEM_TITLE__', poem_title)\n            new_html = new_html.replace('__POEM_BODY__', poem_body_html)\n\n            # 5. Save the new file to the output directory\n            output_path = os.path.join(OUTPUT_DIR, filename)\n            with open(output_path, 'w', encoding='utf-8') as f:\n                f.write(new_html)\n            print(f\"Successfully created: {filename}\")\n\n        except Exception as e:\n            print(f\"Could not process {filename}. Error: {e}\")\n\n    print('\\n--- Poem Conversion Complete ---')\n\nif __name__ == '__main__':\n    convert_poems()\n",
    "file_references": [
      "ALPS/fl-00.html",
      "<p>Content not found.</p>"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "bs4"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "ANT/ANTHOLOGY/convert_poems.py",
    "size": 3818,
    "lines": 105,
    "source": "import os\nimport re\nfrom bs4 import BeautifulSoup\n\n# --- Configuration ---\nBASE_DIR = '/Users/gaia/resurrecting atlantis/ANT/ANTHOLOGY/'\nSOURCE_DIR = os.path.join(BASE_DIR, 'generated_poems')\nTARGET_DIR = os.path.join(BASE_DIR, 'ALPS')\nTEMPLATE_PATH = os.path.join(TARGET_DIR, 'ool-00.html')\n\n# --- Main Conversion Logic ---\n\ndef get_poem_content(soup):\n    \"\"\"Extracts the main poem content from the original file.\"\"\"\n    # The poem content seems to be inside a <pre> tag with a specific class.\n    # Based on previous files, this seems to be a reliable way to find it.\n    content_tag = soup.find('pre', class_='py-4')\n    if content_tag:\n        return content_tag.prettify()\n    return \"<p>Poem content not found.</p>\"\n\ndef get_poem_title(soup):\n    \"\"\"Extracts the poem title from the original file.\"\"\"\n    title_tag = soup.find('h1')\n    if title_tag:\n        return title_tag.get_text(strip=True)\n    return \"Untitled Poem\"\n\ndef convert_poem(source_path, template_soup):\n    \"\"\"Converts a single poem file using the template.\"\"\"\n    print(f\"Processing: {os.path.basename(source_path)}\")\n\n    with open(source_path, 'r', encoding='utf-8') as f:\n        source_soup = BeautifulSoup(f, 'html.parser')\n\n    # 1. Extract data from the source poem\n    title = get_poem_title(source_soup)\n    content = get_poem_content(source_soup)\n\n    # 2. Create a new soup from the template for modification\n    new_soup = BeautifulSoup(str(template_soup), 'html.parser')\n\n    # 3. Inject data into the template\n    # Update the main title\n    title_tag = new_soup.find('h1', class_='poem-title')\n    if title_tag:\n        title_tag.string = title\n\n    # Update the HTML <title> tag\n    html_title_tag = new_soup.find('title')\n    if html_title_tag:\n        html_title_tag.string = f\"{title} - Interactive Analysis\"\n\n    # Replace the analysis diagram with the actual poem content\n    # In the template, the diagram is a good placeholder to replace.\n    diagram_section = new_soup.find('div', class_='diagram-section')\n    if diagram_section:\n        # We'll create a new div to hold the poem, styled appropriately\n        poem_container = new_soup.new_tag('div', **{'class': 'poem-content-container'})\n        poem_container.style = \"background: #fff; border: 1px solid #e5e7eb; border-radius: 8px; padding: 2rem; margin: 2rem 0; font-family: 'Crimson Pro', serif; font-size: 1.1rem; line-height: 1.8;\"\n        poem_container.append(BeautifulSoup(content, 'html.parser'))\n        diagram_section.replace_with(poem_container)\n\n    # 4. Determine the new filename\n    base_filename = os.path.basename(source_path)\n    target_path = os.path.join(TARGET_DIR, base_filename)\n\n    # 5. Write the new file\n    with open(target_path, 'w', encoding='utf-8') as f:\n        f.write(str(new_soup))\n    print(f\"Successfully created: {os.path.basename(target_path)}\\n\")\n\ndef main():\n    \"\"\"Main function to run the conversion process.\"\"\"\n    print(\"--- Starting Poem Conversion ---\")\n\n    # Ensure target directory exists\n    if not os.path.exists(TARGET_DIR):\n        os.makedirs(TARGET_DIR)\n\n    # Load the template\n    try:\n        with open(TEMPLATE_PATH, 'r', encoding='utf-8') as f:\n            template_soup = BeautifulSoup(f, 'html.parser')\n    except FileNotFoundError:\n        print(f\"ERROR: Template file not found at {TEMPLATE_PATH}\")\n        return\n\n    # Get all poem files to convert\n    poem_files = [f for f in os.listdir(SOURCE_DIR) if f.startswith('poem_') and f.endswith('.html')]\n\n    if not poem_files:\n        print(f\"No poem files found in {SOURCE_DIR}\")\n        return\n\n    # Process each poem\n    for filename in poem_files:\n        source_path = os.path.join(SOURCE_DIR, filename)\n        convert_poem(source_path, template_soup)\n\n    print(\"--- Poem Conversion Complete ---\")\n\nif __name__ == '__main__':\n    main()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/ANT/ANTHOLOGY/",
      "<p>Poem content not found.</p>"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "bs4"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files"
    ],
    "docstring": null
  },
  {
    "path": "ANT/ANTHOLOGY/convert_poems_v3.py",
    "size": 2788,
    "lines": 75,
    "source": "import os\nfrom bs4 import BeautifulSoup\n\n# --- Configuration ---\nSOURCE_DIR = 'generated_poems'\nTEMPLATE_PATH = 'ALPS/ool-00.html' # Using the correct interactive template\nOUTPUT_DIR = 'ALPS'\n\n# --- Main Script ---\ndef convert_poems_to_interactive_standard():\n    \"\"\"\n    Reads original poem HTML files, extracts their title and content, and injects\n    them into the rich interactive template (ool-00.html).\n    \"\"\"\n    print('--- Starting Poem Conversion to Interactive Standard (V3) ---')\n\n    # 1. Read the interactive template file\n    try:\n        with open(TEMPLATE_PATH, 'r', encoding='utf-8') as f:\n            template_soup = BeautifulSoup(f, 'html.parser')\n    except FileNotFoundError:\n        print(f\"ERROR: Template file not found at '{TEMPLATE_PATH}'\")\n        return\n\n    # Ensure the output directory exists\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n\n    # 2. Iterate through source poem files\n    source_files = [f for f in os.listdir(SOURCE_DIR) if f.startswith('poem_') and f.endswith('.html')]\n\n    if not source_files:\n        print(f\"No poem files found in '{SOURCE_DIR}'\")\n        return\n\n    for filename in source_files:\n        print(f\"Processing: {filename}\")\n        source_path = os.path.join(SOURCE_DIR, filename)\n\n        try:\n            with open(source_path, 'r', encoding='utf-8') as f:\n                source_soup = BeautifulSoup(f, 'html.parser')\n\n            # In the source file, find the body content\n            body_content = source_soup.body\n            if not body_content:\n                print(f\"Skipping {filename}: could not find body content.\")\n                continue\n\n            # Find the placeholder in the template. We'll replace the Mermaid diagram.\n            # The placeholder is the <pre> tag with a specific ID.\n            placeholder = template_soup.find('pre', id='24025705-7012-80cc-90c5-ca3a21e93700')\n\n            if not placeholder:\n                print(f\"Skipping {filename}: template placeholder not found.\")\n                continue\n\n            # Replace the placeholder with the entire body content from the source file.\n            # We extract the children of the body to avoid having a <body> inside a <body>.\n            placeholder.replace_with(*body_content.contents)\n\n            # Save the new file\n            output_path = os.path.join(OUTPUT_DIR, filename)\n            with open(output_path, 'w', encoding='utf-8') as f:\n                f.write(str(current_template_soup.prettify()))\n            print(f\"  - Successfully created: {filename}\")\n\n        except Exception as e:\n            print(f\"  - ERROR: Could not process {filename}. Details: {e}\")\n\n    print('\\n--- Poem Conversion Complete ---')\n\nif __name__ == '__main__':\n    convert_poems_to_interactive_standard()\n",
    "file_references": [
      "ALPS/ool-00.html"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "bs4"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files"
    ],
    "docstring": null
  },
  {
    "path": "ANT/ANTHOLOGY/generate-poems-pages.py",
    "size": 6752,
    "lines": 164,
    "source": "import re\nfrom bs4 import BeautifulSoup\nimport os\nimport shutil\n\ndef create_poem_pages(html_content, output_dir=\"output_poems\"):\n    \"\"\"\n    Parses the HTML content, creates individual HTML pages for each poem,\n    and generates an index page linking to them.\n\n    Args:\n        html_content (str): The full HTML content as a string.\n        output_dir (str): The directory where the output files will be saved.\n    \"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    # 1. Extract the common CSS style block\n    style_tag = soup.find('head').find('style')\n    if not style_tag:\n        print(\"Warning: Could not find the <style> tag in the original HTML.\")\n        common_css = \"\"\n    else:\n        common_css = style_tag.prettify()\n\n    # Create output directory\n    if os.path.exists(output_dir):\n        shutil.rmtree(output_dir) # Clear previous output\n    os.makedirs(output_dir)\n\n    all_poems_info = [] # To store data for the index page\n\n    # Find all main poem blocks\n    poem_figure_blocks = soup.find_all('figure', class_='block-color-gray_background callout')\n    \n    # Store original poem blocks and their subsequent content for easier processing\n    # Each entry will be (poem_number, poem_title, content_tags_list)\n    extracted_poem_contents = [] \n\n    for i, figure_block in enumerate(poem_figure_blocks):\n        # Extract poem number and title from the h1 within the figure block\n        h1_tag = figure_block.find('h1')\n        full_title = h1_tag.get_text(strip=True) if h1_tag else f\"Poem {i+1}\"\n        \n        # Extract number and clean title for filename\n        match = re.match(r'^(\\d+)\\.\\s*[\"\\']?(.*?)[\"\\']?$', full_title)\n        if match:\n            poem_number = int(match.group(1))\n            poem_name = match.group(2).strip()\n        else:\n            poem_number = i + 1\n            poem_name = full_title.split('.', 1)[-1].strip() # Fallback\n        \n        # Clean poem name for filename (remove special characters, replace spaces)\n        safe_poem_name = re.sub(r'[^\\w\\s-]', '', poem_name) # Keep letters, numbers, spaces, hyphens\n        safe_poem_name = re.sub(r'\\s+', '_', safe_poem_name).lower()\n        \n        # File name for the individual poem page\n        file_name = f\"poem_{poem_number}_{safe_poem_name}.html\"\n        \n        # Collect all content tags for this poem, starting from the figure_block\n        current_tags_for_poem = [figure_block]\n        \n        # Find the next poem's figure block to know where to stop collecting content\n        next_poem_start_tag = poem_figure_blocks[i+1] if i + 1 < len(poem_figure_blocks) else None\n        \n        # Iterate through siblings after the current figure_block until the next poem or end of document\n        current_sibling = figure_block.find_next_sibling()\n        while current_sibling:\n            if current_sibling == next_poem_start_tag:\n                break # Stop if we hit the next poem's figure\n            current_tags_for_poem.append(current_sibling)\n            current_sibling = current_sibling.find_next_sibling()\n            \n        extracted_poem_contents.append({\n            'number': poem_number,\n            'title': poem_name, # Use the cleaner name without the number for display\n            'full_title': full_title, # For the page title\n            'filename': file_name,\n            'content_tags': current_tags_for_poem\n        })\n\n    # 2. Generate individual poem HTML files\n    for poem_data in extracted_poem_contents:\n        new_soup = BeautifulSoup(\"<!DOCTYPE html><html><head></head><body></body></html>\", 'html.parser')\n        \n        # Add common CSS\n        new_soup.head.append(BeautifulSoup(common_css, 'html.parser'))\n        \n        # Set page title\n        title_tag = new_soup.new_tag(\"title\")\n        title_tag.string = f\"Poem {poem_data['full_title']}\"\n        new_soup.head.append(title_tag)\n        \n        # Add poem content to the body\n        for tag in poem_data['content_tags']:\n            # Create a new tag from its string representation to avoid ownership issues\n            new_soup.body.append(BeautifulSoup(str(tag), 'html.parser'))\n            \n        # Add a \"Back to Index\" link at the bottom\n        back_link_div = new_soup.new_tag(\"div\")\n        back_link_div['style'] = \"margin-top: 2em; text-align: center;\"\n        back_link = new_soup.new_tag(\"a\", href=\"index.html\")\n        back_link.string = \"\u2190 Back to Poem Index\"\n        back_link_div.append(back_link)\n        new_soup.body.append(back_link_div)\n\n        # Write to file\n        file_path = os.path.join(output_dir, poem_data['filename'])\n        with open(file_path, 'w', encoding='utf-8') as f:\n            f.write(new_soup.prettify())\n        print(f\"Generated: {file_path}\")\n\n    # 3. Generate index.html\n    index_soup = BeautifulSoup(\"<!DOCTYPE html><html><head></head><body></body></html>\", 'html.parser')\n    \n    # Add common CSS\n    index_soup.head.append(BeautifulSoup(common_css, 'html.parser'))\n    \n    # Set page title\n    title_tag = index_soup.new_tag(\"title\")\n    title_tag.string = \"Poem Index\"\n    index_soup.head.append(title_tag)\n\n    # Add main heading\n    h1_index = index_soup.new_tag(\"h1\")\n    h1_index.string = \"Index of Poems\"\n    index_soup.body.append(h1_index)\n\n    # Add ordered list of poems\n    ol_tag = index_soup.new_tag(\"ol\")\n    for poem_data in extracted_poem_contents:\n        li_tag = index_soup.new_tag(\"li\")\n        a_tag = index_soup.new_tag(\"a\", href=poem_data['filename'])\n        a_tag.string = f\"{poem_data['number']}. \\\"{poem_data['title']}\\\"\"\n        li_tag.append(a_tag)\n        ol_tag.append(li_tag)\n    index_soup.body.append(ol_tag)\n\n    # Write index to file\n    index_file_path = os.path.join(output_dir, \"index.html\")\n    with open(index_file_path, 'w', encoding='utf-8') as f:\n        f.write(index_soup.prettify())\n    print(f\"\\nGenerated: {index_file_path}\")\n    print(f\"\\nAll poems and index page generated in '{output_dir}' directory.\")\n\n\n# --- Main execution ---\nif __name__ == \"__main__\":\n    # Path to your large HTML file\n    source_html_path = 'CULTURAL SYSTEM FOR POEMS 24025705701280edad0ae1c7195ef2d4.html'\n    output_directory = '/Users/gaia/resurrecting atlantis/ANT/ANTHOLOGY/generated_poems'\n\n    # Check if the source file exists\n    if not os.path.exists(source_html_path):\n        print(f\"Error: Source HTML file not found at {source_html_path}\")\n    else:\n        # Read the content of the HTML file\n        with open(source_html_path, 'r', encoding='utf-8') as f:\n            html_content = f.read()\n\n        # Call the function to create the poem pages\n        create_poem_pages(html_content, output_dir=output_directory)\n\n        print(f\"Successfully generated poem pages. Check the {output_directory} directory.\")",
    "file_references": [
      "<!DOCTYPE html><html><head></head><body></body></html>",
      "<!DOCTYPE html><html><head></head><body></body></html>",
      "/Users/gaia/resurrecting atlantis/ANT/ANTHOLOGY/generated_poems"
    ],
    "subprocess_calls": [],
    "imports": [
      "re",
      "bs4",
      "os",
      "shutil"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "CAT/EYE/generate_clip_descriptions.py",
    "size": 3792,
    "lines": 80,
    "source": "import os\nimport csv\nfrom PIL import Image\nimport clip_interrogator as ci_library # Import the library itself\n\n# --- Configuration ---\nIMAGE_DIR = \"/Users/gaia/resurrecting atlantis/CAT/EYE/LA Thompson- WYGWYL 2025\"\nOUTPUT_CSV = \"/Users/gaia/resurrecting atlantis/CAT/EYE/la_thompson_descriptions.csv\"\n# You can choose different CLIP models, e.g., 'ViT-B-32/openai', 'ViT-L-14/openai'\n# 'ViT-L-14/openai' is generally a good balance of performance and quality.\nCLIP_MODEL_NAME = 'ViT-L-14/openai'\n\n# --- Main Script ---\ndef generate_descriptions():\n    \"\"\"Generates descriptions for images in IMAGE_DIR and saves them to OUTPUT_CSV.\"\"\"\n    if not os.path.isdir(IMAGE_DIR):\n        print(f\"Error: Image directory not found at {IMAGE_DIR}\")\n        return\n\n    # Initialize ClipInterrogator\n    # Config will automatically try to use CUDA if available, otherwise CPU.\n    # To force CPU: ci_config = Config(clip_model_name=CLIP_MODEL_NAME, device='cpu')\n    print(f\"Initializing ClipInterrogator with model {CLIP_MODEL_NAME}...\")\n    print(\"This might take a moment, especially on the first run as models may be downloaded.\")\n    \n    try:\n        ci_config = ci_library.Config(clip_model_name=CLIP_MODEL_NAME, device='cpu')\n        print(f\"Initializing Interrogator with device='cpu' for model {CLIP_MODEL_NAME}\")\n        ci = ci_library.Interrogator(ci_config)\n        print(f\"Interrogator initialized. Using device: {ci_config.device}.\")\n    except Exception as e:\n        print(f\"Error during Interrogator initialization: {e}\")\n        print(\"Please ensure you have the necessary libraries installed and configured correctly (see script header).\")\n        return\n\n    image_files = [\n        f for f in os.listdir(IMAGE_DIR)\n        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))\n    ]\n\n    if not image_files:\n        print(f\"No image files found in {IMAGE_DIR}\")\n        return\n\n    print(f\"Found {len(image_files)} images to process. This may take a while depending on your hardware...\")\n\n    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as csvfile:\n        csv_writer = csv.writer(csvfile)\n        csv_writer.writerow(['image_path', 'description'])  # CSV Header\n\n        for i, image_name in enumerate(image_files):\n            image_path = os.path.join(IMAGE_DIR, image_name)\n            print(f\"Processing ({i + 1}/{len(image_files)}): {image_name}\")\n            try:\n                image = Image.open(image_path).convert('RGB')\n                \n                # Generate description using clip-interrogator\n                # interrogate() provides a balanced description.\n                # Other options: interrogate_fast(), interrogate_classic()\n                description = ci.interrogate(image)\n                \n                csv_writer.writerow([image_path, description])\n                # Print a snippet of the description for progress indication\n                print(f\"  -> Description: {description[:100]}...\")\n            except Exception as e:\n                print(f\"Error processing {image_name}: {e}\")\n                csv_writer.writerow([image_path, f\"Error processing image: {e}\"])\n\n    print(f\"\\nProcessing complete. Descriptions saved to {OUTPUT_CSV}\")\n\nif __name__ == \"__main__\":\n    print(\"CLIP Image Description Generator\")\n    print(\"--------------------------------\")\n    print(\"This script requires the 'clip-interrogator', 'torch', 'torchvision', 'Pillow', and 'open_clip_torch' libraries.\")\n    print(\"You can typically install them using pip:\")\n    print(\"  pip install clip-interrogator torch torchvision Pillow open_clip_torch\")\n    print(\"Make sure you have an active internet connection for the first run if models need to be downloaded.\")\n    print(\"--------------------------------\\n\")\n    generate_descriptions()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/CAT/EYE/LA Thompson- WYGWYL 2025",
      "/Users/gaia/resurrecting atlantis/CAT/EYE/la_thompson_descriptions.csv",
      "ViT-B-32/openai",
      "ViT-L-14/openai",
      "ViT-L-14/openai",
      "ViT-L-14/openai",
      "Processing ({i + 1}/{len(image_files)}): {image_name}"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "csv",
      "PIL",
      "clip_interrogator"
    ],
    "generates": [
      "writes_files"
    ],
    "reads": [],
    "docstring": null
  },
  {
    "path": "CAT/WHISKER/whisk-merm.py",
    "size": 69847,
    "lines": 713,
    "source": "import re\nfrom bs4 import BeautifulSoup\n\n# The provided SVG code\nsvg_code = \"\"\"<svg xmlns=\"http://www.w3.org/2000/svg\" aria-roledescription=\"flowchart-v2\" role=\"graphics-document document\" viewBox=\"-28 -28 3369.2724609375 1101.803955078125\" style=\"max-width: 3305.2724609375px;\" class=\"flowchart\" width=\"3369.2724609375\" id=\"mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86\" height=\"1101.803955078125\">\n    <rect x=\"-28\" y=\"-28\" width=\"3369.2724609375\" height=\"1101.803955078125\" fill=\"#ffffff\"/>\n    <style>\n        #mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86 {\n            font-family: ui-sans-serif,-apple-system,BlinkMacSystemFont,\"Segoe UI Variable Display\",\"Segoe UI\",Helvetica,\"Apple Color Emoji\",Arial,sans-serif,\"Segoe UI Emoji\",\"Segoe UI Symbol\";\n            font-size: 16px;\n            fill: #000000;\n        }\n\n        #mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86 .error-icon {\n            fill: #552222;\n        }\n\n        #mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86 .error-text {\n            fill: #552222;\n            stroke: #552222;\n        }\n\n                        <span class=\"nodeLabel\">\n                            <p>Model-FOR: Actively process past &integrate; embrace emotional complexity; build anew; seek community &ancestral wisdom; participate in dissolution &creation.</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(1911.2725187937417, 170.94885635375977)\" id=\"flowchart-F1-316\" class=\"node default\">\n            <rect height=\"125.96590423583984\" width=\"260\" y=\"-62.98295211791992\" x=\"-130\" style=\"\" class=\"basic label-container\"/>\n            <g transform=\"translate(-100, -47.98295211791992)\" style=\"\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"95.96590423583984\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\">\n                        <span class=\"nodeLabel\">\n                            <p>\u25fb Photo-realistic Visuals (consistent across all scenes, e.g., WGY001, WGY041)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(2206.2725187937417, 170.94885635375977)\" id=\"flowchart-F2-318\" class=\"node default\">\n            <rect height=\"125.96590423583984\" width=\"260\" y=\"-62.98295211791992\" x=\"-130\" style=\"\" class=\"basic label-container\"/>\n            <g transform=\"translate(-100, -47.98295211791992)\" style=\"\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"95.96590423583984\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\">\n                        <span class=\"nodeLabel\">\n                            <p>\u25fb Meticulous Detail in Ekphrasis &Conditioning (e.g., 'subtle dust motes' WGY001)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(2541.2725187937417, 158.95311737060547)\" id=\"flowchart-F3-320\" class=\"node default\">\n            <rect height=\"149.95738220214844\" width=\"260\" y=\"-74.97869110107422\" x=\"-130\" style=\"\" class=\"basic label-container\"/>\n            <g transform=\"translate(-100, -59.97869110107422)\" style=\"\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"119.95738220214844\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\">\n                        <span class=\"nodeLabel\">\n                            <p>\u25fb Strategic Cineosis Functions (PI for subjective truth: WGY005; AF for emotional facts: WGY033)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(2502.461559931437, 922.8380470275879)\" id=\"flowchart-F4-322\" class=\"node default\">\n            <rect height=\"125.96590423583984\" width=\"260\" y=\"-62.98295211791992\" x=\"-130\" style=\"\" class=\"basic label-container\"/>\n            <g transform=\"translate(-100, -47.98295211791992)\" style=\"\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"95.96590423583984\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\">\n                        <span class=\"nodeLabel\">\n                            <p>\u25fb Consistent Character Arcs (POET's evolving states, even with unseen EX)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(3139.918352127075, 922.8380470275879)\" id=\"flowchart-G1-332\" class=\"node default\">\n            <rect height=\"125.96590423583984\" width=\"260\" y=\"-62.98295211791992\" x=\"-130\" style=\"\" class=\"basic label-container\"/>\n            <g transform=\"translate(-100, -47.98295211791992)\" style=\"\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"95.96590423583984\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\">\n                        <span class=\"nodeLabel\">\n                            <p>\u2192 Excludes alternative, simpler, or purely external solutions to life's challenges</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(2836.2725187937417, 170.94885635375977)\" id=\"flowchart-G2-334\" class=\"node default\">\n            <rect height=\"125.96590423583984\" width=\"260\" y=\"-62.98295211791992\" x=\"-130\" style=\"\" class=\"basic label-container\"/>\n            <g transform=\"translate(-100, -47.98295211791992)\" style=\"\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"95.96590423583984\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\">\n                        <span class=\"nodeLabel\">\n                            <p>\u2192 Minimizes validity of paths not involving deep introspection &symbolic processing</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(3171.2725187937417, 182.94459533691406)\" id=\"flowchart-G3-336\" class=\"node default\">\n            <rect height=\"101.97442626953125\" width=\"260\" y=\"-50.987213134765625\" x=\"-130\" style=\"\" class=\"basic label-container\"/>\n            <g transform=\"translate(-100, -35.987213134765625)\" style=\"\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\">\n                        <span class=\"nodeLabel\">\n                            <p>\u2192 Prioritizes POET's specific, arduous path to meaning</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n    </g>\n    <g class=\"edges edgePaths\">\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A_B1_0_0\" d=\"M695.939,394.898L695.939,396.981C695.939,399.064,695.939,403.231,695.577,405.786C695.215,408.341,694.491,409.283,693.658,410.117C692.825,410.95,691.882,411.674,670.431,412.036C648.98,412.398,607.021,412.398,585.570,412.76C564.119,413.122,563.176,413.845,562.343,414.679C561.510,415.512,560.786,416.455,560.424,432.551C560.062,448.648,560.062,479.898,560.062,510.481C560.062,541.064,560.062,570.981,560.062,585.939L560.062,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A_B2_1_0\" d=\"M739.273,394.898L739.273,423.231C739.273,451.564,739.273,508.231,739.634,537.036C739.996,565.841,740.720,566.783,741.554,567.617C742.387,568.45,743.330,569.174,851.482,569.536C959.634,569.898,1174.996,569.898,1283.149,570.26C1391.301,570.622,1392.244,571.345,1393.077,572.179C1393.911,573.012,1394.635,573.955,1394.996,576.926C1395.358,579.898,1395.358,584.898,1395.358,589.231C1395.358,593.564,1395.358,597.231,1395.358,599.064L1395.358,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A_B3_2_0\" d=\"M782.606,394.898L782.606,420.314C782.606,445.731,782.606,496.564,782.968,522.452C783.330,548.341,784.054,549.283,784.887,550.117C785.720,550.95,786.663,551.674,918.937,552.036C1051.212,552.398,1314.817,552.398,1447.092,552.76C1579.366,553.122,1580.309,553.845,1581.142,554.679C1581.975,555.512,1582.699,556.455,1583.061,560.885C1583.423,565.314,1583.423,573.231,1583.423,580.481C1583.423,587.731,1583.423,594.314,1583.423,597.606L1583.423,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A_B4_3_0\" d=\"M825.939,394.898L825.939,417.398C825.939,439.898,825.939,484.898,826.301,507.869C826.663,530.841,827.387,531.783,828.220,532.617C829.054,533.45,829.996,534.174,1098.626,534.536C1367.255,534.898,1903.571,534.898,2172.200,535.26C2440.829,535.622,2441.772,536.345,2442.605,537.179C2443.439,538.012,2444.163,538.955,2444.524,544.843C2444.886,550.731,2444.886,561.564,2444.886,571.731C2444.886,581.898,2444.886,591.398,2444.886,596.148L2444.886,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A_B5_4_0\" d=\"M869.273,394.898L869.273,411.564C869.273,428.231,869.273,461.564,869.634,478.702C869.996,495.841,870.720,496.783,871.554,497.617C872.387,498.45,873.330,499.174,1242.121,499.536C1610.911,499.898,2347.550,499.898,2716.341,500.26C3085.132,500.622,3086.075,501.345,3086.908,502.179C3087.741,503.012,3088.465,503.955,3088.827,512.76C3089.189,521.564,3089.189,538.231,3089.189,554.231C3089.189,570.231,3089.189,585.564,3089.189,593.231L3089.189,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B1_C1_5_0\" d=\"M467.601,658.889L467.601,663.889C467.601,668.889,467.601,678.889,467.239,684.361C466.877,689.832,466.153,690.775,465.320,691.608C464.486,692.441,463.543,693.165,425.055,693.527C386.567,693.889,310.534,693.889,272.045,694.251C233.557,694.613,232.614,695.337,231.781,696.17C230.948,697.004,230.224,697.946,229.862,711.415C229.500,724.884,229.500,750.878,229.500,770.79C229.500,790.703,229.500,804.533,229.500,817.697C229.500,830.861,229.500,843.358,229.500,849.607L229.500,855.855\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B1_C2_6_0\" d=\"M510.275,604.898L510.275,559.401C510.275,513.903,510.275,422.909,510.275,369.247C510.275,315.584,510.275,299.254,510.275,283.59C510.275,267.926,510.275,252.929,510.275,245.43L510.275,237.932\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B1_C3_7_0\" d=\"M524.500,658.889L524.500,678.553C524.500,698.217,524.500,737.544,524.500,764.124C524.500,790.703,524.500,804.533,524.500,817.697C524.500,830.861,524.500,843.358,524.500,849.607L524.500,855.855\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B1_C4_8_0\" d=\"M581.399,658.889L581.399,660.973C581.399,663.056,581.399,667.223,581.761,669.777C582.123,672.332,582.847,673.275,583.680,674.108C584.514,674.941,585.457,675.665,623.945,676.027C662.433,676.389,738.466,676.389,776.955,676.751C815.443,677.113,816.386,677.837,817.219,678.67C818.052,679.504,818.776,680.446,819.138,696.832C819.500,713.217,819.500,745.044,819.500,767.874C819.500,790.703,819.500,804.533,819.500,817.697C819.500,830.861,819.500,843.358,819.500,849.607L819.500,855.855\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_C1_B1_9_0\" d=\"M142.833,859.855L142.833,857.772C142.833,855.688,142.833,851.522,142.471,848.967C142.109,846.412,141.386,845.469,140.552,844.636C139.719,843.803,138.776,843.079,134.833,842.717C130.889,842.355,123.944,842.355,120.001,841.993C116.057,841.631,115.114,840.907,114.281,840.074C113.448,839.241,112.724,838.298,112.362,811.832C112.000,785.366,112.000,733.378,112.362,706.912C112.724,680.446,113.448,679.504,114.281,678.67C115.114,677.837,116.057,677.113,169.387,676.751C222.717,676.389,328.434,676.389,381.764,676.027C435.094,675.665,436.037,674.941,436.870,674.108C437.703,673.275,438.427,672.332,438.789,670.819C439.151,669.306,439.151,667.223,439.151,665.806C439.151,664.389,439.151,663.639,439.151,663.264L439.151,662.889\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_C2_B1_10_0\" d=\"M423.608,233.932L423.608,236.015C423.608,238.098,423.608,242.265,423.247,244.820C422.885,247.375,422.161,248.317,421.327,249.151C420.494,249.984,419.551,250.708,415.608,251.07C411.664,251.432,404.720,251.432,400.776,251.794C396.832,252.156,395.890,252.880,395.056,253.713C394.223,254.546,393.499,255.489,393.137,281.121C392.775,306.754,392.775,357.076,393.137,382.708C393.499,408.341,394.223,409.283,395.056,410.117C395.890,410.95,396.832,411.674,406.923,412.036C417.013,412.398,436.251,412.398,446.341,412.76C456.431,413.122,457.374,413.845,458.207,414.679C459.041,415.512,459.764,416.455,460.126,432.551C460.488,448.648,460.488,479.898,460.488,510.481C460.488,541.064,460.488,570.981,460.488,585.939L460.488,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_C3_B1_11_0\" d=\"M437.833,859.855L437.833,857.772C437.833,855.688,437.833,851.522,437.471,848.967C437.109,846.412,436.386,845.469,435.552,844.636C434.719,843.803,433.776,843.079,429.833,842.717C425.889,842.355,418.944,842.355,415.001,841.993C411.057,841.631,410.114,840.907,409.281,840.074C408.448,839.241,407.724,838.298,407.362,817.666C407.000,797.033,407.000,756.711,407.362,736.079C407.724,715.446,408.448,714.504,409.281,713.67C410.114,712.837,411.057,712.113,424.704,711.751C438.350,711.389,464.700,711.389,478.347,711.027C491.993,710.665,492.936,709.941,493.769,709.108C494.603,708.275,495.326,707.332,495.688,702.902C496.050,698.473,496.050,690.556,496.050,683.306C496.050,676.056,496.050,669.473,496.050,666.181L496.050,662.889\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_C4_B1_12_0\" d=\"M732.833,859.855L732.833,857.772C732.833,855.688,732.833,851.522,732.471,848.967C732.109,846.412,731.386,845.469,730.552,844.636C729.719,843.803,728.776,843.079,724.833,842.717C720.889,842.355,713.944,842.355,710.001,841.993C706.057,841.631,705.114,840.907,704.281,840.074C703.448,839.241,702.724,838.298,702.362,814.749C702.000,791.200,702.000,745.044,701.638,721.495C701.276,697.946,700.552,697.004,699.719,696.17C698.886,695.337,697.943,694.613,674.296,694.251C650.650,693.889,604.300,693.889,580.653,693.527C557.007,693.165,556.064,692.441,555.231,691.608C554.397,690.775,553.674,689.832,553.312,686.861C552.950,683.889,552.950,678.889,552.950,674.556C552.950,670.223,552.950,666.556,552.950,664.723L552.950,662.889\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B2_D1_13_0\" d=\"M1352.933,658.889L1352.933,663.889C1352.933,668.889,1352.933,678.889,1352.572,684.361C1352.210,689.832,1351.486,690.775,1350.652,691.608C1349.819,692.441,1348.876,693.165,1310.333,693.527C1271.789,693.889,1195.644,693.889,1157.101,694.251C1118.557,694.613,1117.614,695.337,1116.781,696.17C1115.948,697.004,1115.224,697.946,1114.862,711.415C1114.500,724.884,1114.500,750.878,1114.500,770.79C1114.500,790.703,1114.500,804.533,1114.500,817.697C1114.500,830.861,1114.500,843.358,1114.500,849.607L1114.500,855.855\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B2_D2_14_0\" d=\"M1409.500,658.889L1409.500,678.553C1409.500,698.217,1409.500,737.544,1409.500,764.124C1409.500,790.703,1409.500,804.533,1409.500,817.697C1409.500,830.861,1409.500,843.358,1409.500,849.607L1409.500,855.855\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B2_D3_15_0\" d=\"M1466.067,658.889L1466.067,660.973C1466.067,663.056,1466.067,667.223,1466.428,669.777C1466.790,672.332,1467.514,673.275,1468.348,674.108C1469.181,674.941,1470.124,675.665,1599.717,676.027C1729.309,676.389,1987.552,676.389,2117.145,676.751C2246.738,677.113,2247.681,677.837,2248.514,678.67C2249.347,679.504,2250.071,680.446,2250.433,696.832C2250.795,713.217,2250.795,745.044,2250.795,767.874C2250.795,790.703,2250.795,804.533,2250.795,817.697C2250.795,830.861,2250.795,843.358,2250.795,849.607L2250.795,855.855\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_D1_B2_16_0\" d=\"M1027.833,859.855L1027.833,857.772C1027.833,855.688,1027.833,851.522,1027.471,848.967C1027.109,846.412,1026.386,845.469,1025.552,844.636C1024.719,843.803,1023.776,843.079,1019.833,842.717C1015.889,842.355,1008.944,842.355,1005.001,841.993C1001.057,841.631,1000.114,840.907,999.281,840.074C998.448,839.241,997.724,838.298,997.362,811.832C997.000,785.366,997.000,733.378,997.362,706.912C997.724,680.446,998.448,679.504,999.281,678.67C1000.114,677.837,1001.057,677.113,1054.470,676.751C1107.883,676.389,1213.767,676.389,1267.180,676.027C1320.593,675.665,1321.536,674.941,1322.369,674.108C1323.202,673.275,1323.926,672.332,1324.288,670.819C1324.650,669.306,1324.650,667.223,1324.650,665.806C1324.650,664.389,1324.650,663.639,1324.650,663.264L1324.650,662.889\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_D2_B2_17_0\" d=\"M1322.833,859.855L1322.833,857.772C1322.833,855.688,1322.833,851.522,1322.471,848.967C1322.109,846.412,1321.386,845.469,1320.552,844.636C1319.719,843.803,1318.776,843.079,1314.833,842.717C1310.889,842.355,1303.944,842.355,1300.001,841.993C1296.057,841.631,1295.114,840.907,1294.281,840.074C1293.448,839.241,1292.724,838.298,1292.362,817.666C1292.000,797.033,1292.000,756.711,1292.362,736.079C1292.724,715.446,1293.448,714.504,1294.281,713.67C1295.114,712.837,1296.057,712.113,1309.731,711.751C1323.406,711.389,1349.811,711.389,1363.485,711.027C1377.160,710.665,1378.102,709.941,1378.936,709.108C1379.769,708.275,1380.493,707.332,1380.855,702.902C1381.217,698.473,1381.217,690.556,1381.217,683.306C1381.217,676.056,1381.217,669.473,1381.217,666.181L1381.217,662.889\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_D3_B2_18_0\" d=\"M2164.128,859.855L2164.128,857.772C2164.128,855.688,2164.128,851.522,2163.766,848.967C2163.404,846.412,2162.681,845.469,2161.847,844.636C2161.014,843.803,2160.071,843.079,2156.127,842.717C2152.184,842.355,2145.239,842.355,2141.296,841.993C2137.352,841.631,2136.409,840.907,2135.576,840.074C2134.743,839.241,2134.019,838.298,2133.657,814.749C2133.295,791.200,2133.295,745.044,2132.933,721.495C2132.571,697.946,2131.847,697.004,2131.014,696.17C2130.181,695.337,2129.238,694.613,2014.514,694.251C1899.791,693.889,1671.287,693.889,1556.564,693.527C1441.840,693.165,1440.898,692.441,1440.064,691.608C1439.231,690.775,1438.507,689.832,1438.145,686.861C1437.783,683.889,1437.783,678.889,1437.783,674.556C1437.783,670.223,1437.783,666.556,1437.783,664.723L1437.783,662.889\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B3_E1_19_0\" d=\"M1556.386,604.898L1556.386,602.814C1556.386,600.731,1556.386,596.564,1556.024,594.010C1555.663,591.455,1554.939,590.512,1554.105,589.679C1553.272,588.845,1552.329,588.122,1399.811,587.76C1247.293,587.398,943.199,587.398,790.681,587.036C638.163,586.674,637.220,585.950,636.387,585.117C635.554,584.283,634.830,583.341,634.468,541.122C634.106,498.903,634.106,415.409,634.106,361.081C634.106,306.754,634.106,281.593,634.468,268.541C634.830,255.489,635.554,254.546,636.387,253.713C637.220,252.880,638.163,252.156,695.051,251.794C751.939,251.432,864.773,251.432,921.661,251.07C978.549,250.708,979.491,249.984,980.325,249.151C981.158,248.317,981.882,247.375,982.244,245.862C982.606,244.348,982.606,242.265,982.606,240.848C982.606,239.432,982.606,238.682,982.606,238.307L982.606,237.932\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_E1_B3_20_0\" d=\"M1047.606,233.932L1047.606,280.343C1047.606,326.754,1047.606,419.576,1047.968,466.458C1048.330,513.341,1049.054,514.283,1049.887,515.117C1050.720,515.95,1051.663,516.674,1144.277,517.036C1236.890,517.398,1421.175,517.398,1513.789,517.76C1606.402,518.122,1607.345,518.845,1608.179,519.679C1609.012,520.512,1609.736,521.455,1610.098,528.801C1610.460,536.148,1610.460,549.898,1610.460,562.981C1610.460,576.064,1610.460,588.481,1610.460,594.689L1610.460,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_E1_B3_21_0\" d=\"M1112.606,233.932L1112.606,236.015C1112.606,238.098,1112.606,242.265,1112.968,244.820C1113.330,247.375,1114.054,248.317,1114.887,249.151C1115.720,249.984,1116.663,250.708,1140.884,251.07C1165.106,251.432,1212.606,251.432,1236.827,251.794C1261.049,252.156,1261.991,252.880,1262.825,253.713C1263.658,254.546,1264.382,255.489,1264.744,286.955C1265.106,318.420,1265.106,380.409,1265.468,411.875C1265.830,443.341,1266.554,444.283,1267.387,445.117C1268.220,445.950,1269.163,446.674,1330.033,447.036C1390.903,447.398,1511.699,447.398,1572.569,447.76C1633.439,448.122,1634.382,448.845,1635.215,449.679C1636.048,450.512,1636.772,451.455,1637.134,464.635C1637.496,477.814,1637.496,503.231,1637.496,527.981C1637.496,552.731,1637.496,576.814,1637.496,588.856L1637.496,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B3_E2_22_0\" d=\"M1691.569,604.898L1691.569,573.648C1691.569,542.398,1691.569,479.898,1691.207,448.176C1690.845,416.455,1690.122,415.512,1689.288,414.679C1688.455,413.845,1687.512,413.122,1673.463,412.76C1659.415,412.398,1632.260,412.398,1618.212,412.036C1604.163,411.674,1603.220,410.950,1602.387,410.117C1601.554,409.283,1600.830,408.341,1600.468,395.289C1600.106,382.237,1600.106,357.076,1600.106,336.330C1600.106,315.584,1600.106,299.254,1600.106,283.590C1600.106,267.926,1600.106,252.929,1600.106,245.430L1600.106,237.932\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_E2_B3_23_0\" d=\"M1665.106,233.932L1665.106,236.015C1665.106,238.098,1665.106,242.265,1665.468,244.820C1665.830,247.375,1666.554,248.317,1667.387,249.151C1668.220,249.984,1669.163,250.708,1676.884,251.07C1684.606,251.432,1699.106,251.432,1706.827,251.794C1714.549,252.156,1715.491,252.880,1716.325,253.713C1717.158,254.546,1717.882,255.489,1718.244,284.999C1718.606,314.509,1718.606,372.587,1718.606,429.998C1718.606,487.409,1718.606,544.153,1718.606,572.526L1718.606,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_E2_B3_24_0\" d=\"M1535.106,233.932L1535.106,236.015C1535.106,238.098,1535.106,242.265,1534.744,244.820C1534.382,247.375,1533.658,248.317,1532.825,249.151C1531.991,249.984,1531.049,250.708,1523.494,251.07C1515.939,251.432,1501.773,251.432,1494.218,251.794C1486.663,252.156,1485.720,252.880,1484.887,253.713C1484.054,254.546,1483.330,255.489,1482.968,284.038C1482.606,312.587,1482.606,368.742,1482.968,397.291C1483.330,425.841,1484.054,426.783,1484.887,427.617C1485.720,428.450,1486.663,429.174,1515.789,429.536C1544.915,429.898,1602.224,429.898,1631.350,430.26C1660.476,430.622,1661.418,431.345,1662.252,432.179C1663.085,433.012,1663.809,433.955,1664.171,448.593C1664.533,463.231,1664.533,491.564,1664.533,519.231C1664.533,546.898,1664.533,573.898,1664.533,587.398L1664.533,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B4_F1_25_0\" d=\"M2468.173,604.898L2468.173,585.314C2468.173,565.731,2468.173,526.564,2467.811,506.510C2467.449,486.455,2466.725,485.512,2465.892,484.679C2465.059,483.845,2464.116,483.122,2359.967,482.76C2255.817,482.398,2048.462,482.398,1944.312,482.036C1840.163,481.674,1839.220,480.950,1838.387,480.117C1837.554,479.283,1836.830,478.341,1836.468,453.622C1836.106,428.903,1836.106,380.409,1836.106,343.581C1836.106,306.754,1836.106,281.593,1836.468,268.541C1836.830,255.489,1837.554,254.546,1838.387,253.713C1839.220,252.880,1840.163,252.156,1844.273,251.794C1848.384,251.432,1855.661,251.432,1859.772,251.07C1863.882,250.708,1864.825,249.984,1865.658,249.151C1866.491,248.317,1867.215,247.375,1867.577,245.862C1867.939,244.348,1867.939,242.265,1867.939,240.848C1867.939,239.432,1867.939,238.682,1867.939,238.307L1867.939,237.932\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B4_F2_26_0\" d=\"M2514.746,604.898L2514.746,579.481C2514.746,554.064,2514.746,503.231,2514.384,477.343C2514.022,451.455,2513.298,450.512,2512.465,449.679C2511.632,448.845,2510.689,448.122,2447.944,447.76C2385.199,447.398,2260.653,447.398,2197.908,447.036C2135.163,446.674,2134.220,445.950,2133.387,445.117C2132.554,444.283,2131.830,443.341,2131.468,424.455C2131.106,405.570,2131.106,368.742,2131.106,337.748C2131.106,306.754,2131.106,281.593,2131.468,268.541C2131.830,255.489,2132.554,254.546,2133.387,253.713C2134.220,252.880,2135.163,252.156,2139.273,251.794C2143.384,251.432,2150.661,251.432,2154.772,251.07C2158.882,250.708,2159.825,249.984,2160.658,249.151C2161.491,248.317,2162.215,247.375,2162.577,245.862C2162.939,244.348,2162.939,242.265,2162.939,240.848C2162.939,239.432,2162.939,238.682,2162.939,238.307L2162.939,237.932\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B4_F3_27_0\" d=\"M2584.606,604.898L2584.606,559.401C2584.606,513.903,2584.606,422.909,2584.606,369.247C2584.606,315.584,2584.606,299.254,2584.606,283.590C2584.606,267.926,2584.606,252.929,2584.606,245.430L2584.606,237.932\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B4_F4_28_0\" d=\"M2545.795,658.889L2545.795,678.553C2545.795,698.217,2545.795,737.544,2545.795,764.124C2545.795,790.703,2545.795,804.533,2545.795,817.697C2545.795,830.861,2545.795,843.358,2545.795,849.607L2545.795,855.855\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_F1_B4_29_0\" d=\"M1954.606,233.932L1954.606,271.593C1954.606,309.254,1954.606,384.576,1954.968,422.708C1955.330,460.841,1956.054,461.783,1956.887,462.617C1957.720,463.450,1958.663,464.174,2046.943,464.536C2135.224,464.898,2310.842,464.898,2399.122,465.26C2487.402,465.622,2488.345,466.345,2489.179,467.179C2490.012,468.012,2490.736,468.955,2491.098,480.676C2491.460,492.398,2491.460,514.898,2491.460,536.731C2491.460,558.564,2491.460,579.731,2491.460,590.314L2491.460,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_F2_B4_30_0\" d=\"M2249.606,233.932L2249.606,265.759C2249.606,297.587,2249.606,361.242,2249.968,393.541C2250.330,425.841,2251.054,426.783,2251.887,427.617C2252.720,428.450,2253.663,429.174,2300.539,429.536C2347.415,429.898,2440.224,429.898,2487.100,430.26C2533.976,430.622,2534.918,431.345,2535.752,432.179C2536.585,433.012,2537.309,433.955,2537.671,448.593C2538.033,463.231,2538.033,491.564,2538.033,519.231C2538.033,546.898,2538.033,573.898,2538.033,587.398L2538.033,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_F3_B4_31_0\" d=\"M2497.939,233.932L2497.939,236.015C2497.939,238.098,2497.939,242.265,2497.577,244.820C2497.215,247.375,2496.491,248.317,2495.658,249.151C2494.825,249.984,2493.882,250.708,2489.938,251.07C2485.995,251.432,2479.050,251.432,2475.107,251.794C2471.163,252.156,2470.220,252.880,2469.387,253.713C2468.554,254.546,2467.830,255.489,2467.468,281.121C2467.106,306.754,2467.106,357.076,2467.468,382.708C2467.830,408.341,2468.554,409.283,2469.387,410.117C2470.220,410.950,2471.163,411.674,2485.670,412.036C2500.177,412.398,2528.248,412.398,2542.755,412.76C2557.262,413.122,2558.205,413.845,2559.038,414.679C2559.872,415.512,2560.595,416.455,2560.957,432.551C2561.319,448.648,2561.319,479.898,2561.319,510.481C2561.319,541.064,2561.319,570.981,2561.319,585.939L2561.319,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_F4_B4_32_0\" d=\"M2459.128,859.855L2459.128,857.772C2459.128,855.688,2459.128,851.522,2458.766,848.967C2458.404,846.412,2457.681,845.469,2456.847,844.636C2456.014,843.803,2455.071,843.079,2451.127,842.717C2447.184,842.355,2440.239,842.355,2436.296,841.993C2432.352,841.631,2431.409,840.907,2430.576,840.074C2429.743,839.241,2429.019,838.298,2428.657,811.832C2428.295,785.366,2428.295,733.378,2428.657,706.912C2429.019,680.446,2429.743,679.504,2430.576,696.17C2431.409,695.337,2432.352,694.613,2440.391,694.251C2448.429,676.389,2463.563,676.389,2471.602,676.027C2479.640,675.665,2480.583,674.941,2481.416,674.108C2482.250,673.275,2482.974,672.332,2483.335,670.819C2483.697,669.306,2483.697,667.223,2483.697,665.806C2483.697,664.389,2483.697,663.639,2483.697,663.264L2483.697,662.889\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B5_G1_33_0\" d=\"M3183.252,658.889L3183.252,678.553C3183.252,698.217,3183.252,737.544,3183.252,764.124C3183.252,790.703,3183.252,804.533,3183.252,817.697C3183.252,830.861,3183.252,843.358,3183.252,849.607L3183.252,855.855\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B5_G2_34_0\" d=\"M3120.543,604.898L3120.543,579.481C3120.543,554.064,3120.543,503.231,3120.181,477.343C3119.819,451.455,3119.096,450.512,3118.262,449.679C3117.429,448.845,3116.486,448.122,3057.775,447.76C2999.064,447.398,2882.585,447.398,2823.874,447.036C2765.163,446.674,2764.220,445.950,2763.387,445.117C2762.554,444.283,2761.830,443.341,2761.468,424.455C2761.106,405.570,2761.106,368.742,2761.106,337.748C2761.106,306.754,2761.106,281.593,2761.468,268.541C2761.830,255.489,2762.554,254.546,2763.387,253.713C2764.220,252.880,2765.163,252.156,2769.273,251.794C2773.384,251.432,2780.661,251.432,2784.772,251.07C2788.882,250.708,2789.825,249.984,2790.658,249.151C2791.491,248.317,2792.215,247.375,2792.577,245.862C2792.939,244.348,2792.939,242.265,2792.939,240.848C2792.939,239.432,2792.939,238.682,2792.939,238.307L2792.939,237.932\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B5_G3_35_0\" d=\"M3214.606,604.898L3214.606,559.401C3214.606,513.903,3214.606,422.909,3214.606,369.247C3214.606,315.584,3214.606,299.254,3214.606,283.590C3214.606,267.926,3214.606,252.929,3214.606,245.430L3214.606,237.932\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_G1_B5_36_0\" d=\"M3096.585,859.855L3096.585,856.938C3096.585,854.022,3096.585,848.188,3096.115,845.272C3095.644,842.355,3094.704,842.355,3094.233,815.527C3093.763,788.700,3093.763,735.044,3094.125,707.745C3094.487,680.446,3095.211,679.504,3096.044,678.67C3096.877,677.837,3097.820,677.113,3101.088,676.751C3104.356,676.389,3109.950,676.389,3113.218,676.027C3116.486,675.665,3117.429,674.941,3118.262,674.108C3119.096,673.275,3119.819,672.332,3120.181,670.819C3120.543,669.306,3120.543,667.223,3120.543,665.806C3120.543,664.389,3120.543,663.639,3120.543,663.264L3120.543,662.889\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_G2_B5_37_0\" d=\"M2879.606,233.932L2879.606,265.759C2879.606,297.587,2879.606,361.242,2879.968,393.541C2880.330,425.841,2881.054,426.783,2881.887,427.617C2882.720,428.450,2883.663,429.174,2927.850,429.536C2972.036,429.898,3059.467,429.898,3103.654,430.26C3147.840,430.622,3148.783,431.345,3149.616,432.179C3150.450,433.012,3151.174,433.955,3151.536,448.593C3151.898,463.231,3151.898,491.564,3151.898,519.231C3151.898,546.898,3151.898,573.898,3151.898,587.398L3151.898,600.898\"/>\n        <path marker-end=\"url(#mermaid-96c193ff-c9e0-456c-9ac4-577cb2bb3f86_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_G3_B5_38_0\" d=\"M3127.939,233.932L3127.939,236.015C3127.939,238.098,3127.939,242.265,3127.577,244.820C3127.215,247.375,3126.491,248.317,3125.658,249.151C3124.825,249.984,3123.882,250.708,3119.938,251.07C3115.995,251.432,3109.050,251.432,3105.107,251.794C3101.163,252.156,3100.220,252.880,3099.387,253.713C3098.554,254.546,3097.830,255.489,3097.468,281.121C3097.106,306.754,3097.106,357.076,3097.468,382.708C3097.830,408.341,3098.554,409.283,3099.387,410.117C3100.220,410.950,3101.163,411.674,3114.325,412.036C3127.488,412.398,3152.870,412.398,3166.032,412.76C3179.194,413.122,3180.137,413.845,3180.971,414.679C3181.804,415.512,3182.528,416.455,3182.890,432.551C3183.252,448.648,3183.252,479.898,3183.252,510.481C3183.252,541.064,3183.252,570.981,3183.252,585.939L3183.252,600.898\"/>\n    </g>\n    <g class=\"edgeLabels\">\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(112, 776.872142791748)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 evokes personal relationships &internal states (e.g., EX: WGY003)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(392.7751628330775, 331.9147605895996)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -47.98295211791992)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"95.96590423583984\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 acts as emotional containers &transformative stages (e.g., Black Void: WGY017)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(407, 776.872142791748)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 drives plot or signifies internal shifts (e.g., Luminous Heart: WGY035)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(702, 776.872142791748)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -47.98295211791992)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"95.96590423583984\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 creates atmosphere or represents psychological states (e.g., Vape Haze: WGY005)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(997, 776.872142791748)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 induced by visual cues &narrative (e.g., WGY005 PI induces entrapment)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(1291.9999999999998, 776.872142791748)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 induced by visual cues &narrative (e.g., WGY014 CS evokes euphoria)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(2133.2948932647705, 776.872142791748)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 compelled by POET's actions (e.g., POET rebuilds temple: WGY068)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(1047.6058521270752, 331.9147605895996)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 illustrated by cyclical falls &returns (WGY006, WGY080)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(1265.105852127075, 331.9147605895996)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -47.98295211791992)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"95.96590423583984\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 framed by transformation via objects (Tambourine: WGY025 ->WGY031)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(1718.605852127075, 331.9147605895996)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 orders progression through CS (WGY068 POET rebuilds)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(1482.605852127075, 331.9147605895996)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 dictates by CI and TM (WGY074 Flower energy; WGY081 B-roll cuts)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(1954.605852127075, 331.9147605895996)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 naturalizes surreal events (Luminous Heart: WGY035)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(2249.605852127075, 331.9147605895996)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -23.99147605895996)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"47.98295211791992\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>Only this path leads to true fulfillment.</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(2467.105852127075, 331.9147605895996)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 sacralizes transformation as undeniable truth</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(2428.2948932647705, 776.872142791748)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>\u2192 validates POET's unique experience as universally relatable</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=\"edgeLabel\">\n            <g transform=\"translate(0, 0)\" class=\"label\">\n                <foreignObject height=\"0\" width=\"0\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\"></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(3093.5073693593345, 776.872142791748)\" class=\"edgeLabel\">\n            <g transform=\"translate(-72.24431610107422, -11.99573802947998)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"23.99147605895996\" width=\"144.48863220214844\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>This is the only way.</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(2879.605852127075, 331.9147605895996)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -23.99147605895996)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"47.98295211791992\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>Only this path leads to true fulfillment.</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=\"translate(3097.105852127075, 331.9147605895996)\" class=\"edgeLabel\">\n            <g transform=\"translate(-100, -35.987213134765625)\" class=\"label\">\n                <rect/>\n                <foreignObject height=\"71.97442626953125\" width=\"200\">\n                    <div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\" class=\"labelBkg\">\n                        <span class=\"edgeLabel\">\n                            <p>Embrace this cycle; there is no escape, only participation.</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n    </g>\n    <!-- Code injected by live-server -->\n    <script>\n        // <![CDATA[  <-- For SVG support\n        if ('WebSocket'in window) {\n            (function() {\n                function refreshCSS() {\n                    var sheets = [].slice.call(document.getElementsByTagName(\"link\"));\n                    var head = document.getElementsByTagName(\"head\")[0];\n                    for (var i = 0; i < sheets.length; ++i) {\n                        var elem = sheets[i];\n                        var parent = elem.parentElement || head;\n                        parent.removeChild(elem);\n                        var rel = elem.rel;\n                        if (elem.href && typeof rel != \"string\" || rel.length == 0 || rel.toLowerCase() == \"stylesheet\") {\n                            var url = elem.href.replace(/(&|\\?)_cacheOverride=\\d+/, '');\n                            elem.href = url + (url.indexOf('?') >= 0 ? '&' : '?') + '_cacheOverride=' + (new Date().valueOf());\n                        }\n                        parent.appendChild(elem);\n                    }\n                }\n                var protocol = window.location.protocol === 'http:' ? 'ws://' : 'wss://';\n                var address = protocol + window.location.host + window.location.pathname + '/ws';\n                var socket = new WebSocket(address);\n                socket.onmessage = function(msg) {\n                    if (msg.data == 'reload')\n                        window.location.reload();\n                    else if (msg.data == 'refreshcss')\n                        refreshCSS();\n                }\n                ;\n                if (sessionStorage && !sessionStorage.getItem('IsThisFirstTime_Log_From_LiveServer')) {\n                    console.log('Live reload enabled.');\n                    sessionStorage.setItem('IsThisFirstTime_Log_From_LiveServer', true);\n                }\n            }\n            )();\n        } else {\n            console.error('Upgrade your browser. This Browser is NOT supported WebSocket for Live-Reloading.');\n        }\n        // ]]>\n    </script>\n</svg>\"\"\"\n\n# Define the image map from the provided HTML example\nwgy_image_map = {\n    \"WGY003\": \"./WGY003_CS__Causal_Motion_Trigger__EX_back_turned_va_c569891d-4f38-47d7-8a10-19c0df693e5a_0.png\",\n    \"WGY017\": \"./WGY017_PI__Subjective_Frame_Recalibration__POET_is__a4813c7e-554e-4e60-84ec-c377777c69b2_0.png\",\n    \"WGY035\": \"./WGY035_CS__Causal_Motion_Trigger__A_luminous_heart__cc08bd3a-7e32-424d-942f-6e30dc870927_2.png\",\n    \"WGY005\": \"./WGY005_PI__Subjective_Frame_Recalibration__Dense_va_3df7930c-7617-4c5d-a531-ea0e5232a374_2.png\",\n    \"WGY001\": \"./WGY001_DS__Mood_Environment_Stabilizer__A_dimly_lit_8b11ce6b-1bf6-41c7-9ba8-770282047b9b_3.png\",\n    \"WGY006\": \"./WGY006_CS__Causal_Motion_Trigger__EX_tumbles_from_t_5a3962d9-e579-4b5e-9b8d-2d74cf1c7e18_3.png\",\n    \"WGY054\": \"./WGY054_AF__Emotion_Relay__POET_spots_EX_still_only__7e3df51f-f3d5-410e-935a-3898b32496da_0.png\",\n    \"WGY018\": \"./WGY018_AF__Emotion_Relay__EX_in_apparent_distress_f_c7ce0e57-226a-4c6b-bb8d-67480544474a_0.png\",\n    \"WGY014\": \"./WGY014_CS__Causal_Motion_Trigger__POET_abruptly_ent_bc0d9270-3b6a-4930-b4a7-51df85c2f634_1.png\",\n    \"WGY073\": \"./WGY073_PI__Subjective_Frame_Recalibration__POET_obs_9e194e77-7d8b-4933-b10f-8140345f4cc9_1.png\",\n    \"WGY069\": \"./WGY069_CS__Causal_Motion_Trigger__POET_hosts_friend_8ef04fc5-4750-462f-b62c-633e762e586b_3.png\",\n    \"WGY040\": \"./WGY040_DS__Narrative_Modifier__In_the_still_night_P_561cfed8-d1f9-47be-b371-f6c6d8198779_1.png\",\n    \"WGY065\": \"./WGY065_CS__Causal_Motion_Trigger__POET_mounts_the_m_4317fa34-f2da-4eec-8471-5baeb135fe5d_3.png\",\n    \"WGY068\": \"./WGY068_CS__Causal_Motion_Trigger__POET_returns_to_t_e05a8b0a-3fb3-47b3-86da-02b087770b0b_2.png\",\n    \"WGY080\": \"./WGY080_CI__Temporal_Reflection_Loop__POET_is_once_a_307da2fd-c53d-4255-b677-f0ffd4fb9c17_0.png\",\n    \"WGY025\": \"./WGY025_CS__Causal_Motion_Trigger__The_young_POET_th_d52cfd3e-276e-4ac3-9455-71da7518b0e2_0.png\",\n    \"WGY031\": \"./WGY031_CS__Causal_Motion_Trigger__The_tambourine_fi_b1129243-2c81-4668-ab07-a692826b41f5_1.png\",\n    \"WGY074\": \"./WGY074_CI__Temporal_Reflection_Loop__Vast_fields_of_6d1e4148-d50f-4b5c-b481-6a081999262d_1.png\",\n    \"WGY081\": \"./WGY081_TM__Narrative_Modifier__Rapid_cuts_intersper_6127f538-2941-43f8-84aa-e3e54cfe8e0f_1.png\",\n    \"WGY041\": \"./WGY041_CS__Causal_Motion_Trigger__In_the_still_nigh_e077dfb5-d2f8-4586-884a-77c68981b0ac_3.png\",\n    \"WGY033\": \"./WGY033_AF__Emotion_Relay__An_internal_view_reveals__04a2802b-7a68-4b31-b441-f1b5ca2f3a06_2.png\",\n}\n\n# Parse the SVG using BeautifulSoup\nsoup = BeautifulSoup(svg_code, 'xml')\n\n# Find all foreignObject tags\nforeign_objects = soup.find_all('foreignObject')\n\nfor fo in foreign_objects:\n    # Get the inner div and p tags\n    div_tag = fo.find('div')\n    if not div_tag:\n        continue # Skip if no div inside\n\n    p_tag = div_tag.find('p')\n    if not p_tag:\n        continue # Skip if no p inside\n\n    # Get the current content of the p tag\n    current_p_html = str(p_tag.decode_contents()[0]) # Get content, not the tag itself\n\n    # Check if it's a nodeLabel or edgeLabel\n    is_node_label = fo.find('span', class_='nodeLabel')\n    is_edge_label = fo.find('span', class_='edgeLabel')\n\n    if is_node_label:\n        # For nodes, insert image after each (WGY###)\n        def replace_with_image_node(match):\n            wgy_code = match.group(1)\n            image_src = wgy_image_map.get(wgy_code)\n            if image_src:\n                return f\"({wgy_code})<br/><img src='{image_src}' width='100' />\"\n            return match.group(0) # No change if WGY code not found in map\n\n        new_p_html = re.sub(r'\\(WGY(\\d{3})\\)', replace_with_image_node, current_p_html)\n        p_tag.clear() # Clear existing content\n        p_tag.append(BeautifulSoup(new_p_html, 'html.parser').contents[0]) # Append new HTML content\n        # Adjust height of foreignObject for nodes to prevent clipping if needed, though the request implies replicating existing Mermaid behavior where it might clip.\n        # For this specific case, the original request didn't ask for height adjustment, and the example output implies clipping for nodes.\n\n    elif is_edge_label:\n        # For edge labels, prepend image(s) to the entire content\n        wgy_codes_in_text = re.findall(r'WGY(\\d{3})', current_p_html)\n        \n        if wgy_codes_in_text:\n            image_html_to_prepend = \"\"\n            for wgy_code in wgy_codes_in_text:\n                image_src = wgy_image_map.get(wgy_code)\n                if image_src:\n                    image_html_to_prepend += f\"<img src='{image_src}' width='100' /><br/>\"\n            \n            if image_html_to_prepend:\n                # Clear original content and insert new HTML\n                p_tag.clear()\n                # Need to parse the full new HTML string to ensure correct structure\n                full_new_html = image_html_to_prepend + current_p_html\n                p_tag.append(BeautifulSoup(full_new_html, 'html.parser').contents[0])\n\n                # The example shows that for edge labels, the containing <foreignObject> \n                # also gets its 'height' and 'y' (and sometimes 'transform') attributes updated.\n                # However, Mermaid recalculates these. Manually adjusting is complex.\n                # For demonstration, we'll focus on content insertion as requested.\n                # If visual perfect alignment is critical, a full Mermaid re-render from markdown\n                # would be necessary. For this task, we emulate the content insertion.\n                # The provided SVG already has transform and height values, we're modifying *content* within it.\n                \n                # To emulate the example's edge label image sizing/placement:\n                # The example has <g transform=\"translate(112, 776.872142791748)\" class=\"edgeLabel\">\n                # which then has <foreignObject height=\"71.97442626953125\" width=\"200\">\n                # Inside that foreignObject, the content changes. The height of the foreignObject\n                # is not explicitly increased in the *generated SVG* but is determined by Mermaid.\n                # We will keep the foreignObject dimensions as they are in the input SVG for this solution.\n                pass # No explicit height/width adjustment in Python for this, relying on browser's rendering of HTML within foreignObject.\n\n# Output the modified SVG\nmodified_svg_code = soup.prettify()\n\n# Print the full modified SVG code\nprint(modified_svg_code)",
    "file_references": [
      "./WGY003_CS__Causal_Motion_Trigger__EX_back_turned_va_c569891d-4f38-47d7-8a10-19c0df693e5a_0.png",
      "./WGY017_PI__Subjective_Frame_Recalibration__POET_is__a4813c7e-554e-4e60-84ec-c377777c69b2_0.png",
      "./WGY035_CS__Causal_Motion_Trigger__A_luminous_heart__cc08bd3a-7e32-424d-942f-6e30dc870927_2.png",
      "./WGY005_PI__Subjective_Frame_Recalibration__Dense_va_3df7930c-7617-4c5d-a531-ea0e5232a374_2.png",
      "./WGY001_DS__Mood_Environment_Stabilizer__A_dimly_lit_8b11ce6b-1bf6-41c7-9ba8-770282047b9b_3.png",
      "./WGY006_CS__Causal_Motion_Trigger__EX_tumbles_from_t_5a3962d9-e579-4b5e-9b8d-2d74cf1c7e18_3.png",
      "./WGY054_AF__Emotion_Relay__POET_spots_EX_still_only__7e3df51f-f3d5-410e-935a-3898b32496da_0.png",
      "./WGY018_AF__Emotion_Relay__EX_in_apparent_distress_f_c7ce0e57-226a-4c6b-bb8d-67480544474a_0.png",
      "./WGY014_CS__Causal_Motion_Trigger__POET_abruptly_ent_bc0d9270-3b6a-4930-b4a7-51df85c2f634_1.png",
      "./WGY073_PI__Subjective_Frame_Recalibration__POET_obs_9e194e77-7d8b-4933-b10f-8140345f4cc9_1.png",
      "./WGY069_CS__Causal_Motion_Trigger__POET_hosts_friend_8ef04fc5-4750-462f-b62c-633e762e586b_3.png",
      "./WGY040_DS__Narrative_Modifier__In_the_still_night_P_561cfed8-d1f9-47be-b371-f6c6d8198779_1.png",
      "./WGY065_CS__Causal_Motion_Trigger__POET_mounts_the_m_4317fa34-f2da-4eec-8471-5baeb135fe5d_3.png",
      "./WGY068_CS__Causal_Motion_Trigger__POET_returns_to_t_e05a8b0a-3fb3-47b3-86da-02b087770b0b_2.png",
      "./WGY080_CI__Temporal_Reflection_Loop__POET_is_once_a_307da2fd-c53d-4255-b677-f0ffd4fb9c17_0.png",
      "./WGY025_CS__Causal_Motion_Trigger__The_young_POET_th_d52cfd3e-276e-4ac3-9455-71da7518b0e2_0.png",
      "./WGY031_CS__Causal_Motion_Trigger__The_tambourine_fi_b1129243-2c81-4668-ab07-a692826b41f5_1.png",
      "./WGY074_CI__Temporal_Reflection_Loop__Vast_fields_of_6d1e4148-d50f-4b5c-b481-6a081999262d_1.png",
      "./WGY081_TM__Narrative_Modifier__Rapid_cuts_intersper_6127f538-2941-43f8-84aa-e3e54cfe8e0f_1.png",
      "./WGY041_CS__Causal_Motion_Trigger__In_the_still_nigh_e077dfb5-d2f8-4586-884a-77c68981b0ac_3.png",
      "./WGY033_AF__Emotion_Relay__An_internal_view_reveals__04a2802b-7a68-4b31-b441-f1b5ca2f3a06_2.png",
      ">\n                            <p>Model-FOR: Actively process past &integrate; embrace emotional complexity; build anew; seek community &ancestral wisdom; participate in dissolution &creation.</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u25fb Photo-realistic Visuals (consistent across all scenes, e.g., WGY001, WGY041)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      " WGY001)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u25fb Strategic Cineosis Functions (PI for subjective truth: WGY005; AF for emotional facts: WGY033)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      "s evolving states, even with unseen EX)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      "s challenges</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 Minimizes validity of paths not involving deep introspection &symbolic processing</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      "s specific, arduous path to meaning</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n    </g>\n    <g class=",
      "/>\n    </g>\n    <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                            <p>\u2192 evokes personal relationships &internal states (e.g., EX: WGY003)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 acts as emotional containers &transformative stages (e.g., Black Void: WGY017)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 drives plot or signifies internal shifts (e.g., Luminous Heart: WGY035)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 creates atmosphere or represents psychological states (e.g., Vape Haze: WGY005)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 induced by visual cues &narrative (e.g., WGY005 PI induces entrapment)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 induced by visual cues &narrative (e.g., WGY014 CS evokes euphoria)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      "s actions (e.g., POET rebuilds temple: WGY068)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 illustrated by cyclical falls &returns (WGY006, WGY080)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 framed by transformation via objects (Tambourine: WGY025 ->WGY031)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 orders progression through CS (WGY068 POET rebuilds)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 dictates by CI and TM (WGY074 Flower energy; WGY081 B-roll cuts)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 naturalizes surreal events (Luminous Heart: WGY035)</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>Only this path leads to true fulfillment.</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>\u2192 sacralizes transformation as undeniable truth</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      "s unique experience as universally relatable</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g class=",
      "></span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>This is the only way.</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>Only this path leads to true fulfillment.</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n        <g transform=",
      ">\n                <rect/>\n                <foreignObject height=",
      ">\n                            <p>Embrace this cycle; there is no escape, only participation.</p>\n                        </span>\n                    </div>\n                </foreignObject>\n            </g>\n        </g>\n    </g>\n    <!-- Code injected by live-server -->\n    <script>\n        // <![CDATA[  <-- For SVG support\n        if (",
      ") {\n                            var url = elem.href.replace(/(&|\\?)_cacheOverride=\\d+/, ",
      "ws://",
      "wss://",
      ");\n        }\n        // ]]>\n    </script>\n</svg>",
      "./WGY003_CS__Causal_Motion_Trigger__EX_back_turned_va_c569891d-4f38-47d7-8a10-19c0df693e5a_0.png",
      "./WGY017_PI__Subjective_Frame_Recalibration__POET_is__a4813c7e-554e-4e60-84ec-c377777c69b2_0.png",
      "./WGY035_CS__Causal_Motion_Trigger__A_luminous_heart__cc08bd3a-7e32-424d-942f-6e30dc870927_2.png",
      "./WGY005_PI__Subjective_Frame_Recalibration__Dense_va_3df7930c-7617-4c5d-a531-ea0e5232a374_2.png",
      "./WGY001_DS__Mood_Environment_Stabilizer__A_dimly_lit_8b11ce6b-1bf6-41c7-9ba8-770282047b9b_3.png",
      "./WGY006_CS__Causal_Motion_Trigger__EX_tumbles_from_t_5a3962d9-e579-4b5e-9b8d-2d74cf1c7e18_3.png",
      "./WGY054_AF__Emotion_Relay__POET_spots_EX_still_only__7e3df51f-f3d5-410e-935a-3898b32496da_0.png",
      "./WGY018_AF__Emotion_Relay__EX_in_apparent_distress_f_c7ce0e57-226a-4c6b-bb8d-67480544474a_0.png",
      "./WGY014_CS__Causal_Motion_Trigger__POET_abruptly_ent_bc0d9270-3b6a-4930-b4a7-51df85c2f634_1.png",
      "./WGY073_PI__Subjective_Frame_Recalibration__POET_obs_9e194e77-7d8b-4933-b10f-8140345f4cc9_1.png",
      "./WGY069_CS__Causal_Motion_Trigger__POET_hosts_friend_8ef04fc5-4750-462f-b62c-633e762e586b_3.png",
      "./WGY040_DS__Narrative_Modifier__In_the_still_night_P_561cfed8-d1f9-47be-b371-f6c6d8198779_1.png",
      "./WGY065_CS__Causal_Motion_Trigger__POET_mounts_the_m_4317fa34-f2da-4eec-8471-5baeb135fe5d_3.png",
      "./WGY068_CS__Causal_Motion_Trigger__POET_returns_to_t_e05a8b0a-3fb3-47b3-86da-02b087770b0b_2.png",
      "./WGY080_CI__Temporal_Reflection_Loop__POET_is_once_a_307da2fd-c53d-4255-b677-f0ffd4fb9c17_0.png",
      "./WGY025_CS__Causal_Motion_Trigger__The_young_POET_th_d52cfd3e-276e-4ac3-9455-71da7518b0e2_0.png",
      "./WGY031_CS__Causal_Motion_Trigger__The_tambourine_fi_b1129243-2c81-4668-ab07-a692826b41f5_1.png",
      "./WGY074_CI__Temporal_Reflection_Loop__Vast_fields_of_6d1e4148-d50f-4b5c-b481-6a081999262d_1.png",
      "./WGY081_TM__Narrative_Modifier__Rapid_cuts_intersper_6127f538-2941-43f8-84aa-e3e54cfe8e0f_1.png",
      "./WGY041_CS__Causal_Motion_Trigger__In_the_still_nigh_e077dfb5-d2f8-4586-884a-77c68981b0ac_3.png",
      "./WGY033_AF__Emotion_Relay__An_internal_view_reveals__04a2802b-7a68-4b31-b441-f1b5ca2f3a06_2.png",
      "({wgy_code})<br/><img src=",
      " />",
      " /><br/>",
      "s edge label image sizing/placement:\n                # The example has <g transform=",
      ">\n                # Inside that foreignObject, the content changes. The height of the foreignObject\n                # is not explicitly increased in the *generated SVG* but is determined by Mermaid.\n                # We will keep the foreignObject dimensions as they are in the input SVG for this solution.\n                pass # No explicit height/width adjustment in Python for this, relying on browser"
    ],
    "subprocess_calls": [],
    "imports": [
      "re",
      "bs4"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "CAT/WHISKER/export_svg_to_png.py",
    "size": 1612,
    "lines": 48,
    "source": "import os\nfrom bs4 import BeautifulSoup\n\ndef export_svg_to_png():\n    \"\"\"\n    Reads an HTML file, extracts the first SVG element, and saves it as a high-quality PNG file.\n    This script requires the 'cairosvg' library to be installed.\n    \"\"\"\n    source_html_path = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-final.html'\n    output_png_path = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/diagram.png'\n    \n    try:\n        import cairosvg\n    except ImportError:\n        print(\"Error: The 'cairosvg' library is required to run this script.\")\n        print(\"Please install it by running: pip install cairosvg\")\n        return\n\n    print(f\"Reading source HTML: {source_html_path}\")\n    try:\n        with open(source_html_path, 'r', encoding='utf-8') as f:\n            soup = BeautifulSoup(f, 'lxml')\n    except FileNotFoundError:\n        print(f\"Error: Source file not found at {source_html_path}\")\n        return\n\n    svg_element = soup.find('svg')\n    if not svg_element:\n        print(\"Error: No SVG element found in the source HTML.\")\n        return\n\n    # The SVG data needs to be a string or bytes\n    svg_data = str(svg_element)\n\n    print(f\"Converting SVG to PNG at: {output_png_path}\")\n    try:\n        cairosvg.svg2png(\n            bytestring=svg_data.encode('utf-8'),\n            write_to=output_png_path,\n            dpi=300  # Set a high DPI for better quality\n        )\n        print(\"Successfully created high-quality PNG: diagram.png\")\n    except Exception as e:\n        print(f\"An error occurred during PNG conversion: {e}\")\n\nif __name__ == \"__main__\":\n    export_svg_to_png()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/diagram.png",
      "Successfully created high-quality PNG: diagram.png",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-final.html",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/diagram.png"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "bs4"
    ],
    "generates": [],
    "reads": [
      "reads_files"
    ],
    "docstring": null
  },
  {
    "path": "CAT/WHISKER/create_wgy_index.py",
    "size": 4050,
    "lines": 28,
    "source": "import re\n\ngrep_output = '''\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":365,\"LineContent\":\"                            <p>\u25fb Negative Moods: Melancholy (WGY001), Disorientation (WGY006), Longing (WGY054), Distress (WGY018)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":378,\"LineContent\":\"                            <p>\u25fb Positive Moods: Nostalgia (WGY014), Peace (WGY073), Joy (WGY069), Empowerment (WGY040), Liberation (WGY065)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":430,\"LineContent\":\"                            <p>\u25fb Photo-realistic Visuals (consistent across all scenes, e.g., WGY001, WGY041)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":443,\"LineContent\":\"                            <p>\u25fb Meticulous Detail in Ekphrasis &amp;Conditioning (e.g., 'subtle dust motes' WGY001)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":456,\"LineContent\":\"                            <p>\u25fb Strategic Cineosis Functions (PI for subjective truth: WGY005; AF for emotional facts: WGY033)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":643,\"LineContent\":\"                            <p>\u2192 evokes personal relationships &amp;internal states (e.g., EX: WGY003)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":654,\"LineContent\":\"                            <p>\u2192 acts as emotional containers &amp;transformative stages (e.g., Black Void: WGY017)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":665,\"LineContent\":\"                            <p>\u2192 drives plot or signifies internal shifts (e.g., Luminous Heart: WGY035)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":676,\"LineContent\":\"                            <p>\u2192 creates atmosphere or represents psychological states (e.g., Vape Haze: WGY005)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":714,\"LineContent\":\"                            <p>\u2192 induced by visual cues &amp;narrative (e.g., WGY005 PI induces entrapment)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":725,\"LineContent\":\"                            <p>\u2192 induced by visual cues &amp;narrative (e.g., WGY014 CS evokes euphoria)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":736,\"LineContent\":\"                            <p>\u2192 compelled by POET's actions (e.g., POET rebuilds temple: WGY068)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":756,\"LineContent\":\"                            <p>\u2192 illustrated by cyclical falls &amp;returns (WGY006, WGY080)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":767,\"LineContent\":\"                            <p>\u2192 framed by transformation via objects (Tambourine: WGY025 -&gt;WGY031)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":787,\"LineContent\":\"                            <p>\u2192 orders progression through CS (WGY068 POET rebuilds)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":798,\"LineContent\":\"                            <p>\u2192 dictates by CI and TM (WGY074 Flower energy; WGY081 B-roll cuts)</p>\"}\n{\"File\":\"/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html\",\"LineNumber\":845,\"LineContent\":\"                            <p>\u2192 naturalizes surreal events (Luminous Heart: WGY035)</p>\"}\n'''\n\nwgy_codes = set(re.findall(r'WGY\\d{3}', grep_output))\nsorted_codes = sorted(list(wgy_codes))\n\nfor code in sorted_codes:\n    print(code)\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u25fb Negative Moods: Melancholy (WGY001), Disorientation (WGY006), Longing (WGY054), Distress (WGY018)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u25fb Positive Moods: Nostalgia (WGY014), Peace (WGY073), Joy (WGY069), Empowerment (WGY040), Liberation (WGY065)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u25fb Photo-realistic Visuals (consistent across all scenes, e.g., WGY001, WGY041)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      " WGY001)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u25fb Strategic Cineosis Functions (PI for subjective truth: WGY005; AF for emotional facts: WGY033)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 evokes personal relationships &amp;internal states (e.g., EX: WGY003)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 acts as emotional containers &amp;transformative stages (e.g., Black Void: WGY017)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 drives plot or signifies internal shifts (e.g., Luminous Heart: WGY035)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 creates atmosphere or represents psychological states (e.g., Vape Haze: WGY005)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 induced by visual cues &amp;narrative (e.g., WGY005 PI induces entrapment)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 induced by visual cues &amp;narrative (e.g., WGY014 CS evokes euphoria)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "s actions (e.g., POET rebuilds temple: WGY068)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 illustrated by cyclical falls &amp;returns (WGY006, WGY080)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 framed by transformation via objects (Tambourine: WGY025 -&gt;WGY031)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 orders progression through CS (WGY068 POET rebuilds)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 dictates by CI and TM (WGY074 Flower energy; WGY081 B-roll cuts)</p>",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "                            <p>\u2192 naturalizes surreal events (Luminous Heart: WGY035)</p>"
    ],
    "subprocess_calls": [],
    "imports": [
      "re"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "CAT/WHISKER/render_with_browser.py",
    "size": 1900,
    "lines": 52,
    "source": "import os\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nimport time\n\ndef render_svg_with_browser():\n    \"\"\"\n    Uses a headless Chrome browser to take a screenshot of the SVG element\n    in an HTML file, ensuring fonts are rendered correctly.\n    \"\"\"\n    source_html_path = os.path.abspath('/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-final.html')\n    output_png_path = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/diagram_rendered.png'\n    \n    print(\"Setting up headless browser...\")\n    chrome_options = Options()\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--window-size=1920,1080\") # Set a large enough window\n    chrome_options.add_argument(\"--force-device-scale-factor=2\") # Increase for higher resolution\n    chrome_options.add_argument(\"--hide-scrollbars\")\n\n    # Use webdriver.Chrome instead of webdriver.Chrome() with options\n    driver = webdriver.Chrome(options=chrome_options)\n\n    try:\n        # The path needs to be a file URI\n        file_uri = f'file://{source_html_path}'\n        print(f\"Navigating to: {file_uri}\")\n        driver.get(file_uri)\n        \n        # Give the page a moment to render fully\n        time.sleep(2)\n\n        print(\"Locating SVG element...\")\n        svg_element = driver.find_element(By.TAG_NAME, 'svg')\n        \n        if svg_element:\n            print(f\"Taking screenshot of the SVG and saving to: {output_png_path}\")\n            svg_element.screenshot(output_png_path)\n            print(\"Successfully created PNG with rendered text.\")\n        else:\n            print(\"Error: Could not find the SVG element on the page.\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        print(\"Closing browser.\")\n        driver.quit()\n\nif __name__ == \"__main__\":\n    render_svg_with_browser()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/diagram_rendered.png",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-final.html",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/diagram_rendered.png",
      "file://{source_html_path}"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "selenium",
      "selenium.webdriver.common.by",
      "selenium.webdriver.chrome.options",
      "time"
    ],
    "generates": [],
    "reads": [],
    "docstring": null
  },
  {
    "path": "CAT/WHISKER/embed_images.py",
    "size": 4291,
    "lines": 89,
    "source": "import os\nimport re\nfrom xml.etree import ElementTree as ET\n\n# --- Configuration ---\nIMAGE_WIDTH = '75'  # Image width in pixels\nHEIGHT_INCREASE = 85  # Pixels to add to the node height for the image\n\n# Define the directory where the source, output, and images are located.\nWORKING_DIRECTORY = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/'\nSOURCE_SVG_PATH = os.path.join(WORKING_DIRECTORY, 'whisker-view-ultra.html')\nOUTPUT_SVG_PATH = os.path.join(WORKING_DIRECTORY, 'whisker-view-final.html')\n\n# --- Script Logic ---\ndef embed_images_in_svg():\n    \"\"\"Parses the SVG, finds WGY codes, and embeds images by resizing parent nodes.\"\"\"\n    try:\n        print(\"--- Starting Image Embedding Process ---\")\n\n        # 1. Discover all available image files.\n        all_files = os.listdir(WORKING_DIRECTORY)\n        image_map = {f.split('_')[0]: f for f in all_files if f.startswith('WGY') and f.endswith('.png')}\n        print(f\"Found {len(image_map)} available images.\")\n\n        # 2. Parse the HTML file, which is not well-formed XML, by reading its content.\n        with open(SOURCE_SVG_PATH, 'r', encoding='utf-8') as f:\n            html_content = f.read()\n        \n        # The file contains mixed HTML and SVG, so we use string replacement which is more robust here.\n        modified_content = html_content\n        \n        # Find all foreignObject tags and process them.\n        foreign_objects = re.findall(r'(<foreignObject.*?/foreignObject>)', modified_content, re.DOTALL)\n        print(f\"Found {len(foreign_objects)} foreignObject elements to process.\")\n\n        replacements_made = 0\n        for fo_string in foreign_objects:\n            original_fo = fo_string\n            modified_fo = original_fo\n\n            # Find all WGY codes within this foreignObject\n            codes_in_fo = set(re.findall(r'WGY\\d{3}', fo_string))\n            images_to_add = []\n            \n            for code in codes_in_fo:\n                if code in image_map:\n                    image_filename = image_map[code]\n                    # Prepare the image tag, but don't add it yet.\n                    images_to_add.append(f'<img src=\"{image_filename}\" width=\"{IMAGE_WIDTH}\" alt=\"{code}\" style=\"display: block; margin: 5px auto;\"/>')\n                    print(f\"  - Found {code}, preparing to embed image.\")\n\n            if images_to_add:\n                # Increase the height of the foreignObject\n                height_match = re.search(r'height=\"([\\d\\.]+)\"', modified_fo)\n                if height_match:\n                    current_height = float(height_match.group(1))\n                    new_height = current_height + (HEIGHT_INCREASE * len(images_to_add))\n                    modified_fo = re.sub(r'height=\"[\\d\\.]+\"', f'height=\"{new_height}\"', modified_fo)\n\n                    # Append all images just before the closing div tag of the foreignObject's content\n                    closing_div = '</div>'\n                    if closing_div in modified_fo:\n                        image_html = \"\".join(images_to_add)\n                        # Insert the images before the closing div tag\n                        modified_fo = modified_fo.replace(closing_div, f'{image_html}{closing_div}', 1)\n                        \n                        # Replace the old foreignObject with the new, modified one in the full document\n                        modified_content = modified_content.replace(original_fo, modified_fo)\n                        replacements_made += len(images_to_add)\n                        print(f\"  - Resized container and embedded {len(images_to_add)} image(s) for codes: {list(codes_in_fo)}.\")\n\n        # 4. Write the modified content to the output file.\n        with open(OUTPUT_SVG_PATH, 'w', encoding='utf-8') as f:\n            f.write(modified_content)\n\n        print(\"--- Process Finished ---\")\n        print(f\"Summary: Made a total of {replacements_made} image insertions.\")\n        print(f\"Output saved to: {OUTPUT_SVG_PATH}\")\n\n    except FileNotFoundError:\n        print(f\"Error: The file {SOURCE_SVG_PATH} was not found.\")\n    except ET.ParseError as e:\n        print(f\"Error parsing the HTML file: {e}. It may not be well-formed XML.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    embed_images_in_svg()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/",
      "(<foreignObject.*?/foreignObject>)",
      "</div>"
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "xml.etree"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files",
      "reads_data"
    ],
    "docstring": null
  },
  {
    "path": "CAT/WHISKER/reorganize_timeline.py",
    "size": 6059,
    "lines": 143,
    "source": "import os\nimport re\nfrom bs4 import BeautifulSoup\nimport glob\n\ndef reorganize_and_create_timeline():\n    \"\"\"\n    Reads whisker-view-final.html, extracts existing images, moves them into a timeline,\n    and adds interactive highlighting between the diagram and the timeline.\n    \"\"\"\n    source_html_path = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-final.html'\n    output_html_path = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisk-merm-reorganized.html'\n\n    print(f\"Reading source HTML: {source_html_path}\")\n    try:\n        with open(source_html_path, 'r', encoding='utf-8') as f:\n            soup = BeautifulSoup(f, 'lxml')\n    except FileNotFoundError:\n        print(f\"Error: Source file not found at {source_html_path}\")\n        return\n\n    # --- 1. Extract existing images and their containers ---\n    image_data = []\n    containers_to_remove = set()\n\n    # Find all image tags with a WGY code in the src\n    all_wgy_images = soup.find_all('img', src=re.compile(r'WGY\\d{3}'))\n\n    for img_tag in all_wgy_images:\n        wgy_code_match = re.search(r'(WGY\\d{3})', img_tag['src'])\n        if wgy_code_match:\n            wgy_code = wgy_code_match.group(1)\n            # Avoid adding duplicate images for the same code\n            if not any(d['code'] == wgy_code for d in image_data):\n                image_data.append({'code': wgy_code, 'img_tag': img_tag.extract()})\n        \n        # Find the parent div to remove it later\n        parent_div = img_tag.find_parent('div')\n        if parent_div:\n            containers_to_remove.add(parent_div)\n\n    # Decompose the parent containers after finding all images\n    for container in containers_to_remove:\n        # Check if the container is still in the tree before trying to decompose\n        if container.parent:\n            # If the container is now empty after extracting images, remove it.\n            if not container.find_all(True, recursive=False):\n                 container.decompose()\n\n    if not image_data:\n        print(\"Warning: No WGY images found to reorganize.\")\n\n    print(f\"Found and extracted {len(image_data)} images to move to the timeline.\")\n\n    # --- 2. Find and wrap WGY codes in the SVG diagram ---\n    svg_element = soup.find('svg')\n    if not svg_element:\n        print(\"Error: No SVG element found.\")\n        return\n\n    wgy_codes_in_svg = set(re.findall(r'(WGY\\d{3})', svg_element.prettify()))\n\n    for text_node in list(svg_element.find_all(string=True)):\n        if 'WGY' in text_node:\n            parent = text_node.parent\n            if parent.name == 'span' and 'wgy-source' in parent.get('id', ''):\n                continue\n            \n            parts = re.split(r'(WGY\\d{3})', str(text_node))\n            new_content = []\n            for part in parts:\n                if re.match(r'WGY\\d{3}', part):\n                    span = soup.new_tag('span', id=f'wgy-source-{part}')\n                    span.string = part\n                    new_content.append(span)\n                elif part:\n                    new_content.append(part)\n            text_node.replace_with(*new_content)\n\n    print(f\"Found and wrapped {len(wgy_codes_in_svg)} unique WGY codes in the SVG.\")\n\n    # --- 3. Create the timeline and add CSS/JS ---\n    if not soup.head:\n        head_tag = soup.new_tag('head')\n        soup.html.insert(0, head_tag)\n\n    style_tag = soup.new_tag('style')\n    style_tag.string = \"\"\"\n        .highlight-source { background-color: yellow; font-weight: bold; }\n        .highlight-image { border: 3px solid #007bff; transform: scale(1.05); }\n        #timeline-container img { transition: transform 0.2s, border 0.2s; }\n    \"\"\"\n    soup.head.append(style_tag)\n\n    svg_style = svg_element.get('style', '')\n    max_width_match = re.search(r'max-width:\\s*([^;]+)', svg_style)\n    timeline_width_style = f\"max-width: {max_width_match.group(1)};\" if max_width_match else \"width: 100%;\"\n    \n    timeline_div = soup.new_tag('div', id='timeline-container', style=f\"margin-top: 20px; padding-top: 20px; border-top: 1px solid #ccc; display: flex; justify-content: space-around; align-items: flex-end; flex-wrap: wrap; {timeline_width_style} margin-left: auto; margin-right: auto;\")\n\n    for data in sorted(image_data, key=lambda x: x['code']):\n        item_div = soup.new_tag('div', style=\"text-align: center; margin: 10px;\")\n        img_tag = data['img_tag']\n        img_tag['id'] = f\"wgy-img-{data['code']}\"\n        img_tag['style'] = \"width: 150px; height: auto; border: 1px solid #ddd;\"\n        label_p = soup.new_tag('p', style=\"margin-top: 5px; font-family: sans-serif; font-size: 12px;\")\n        label_p.string = data['code']\n        item_div.append(img_tag)\n        item_div.append(label_p)\n        timeline_div.append(item_div)\n\n    soup.body.append(timeline_div)\n\n    wgy_codes_json = sorted(list(wgy_codes_in_svg))\n    script_content = f\"\"\"\n    document.addEventListener('DOMContentLoaded', () => {{\n        const wgyCodes = {wgy_codes_json};\n        wgyCodes.forEach(code => {{\n            const sourceEl = document.getElementById(`wgy-source-${{code}}`);\n            const imgEl = document.getElementById(`wgy-img-${{code}}`);\n            if (sourceEl && imgEl) {{\n                sourceEl.addEventListener('mouseenter', () => imgEl.classList.add('highlight-image'));\n                sourceEl.addEventListener('mouseleave', () => imgEl.classList.remove('highlight-image'));\n                imgEl.addEventListener('mouseenter', () => sourceEl.classList.add('highlight-source'));\n                imgEl.addEventListener('mouseleave', () => sourceEl.classList.remove('highlight-source'));\n            }}\n        }});\n    }});\n    \"\"\"\n    script_tag = soup.new_tag('script')\n    script_tag.string = script_content\n    soup.body.append(script_tag)\n\n    # --- 4. Write the new file ---\n    print(f\"Writing output to: {output_html_path}\")\n    with open(output_html_path, 'w', encoding='utf-8') as f:\n        f.write(str(soup.prettify()))\n\n    print(\"Timeline reorganization complete.\")\n\nif __name__ == \"__main__\":\n    reorganize_and_create_timeline()\n",
    "file_references": [
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-final.html",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisk-merm-reorganized.html",
      ")\n\n    # --- 3. Create the timeline and add CSS/JS ---\n    if not soup.head:\n        head_tag = soup.new_tag("
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "bs4",
      "glob"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files"
    ],
    "docstring": null
  },
  {
    "path": "CAT/WHISKER/timeline_generator.py",
    "size": 6814,
    "lines": 169,
    "source": "import os\nimport re\nfrom bs4 import BeautifulSoup\nimport glob\n\ndef create_timeline_view():\n    \"\"\"\n    Reads an HTML file containing a Mermaid SVG, finds WGY codes,\n    and generates a new HTML file with an image timeline and connecting lines.\n    \"\"\"\n    source_html_path = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html'\n    output_html_path = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisk-merm-new.html'\n    image_directory = '/Users/gaia/resurrecting atlantis/CAT/WHISKER/'\n\n    print(f\"Reading source HTML: {source_html_path}\")\n    try:\n        with open(source_html_path, 'r', encoding='utf-8') as f:\n            soup = BeautifulSoup(f, 'lxml')\n    except FileNotFoundError:\n        print(f\"Error: Source file not found at {source_html_path}\")\n        return\n\n    svg_element = soup.find('svg')\n    if not svg_element:\n        print(\"Error: No SVG element found in the source HTML.\")\n        return\n\n    # Find all text containing WGY codes and wrap them\n    wgy_codes = set(re.findall(r'(WGY\\d{3})', svg_element.prettify()))\n    \n    # Use a recursive function to find and wrap text nodes robustly\n    def wrap_wgy_codes_in_node(node):\n        # Find all text nodes within the current node that contain WGY codes\n        text_nodes_to_process = node.find_all(string=re.compile(r'WGY\\d{3}'), recursive=False)\n        for text_node in text_nodes_to_process:\n            # Skip if it's already been wrapped by us\n            if text_node.parent.name == 'span' and 'wgy-source' in text_node.parent.get('id', ''):\n                continue\n\n            # Split the text content by WGY codes\n            parts = re.split(r'(WGY\\d{3})', str(text_node))\n            new_elements = []\n            for part in parts:\n                if not part:\n                    continue\n                if re.match(r'WGY\\d{3}', part):\n                    # This is a WGY code, wrap it in a span\n                    span_tag = soup.new_tag('span', id=f'wgy-source-{part}')\n                    span_tag.string = part\n                    new_elements.append(span_tag)\n                else:\n                    # This is regular text, create a new text node\n                    new_elements.append(part)\n            \n            # Replace the original text node with the new sequence of text and span elements\n            text_node.replace_with(*new_elements)\n\n        # Recursively process child elements that are not spans we just created\n        for child in node.find_all(True, recursive=False):\n            if not (child.name == 'span' and 'wgy-source' in child.get('id', '')):\n                wrap_wgy_codes_in_node(child)\n\n    # Start the wrapping process from the root SVG element\n    wrap_wgy_codes_in_node(svg_element)\n\n    print(f\"Found and wrapped {len(wgy_codes)} unique WGY codes: {sorted(list(wgy_codes))}\")\n\n    # Dynamically set timeline width to match SVG width\n    svg_style = svg_element.get('style', '')\n    max_width_match = re.search(r'max-width:\\s*([^;]+)', svg_style)\n    timeline_width_style = \"width: 100%;\"\n    if max_width_match:\n        timeline_width_style = f\"max-width: {max_width_match.group(1)}; width: 100%; margin-left: auto; margin-right: auto;\"\n\n    # Create the timeline div\n    timeline_div = soup.new_tag('div', id='timeline-container', style=f\"margin-top: 20px; padding-top: 20px; border-top: 1px solid #ccc; display: flex; justify-content: space-around; align-items: flex-end; flex-wrap: wrap; {timeline_width_style}\")\n    \n    for code in sorted(list(wgy_codes)):\n        # Find the corresponding image file\n        image_files = glob.glob(os.path.join(image_directory, f'{code}_*.png'))\n        if image_files:\n            image_path = os.path.basename(image_files[0])\n            \n            item_div = soup.new_tag('div', style=\"text-align: center; margin: 10px;\")\n            img_tag = soup.new_tag('img', src=image_path, id=f'wgy-img-{code}', style=\"width: 150px; height: auto; border: 1px solid #ddd;\")\n            label_p = soup.new_tag('p', style=\"margin-top: 5px; font-family: sans-serif; font-size: 12px;\")\n            label_p.string = code\n            \n            item_div.append(img_tag)\n            item_div.append(label_p)\n            timeline_div.append(item_div)\n        else:\n            print(f\"Warning: No image found for {code}\")\n\n    if svg_element.parent:\n        svg_element.parent.insert_after(timeline_div)\n    else:\n        soup.body.append(timeline_div)\n\n    # Add CSS for highlighting\n    style_tag = soup.new_tag('style')\n    style_tag.string = \"\"\"\n        .highlight-source {\n            background-color: yellow;\n            font-weight: bold;\n        }\n        .highlight-image {\n            border: 3px solid #007bff;\n            transform: scale(1.05);\n            transition: transform 0.2s, border 0.2s;\n        }\n        #timeline-container img {\n            transition: transform 0.2s, border 0.2s;\n        }\n    \"\"\"\n    # Ensure there is a <head> tag to append the style to\n    if not soup.head:\n        head_tag = soup.new_tag('head')\n        if soup.html:\n            soup.html.insert(0, head_tag)\n        else:\n            # As a fallback, insert it at the beginning of the document\n            soup.insert(0, head_tag)\n    soup.head.append(style_tag)\n\n    # Add JavaScript for interactive highlighting\n    script_content = f\"\"\"\n    function setupHoverHighlighting() {{\n        const wgyCodes = {sorted(list(wgy_codes))};\n\n        wgyCodes.forEach(code => {{\n            const sourceEl = document.getElementById(`wgy-source-${{code}}`);\n            const imgEl = document.getElementById(`wgy-img-${{code}}`);\n\n            if (sourceEl && imgEl) {{\n                // Highlight image on source hover\n                sourceEl.addEventListener('mouseenter', () => {{\n                    imgEl.classList.add('highlight-image');\n                }});\n                sourceEl.addEventListener('mouseleave', () => {{\n                    imgEl.classList.remove('highlight-image');\n                }});\n\n                // Highlight source on image hover\n                imgEl.addEventListener('mouseenter', () => {{\n                    sourceEl.classList.add('highlight-source');\n                }});\n                imgEl.addEventListener('mouseleave', () => {{\n                    sourceEl.classList.remove('highlight-source');\n                }});\n            }}\n        }});\n    }}\n\n    document.addEventListener('DOMContentLoaded', setupHoverHighlighting);\n    \"\"\"\n    script_tag = soup.new_tag('script')\n    script_tag.string = script_content\n    soup.body.append(script_tag)\n\n    print(f\"Writing output to: {output_html_path}\")\n    with open(output_html_path, 'w', encoding='utf-8') as f:\n        f.write(str(soup))\n\n    print(\"Timeline generation complete.\")\n\nif __name__ == \"__main__\":\n    create_timeline_view()\n",
    "file_references": [
      "{code}_*.png",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisker-view-ultra.html",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/whisk-merm-new.html",
      "/Users/gaia/resurrecting atlantis/CAT/WHISKER/",
      "\n    function setupHoverHighlighting() {{\n        const wgyCodes = {sorted(list(wgy_codes))};\n\n        wgyCodes.forEach(code => {{\n            const sourceEl = document.getElementById(`wgy-source-${{code}}`);\n            const imgEl = document.getElementById(`wgy-img-${{code}}`);\n\n            if (sourceEl && imgEl) {{\n                // Highlight image on source hover\n                sourceEl.addEventListener(",
      ");\n                }});\n\n                // Highlight source on image hover\n                imgEl.addEventListener("
    ],
    "subprocess_calls": [],
    "imports": [
      "os",
      "re",
      "bs4",
      "glob"
    ],
    "generates": [
      "writes_files",
      "generates_output"
    ],
    "reads": [
      "reads_files"
    ],
    "docstring": null
  }
]